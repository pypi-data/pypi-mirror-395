"""
Ensure that the tokens generated by the offline vLLM engine are the same as the tokens in the API mode.

We do this by just spinning up one of each and seeing what it outputs. This runs on CPU, so we use a tiny model.
"""

# this test is a good idea but it doesn't fit in our GHA CI, so it's only run locally anyway
# for now it's a manual script

import itertools
import random

import pytest
from kani import Kani
from kani.ext.vllm import VLLMEngine, VLLMOpenAIEngine, VLLMServerEngine
from pytest_lazy_fixtures import lf
from vllm import SamplingParams

from tests.conftest import MODEL_ID, PROMPTS, SEED


# define engines to test with
@pytest.fixture()
async def offline_engine():
    model = VLLMEngine(
        model_id=MODEL_ID,
        max_context_size=8192,
        model_load_kwargs={"seed": SEED, "gpu_memory_utilization": 0.25},
        sampling_params=SamplingParams(temperature=0, max_tokens=2048),
    )
    yield model
    await model.close()
    del model


@pytest.fixture()
async def api_engine():
    model = VLLMServerEngine(
        model_id=MODEL_ID,
        max_context_size=8192,
        vllm_args={"seed": SEED, "gpu_memory_utilization": 0.25},
        vllm_port=31415,
        timeout=3000,
        temperature=0,
        max_tokens=2048,
    )
    yield model
    await model.close()
    del model


@pytest.fixture()
async def openai_engine():
    model = VLLMOpenAIEngine(
        model_id=MODEL_ID,
        max_context_size=8192,
        vllm_args={"seed": SEED, "gpu_memory_utilization": 0.25},
        vllm_port=31416,
        timeout=3000,
        temperature=0,
        max_tokens=2048,
    )
    yield model
    await model.close()
    del model


# define helpers to call the engines with
async def infer_with_engine(engine, prompt, stream=False):
    ai = Kani(engine)
    if stream:
        msg = await ai.chat_round_stream(prompt)
        resp = msg.text
    else:
        resp = await ai.chat_round_str(prompt)
    return resp


# pairwise equivalence tests
@pytest.mark.parametrize(
    ["e1", "e2"],
    itertools.pairwise([lf("offline_engine"), lf("api_engine"), lf("openai_engine")]),
)
async def test_equivalence(e1, e2):
    prompt = random.choice(PROMPTS)
    resp1 = await infer_with_engine(e1, prompt)
    print(f"{type(e1).__name__}: {resp1}")
    resp2 = await infer_with_engine(e2, prompt)
    print(f"{type(e2).__name__}: {resp2}")
    assert resp1 == resp2


@pytest.mark.parametrize("engine", [lf("offline_engine"), lf("api_engine"), lf("openai_engine")])
async def test_equivalence_stream(engine):
    prompt = random.choice(PROMPTS)
    resp1 = await infer_with_engine(engine, prompt, stream=False)
    print(f"{type(engine).__name__} (no stream): {resp1}")
    resp2 = await infer_with_engine(engine, prompt, stream=True)
    print(f"{type(engine).__name__} (streaming): {resp2}")
    assert resp1 == resp2
