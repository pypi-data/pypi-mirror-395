"""
Insight Agent - Automatic Narrative Generation
===============================================

Converts data analysis into human-readable business insights.
"""

import pandas as pd
import numpy as np
from typing import Union, Dict, List, Optional
from datetime import datetime
from autodatamind.core.reader import read_data
from autodatamind.core.cleaner import autoclean
from autodatamind.core.utils import detect_problem_type, format_number


def generate_insights(data: Union[str, pd.DataFrame],
                      target: Optional[str] = None,
                      auto_clean: bool = True,
                      save_report: bool = True,
                      report_path: Optional[str] = None,
                      verbose: bool = True) -> str:
    """
    Generate natural language insights from data.
    
    Parameters
    ----------
    data : str or DataFrame
        Path to file or DataFrame
    target : str, optional
        Target column for focused analysis
    auto_clean : bool
        Automatically clean data
    save_report : bool
        Save report to file
    report_path : str, optional
        Custom report save path
    verbose : bool
        Print report to console
    
    Returns
    -------
    str
        Generated narrative report
    
    Examples
    --------
    >>> import autodatamind as adm
    >>> report = adm.generate_insights("sales.csv")
    >>> report = adm.generate_insights(df, target="revenue")
    """
    # Load data
    df = read_data(data)
    original_rows = len(df)
    
    # Auto-clean
    if auto_clean:
        df = autoclean(df, verbose=False)
    
    # Generate narrative
    narrative = _build_narrative(df, target, original_rows)
    
    # Print if verbose
    if verbose:
        print(narrative)
    
    # Save report
    if save_report:
        if report_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_path = f"autodatamind_insights_{timestamp}.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(narrative)
        
        if verbose:
            print(f"\n✓ Report saved: {report_path}")
    
    return narrative


def _build_narrative(df: pd.DataFrame, target: Optional[str], original_rows: int) -> str:
    """Build comprehensive narrative report."""
    lines = []
    
    # Header
    lines.append("=" * 80)
    lines.append("AUTODATAMIND INSIGHTS REPORT")
    lines.append("=" * 80)
    lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    
    # Executive Summary
    lines.append("EXECUTIVE SUMMARY")
    lines.append("-" * 80)
    summary = _generate_executive_summary(df, target, original_rows)
    lines.append(summary)
    lines.append("")
    
    # Data Overview
    lines.append("DATA OVERVIEW")
    lines.append("-" * 80)
    overview = _generate_data_overview(df, original_rows)
    lines.append(overview)
    lines.append("")
    
    # Key Findings
    lines.append("KEY FINDINGS")
    lines.append("-" * 80)
    findings = _generate_key_findings(df, target)
    for finding in findings:
        lines.append(f"• {finding}")
    lines.append("")
    
    # Statistical Insights
    lines.append("STATISTICAL INSIGHTS")
    lines.append("-" * 80)
    stats_insights = _generate_statistical_insights(df)
    for insight in stats_insights:
        lines.append(f"• {insight}")
    lines.append("")
    
    # Recommendations
    lines.append("RECOMMENDATIONS")
    lines.append("-" * 80)
    recommendations = _generate_recommendations(df, target)
    for i, rec in enumerate(recommendations, 1):
        lines.append(f"{i}. {rec}")
    lines.append("")
    
    # Data Quality Assessment
    lines.append("DATA QUALITY ASSESSMENT")
    lines.append("-" * 80)
    quality = _assess_data_quality(df, original_rows)
    lines.append(quality)
    lines.append("")
    
    # Footer
    lines.append("=" * 80)
    lines.append("Report generated by AutoDataMind - Native Intelligence Layer")
    lines.append("=" * 80)
    
    return "\n".join(lines)


def _generate_executive_summary(df: pd.DataFrame, target: Optional[str], original_rows: int) -> str:
    """Generate executive summary."""
    summary_parts = []
    
    # Dataset description
    summary_parts.append(
        f"This dataset contains {format_number(len(df))} records with {len(df.columns)} "
        f"variables. "
    )
    
    # Cleaning note
    if len(df) < original_rows:
        removed = original_rows - len(df)
        summary_parts.append(
            f"After automatic cleaning, {format_number(removed)} records "
            f"({removed/original_rows*100:.1f}%) were removed due to quality issues. "
        )
    
    # Target analysis
    if target and target in df.columns:
        problem_type = detect_problem_type(df, target)
        summary_parts.append(
            f"The target variable '{target}' suggests a {problem_type.replace('_', ' ')} "
            f"problem. "
        )
        
        if df[target].dtype in ['int64', 'float64']:
            target_mean = df[target].mean()
            target_std = df[target].std()
            summary_parts.append(
                f"The target has a mean of {format_number(target_mean)} with a "
                f"standard deviation of {format_number(target_std)}. "
            )
    
    # Numeric vs categorical
    numeric_count = len(df.select_dtypes(include=[np.number]).columns)
    categorical_count = len(df.columns) - numeric_count
    summary_parts.append(
        f"The dataset comprises {numeric_count} numeric and {categorical_count} "
        f"categorical variables."
    )
    
    return "".join(summary_parts)


def _generate_data_overview(df: pd.DataFrame, original_rows: int) -> str:
    """Generate data overview section."""
    lines = []
    
    lines.append(f"Original Records: {format_number(original_rows)}")
    lines.append(f"Clean Records: {format_number(len(df))}")
    lines.append(f"Total Variables: {len(df.columns)}")
    lines.append(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # Missing data
    total_missing = df.isnull().sum().sum()
    total_cells = len(df) * len(df.columns)
    missing_pct = (total_missing / total_cells * 100) if total_cells > 0 else 0
    lines.append(f"Missing Values: {format_number(total_missing)} ({missing_pct:.2f}%)")
    
    return "\n".join(lines)


def _generate_key_findings(df: pd.DataFrame, target: Optional[str]) -> List[str]:
    """Generate key findings."""
    findings = []
    
    # Numeric insights
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        # Find highest variance column
        variances = df[numeric_cols].var()
        highest_var_col = variances.idxmax()
        findings.append(
            f"'{highest_var_col}' shows the highest variability in the dataset, "
            f"with a coefficient of variation of {df[highest_var_col].std()/df[highest_var_col].mean():.2f}"
        )
        
        # Correlations
        if len(numeric_cols) >= 2:
            corr_matrix = df[numeric_cols].corr()
            # Get strongest correlation (excluding diagonal)
            np.fill_diagonal(corr_matrix.values, 0)
            max_corr_idx = np.unravel_index(np.abs(corr_matrix).values.argmax(), corr_matrix.shape)
            col1, col2 = corr_matrix.columns[max_corr_idx[0]], corr_matrix.columns[max_corr_idx[1]]
            corr_val = corr_matrix.iloc[max_corr_idx[0], max_corr_idx[1]]
            
            if abs(corr_val) > 0.5:
                findings.append(
                    f"Strong {'positive' if corr_val > 0 else 'negative'} correlation "
                    f"({corr_val:.3f}) detected between '{col1}' and '{col2}'"
                )
    
    # Categorical insights
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    if len(categorical_cols) > 0:
        # Find most diverse categorical
        nunique_counts = df[categorical_cols].nunique()
        most_diverse = nunique_counts.idxmax()
        findings.append(
            f"'{most_diverse}' has the highest diversity with "
            f"{nunique_counts[most_diverse]} unique categories"
        )
    
    # Target-specific findings
    if target and target in df.columns:
        if df[target].dtype in ['int64', 'float64']:
            # Outliers in target
            Q1, Q3 = df[target].quantile([0.25, 0.75])
            IQR = Q3 - Q1
            outliers = ((df[target] < Q1 - 1.5*IQR) | (df[target] > Q3 + 1.5*IQR)).sum()
            if outliers > 0:
                findings.append(
                    f"Target variable contains {outliers} potential outliers "
                    f"({outliers/len(df)*100:.1f}% of data)"
                )
    
    return findings


def _generate_statistical_insights(df: pd.DataFrame) -> List[str]:
    """Generate statistical insights."""
    insights = []
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols[:5]:  # Top 5 numeric columns
        mean_val = df[col].mean()
        median_val = df[col].median()
        
        # Skewness insight
        if abs(mean_val - median_val) / (df[col].std() + 1e-10) > 0.5:
            direction = "right" if mean_val > median_val else "left"
            insights.append(
                f"'{col}' shows {direction}-skewed distribution "
                f"(mean: {format_number(mean_val)}, median: {format_number(median_val)})"
            )
    
    return insights


def _generate_recommendations(df: pd.DataFrame, target: Optional[str]) -> List[str]:
    """Generate actionable recommendations."""
    recommendations = []
    
    # Missing data recommendation
    missing_cols = df.columns[df.isnull().any()].tolist()
    if missing_cols:
        recommendations.append(
            f"Consider investigating the {len(missing_cols)} columns with missing values: "
            f"{', '.join(missing_cols[:3])}{'...' if len(missing_cols) > 3 else ''}"
        )
    
    # Feature engineering
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) >= 2:
        recommendations.append(
            "Consider creating interaction features between highly correlated numeric variables "
            "to capture non-linear relationships"
        )
    
    # Categorical encoding
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    high_cardinality = [col for col in categorical_cols if df[col].nunique() > 10]
    if high_cardinality:
        recommendations.append(
            f"Apply target encoding or embeddings for high-cardinality categorical "
            f"features: {', '.join(high_cardinality[:3])}"
        )
    
    # Scaling recommendation
    if len(numeric_cols) > 0:
        ranges = df[numeric_cols].max() - df[numeric_cols].min()
        if ranges.max() / (ranges.min() + 1e-10) > 10:
            recommendations.append(
                "Apply feature scaling (StandardScaler or MinMaxScaler) due to "
                "significant differences in feature ranges"
            )
    
    # Model recommendation
    if target:
        problem_type = detect_problem_type(df, target)
        if problem_type == 'regression':
            recommendations.append(
                "For regression tasks, consider trying ensemble methods like "
                "RandomForest, GradientBoosting, or XGBoost"
            )
        elif 'classification' in problem_type:
            recommendations.append(
                "For classification, evaluate multiple models: Logistic Regression, "
                "RandomForest, and Neural Networks"
            )
    
    return recommendations


def _assess_data_quality(df: pd.DataFrame, original_rows: int) -> str:
    """Assess overall data quality."""
    quality_lines = []
    
    # Completeness
    completeness = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
    quality_lines.append(f"Completeness Score: {completeness:.1f}%")
    
    # Consistency
    consistency = (len(df) / original_rows) * 100
    quality_lines.append(f"Consistency Score: {consistency:.1f}%")
    
    # Overall assessment
    overall = (completeness + consistency) / 2
    
    if overall >= 90:
        assessment = "EXCELLENT - Data is high quality and ready for modeling"
    elif overall >= 75:
        assessment = "GOOD - Data is suitable for analysis with minor improvements"
    elif overall >= 60:
        assessment = "FAIR - Consider additional data cleaning and validation"
    else:
        assessment = "POOR - Significant data quality issues require attention"
    
    quality_lines.append(f"Overall Quality: {assessment}")
    
    return "\n".join(quality_lines)


class InsightAgent:
    """
    Insight Agent for automatic narrative generation.
    """
    
    def __init__(self):
        pass
    
    def generate(self, data: Union[str, pd.DataFrame], **kwargs) -> str:
        """Generate insights narrative."""
        return generate_insights(data, **kwargs)
