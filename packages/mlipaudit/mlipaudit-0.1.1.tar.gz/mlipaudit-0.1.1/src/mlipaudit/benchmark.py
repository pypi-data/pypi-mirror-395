# Copyright 2025 InstaDeep Ltd
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import zipfile
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Literal, TypeAlias

from ase import Atom
from ase.calculators.calculator import Calculator as ASECalculator
from huggingface_hub import hf_hub_download
from mlip.models import ForceField
from pydantic import BaseModel, Field

from mlipaudit.exceptions import ChemicalElementsMissingError
from mlipaudit.run_mode import RunMode

RunModeAsString: TypeAlias = Literal["dev", "fast", "standard"]


class BenchmarkResult(BaseModel):
    """A base model for all benchmark results.

    Attributes:
        failed: Whether all the simulations or inferences failed
            and no analysis could be performed. Defaults to False.
        score: The final score for the benchmark between
            0 and 1.
    """

    failed: bool = False
    score: float | None = Field(ge=0, le=1, default=None)


class ModelOutput(BaseModel):
    """A base model for all intermediate model outputs."""


class Benchmark(ABC):
    """An Abstract Base Class for structuring MLIP benchmark calculations.

    This class uses the Template Method pattern. Each concrete benchmark
    must implement the `run_model` and `analyze` methods. Benchmarks must
    be designed to first call `run_model` followed by `analyze`. Intermediate
    calculations generated by `run_model` will be stored in the instance variable
    `model_output`. Results generated by `analyze` will be stored in the instance
    variable `model_output`.

    Subclasses should also define the class attribute `name`, giving the
    benchmark a unique name, as well as `input_data_url` if necessary, specifying where
    any input data should be downloaded from.

    Attributes:
        name: The unique benchmark name that should be used to run the benchmark
            from the CLI and that will determine the output folder name for the result
            file.
        category: A string that describes the category of the benchmark, used for
            example, in the UI app for grouping. Default, if not overridden,
            is "General".
        result_class: A reference to the type of `BenchmarkResult` that will determine
            the return type of `self.analyze()`.
        model_output_class: A reference to the type of `ModelOutput` class that will
            be used to store the outcome of the `self.run_model()` function.
        required_elements: The set of element types that are present in the benchmark's
            input files.
        skip_if_elements_missing: Whether the benchmark should be skipped entirely
            if there are some element types that the model cannot handle. If False,
            the benchmark must have its own custom logic to handle missing element
            types. Defaults to True.
        reusable_output_id: An optional ID that references other benchmarks with
            identical input systems and `ModelOutput` signatures (in form of a tuple).
            If present, a user or the CLI can make use of this information to reuse
            cached model outputs from another benchmark carrying the same ID instead of
            rerunning simulations or inference.
    """

    name: str = ""
    category: str = "General"
    result_class: type[BenchmarkResult] | None = None
    model_output_class: type[ModelOutput] | None = None

    required_elements: set[str] | None = None
    skip_if_elements_missing: bool = True

    reusable_output_id: tuple[str, ...] | None = None

    def __init__(
        self,
        force_field: ForceField | ASECalculator,
        data_input_dir: str | os.PathLike = "./data",
        run_mode: RunMode | RunModeAsString = RunMode.STANDARD,
    ) -> None:
        """Initializes the benchmark.

        Args:
            force_field: The force field model to be benchmarked.
            data_input_dir: The local input data directory. Defaults to
                "./data". If the subdirectory "{data_input_dir}/{benchmark_name}"
                exists, the benchmark expects the relevant data to be in there,
                otherwise it will download it from HuggingFace.
            run_mode: Whether to run the standard benchmark length, a faster version,
                or a very fast development version. Subclasses
                should ensure that when `RunMode.DEV`, their benchmark runs in a
                much shorter timeframe, by running on a reduced number of
                test cases, for instance. Implementing `RunMode.FAST` being different
                from `RunMode.STANDARD` is optional and only recommended for very
                long-running benchmarks. This argument can also be passed as a string
                "dev", "fast", or "standard".

        Raises:
            ChemicalElementsMissingError: If initialization is attempted
                with a force field that cannot perform inference on the
                required elements.
            ValueError: If force field type is not compatible.
        """
        self.run_mode = run_mode
        if not isinstance(self.run_mode, RunMode):
            self.run_mode = RunMode(run_mode)

        self.force_field = force_field

        if not (
            isinstance(self.force_field, ForceField)
            or isinstance(self.force_field, ASECalculator)
        ):
            raise ValueError(
                "Provided force field must be either a mlip-compatible "
                "force field object or an ASE calculator."
            )

        self._handle_missing_element_types()
        self.data_input_dir = Path(data_input_dir)

        self.model_output: ModelOutput | None = None

        self._download_data()

    def __init_subclass__(cls, **kwargs: Any):
        """Called when a class inherits from `Benchmark`.

        Used to validate that the required class attributes are defined.
        """
        super().__init_subclass__(**kwargs)
        if not cls.name:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'name' attribute."
            )
        if cls.result_class is None:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'result_class' attribute."
            )
        if cls.model_output_class is None:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'model_output_class' attribute."
            )
        if cls.required_elements is None:
            raise NotImplementedError(
                f"{cls.__name__} must override the 'required_elements' attribute."
            )

    @classmethod
    def get_missing_element_types(cls, force_field: ForceField) -> set[str]:
        """Fetch the set of missing allowed element types from
        the force field to run the benchmark.

        Args:
            force_field: The force field model to be benchmarked.

        Returns:
            The set of element types that the model cannot handle
                that are part of the input data.
        """
        ff_allowed_element_types = set(
            Atom(symbol=atomic_number).symbol
            for atomic_number in force_field.allowed_atomic_numbers
        )
        missing_element_types = cls.required_elements - ff_allowed_element_types  # type: ignore
        return missing_element_types

    def _handle_missing_element_types(self):
        if self.skip_if_elements_missing:
            missing_element_types = self.get_missing_element_types(self.force_field)
            if missing_element_types:
                raise ChemicalElementsMissingError(
                    f"The following element types are missing: {missing_element_types}"
                )

    @classmethod
    def check_can_run_model(cls, force_field: ForceField) -> bool:
        """Checks if a model can be run on a specific benchmark
        by ensuring that the model can handle all the element
        types in the benchmark's input files.

        Args:
            force_field: The force field model to be benchmarked.

        Returns:
            Whether the model can be run on the benchmark.
        """
        if cls.skip_if_elements_missing:
            missing_element_types = cls.get_missing_element_types(force_field)
            if missing_element_types:
                return False

        return True

    def _download_data(self) -> None:
        """Download the data from the data input directory if not already exists."""
        already_exists = (self.data_input_dir / self.name).exists()
        if not already_exists:
            hf_hub_download(
                repo_id="InstaDeepAI/MLIPAudit-data",
                filename=f"{self.name}.zip",
                local_dir=self.data_input_dir,
                repo_type="dataset",
            )
            with zipfile.ZipFile(self.data_input_dir / f"{self.name}.zip", "r") as z:
                z.extractall(self.data_input_dir)

    @abstractmethod
    def run_model(self) -> None:
        """Generates any necessary data with `self.force_field`.

        Subclasses must implement this method. Raw data from simulations,
        single-point energy calculations or other types of calculations
        will be stored in the instance variable `model_output`.
        """
        pass

    @abstractmethod
    def analyze(self) -> BenchmarkResult:
        """Performs all post-inference or simulation analysis.

        Subclasses must implement this method. This method
        processes the raw data generated from the generation step
        to compute final metrics. Subclasses are also responsible
        for computing the final score for the benchmark.

        Returns:
            A class-specific instance of `BenchmarkResult`.
        """
        pass
