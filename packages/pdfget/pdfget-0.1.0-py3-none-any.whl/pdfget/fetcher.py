#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ–‡çŒ®è·å–å™¨ - Linusé£æ ¼
åªåšä¸€ä»¶äº‹ï¼šä¸‹è½½å¼€æ”¾è·å–æ–‡çŒ®
éµå¾ªKISSåŸåˆ™ï¼šKeep It Simple, Stupid
"""

import hashlib
import json
import re
import time
from pathlib import Path
from urllib.parse import quote

import requests

import logging


class PaperFetcher:
    """ç®€å•æ–‡çŒ®è·å–å™¨"""

    def __init__(self, cache_dir: str = "data/cache", output_dir: str = "data/pdfs"):
        """
        åˆå§‹åŒ–è·å–å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            output_dir: PDFè¾“å‡ºç›®å½•
        """
        self.logger = logging.getLogger("PaperFetcher")
        self.cache_dir = Path(cache_dir)
        self.output_dir = Path(output_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ç®€å•çš„HTTPä¼šè¯
        self.session = requests.Session()
        self.session.headers.update(
            {"User-Agent": "Mozilla/5.0 (compatible; PaperFetcher/1.0)"}
        )

    def parse_query(self, query: str) -> str:
        """
        è§£æé«˜çº§æ£€ç´¢è¯ä¸ºEurope PMCæ ¼å¼

        æ”¯æŒçš„è¯­æ³•ï¼š
        - å¸ƒå°”è¿ç®—ç¬¦ï¼šAND, OR, NOT
        - å­—æ®µæ£€ç´¢ï¼štitle:, author:, journal:
        - çŸ­è¯­æ£€ç´¢ï¼š"exact phrase"

        Args:
            query: ç”¨æˆ·è¾“å…¥çš„æ£€ç´¢è¯

        Returns:
            Europe PMCæ ¼å¼çš„æ£€ç´¢è¯
        """
        # å¤„ç†çŸ­è¯­æ£€ç´¢ï¼ˆå¼•å·åŒ…å›´çš„å†…å®¹ï¼‰
        phrase_pattern = r'"([^"]+)"'
        phrases = re.findall(phrase_pattern, query)

        # ä¸´æ—¶æ›¿æ¢çŸ­è¯­ä¸ºå ä½ç¬¦
        for i, phrase in enumerate(phrases):
            query = query.replace(f'"{phrase}"', f"__PHRASE_{i}__")

        # å¤„ç†å­—æ®µæ£€ç´¢
        field_mappings = {
            "title:": "TITLE:",
            "author:": "AUTHOR:",
            "journal:": "JOURNAL:",
            "abstract:": "ABSTRACT:",
        }

        for user_field, pmc_field in field_mappings.items():
            query = query.replace(user_field, pmc_field)

        # æ¢å¤çŸ­è¯­ï¼Œå¹¶æ·»åŠ å¿…è¦çš„å¼•å·
        for i, phrase in enumerate(phrases):
            query = query.replace(f"__PHRASE_{i}__", f'"{phrase}"')

        # å¤„ç†å¸ƒå°”è¿ç®—ç¬¦ï¼ˆç¡®ä¿å¤§å†™ï¼‰
        query = (
            query.replace(" and ", " AND ")
            .replace(" or ", " OR ")
            .replace(" not ", " NOT ")
        )

        return query.strip()

    def search_papers(self, query: str, limit: int = 50) -> list[dict]:
        """
        é€šè¿‡Europe PMCæœç´¢æ–‡çŒ®

        Args:
            query: æ£€ç´¢è¯ï¼ˆæ”¯æŒé«˜çº§è¯­æ³•ï¼‰
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶

        Returns:
            æ–‡çŒ®åˆ—è¡¨ï¼ŒåŒ…å«DOIã€æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
        """
        self.logger.info(f"ğŸ” æœç´¢æ–‡çŒ®: {query}")

        # è§£ææ£€ç´¢è¯
        parsed_query = self.parse_query(query)
        self.logger.debug(f"  è§£æå: {parsed_query}")

        # æ„å»ºæœç´¢URL
        search_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
        params = {
            "query": parsed_query,
            "resulttype": "core",
            "format": "json",
            "pageSize": min(limit, 1000),  # Europe PMCé™åˆ¶æœ€å¤š1000æ¡
            "cursorMark": "*",
        }

        try:
            response = self.session.get(search_url, params=params, timeout=30)  # type: ignore[arg-type]
            response.raise_for_status()

            data = response.json()

            if data.get("hitCount", 0) == 0:
                self.logger.info("  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                return []

            # å¤„ç†ç»“æœ
            papers = []
            results = data.get("resultList", {}).get("result", [])

            for i, record in enumerate(results[:limit]):
                # è·å–æœŸåˆŠä¿¡æ¯
                journal_info = record.get("journalInfo", {})

                paper = {
                    "title": record.get("title", ""),
                    "authors": [
                        a.strip() for a in record.get("authorString", "").split(",")
                    ]
                    if record.get("authorString")
                    else [],
                    "journal": journal_info.get("journal", {}).get("title", ""),
                    "year": record.get("pubYear", ""),
                    "doi": record.get("doi", ""),
                    "pmcid": record.get("pmcid", ""),
                    "pmid": record.get("pmid", ""),
                    "abstract": record.get("abstractText", ""),
                    "isOpenAccess": bool(
                        record.get("pmcid")
                    ),  # æœ‰PMCIDé€šå¸¸è¡¨ç¤ºå¼€æ”¾è·å–
                    "source": "Europe PMC",
                    # æ–°å¢çš„10ä¸ªå­—æ®µ
                    "affiliation": record.get("affiliation", ""),
                    "volume": journal_info.get("volume", ""),
                    "issue": journal_info.get("issue", ""),
                    "pages": record.get("pageInfo", ""),
                    "license": record.get("license", ""),
                    "citedBy": record.get("citedByCount", 0),
                    "keywords": record.get("keywordList", []),
                    "meshTerms": record.get("meshHeadingList", []),
                    "grants": record.get("grantsList", []),
                    "hasData": record.get("hasData") == "Y",
                    "hasSuppl": record.get("hasSuppl") == "Y",
                }
                papers.append(paper)

                self.logger.info(
                    f"  ğŸ“„ {i + 1}/{min(len(results), limit)}: {paper['title'][:60]}..."
                )

            self.logger.info(f"  âœ… æ‰¾åˆ° {len(papers)} ç¯‡æ–‡çŒ®")
            return papers

        except requests.exceptions.Timeout:
            self.logger.error("  âŒ æœç´¢è¶…æ—¶")
            return []
        except requests.exceptions.ConnectionError:
            self.logger.error("  âŒ è¿æ¥å¤±è´¥")
            return []
        except Exception as e:
            self.logger.error(f"  âŒ æœç´¢å¤±è´¥: {str(e)}")
            return []

    def fetch_by_doi(self, doi: str, timeout: int = 30) -> dict:
        """
        é€šè¿‡DOIè·å–æ–‡çŒ®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        ç­–ç•¥ï¼š
        1. åªå¤„ç†å¼€æ”¾è·å–æ–‡çŒ®ï¼ˆæœ‰PMCIDï¼‰
        2. å¿«é€Ÿå¤±è´¥ï¼Œä¸é‡è¯•
        3. ç®€å•ç¼“å­˜
        4. ä¸æå¤æ‚çš„ç½‘ç»œç›‘æ§å’Œè‡ªé€‚åº”é‡è¯•

        Args:
            doi: æ–‡çŒ®DOI
            timeout: è¶…æ—¶æ—¶é—´

        Returns:
            è·å–ç»“æœå­—å…¸
        """
        self.logger.info(f"ğŸ” è·å–æ–‡çŒ®: {doi}")

        # æ£€æŸ¥ç¼“å­˜
        cached_result = self._get_cache(doi)
        if cached_result:
            self.logger.info("  ğŸ“¦ ä»ç¼“å­˜åŠ è½½")
            return cached_result

        # åªä½¿ç”¨Europe PMCï¼ˆä¸»è¦çš„å¼€æ”¾è·å–æºï¼‰
        result = self._fetch_from_pmc(doi, timeout)

        # ç¼“å­˜ç»“æœ
        self._save_cache(doi, result)

        if result.get("success"):
            self.logger.info("  âœ… è·å–æˆåŠŸ")
        else:
            self.logger.info(f"  âŒ è·å–å¤±è´¥: {result.get('error', 'Unknown error')}")

        return result

    def _fetch_from_pmc(self, doi: str, timeout: int) -> dict:
        """ä»Europe PMCè·å–æ–‡çŒ®"""
        try:
            # æœç´¢PMCID
            search_url = f"https://www.ebi.ac.uk/europepmc/webservices/rest/search?query=DOI:{quote(doi)}&resulttype=core&format=json"
            self.logger.debug(f"  ğŸ” Europe PMC URL: {search_url}")

            response = self.session.get(search_url, timeout=timeout)
            response.raise_for_status()

            data = response.json()
            if data.get("hitCount", 0) == 0:
                return {
                    "success": False,
                    "error": "Not found in Europe PMC",
                    "doi": doi,
                }

            record = data["resultList"]["result"][0]
            pmcid = record.get("pmcid")

            if not pmcid:
                self.logger.info("  â­ï¸ æ— PMCIDï¼Œéå¼€æ”¾è·å–æ–‡çŒ®")
                return {
                    "success": False,
                    "error": "Not open access (no PMCID)",
                    "doi": doi,
                }

            self.logger.info(f"  ğŸ“„ æ‰¾åˆ°PMCID: {pmcid}")

            # å°è¯•ä¸‹è½½PDF
            pdf_result = self._download_pdf(pmcid, doi)

            if pdf_result["success"]:
                return {
                    "success": True,
                    "doi": doi,
                    "pmcid": pmcid,
                    "pdf_path": pdf_result["path"],
                    "content_type": "pdf",
                    "title": record.get("title"),
                    "journal": record.get("journalInfo", {})
                    .get("journal", {})
                    .get("title"),
                    "authors": [
                        a.strip() for a in record.get("authorString", "").split(",")
                    ]
                    if record.get("authorString")
                    else [],
                    "year": record.get("pubYear"),
                    "abstract": record.get("abstractText"),
                }

            # PDFä¸‹è½½å¤±è´¥ï¼Œè¿”å›å…¨æ–‡HTMLé“¾æ¥
            return {
                "success": True,
                "doi": doi,
                "pmcid": pmcid,
                "full_text_url": f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/",
                "content_type": "html",
                "title": record.get("title"),
                "authors": [
                    a.strip() for a in record.get("authorString", "").split(",")
                ]
                if record.get("authorString")
                else [],
                "year": record.get("pubYear"),
                "abstract": record.get("abstractText"),
            }

        except requests.exceptions.Timeout:
            return {"success": False, "error": "Request timeout", "doi": doi}
        except requests.exceptions.ConnectionError:
            return {"success": False, "error": "Connection error", "doi": doi}
        except Exception as e:
            return {"success": False, "error": str(e), "doi": doi}

    def _download_pdf(self, pmcid: str, doi: str) -> dict:
        """ä¸‹è½½PDFæ–‡ä»¶"""
        # å°è¯•å‡ ä¸ªå¸¸è§çš„PDF URL
        pdf_urls = [
            f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/",
            f"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmcid}/pdf/{pmcid}.pdf",
            f"https://europepmc.org/articles/{pmcid}?pdf=render",
        ]

        for i, pdf_url in enumerate(pdf_urls):
            try:
                self.logger.debug(f"  ğŸ“¥ å°è¯•PDFæº {i + 1}: {pdf_url}")
                response = self.session.get(pdf_url, timeout=30, stream=True)
                response.raise_for_status()

                content_type = response.headers.get("content-type", "").lower()
                if "application/pdf" not in content_type:
                    continue

                # ä¿å­˜æ–‡ä»¶
                safe_doi = "".join(c for c in doi if c.isalnum() or c in "-._")
                filename = f"{pmcid}_{safe_doi}.pdf"
                file_path = self.output_dir / filename

                with open(file_path, "wb") as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                self.logger.info(f"  ğŸ’¾ PDFä¿å­˜æˆåŠŸ: {file_path}")
                return {"success": True, "path": str(file_path)}

            except Exception as e:
                self.logger.debug(f"  âš ï¸ PDFæº {i + 1} å¤±è´¥: {str(e)}")
                continue

        return {"success": False, "error": "All PDF sources failed"}

    def _get_cache(self, doi: str) -> dict | None:
        """ç®€å•ç¼“å­˜æ£€æŸ¥"""
        cache_file = (
            self.cache_dir / f"cache_{hashlib.md5(doi.encode()).hexdigest()}.json"
        )

        if cache_file.exists():
            try:
                with open(cache_file, "r") as f:
                    data = json.load(f)

                # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦è¿˜å­˜åœ¨
                if data.get("pdf_path") and not Path(data["pdf_path"]).exists():
                    self.logger.debug("ç¼“å­˜çš„PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ¸…é™¤ç¼“å­˜")
                    cache_file.unlink()
                    return None

                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                if time.time() - data.get("timestamp", 0) > 86400:
                    self.logger.debug("ç¼“å­˜å·²è¿‡æœŸ")
                    cache_file.unlink()
                    return None

                return data  # type: ignore

            except Exception as e:
                self.logger.debug(f"ç¼“å­˜è¯»å–å¤±è´¥: {str(e)}")
                cache_file.unlink()
                return None

        return None

    def _save_cache(self, doi: str, result: dict) -> None:
        """ä¿å­˜ç¼“å­˜"""
        try:
            cache_file = (
                self.cache_dir / f"cache_{hashlib.md5(doi.encode()).hexdigest()}.json"
            )
            result["timestamp"] = time.time()

            with open(cache_file, "w") as f:
                json.dump(result, f, indent=2)

        except Exception as e:
            self.logger.debug(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}")

    def fetch_batch(self, dois: list[str], delay: float = 1.0) -> list[dict]:
        """
        æ‰¹é‡è·å–æ–‡çŒ®ï¼ˆç®€åŒ–ç‰ˆï¼‰

        Args:
            dois: DOIåˆ—è¡¨
            delay: è¯·æ±‚é—´å»¶è¿Ÿï¼ˆç§’ï¼‰

        Returns:
            ç»“æœåˆ—è¡¨
        """
        self.logger.info(f"ğŸš€ æ‰¹é‡è·å– {len(dois)} ç¯‡æ–‡çŒ®")
        results = []

        for i, doi in enumerate(dois, 1):
            self.logger.info(f"\nğŸ“„ è¿›åº¦: {i}/{len(dois)}")

            try:
                result = self.fetch_by_doi(doi)
                results.append(result)
            except Exception as e:
                self.logger.error(f"è·å–æ–‡çŒ®å¤±è´¥ ({doi}): {e}")
                results.append({"doi": doi, "success": False, "error": str(e)})

            # ç®€å•å»¶è¿Ÿï¼Œé¿å…è¢«é™åˆ¶
            if i < len(dois):
                time.sleep(delay)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r.get("success"))
        self.logger.info(f"\nğŸ“Š æ‰¹é‡è·å–å®Œæˆ: {success_count}/{len(dois)} æˆåŠŸ")

        return results
