#!/usr/bin/env python3
"""
PDFä¸‹è½½å™¨ä¸»ç¨‹åº
ç‹¬ç«‹çš„æ–‡çŒ®PDFä¸‹è½½å·¥å…·
"""

import argparse
import json
import time
from pathlib import Path

import logging

from .fetcher import PaperFetcher
from .downloader import ConcurrentDownloader
from .config import TIMEOUT, DELAY, LOG_LEVEL, LOG_FORMAT


def main() -> None:
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="PDFæ–‡çŒ®ä¸‹è½½å™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æœç´¢æ–‡çŒ®
  python -m pdfget -s "machine learning cancer"
  python -m pdfget -s "deep learning" -l 20 -d

  # å¹¶å‘ä¸‹è½½ï¼ˆå¤šçº¿ç¨‹ï¼‰
  python -m pdfget -s "cancer immunotherapy" -l 20 -d -t 5
  python -m pdfget -i dois.csv -t 3

  # ä¸‹è½½å•ä¸ªæ–‡çŒ®
  python -m pdfget --doi 10.1016/j.cell.2020.01.021
        """,
    )

    # è¾“å…¥é€‰é¡¹
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--doi", help="å•ä¸ªDOI")
    group.add_argument("-i", help="è¾“å…¥æ–‡ä»¶ï¼ˆCSVæˆ–TXTï¼‰")
    group.add_argument("-s", help="æœç´¢æ–‡çŒ®")

    # å¯é€‰å‚æ•°
    parser.add_argument("-c", default="doi", help="CSVåˆ—åï¼ˆé»˜è®¤: doiï¼‰")
    parser.add_argument("-o", default="data/pdfs", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--delay", type=float, default=DELAY, help="è¯·æ±‚å»¶è¿Ÿç§’æ•°")
    parser.add_argument("-l", type=int, default=50, help="æœç´¢ç»“æœæ•°é‡")
    parser.add_argument("-d", action="store_true", help="ä¸‹è½½PDF")
    parser.add_argument("-t", type=int, default=3, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤3ï¼‰")
    parser.add_argument("-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.DEBUG if args.v else LOG_LEVEL, format=LOG_FORMAT)
    logger = logging.getLogger("PDFDownloader")

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    fetcher = PaperFetcher(cache_dir="data/cache", output_dir="data/pdfs")

    logger.info("ğŸš€ PDFä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"   è¾“å‡ºç›®å½•: {args.o}")

    try:
        if args.doi:
            # å•ä¸ªDOIä¸‹è½½
            logger.info(f"\nğŸ“„ ä¸‹è½½å•ä¸ªæ–‡çŒ®: {args.doi}")
            result = fetcher.fetch_by_doi(args.doi, timeout=TIMEOUT)

            if result.get("success"):
                logger.info("âœ… ä¸‹è½½æˆåŠŸ!")
                if result.get("pdf_path"):
                    logger.info(f"   PDFè·¯å¾„: {result['pdf_path']}")
                else:
                    logger.info(f"   HTMLé“¾æ¥: {result.get('full_text_url')}")
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {result.get('error', 'Unknown error')}")

        elif args.s:
            # æœç´¢æ–‡çŒ®
            logger.info(f"\nğŸ” æœç´¢æ–‡çŒ®: {args.s}")
            papers = fetcher.search_papers(args.s, limit=args.l)

            if not papers:
                logger.error("âŒ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡çŒ®")
                exit(1)

            # æ˜¾ç¤ºæœç´¢ç»“æœ
            logger.info(f"\nğŸ“Š æœç´¢ç»“æœ ({len(papers)} ç¯‡):")
            for i, paper in enumerate(papers, 1):
                logger.info(f"\n{i}. {paper['title']}")
                logger.info(
                    f"   ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}"
                )
                logger.info(f"   æœŸåˆŠ: {paper['journal']} ({paper['year']})")
                if paper["doi"]:
                    logger.info(f"   DOI: {paper['doi']}")
                logger.info(f"   å¼€æ”¾è·å–: {'æ˜¯' if paper['isOpenAccess'] else 'å¦'}")

            # ä¿å­˜æœç´¢ç»“æœ
            search_results_file = (
                Path(args.o) / f"search_results_{int(time.time())}.json"
            )
            search_results_file.parent.mkdir(parents=True, exist_ok=True)

            with open(search_results_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "query": args.s,
                        "timestamp": time.time(),
                        "total": len(papers),
                        "results": papers,
                    },
                    f,
                    indent=2,
                    ensure_ascii=False,
                )

            logger.info(f"\nğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {search_results_file}")

            # å¦‚æœéœ€è¦ä¸‹è½½PDF
            if args.d:
                logger.info("\nğŸ“¥ å¼€å§‹ä¸‹è½½PDF...")

                # åªä¸‹è½½æœ‰PMCIDçš„å¼€æ”¾è·å–æ–‡çŒ®
                oa_papers = [p for p in papers if p["pmcid"]]
                logger.info(f"   æ‰¾åˆ° {len(oa_papers)} ç¯‡å¼€æ”¾è·å–æ–‡çŒ®")

                if oa_papers:
                    # æ„é€ DOIåˆ—è¡¨
                    dois = [p["doi"] for p in oa_papers if p["doi"]]

                    if dois:
                        # æ ¹æ®çº¿ç¨‹æ•°å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘ä¸‹è½½
                        if len(dois) > 1 and args.t > 1:
                            logger.info(
                                f"\nğŸš€ ä½¿ç”¨ {args.t} ä¸ªçº¿ç¨‹å¹¶å‘ä¸‹è½½ {len(dois)} ç¯‡æ–‡çŒ®"
                            )
                            concurrent_downloader = ConcurrentDownloader(
                                max_workers=args.t,
                                base_delay=args.delay,
                                fetcher=fetcher,
                            )
                            results = concurrent_downloader.download_batch(
                                dois, timeout=TIMEOUT
                            )
                        else:
                            # å•çº¿ç¨‹ä¸‹è½½ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                            results = fetcher.fetch_batch(dois, delay=args.delay)

                        # ç»Ÿè®¡ç»“æœ
                        success_count = sum(1 for r in results if r.get("success"))
                        pdf_count = sum(1 for r in results if r.get("pdf_path"))
                        html_count = sum(1 for r in results if r.get("full_text_url"))

                        logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
                        logger.info(f"   æ€»è®¡: {len(results)}")
                        logger.info(f"   æˆåŠŸ: {success_count}")
                        logger.info(f"   PDF: {pdf_count}")
                        logger.info(f"   HTML: {html_count}")
                        logger.info(f"   å¤±è´¥: {len(results) - success_count}")

                        # ä¿å­˜ä¸‹è½½ç»“æœ
                        if success_count > 0:
                            download_results_file = (
                                Path(args.o) / "download_results.json"
                            )
                            with open(
                                download_results_file, "w", encoding="utf-8"
                            ) as f:
                                json.dump(
                                    {
                                        "timestamp": time.time(),
                                        "total": len(results),
                                        "success": success_count,
                                        "results": results,
                                    },
                                    f,
                                    indent=2,
                                    ensure_ascii=False,
                                )

                            logger.info(
                                f"\nğŸ’¾ ä¸‹è½½ç»“æœå·²ä¿å­˜åˆ°: {download_results_file}"
                            )

        else:
            # æ‰¹é‡ä¸‹è½½
            logger.info(f"\nğŸ“š æ‰¹é‡ä¸‹è½½: {args.i}")

            # è¯»å–DOIåˆ—è¡¨
            input_path = Path(args.i)
            if not input_path.exists():
                logger.error(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.i}")
                exit(1)

            if input_path.suffix.lower() == ".csv":
                # è¯»å–CSVæ–‡ä»¶
                import pandas as pd

                try:
                    df = pd.read_csv(input_path)
                    if args.c not in df.columns:
                        logger.error(f"âŒ CSVæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åˆ—: {args.c}")
                        exit(1)

                    dois = df[args.c].dropna().unique().tolist()
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªå”¯ä¸€DOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
                    exit(1)

            else:
                # è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªDOIï¼‰
                try:
                    with open(input_path, "r") as f:
                        dois = [line.strip() for line in f if line.strip()]
                    logger.info(f"   æ‰¾åˆ° {len(dois)} ä¸ªDOI")

                except Exception as e:
                    logger.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                    exit(1)

            # æ ¹æ®çº¿ç¨‹æ•°å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘ä¸‹è½½
            if len(dois) > 1 and args.t > 1:
                logger.info(f"\nğŸš€ ä½¿ç”¨ {args.t} ä¸ªçº¿ç¨‹å¹¶å‘ä¸‹è½½ {len(dois)} ç¯‡æ–‡çŒ®")
                concurrent_downloader = ConcurrentDownloader(
                    max_workers=args.t, base_delay=args.delay, fetcher=fetcher
                )
                results = concurrent_downloader.download_batch(dois, timeout=TIMEOUT)
            else:
                # å•çº¿ç¨‹ä¸‹è½½ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                results = fetcher.fetch_batch(dois, delay=args.delay)

            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if r.get("success"))
            pdf_count = sum(1 for r in results if r.get("pdf_path"))
            html_count = sum(1 for r in results if r.get("full_text_url"))

            logger.info("\nğŸ“Š ä¸‹è½½ç»Ÿè®¡:")
            logger.info(f"   æ€»è®¡: {len(results)}")
            logger.info(f"   æˆåŠŸ: {success_count}")
            logger.info(f"   PDF: {pdf_count}")
            logger.info(f"   HTML: {html_count}")
            logger.info(f"   å¤±è´¥: {len(results) - success_count}")

            # ä¿å­˜ç»“æœ
            if success_count > 0:
                output_file = Path(args.o) / "download_results.json"
                output_file.parent.mkdir(parents=True, exist_ok=True)

                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(
                        {
                            "timestamp": time.time(),
                            "total": len(results),
                            "success": success_count,
                            "results": results,
                        },
                        f,
                        indent=2,
                        ensure_ascii=False,
                    )

                logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        exit(1)
    except Exception as e:
        logger.error(f"\nğŸ’¥ å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        exit(1)

    logger.info("\nâœ¨ ä¸‹è½½å®Œæˆ")
    exit(0)


if __name__ == "__main__":
    main()
