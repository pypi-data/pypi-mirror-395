# check this documentation
# https://python.langchain.com/docs/how_to/semantic-chunker/
# https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb
# https://python.langchain.com/docs/how_to/embed_text/
import logging
import uuid
from typing import Any, List

from langchain_core.documents import Document
from langchain_experimental.text_splitter import SemanticChunker

from ...application.interfaces import RagChunker

logger = logging.getLogger(__name__)


class SemanticChunks(RagChunker):
    """
    Class for semantically chunking documents into smaller pieces based on semantic similarity.
    Uses LangChain's SemanticChunker to create semantically coherent document chunks.
    """

    __slots__ = ("embeddings_model",)

    def __init__(self, embeddings_model: Any):
        """
        Initialize a document chunker with an embeddings model.

        Args:
            embeddings_model: The embeddings model to use for semantic chunking
                             (must be compatible with LangChain's embeddings interface)

        Notes:
            By default the semantic chunker uses percentile above 95% to keep sentences separated
            and a minimum chunk size of 200 characters.
        """
        self.text_splitter = SemanticChunker(
            embeddings_model,
            buffer_size=1,
            add_start_index=True,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95,
            min_chunk_size=200,
        )

    def gen_chunks_for_document(self, document: Document) -> List[Document]:
        """
        Split a document into semantically coherent chunks.

        Args:
            documents: The text from documents to split into chunks

        Returns:
            List of strings containing the chunked content

        Raises:
            Exception: If there's an error during the chunking process
        """
        try:
            chunks = self.text_splitter.split_documents([document])
            filtered_chunks = []
            for i, chunk in enumerate(chunks):
                if document.metadata["source"]:
                    chunk.id = f"{uuid.uuid4()}"
                if chunk.page_content is not None and chunk.page_content != "":
                    filtered_chunks.append(chunk)
            logger.info(f"{len(filtered_chunks)} chunks generated successfully")
            return filtered_chunks
        except Exception as e:
            logger.error(f"Failed to get chunks: {str(e)}")
            raise
