"""File metadata model."""

from typing import Any, Generator, Self, TypeAlias

from anystore.store.base import Stats
from anystore.types import SDict
from anystore.util import make_data_checksum
from followthemoney import StatementEntity
from followthemoney.dataset import DefaultDataset
from ftmq.types import StatementEntities
from ftmq.util import make_entity

from ftm_lakehouse.conventions import path
from ftm_lakehouse.util import mime_to_schema


class File(Stats):
    """File metadata model"""

    dataset: str
    """Dataset name"""
    checksum: str
    """SHA1 checksum (often referred to as `content_hash`)"""
    extra: dict[str, Any] = {}
    """Arbitrary extra data (e.g. `memorious` header result)"""

    def to_entity(self) -> StatementEntity:
        """Make an entity for this obj"""
        entity = make_entity(
            {"id": self.id, "schema": mime_to_schema(self.mimetype)},
            entity_type=StatementEntity,
            default_dataset=self.dataset,
        )
        entity.add("contentHash", self.checksum)
        entity.add("fileName", self.name)
        entity.add("fileSize", self.size)
        entity.add("mimeType", self.mimetype)
        return entity

    def make_parents(self) -> StatementEntities:
        """Make parent `Folder` entities"""
        raise NotImplementedError

    @property
    def id(self) -> str:
        """The entity id is generated by a hash of the file path and the
        checksum"""
        return f"file-{make_data_checksum((self.key, self.checksum))}"

    @property
    def archive_path(self) -> str:
        """Relative path in dataset archive"""
        return path.file_path(self.checksum)

    @property
    def archive_path_meta(self) -> str:
        """Relative path for metadata json in dataset archive"""
        return path.file_path_meta(self.checksum)

    @classmethod
    def from_info(cls, info: Stats, checksum: str, **data) -> Self:
        data["dataset"] = data.get("dataset", DefaultDataset.name)
        data["checksum"] = checksum
        return cls(**{**info.model_dump(), **data})

    def to_dict(self) -> SDict:
        return self.model_dump(
            mode="json",
            include={
                "dataset",
                "checksum",
                "key",
                "size",
                "mimetype",
                "created_at",
                "updated_at",
            },
        )


Files: TypeAlias = Generator[File, None, None]
"""Type shorthand for a generator of `File` objects"""
