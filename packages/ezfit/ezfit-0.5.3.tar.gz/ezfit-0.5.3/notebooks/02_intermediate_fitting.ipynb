{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Tutorial: Choosing the Right Optimizer\n",
    "\n",
    "This tutorial explores different optimization methods and when to use them. We'll work with increasingly complex objective function surfaces to understand why different algorithms are needed.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Understand when to use different optimization methods\n",
    "- See how complex objective surfaces affect optimization\n",
    "- Learn about global vs local optimizers\n",
    "- Compare results from different methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ezfit.examples import (\n",
    "    generate_gaussian_data,\n",
    "    generate_linear_data,\n",
    "    generate_multi_peak_data,\n",
    "    generate_rugged_surface_data,\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Case: Linear Fitting\n",
    "\n",
    "For simple, well-behaved functions, the default `curve_fit` method works perfectly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple linear data\n",
    "df_linear = generate_linear_data(n_points=50, slope=2.0, intercept=1.0, seed=42)\n",
    "\n",
    "def line(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "# Default method: curve_fit (Levenberg-Marquardt)\n",
    "model1, ax1, _ = df_linear.fit(line, \"x\", \"y\", \"yerr\", method=\"curve_fit\")\n",
    "plt.show()\n",
    "print(\"Default method (curve_fit):\")\n",
    "print(f\"  m = {model1['m'].value:.4f} ¬± {model1['m'].err:.4f}\")\n",
    "print(f\"  b = {model1['b'].value:.4f} ¬± {model1['b'].err:.4f}\")\n",
    "print(f\"  œá¬≤ = {model1.ùúí2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. When Initial Guesses Matter: Gaussian Peak\n",
    "\n",
    "For functions with multiple parameters or local minima, initial guesses become important. Let's fit a Gaussian peak:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Gaussian peak data\n",
    "df_gauss = generate_gaussian_data(\n",
    "    n_points=100,\n",
    "    amplitude=10.0,\n",
    "    center=5.0,\n",
    "    fwhm=2.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Use built-in Gaussian function\n",
    "from ezfit import gaussian\n",
    "\n",
    "# Try with poor initial guess\n",
    "print(\"=== Poor Initial Guess ===\")\n",
    "model_bad, ax_bad, _ = df_gauss.fit(\n",
    "    gaussian, \"x\", \"y\", \"yerr\",\n",
    "    amplitude={\"value\": 5.0},  # Too low\n",
    "    center={\"value\": 3.0},     # Wrong position\n",
    "    fwhm={\"value\": 1.0}        # Too narrow\n",
    ")\n",
    "plt.show()\n",
    "print(model_bad)\n",
    "\n",
    "# Try with good initial guess\n",
    "print(\"\\n=== Good Initial Guess ===\")\n",
    "model_good, ax_good, _ = df_gauss.fit(\n",
    "    gaussian, \"x\", \"y\", \"yerr\",\n",
    "    amplitude={\"value\": 9.0},  # Close to true value\n",
    "    center={\"value\": 5.0},     # Close to true value\n",
    "    fwhm={\"value\": 2.0}        # Close to true value\n",
    ")\n",
    "plt.show()\n",
    "print(model_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Global Optimization: Rugged Surface\n",
    "\n",
    "When the objective function has multiple local minima, local optimizers can get stuck. This is when you need global optimizers like `differential_evolution`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with complex, multi-modal surface\n",
    "df_rugged = generate_rugged_surface_data(n_points=100, seed=42)\n",
    "\n",
    "# Define a model that approximates the complex function\n",
    "# y = A*sin(x)*exp(-x/B) + C*sin(D*x) + E\n",
    "def complex_model(x, A, B, C, D, E):\n",
    "    \"\"\"Complex model with multiple local minima\"\"\"\n",
    "    return A * np.sin(x) * np.exp(-x / B) + C * np.sin(D * x) + E\n",
    "\n",
    "# Try with local optimizer (may get stuck in local minimum)\n",
    "print(\"=== Local Optimizer (minimize) ===\")\n",
    "try:\n",
    "    model_local, ax_local, _ = df_rugged.fit(\n",
    "        complex_model, \"x\", \"y\", \"yerr\",\n",
    "        method=\"minimize\",\n",
    "        fit_kwargs={\"method\": \"L-BFGS-B\"},\n",
    "        A={\"value\": 1.0, \"min\": -5, \"max\": 5},\n",
    "        B={\"value\": 5.0, \"min\": 1, \"max\": 10},\n",
    "        C={\"value\": 0.5, \"min\": -2, \"max\": 2},\n",
    "        D={\"value\": 3.0, \"min\": 1, \"max\": 5},\n",
    "        E={\"value\": 2.0, \"min\": 0, \"max\": 5}\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"œá¬≤ = {model_local.ùúí2:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {e}\")\n",
    "\n",
    "# Try with global optimizer\n",
    "print(\"\\n=== Global Optimizer (differential_evolution) ===\")\n",
    "model_global, ax_global, _ = df_rugged.fit(\n",
    "    complex_model, \"x\", \"y\", \"yerr\",\n",
    "    method=\"differential_evolution\",\n",
    "    fit_kwargs={\"maxiter\": 1000, \"seed\": 42},\n",
    "    A={\"value\": 1.0, \"min\": -5, \"max\": 5},\n",
    "    B={\"value\": 5.0, \"min\": 1, \"max\": 10},\n",
    "    C={\"value\": 0.5, \"min\": -2, \"max\": 2},\n",
    "    D={\"value\": 3.0, \"min\": 1, \"max\": 5},\n",
    "    E={\"value\": 2.0, \"min\": 0, \"max\": 5}\n",
    ")\n",
    "plt.show()\n",
    "print(f\"œá¬≤ = {model_global.ùúí2:.2f}\")\n",
    "print(\"\\nGlobal optimizer finds better fit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Methods\n",
    "\n",
    "Let's compare multiple optimization methods on the same problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multi-peak data for comparison\n",
    "df_multi = generate_multi_peak_data(n_points=200, seed=42)\n",
    "\n",
    "# Define two-peak model\n",
    "def two_gaussians(x, A1, c1, w1, A2, c2, w2, B):\n",
    "    \"\"\"Sum of two Gaussians plus baseline\"\"\"\n",
    "    from ezfit import gaussian\n",
    "    return gaussian(x, A1, c1, w1) + gaussian(x, A2, c2, w2) + B\n",
    "\n",
    "# Define parameter bounds\n",
    "params = {\n",
    "    \"A1\": {\"value\": 7.0, \"min\": 0, \"max\": 15},\n",
    "    \"c1\": {\"value\": 7.0, \"min\": 5, \"max\": 9},\n",
    "    \"w1\": {\"value\": 2.0, \"min\": 0.5, \"max\": 5},\n",
    "    \"A2\": {\"value\": 5.0, \"min\": 0, \"max\": 15},\n",
    "    \"c2\": {\"value\": 12.0, \"min\": 10, \"max\": 14},\n",
    "    \"w2\": {\"value\": 3.0, \"min\": 0.5, \"max\": 5},\n",
    "    \"B\": {\"value\": 0.5, \"min\": 0, \"max\": 2}\n",
    "}\n",
    "\n",
    "methods = [\"curve_fit\", \"minimize\", \"differential_evolution\", \"dual_annealing\"]\n",
    "results = {}\n",
    "\n",
    "for method in methods:\n",
    "    try:\n",
    "        print(f\"\\n=== {method.upper()} ===\")\n",
    "        model, ax, _ = df_multi.fit(\n",
    "            two_gaussians, \"x\", \"y\", \"yerr\",\n",
    "            method=method,\n",
    "            fit_kwargs={\"seed\": 42} if method in [\"differential_evolution\", \"dual_annealing\"] else {},\n",
    "            **params\n",
    "        )\n",
    "        results[method] = {\"model\": model, \"chi2\": model.ùúí2}\n",
    "        print(f\"œá¬≤ = {model.ùúí2:.2f}\")\n",
    "        plt.close()  # Close plot to save memory\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {e}\")\n",
    "        results[method] = None\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for method, result in results.items():\n",
    "    if result:\n",
    "        print(f\"{method:25s}: œá¬≤ = {result['chi2']:.2f}\")\n",
    "    else:\n",
    "        print(f\"{method:25s}: Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing the Best Fit\n",
    "\n",
    "Let's plot the best result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best method\n",
    "best_method = min([m for m, r in results.items() if r],\n",
    "                  key=lambda m: results[m]['chi2'])\n",
    "\n",
    "print(f\"Best method: {best_method}\")\n",
    "\n",
    "# Re-fit and plot\n",
    "model_best, ax_best, ax_res_best = df_multi.fit(\n",
    "    two_gaussians, \"x\", \"y\", \"yerr\",\n",
    "    method=best_method,\n",
    "    fit_kwargs={\"seed\": 42} if best_method in [\"differential_evolution\", \"dual_annealing\"] else {},\n",
    "    **params\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "print(model_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. When to Use Which Method?\n",
    "\n",
    "### Decision Tree:\n",
    "\n",
    "1. **Simple, well-behaved functions** ‚Üí `curve_fit` (default)\n",
    "\n",
    "   - Fast, reliable for most cases\n",
    "   - Good initial guesses usually sufficient\n",
    "\n",
    "2. **Many local minima or complex surfaces** ‚Üí `differential_evolution` or `dual_annealing`\n",
    "\n",
    "   - Global optimizers that search entire parameter space\n",
    "   - Slower but more robust\n",
    "\n",
    "3. **Need uncertainty estimates** ‚Üí `curve_fit` or `emcee` (MCMC)\n",
    "\n",
    "   - `curve_fit` provides covariance matrix\n",
    "   - `emcee` provides full posterior distributions\n",
    "\n",
    "4. **Linear models** ‚Üí `ridge`, `lasso`, or `bayesian_ridge`\n",
    "   - Specialized methods for linear regression\n",
    "   - Can handle regularization\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "1. ‚úÖ When initial guesses matter\n",
    "2. ‚úÖ The difference between local and global optimizers\n",
    "3. ‚úÖ How to compare different methods\n",
    "4. ‚úÖ When to use each optimization method\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "- Try the advanced tutorial to learn about MCMC and constraints\n",
    "- Experiment with your own complex models\n",
    "- Read about parameter constraints in the documentation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
