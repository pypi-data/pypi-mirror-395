# -*- coding: utf-8 -*-
#!/usr/bin/env python3


"""
Action Chunk to Fast Token è½¬æ¢ç¨‹åº - ç²¾ç®€ç‰ˆ
"""
 
import numpy as np
import sys
import os
from typing import Dict, List
from scipy.fft import idct
# from action_token.tokenizer import FASTTokenizer
from transformers import AutoProcessor


class ActionChunkProcessor:
    """Action Chunkå¤„ç†å™¨"""
    
    def __init__(self, max_len: int = 256, fast_tokenizer_path: str = None):
        self.max_len = max_len
        if fast_tokenizer_path is None:
            fast_tokenizer_path = os.path.join(os.path.dirname(__file__), "fast")
            print(fast_tokenizer_path)
        # åˆå§‹åŒ–FAST tokenizer
        self.fast_tokenizer = AutoProcessor.from_pretrained(fast_tokenizer_path, trust_remote_code=True)
        # print(f"âœ“ FAST tokenizeråŠ è½½æˆåŠŸ")

        # åˆå§‹åŒ–FASTTokenizer
        # self.tokenizer = FASTTokenizer(max_len=max_len, fast_tokenizer_path=fast_tokenizer_path)
        # print(f"âœ“ FASTTokenizeråˆå§‹åŒ–æˆåŠŸ")
    
    def create_dummy_batch(self, batch_size: int = 2, action_horizon: int = 50, action_dim: int = 32) -> Dict:
        """åˆ›å»ºdummy batchæ•°æ®"""
        dummy_batch = {
            "prompt": [
                "Pick up the red block from the table",
                "Move the blue object to the left"
            ][:batch_size],
            "state": np.random.randn(batch_size, action_dim).astype(np.float32),
            "actions": np.random.randn(batch_size, action_horizon, action_dim).astype(np.float32),
        }
        return dummy_batch
    
    def process_action_chunk_to_fast_token(self, action_chunk: np.ndarray) -> List[List[int]]:
        """å°†action chunkè½¬æ¢ä¸ºfast token"""
        fast_tokens = self.fast_tokenizer(action_chunk)
        return fast_tokens
    
    # def tokenize_with_context(self, prompt: str, state: np.ndarray, actions: np.ndarray):
    #     """ä½¿ç”¨ä¸Šä¸‹æ–‡è¿›è¡Œtokenization"""
    #     tokens, token_masks, ar_masks, loss_masks = self.tokenizer.tokenize(prompt, state, actions)
    #     return tokens, token_masks, ar_masks, loss_masks
    
    # def extract_actions_from_tokens(self, action_tokens: List[List[int]], action_horizon: int, action_dim: int) -> np.ndarray:
    #     """ä»tokensä¸­æå–åŠ¨ä½œï¼ˆdetokenizeéªŒè¯ï¼‰"""
    #     # actions = self.tokenizer.extract_actions(tokens, action_horizon, action_dim)
    #     decode_actions, output_dims = self.fast_tokenizer.decode(action_tokens, time_horizon=action_horizon, action_dim=action_dim)
    #     return decode_actions, output_dims

    def extract_actions_from_tokens(self, action_tokens: List[List[int]], action_horizon: int, action_dim: int) -> np.ndarray:
        assert (
            action_horizon is not None and action_dim is not None
        ), "Tokenizer not initialized, call encode() once or pass in time_horizon and action_dim."

        decoded_actions = []
        output_dims = []
        for token in action_tokens:
            try:
                decoded_tokens = self.fast_tokenizer.bpe_tokenizer.decode(token)
                decoded_dct_coeff = np.array(list(map(ord, decoded_tokens))) + self.fast_tokenizer.min_token
                output_dim = len(decoded_dct_coeff)
                output_dims.append(output_dim)
                decoded_dct_coeff = decoded_dct_coeff.reshape(-1, action_dim)
                assert (
                    decoded_dct_coeff.shape
                    == (
                        action_horizon,
                        action_dim,
                    )
                ), f"Decoded DCT coefficients have shape {decoded_dct_coeff.shape}, expected ({action_horizon}, {action_dim})"
            except Exception as e:
                print(f"Error decoding tokens: {e}")
                print(f"Tokens: {token}")
                decoded_dct_coeff = np.zeros((action_horizon, action_dim))
            decoded_actions.append(idct(decoded_dct_coeff / self.fast_tokenizer.scale, axis=0, norm="ortho"))
        return np.stack(decoded_actions), output_dims

    def _extract_actions_from_tokens(self, action_tokens: List[List[int]], action_horizon: int, action_dim: int) -> np.ndarray:
        assert (
            action_horizon is not None and action_dim is not None
        ), "Tokenizer not initialized, call encode() once or pass in time_horizon and action_dim."

        decoded_actions = []
        output_dims = []
        for token in action_tokens:
            try:
                decoded_tokens = self.fast_tokenizer.bpe_tokenizer.decode(token)
                decoded_dct_coeff = np.array(list(map(ord, decoded_tokens))) + self.fast_tokenizer.min_token
                output_dim = len(decoded_dct_coeff)
                output_dims.append(output_dim)
                decoded_dct_coeff = decoded_dct_coeff.reshape(-1, action_dim)
                assert (
                    decoded_dct_coeff.shape
                    == (
                        action_horizon,
                        action_dim,
                    )
                ), f"Decoded DCT coefficients have shape {decoded_dct_coeff.shape}, expected ({action_horizon}, {action_dim})"
            except Exception as e:
                print(f"Error decoding tokens: {e}")
                print(f"Tokens: {token}")
                if len(decoded_dct_coeff.shape) == 2:
                    results = []
                    output_horizon = decoded_dct_coeff.shape[0]
                    needed_rows = 30
                    zero_rows_needed = max(0, needed_rows - output_horizon)
                    results.extend(decoded_dct_coeff[:needed_rows])
                    zero_vector = [0] * action_dim
                    results.extend([zero_vector for _ in range(zero_rows_needed)])
                    decoded_dct_coeff = np.array(results)
                else:
                    decoded_dct_coeff = np.zeros((action_horizon, action_dim))
            decoded_actions.append(idct(decoded_dct_coeff / self.fast_tokenizer.scale, axis=0, norm="ortho"))
        return np.stack(decoded_actions), output_dims

    def _extract_actions_from_tokens_v2(self, action_tokens: List[List[int]], action_horizon: int, action_dim: int) -> np.ndarray:
        assert (
            action_horizon is not None and action_dim is not None
        ), "Tokenizer not initialized, call encode() once or pass in time_horizon and action_dim."

        decoded_actions = []
        output_dims = []
        zero_nums = []
        for token in action_tokens:
            try:
                decoded_tokens = self.fast_tokenizer.bpe_tokenizer.decode(token)
                decoded_dct_coeff = np.array(list(map(ord, decoded_tokens))) + self.fast_tokenizer.min_token
                output_dim = len(decoded_dct_coeff)
                output_dims.append(output_dim)
                decoded_dct_coeff = decoded_dct_coeff.reshape(-1, action_dim)
                assert (
                    decoded_dct_coeff.shape
                    == (
                        action_horizon,
                        action_dim,
                    )
                ), f"Decoded DCT coefficients have shape {decoded_dct_coeff.shape}, expected ({action_horizon}, {action_dim})"
            except Exception as e:
                print(f"Error decoding tokens: {e}")
                print(f"Tokens: {token}")
                if len(decoded_dct_coeff.shape) == 2:
                    decoded_dct_coeff = decoded_dct_coeff
                else:
                    decoded_dct_coeff = np.zeros((action_horizon, action_dim))
            tmp_actions = idct(decoded_dct_coeff / self.fast_tokenizer.scale, axis=0, norm="ortho")
            if tmp_actions.shape == (action_horizon, action_dim):
                decoded_actions.append(tmp_actions)
                zero_nums.append(0)
            else:
                if action_horizon > tmp_actions.shape[0]:
                    delta_size = action_horizon - tmp_actions.shape[0] 
                    new_rows = np.zeros((delta_size, action_dim))
                    pad_actions = np.vstack((tmp_actions, new_rows))
                    decoded_actions.append(pad_actions)
                    zero_nums.append(delta_size)
                else:
                    truncate_actions = tmp_actions[:action_horizon, :]
                    decoded_actions.append(truncate_actions)
                    zero_nums.append(0)

            # decoded_actions.append(idct(decoded_dct_coeff / self.fast_tokenizer.scale, axis=0, norm="ortho"))
        return np.stack(decoded_actions), output_dims, zero_nums

    def validate_encode_decode(self, original_actions: np.ndarray, action_horizon: int, action_dim: int) -> Dict:
        """éªŒè¯ç¼–ç å’Œè§£ç çš„ä¸€è‡´æ€§"""
        print(f"ğŸ” éªŒè¯ç¼–ç è§£ç ä¸€è‡´æ€§...")
        
        # 1. ç›´æ¥ä½¿ç”¨FAST tokenizerç¼–ç 
        fast_tokens = self.fast_tokenizer(original_actions[None])[0]
        
        # 2. ä½¿ç”¨FAST tokenizerè§£ç 
        decoded_actions = self.fast_tokenizer.decode(
            [fast_tokens], 
            time_horizon=action_horizon, 
            action_dim=action_dim
        )[0]
        
        # 3. è®¡ç®—è¯¯å·®
        mse_error = np.mean((original_actions - decoded_actions) ** 2)
        mae_error = np.mean(np.abs(original_actions - decoded_actions))
        
        print(f"  - MSEè¯¯å·®: {mse_error:.6f}")
        print(f"  - MAEè¯¯å·®: {mae_error:.6f}")
        
        return {
            "original_shape": original_actions.shape,
            "decoded_shape": decoded_actions.shape,
            "mse_error": mse_error,
            "mae_error": mae_error,
            "fast_tokens_length": len(fast_tokens)
        }
    
    def process_batch(self, dummy_batch: Dict) -> Dict:
        """å¤„ç†æ•´ä¸ªbatchçš„æ•°æ®"""
        batch_size = len(dummy_batch["prompt"])
        results = {"processed_samples": []}
        
        for i in range(batch_size):
            sample_result = {"sample_id": i}
            
            try:
                # 1. ç›´æ¥è½¬æ¢action chunkä¸ºfast token
                action_chunk = dummy_batch["actions"][i:i+1]
                fast_tokens = self.process_action_chunk_to_fast_token(action_chunk)
                sample_result["fast_tokens"] = fast_tokens[0]
                sample_result["fast_tokens_length"] = len(fast_tokens[0])
                
                # 2. ä½¿ç”¨ä¸Šä¸‹æ–‡è¿›è¡Œtokenization
                tokens, token_masks, ar_masks, loss_masks = self.tokenize_with_context(
                    dummy_batch["prompt"][i],
                    dummy_batch["state"][i],
                    dummy_batch["actions"][i]
                )
                sample_result["context_tokens"] = tokens
                
                # 3. ä»tokensä¸­æå–åŠ¨ä½œï¼ˆdetokenizeéªŒè¯ï¼‰
                extracted_actions = self.extract_actions_from_tokens(
                    tokens, 
                    dummy_batch["actions"].shape[1], 
                    dummy_batch["actions"].shape[2]
                )
                sample_result["extracted_actions"] = extracted_actions
                
                # 4. éªŒè¯ç¼–ç è§£ç ä¸€è‡´æ€§
                validation_result = self.validate_encode_decode(
                    dummy_batch["actions"][i],
                    dummy_batch["actions"].shape[1],
                    dummy_batch["actions"].shape[2]
                )
                sample_result["validation"] = validation_result
                
                sample_result["success"] = True
                
            except Exception as e:
                sample_result["error"] = str(e)
                sample_result["success"] = False
            
            results["processed_samples"].append(sample_result)
        
        return results
    
    def print_results_summary(self, results: Dict):
        """æ‰“å°å¤„ç†ç»“æœæ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ¯ å¤„ç†ç»“æœæ‘˜è¦")
        print("="*50)
        
        successful_samples = 0
        total_fast_tokens = 0
        total_mse_error = 0
        total_mae_error = 0
        
        for sample in results["processed_samples"]:
            sample_id = sample["sample_id"]
            
            if sample.get("success", False):
                successful_samples += 1
                print(f"æ ·æœ¬ {sample_id + 1}: âœ… æˆåŠŸ")
                print(f"  - Fast tokensé•¿åº¦: {sample['fast_tokens_length']}")
                print(f"  - ä¸Šä¸‹æ–‡tokenså½¢çŠ¶: {sample['context_tokens'].shape}")
                print(f"  - æå–åŠ¨ä½œå½¢çŠ¶: {sample['extracted_actions'].shape}")
                
                validation = sample["validation"]
                print(f"  - ç¼–ç è§£ç MSE: {validation['mse_error']:.6f}")
                print(f"  - ç¼–ç è§£ç MAE: {validation['mae_error']:.6f}")
                
                total_fast_tokens += sample['fast_tokens_length']
                total_mse_error += validation['mse_error']
                total_mae_error += validation['mae_error']
            else:
                print(f"æ ·æœ¬ {sample_id + 1}: âŒ å¤±è´¥")
        
        print(f"\nğŸ“Š ç»Ÿè®¡: {successful_samples}/{len(results['processed_samples'])} æˆåŠŸ")
        if successful_samples > 0:
            print(f"å¹³å‡Fast tokensé•¿åº¦: {total_fast_tokens/successful_samples:.1f}")
            print(f"å¹³å‡MSEè¯¯å·®: {total_mse_error/successful_samples:.6f}")
            print(f"å¹³å‡MAEè¯¯å·®: {total_mae_error/successful_samples:.6f}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Action Chunk to Fast Token è½¬æ¢ç¨‹åº")
    
    try:
        # åˆå§‹åŒ–å¤„ç†å™¨
        processor = ActionChunkProcessor(max_len=256)
        
        # åˆ›å»ºdummy batch
        dummy_batch = processor.create_dummy_batch(batch_size=3, action_horizon=50, action_dim=32)
        
        # å¤„ç†batch
        results = processor.process_batch(dummy_batch)
        
        # æ‰“å°ç»“æœæ‘˜è¦
        processor.print_results_summary(results)
        
        print("\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 