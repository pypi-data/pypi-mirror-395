# -*- coding: utf-8 -*-

'''
version v1.0
'''

import os
import json
import shutil
import logging
from pathlib import Path
from typing import List, Dict, Union, Optional, Tuple
import subprocess
from collections import defaultdict, OrderedDict
from logging.handlers import TimedRotatingFileHandler
import sys
import pandas as pd


class DoRobotDataMerger:
    def __init__(self, output_dir: Path = None, log_dir: Path = None, log_to_file: bool = False):
        """
        åˆå§‹åŒ–æ•°æ®åˆå¹¶å·¥å…·

        Args:
            repo_url: ä»£ç ä»“åº“URLï¼Œç”¨äºè‡ªåŠ¨æ›´æ–°ä»£ç ç‰ˆæœ¬
            data_dirs: å•ä¸ªæ•°æ®ç›®å½•è·¯å¾„(å­—ç¬¦ä¸²)æˆ–å¤šä¸ªç›®å½•è·¯å¾„(åˆ—è¡¨)
            output_dir: åˆå¹¶è¾“å‡ºç›®å½•
            log_to_file: æ˜¯å¦å°†æ—¥å¿—ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆé»˜è®¤Falseï¼Œè¾“å‡ºåˆ°æ§åˆ¶å°ï¼‰
        """
        self.repo_url = "git@github.com:liuyou1103/any_to_lerobot.git"
        self.data_dirs = None
        self.output_dir = Path(output_dir)
        self.output_dir_task = None
        self.current_code_version = "v1.0"
        self.supported_data_versions = ["v1.0"]
        self.existing_dirs_list = []

    def _normalize_data_dirs(self, data_dirs: Union[str, List[str]]) -> List[Path]:
        """
        æ ‡å‡†åŒ–æ•°æ®ç›®å½•è¾“å…¥ï¼š
        - å¦‚æœæ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œè·å–å…¶æ‰€æœ‰å­ç›®å½•
        - å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½æ˜¯Pathå¯¹è±¡

        Args:
            data_dirs: å­—ç¬¦ä¸²è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨

        Returns:
            Pathå¯¹è±¡çš„åˆ—è¡¨

        Raises:
            FileNotFoundError: è·¯å¾„ä¸å­˜åœ¨
            ValueError: è¾“å…¥ç±»å‹æ— æ•ˆ
        """
        if isinstance(data_dirs, str):
            path = Path(data_dirs)
            if not path.exists():
                raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
            if path.is_file():
                return [path.parent]
            else:
                subdirs = [d for d in path.iterdir() if d.is_dir()]

                def extract_sort_key(dirname):
                    parts = str(dirname).split('_')
                    if parts:
                        last_part = parts[-1]
                        if last_part.isdigit():
                            return int(last_part)
                    return 0

                subdirs.sort(key=extract_sort_key)
                return subdirs

        elif isinstance(data_dirs, list):
            normalized = []
            for d in data_dirs:
                p = Path(d) if isinstance(d, str) else d
                if not p.exists():
                    raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {p}")
                normalized.append(p)
            return normalized

        else:
            raise ValueError("data_dirs å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨")

    @staticmethod
    def get_img_video_path(each_task_path: Path, camera_images: str, camera_images_path: Path) -> Tuple[
        List[Path], List[Path], List[Path]]:
        """
        è·å–å›¾åƒå’Œè§†é¢‘è·¯å¾„åˆ—è¡¨

        Args:
            each_task_path: ä»»åŠ¡æ ¹ç›®å½•è·¯å¾„
            camera_images: ç›¸æœºåç§°
            camera_images_path: ç›¸æœºå›¾åƒç›®å½•è·¯å¾„

        Returns:
            åŒ…å«ä¸‰ä¸ªåˆ—è¡¨çš„å…ƒç»„ï¼š(img_path_list, video_path_list, label_video_path_list)
        """
        img_path_list = []
        video_path_list = []
        depth_video_path_list = []

        try:
            for episode_index in camera_images_path.iterdir():
                if not episode_index.is_dir():
                    continue

                episode_index_name = episode_index.name
                video_name = episode_index_name + '.mp4'
                depth_video_name = episode_index_name + '.avi'
                video_path = each_task_path / "videos" / "chunk-000" / camera_images / video_name
                depth_video_path = each_task_path / "depth" / "chunk-000" / camera_images / depth_video_name

                # ç¡®ä¿ç›®å½•å­˜åœ¨
                video_path.parent.mkdir(parents=True, exist_ok=True)
                depth_video_path.parent.mkdir(parents=True, exist_ok=True)

                img_path_list.append(episode_index)
                video_path_list.append(video_path)
                depth_video_path_list.append(depth_video_path)

            if len(img_path_list) != len(video_path_list):
                logging.error("å›¾åƒå’Œè§†é¢‘è·¯å¾„æ•°é‡ä¸åŒ¹é…")
                return [], [], []

            logging.debug(f"æ‰¾åˆ° {len(img_path_list)} ç»„å›¾åƒå’Œè§†é¢‘è·¯å¾„")
            return img_path_list, video_path_list, depth_video_path_list
        except Exception as e:
            logging.error(f"è·å–è·¯å¾„åˆ—è¡¨å¤±è´¥: {str(e)}")
            return [], [], []

    @staticmethod
    def read_info_json(each_task_path: Path) -> Optional[int]:
        """
        è¯»å–info.jsonæ–‡ä»¶è·å–FPSå€¼

        Args:
            each_task_path: åŒ…å«meta/info.jsonçš„ç›®å½•è·¯å¾„

        Returns:
            FPSå€¼ï¼Œè¯»å–å¤±è´¥è¿”å›None
        """
        try:
            with open(each_task_path / "info.json", "r", encoding="utf-8") as f:
                data = json.load(f)
                fps = data.get("fps")
                logging.debug(f"è¯»å–åˆ°FPSå€¼: {fps}")
                return fps
        except Exception as e:
            logging.error(f"è¯»å–info.jsonå¤±è´¥: {str(e)}")
            return None

    def _compile_images_to_videos(self, input_dirs: List[Path]) -> None:
        """
        å°†å›¾åƒåºåˆ—ç¼–è¯‘ä¸ºè§†é¢‘æ–‡ä»¶ï¼ˆ.aviå’Œ.mp4æ ¼å¼ï¼‰

        Args:
            input_dirs: åŒ…å«imagesç›®å½•çš„è¾“å…¥è·¯å¾„åˆ—è¡¨
        """
        for input_dir in input_dirs:
            images_dir = input_dir / "images"
            meta_dir = input_dir / "meta"
            fps = self.read_info_json(meta_dir)
            if fps is None:
                raise ValueError("æ— æ³•è¯»å–FPSå€¼ï¼Œè·³è¿‡ç¼–è¯‘")

            camera_images_path_name = []
            for camera_images_path in images_dir.iterdir():
                if not camera_images_path.is_dir():
                    continue

                camera_images = camera_images_path.name
                camera_images_path_name.append(camera_images)
                img_list, video_list, depth_video_path_list = self.get_img_video_path(input_dir, camera_images,
                                                                                      camera_images_path)

                if 'depth' in camera_images:
                    # å¤„ç†æ·±åº¦å›¾åƒ
                    if img_list:
                        for img_path, video_path in zip(img_list, depth_video_path_list):
                            logging.info(f"å¤„ç†æ·±åº¦å›¾åƒ: {img_path} -> {video_path}")
                            if not self.encode_depth_video_frames(img_path, video_path, fps):
                                logging.warning(f"æ·±åº¦è§†é¢‘ç¼–ç å¤±è´¥: {video_path}")
                else:
                    # å¤„ç†æ™®é€šå›¾åƒ
                    if img_list:
                        for img_path, video_path in zip(img_list, video_list):
                            logging.info(f"å¤„ç†æ™®é€šå›¾åƒ(mp4): {img_path} -> {video_path}")
                            if not self.encode_video_frames(img_path, video_path, fps):
                                logging.warning(f"æ™®é€šè§†é¢‘ç¼–ç å¤±è´¥: {video_path}")

            # æ›´æ–°å…ƒæ•°æ®
            with open(meta_dir / "info.json", "r", encoding="utf-8") as f:
                metadata = json.load(f)

            if "features" not in metadata:
                logging.warning("æ— æ•ˆå…ƒæ•°æ®: ç¼ºå°‘'features'å­—æ®µ")
            else:
                for field_name, field_info in metadata["features"].items():
                    if field_name in camera_images_path_name:
                        field_info["dtype"] = "video"

            metadata.update({
                "total_videos": 1,
                "video_path": "videos/chunk-{episode_chunk:03d}/{video_key}/episode_{episode_index:06d}.mp4",
                "image_path": None
            })

            with open(meta_dir / "info.json", "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=4, ensure_ascii=False)

            # åˆ é™¤åŸå§‹imagesç›®å½•
            shutil.rmtree(images_dir)
            logging.info(f"å·²å®Œæˆå›¾åƒç¼–è¯‘: {input_dir}")

    def _get_current_code_version(self) -> str:
        """è·å–å½“å‰ä»£ç ç‰ˆæœ¬"""
        try:
            return "1.0"
        except:
            return "unknown"

    def _check_data_version(self, data_path: Path) -> bool:
        """
        æ£€æŸ¥æ•°æ®ç‰ˆæœ¬æ˜¯å¦ä¸ä»£ç å…¼å®¹

        Args:
            data_path: æ•°æ®ç›®å½•è·¯å¾„

        Returns:
            bool: æ˜¯å¦å…¼å®¹
        """
        version_file = data_path / "meta" / "info.json"
        if not version_file.exists():
            return False

        with open(version_file, "r", encoding="utf-8") as f:
            metadata = json.load(f)
            data_version = metadata.get("dorobot_dataset_version", "unknown")

        return data_version in self.supported_data_versions

    def _update_output_file_name(self, data_path: Path):
        """
        ä»common_record.jsonè·å–ä»»åŠ¡åç§°å’ŒIDï¼Œç”¨äºæ„å»ºè¾“å‡ºç›®å½•

        Args:
            data_path: æ•°æ®ç›®å½•è·¯å¾„
        """
        task_file = data_path / "meta" / "common_record.json"
        if not task_file.exists():
            return False

        with open(task_file, "r", encoding="utf-8") as f:
            taskdata = json.load(f)
            task_id = taskdata.get("task_id", "unknown")
            task_name = taskdata.get("task_name", "unknown")
            output_name = f"{task_name}_{task_id}"
            self.output_dir_task = self.output_dir / output_name
            meta_dir = self.output_dir_task / "meta"
            if meta_dir.exists():
                raise FileExistsError(
                    f"Error: 'meta' directory already exists in {self.output_dir_task / output_name}.")
            self.output_dir_task.mkdir(parents=True, exist_ok=True)

    def _update_code_from_repo(self):
        """ä»ä»“åº“è‡ªåŠ¨æ›´æ–°ä»£ç """
        if not self.repo_url:
            raise ValueError("Repository URL not provided for code update")

        try:
            logging.info(f"Updating code from repository: {self.repo_url}")
            subprocess.run(["git", "pull", self.repo_url], check=True)
            self.current_code_version = self._get_current_code_version()
            logging.info(f"Code updated to version: {self.current_code_version}")
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f"Failed to update code from repository: {e}")

    def _validate_data_structure(self, data_path: Path) -> bool:
        """
        éªŒè¯æ•°æ®ç›®å½•ç»“æ„æ˜¯å¦æ­£ç¡®ï¼š
        1. å¿…é¡»åŒ…å« `meta` ç›®å½•
        2. å¿…é¡»åŒ…å« `images` æˆ– `videos` ç›®å½•ï¼ˆä½†ä¸èƒ½åŒæ—¶å­˜åœ¨ï¼‰
        3. æ‰€æœ‰å¿…éœ€ç›®å½•å¿…é¡»éç©º

        Args:
            data_path: è¦éªŒè¯çš„æ•°æ®ç›®å½•è·¯å¾„

        Returns:
            bool: ç»“æ„æ˜¯å¦æœ‰æ•ˆ
        """
        required_dirs = {"meta", "data"}
        optional_dirs = {"images", "videos"}
        existing_dirs = {d.name for d in data_path.iterdir() if d.is_dir()}
        self.existing_dirs_list.append(existing_dirs)

        # æ£€æŸ¥å¿…éœ€ç›®å½•æ˜¯å¦å­˜åœ¨
        if not required_dirs.issubset(existing_dirs):
            missing = required_dirs - existing_dirs
            logging.error(f"ç¼ºå°‘å¿…éœ€ç›®å½• {missing} in {data_path}")
            return False

        # æ£€æŸ¥ `images` å’Œ `videos` æ˜¯å¦äº’æ–¥
        media_dirs = {d for d in existing_dirs if d in optional_dirs}
        if len(media_dirs) != 1:
            logging.error(f"å¿…é¡»ä¸”åªèƒ½å­˜åœ¨ä¸€ä¸ªåª’ä½“ç›®å½•ï¼ˆimages/videosï¼‰ï¼Œå½“å‰æ‰¾åˆ°: {media_dirs}")
            return False

        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€ç›®å½•æ˜¯å¦éç©º
        for dir_name in required_dirs | media_dirs:
            dir_path = data_path / dir_name
            if not any(dir_path.iterdir()):
                logging.error(f"ç›®å½• {dir_path} ä¸ºç©º")
                return False

        return True

    def _check_consistent_structure(self) -> bool:
        """
        æ£€æŸ¥æ‰€æœ‰è¾“å…¥ç›®å½•çš„ç»“æ„æ˜¯å¦ä¸€è‡´

        Returns:
            bool: æ˜¯å¦ä¸€è‡´
        """
        if not self.existing_dirs_list:
            return False

        reference_structure = self.existing_dirs_list[0]
        reference_optional = reference_structure & {"images", "videos"}

        if len(reference_optional) != 1:
            logging.error(f"å‚è€ƒç»“æ„åŒ…å«æ— æ•ˆçš„å¯é€‰ç›®å½•: {reference_optional}")
            return False

        for structure in self.existing_dirs_list[1:]:
            current_optional = structure & {"images", "videos"}
            if current_optional != reference_optional:
                logging.error(f"ç›®å½•ä¸ä¸€è‡´ã€‚æœŸæœ›: {reference_optional}, æ‰¾åˆ°: {current_optional}")
                return False

            required_dirs = {"meta", "data"}
            if not required_dirs.issubset(structure):
                missing = required_dirs - structure
                logging.error(f"æŸäº›æ–‡ä»¶å¤¹ç¼ºå°‘å¿…éœ€ç›®å½•: {missing}")
                return False

        return True

    def _detect_file_extension(self, input_dir: Path) -> Optional[str]:
        """
        æ£€æµ‹ç›®å½•ä¸­episodeæ–‡ä»¶çš„æ‰©å±•å

        Args:
            input_dir: è¦æœç´¢çš„è¾“å…¥ç›®å½•

        Returns:
            æ£€æµ‹åˆ°çš„æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚ ".mp4"ï¼‰ï¼Œæœªæ‰¾åˆ°è¿”å›None
        """
        for file in input_dir.rglob("episode_[0-9]*.*"):
            if file.is_file():
                return file.suffix.lower() or None
        return None

    def _get_middle_file_path(self, file_path: Path, sub: str) -> Path:
        """
        è·å–æ–‡ä»¶è·¯å¾„ä¸­æŒ‡å®šå­ç›®å½•ä¹‹åçš„è·¯å¾„éƒ¨åˆ†

        Args:
            file_path: å®Œæ•´æ–‡ä»¶è·¯å¾„
            sub: è¦æŸ¥æ‰¾çš„å­ç›®å½•å

        Returns:
            Pathå¯¹è±¡ï¼Œè¡¨ç¤ºsubä¹‹åçš„è·¯å¾„ï¼ˆä¸åŒ…æ‹¬æ–‡ä»¶åï¼‰
        """
        parts = file_path.parts
        try:
            sub_index = len(parts) - 1 - parts[::-1].index(sub)
        except ValueError:
            return Path()

        if sub_index < len(parts) - 1:
            middle_parts = parts[sub_index + 1:-1]
            return Path(*middle_parts) if middle_parts else Path()
        return Path()

    def _reindex_episode_files(self, input_dirs: List[Path], output_dir: Path, sub: str, file_extension: str):
        """
        é‡æ–°ç¼–å·episodeæ–‡ä»¶å¹¶å¤åˆ¶åˆ°è¾“å‡ºç›®å½•

        Args:
            input_dirs: è¾“å…¥ç›®å½•åˆ—è¡¨
            output_dir: è¾“å‡ºæ ¹ç›®å½•
            sub: å­ç›®å½•å
            file_extension: æ–‡ä»¶æ‰©å±•åï¼ˆå¦‚.mp4ï¼‰
        """
        output_dir = output_dir / sub
        relative_path_list = []
        logging.info(f"æ£€æµ‹åˆ°æ–‡ä»¶æ‰©å±•å: {file_extension}")

        episode_counter = 0

        for input_dir in input_dirs:
            for root, _, files in os.walk(input_dir):
                for file in files:
                    file_path = Path(root) / file
                    if file_path.suffix.lower() != file_extension:
                        continue

                    # è·³è¿‡å·²æ­£ç¡®å‘½åçš„æ–‡ä»¶
                    if file.startswith("episode_") and file[8:].isdigit():
                        continue

                    relative_path = self._get_middle_file_path(file_path, sub)
                    episode_counter = relative_path_list.count(relative_path)
                    relative_path_list.append(relative_path)

                    new_filename = f"episode_{episode_counter:06d}{file_extension}"
                    output_path = output_dir / relative_path / new_filename
                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    shutil.copy2(file_path, output_path)

        logging.info(f"æˆåŠŸé‡æ–°ç¼–å· {episode_counter} ä¸ªæ–‡ä»¶åˆ° {output_dir}")

    def _merge_jsonl_files(self, input_dirs: List[Path], output_dir: Path, key_field: Optional[str], sub: str):
        """
        åˆå¹¶JSONLæ–‡ä»¶

        Args:
            input_dirs: è¾“å…¥ç›®å½•åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            key_field: ç”¨äºé‡æ–°ç´¢å¼•çš„å­—æ®µï¼ˆå¯é€‰ï¼‰
            sub: å­ç›®å½•å
        """
        output_dir = output_dir / sub
        output_dir.mkdir(parents=True, exist_ok=True)

        jsonl_files = defaultdict(list)
        json_files = defaultdict(list)

        for input_dir in input_dirs:
            if not input_dir.exists():
                continue

            for file in input_dir.iterdir():
                if file.name == "info.json":
                    continue

                if file.suffix.lower() == ".jsonl":
                    jsonl_files[file.name].append(file)
                elif file.suffix.lower() == ".json":
                    json_files[file.name].append(file)

        # åˆå¹¶JSONLæ–‡ä»¶
        for filename, files in jsonl_files.items():
            relative_path = self._get_middle_file_path(files[0], sub)
            output_path = output_dir / relative_path / filename
            output_path.parent.mkdir(parents=True, exist_ok=True)

            records = []
            if key_field:
                temp_records = []
                for file in files:
                    with open(file, 'r', encoding='utf-8') as f:
                        for line in f:
                            record = json.loads(line.strip())
                            if key_field in record:
                                temp_records.append(record)

                if temp_records:
                    temp_records.sort(key=lambda x: x[key_field])
                    for i, record in enumerate(temp_records):
                        new_record = record.copy()
                        new_record[key_field] = i
                        records.append(new_record)
                    logging.info(f"  åˆå¹¶å®Œæˆ: {filename} (å…± {len(records)} æ¡è®°å½•)")
                else:
                    for file in files:
                        with open(file, 'r', encoding='utf-8') as f:
                            for line in f:
                                records.append(json.loads(line.strip()))
                                logging.info(f"  ä»…ä¿ç•™é¦–æ¡è®°å½•:{filename}")
                                break
                        break

            with open(output_path, 'w', encoding='utf-8') as f:
                for record in records:
                    f.write(json.dumps(record, ensure_ascii=False) + "\n")

        # åˆå¹¶JSONæ–‡ä»¶
        for filename, files in json_files.items():
            relative_path = self._get_middle_file_path(files[0], sub)
            output_path = output_dir / relative_path / filename
            output_path.parent.mkdir(parents=True, exist_ok=True)

            merged_data = None
            for file in files:
                with open(file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    merged_data = data
                    # if isinstance(data, list):
                    #     merged_data.extend(data)
                    # else:
                    #     merged_data.append(data)
                    logging.info(f"  ä»…ä¿ç•™å•æ–‡ä»¶è®°å½•:{filename}")
                    break

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(merged_data, f, ensure_ascii=False, indent=2)

    def _merge_meta_info_files(self, input_dirs: List[Path], output_dir: Path, sub: str):
        """
        åˆå¹¶å…ƒæ•°æ®ä¿¡æ¯æ–‡ä»¶

        Args:
            input_dirs: è¾“å…¥ç›®å½•åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            sub: å­ç›®å½•å
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        file_name = "info.json"
        episode_name = "episodes.jsonl"
        data_count = len(input_dirs)
        lengths = []

        for input_dir in input_dirs:
            info_path = input_dir / file_name
            if not info_path.exists():
                continue

            output_path = output_dir / "meta" / file_name
            episode_path = output_dir / "meta" / episode_name

            # è¯»å–episodeé•¿åº¦
            if episode_path.exists():
                with open(episode_path, 'r', encoding='utf-8') as file:
                    for line in file:
                        data = json.loads(line.strip())
                        lengths.append(data["length"])

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total = sum(lengths) if lengths else 0
            shutil.copy2(info_path, output_path)

            with open(output_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
                features = metadata["features"]
                count = sum(1 for key in features.keys() if key.startswith("observation.images"))

                metadata.update({
                    "total_videos": int(data_count * count),
                    "total_episodes": int(data_count),
                    "total_frames": int(total),
                    "splits": {"train": f"0:{str(data_count)}"}
                })

            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=4, ensure_ascii=False)
            break

    @staticmethod
    def encode_video_frames(imgs_dir: Union[Path, str], video_path: Union[Path, str], fps: int) -> bool:
        """
        ç¼–ç æ™®é€šè§†é¢‘å¸§ï¼ˆè‡ªåŠ¨æ£€æµ‹å›¾ç‰‡åç¼€ï¼‰

        Args:
            imgs_dir: åŒ…å«å›¾åƒåºåˆ—çš„ç›®å½•
            video_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            imgs_dir = Path(imgs_dir)
            video_path = Path(video_path)
            video_path.parent.mkdir(parents=True, exist_ok=True)

            supported_extensions = ['.jpg', '.jpeg', '.png']
            detected_ext = None
            for ext in supported_extensions:
                if list(imgs_dir.glob(f"*{ext}")):
                    detected_ext = ext
                    break

            if not detected_ext:
                raise ValueError(f"æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶ï¼ˆæ”¯æŒ: {', '.join(supported_extensions)}ï¼‰")

            ffmpeg_args = OrderedDict([
                ("-f", "image2"),
                ("-r", str(fps)),
                ("-i", str(imgs_dir / f"frame_%06d{detected_ext}")),
                ("-vcodec", "libx264"),
                ("-pix_fmt", "yuv420p"),
                ("-g", "5"),
                ("-crf", "18"),
                ("-loglevel", "error"),
            ])

            ffmpeg_cmd = ["ffmpeg"] + [item for pair in ffmpeg_args.items() for item in pair] + [str(video_path)]
            subprocess.run(ffmpeg_cmd, check=True)

            if not video_path.exists():
                raise OSError(f"è§†é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {video_path}")

            logging.info(f"æ™®é€šè§†é¢‘ç¼–ç æˆåŠŸ: {video_path}")
            return True
        except Exception as e:
            logging.error(f"æ™®é€šè§†é¢‘ç¼–ç å¤±è´¥: {str(e)}")
            return False

    @staticmethod
    def encode_label_video_frames(imgs_dir: Union[Path, str], video_path: Union[Path, str], fps: int) -> bool:
        """
        ç¼–ç æ ‡ç­¾è§†é¢‘å¸§

        Args:
            imgs_dir: åŒ…å«å›¾åƒåºåˆ—çš„ç›®å½•
            video_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            imgs_dir = Path(imgs_dir)
            video_path = Path(video_path)
            video_path.parent.mkdir(parents=True, exist_ok=True)

            supported_extensions = ['.jpg', '.jpeg', '.png']
            detected_ext = None
            for ext in supported_extensions:
                if list(imgs_dir.glob(f"*{ext}")):
                    detected_ext = ext
                    break

            if not detected_ext:
                raise ValueError(f"æœªæ‰¾åˆ°æ”¯æŒçš„å›¾ç‰‡æ–‡ä»¶ï¼ˆæ”¯æŒ: {', '.join(supported_extensions)}ï¼‰")

            ffmpeg_args = OrderedDict([
                ("-f", "image2"),
                ("-r", str(fps)),
                ("-i", str(imgs_dir / f"frame_%06d{detected_ext}")),
                ("-vcodec", "libx264"),
                ("-pix_fmt", "yuv420p"),
                ("-g", "20"),
                ("-crf", "23"),
                ("-loglevel", "error"),
            ])

            ffmpeg_cmd = ["ffmpeg"] + [item for pair in ffmpeg_args.items() for item in pair] + [str(video_path)]
            subprocess.run(ffmpeg_cmd, check=True)

            if not video_path.exists():
                raise OSError(f"è§†é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {video_path}")

            logging.info(f"æ ‡ç­¾è§†é¢‘ç¼–ç æˆåŠŸ: {video_path}")
            return True
        except Exception as e:
            logging.error(f"æ ‡ç­¾è§†é¢‘ç¼–ç å¤±è´¥: {str(e)}")
            return False

    @staticmethod
    def encode_depth_video_frames(imgs_dir: Union[Path, str], video_path: Union[Path, str], fps: int) -> bool:
        """
        ç¼–ç æ·±åº¦è§†é¢‘å¸§

        Args:
            imgs_dir: åŒ…å«æ·±åº¦å›¾åƒåºåˆ—çš„ç›®å½•
            video_path: è¾“å‡ºè§†é¢‘è·¯å¾„
            fps: å¸§ç‡

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            imgs_dir = Path(imgs_dir)
            video_path = Path(video_path)
            video_path.parent.mkdir(parents=True, exist_ok=True)

            ffmpeg_args = [
                "ffmpeg",
                "-f", "image2",
                "-r", str(fps),
                "-i", str(imgs_dir / "frame_%06d.png"),
                "-vcodec", "ffv1",
                "-loglevel", "error",
                "-pix_fmt", "gray16le",
                "-y",
                str(video_path)
            ]

            subprocess.run(ffmpeg_args, check=True)

            if not video_path.exists():
                raise OSError(f"è§†é¢‘æ–‡ä»¶æœªç”Ÿæˆ: {video_path}")

            logging.info(f"æ·±åº¦è§†é¢‘ç¼–ç æˆåŠŸ: {video_path}")
            return True
        except Exception as e:
            logging.error(f"æ·±åº¦è§†é¢‘ç¼–ç å¤±è´¥: {str(e)}")
            return False

    def prepare_merge(self, data_paths: List[Path]) -> bool:
        """
        åˆå¹¶å‰çš„å‡†å¤‡å·¥ä½œ

        Args:
            data_paths: è¦åˆå¹¶çš„æ•°æ®ç›®å½•è·¯å¾„åˆ—è¡¨

        Returns:
            bool: æ˜¯å¦å‡†å¤‡å¥½è¿›è¡Œåˆå¹¶

        Raises:
            ValueError: æ•°æ®ç‰ˆæœ¬ä¸åŒ¹é…æˆ–ç»“æ„æ— æ•ˆæ—¶æŠ›å‡º
            RuntimeError: å‡†å¤‡å¤±è´¥æ—¶æŠ›å‡º
        """
        logging.info(f"å½“å‰ä»£ç ç‰ˆæœ¬: {self.current_code_version}")
        # 1. æ£€æŸ¥æ•°æ®ç‰ˆæœ¬
        for path in data_paths:
            version_file = path / "meta" / "info.json"
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if not version_file.exists():
                logging.error(f"è·¯å¾„ä¸å­˜åœ¨: {path},æ•°æ®æ ¼å¼éƒ¨ä¸åŒ¹é…ï¼Œè·³è¿‡åˆå¹¶")
                return
            if not self._check_data_version(path):
                logging.error(f"æ•°æ®ç‰ˆæœ¬æ£€æŸ¥å¤±è´¥: {path}")
                # if self.repo_url:
                #     logging.info("å°è¯•ä»ä»“åº“æ›´æ–°ä»£ç ...")
                #     self._update_code_from_repo()
                #     if not self._check_data_version(path):
                #         raise ValueError(f"æ›´æ–°æˆåŠŸä½†æ•°æ®ç‰ˆæœ¬ä¸åŒ¹é…: {path}")
                # else:
                #     raise ValueError(f"æ•°æ®ç‰ˆæœ¬ä¸åŒ¹é…ä¸”æœªæä¾›ä»“åº“URL: {path}")
        # 2. æ£€æŸ¥æ•°æ®ç»“æ„
        for path in data_paths:
            if not self._validate_data_structure(path):
                raise ValueError(f"æ— æ•ˆçš„æ•°æ®ç»“æ„: {path}")
        # 3. æ£€æŸ¥æ‰€æœ‰ç›®å½•ç»“æ„æ˜¯å¦ä¸€è‡´
        if not self._check_consistent_structure():
            raise ValueError("è¾“å…¥æ–‡ä»¶å¤¹çš„ç›®å½•ç»“æ„ä¸ä¸€è‡´")
        # 4.æ›´æ–°è¾“å‡ºç›®å½•åç§°
        for path in data_paths:
            self._update_output_file_name(path)
            break

        return True

    def merge(self, data_dirs):
        """æ‰§è¡Œä¸»åˆå¹¶æµç¨‹"""
        try:
            # åˆå§‹åŒ–ç›®å½•
            self.data_dirs = self._normalize_data_dirs(data_dirs) if data_dirs else []
            self.existing_dirs_list = []
            logging.info(f"åˆå§‹åŒ–æ•°æ®ç›®å½•: {self.data_dirs}")
            # åˆ†ç±»æ•°æ®ç›®å½•
            data_compile_list = [d for d in self.data_dirs if (d / "images").exists()]

            # 1. ç¼–è¯‘å›¾åƒæ•°æ®ä¸ºè§†é¢‘æ ¼å¼
            if data_compile_list:
                logging.info(f"æ­£åœ¨ç¼–è¯‘ {len(data_compile_list)} ä¸ªå›¾åƒæ•°æ®é›†ä¸ºè§†é¢‘æ ¼å¼...")
                self._compile_images_to_videos(data_compile_list)
                logging.info("ç¼–è¯‘å®Œæˆ")

            if not self.prepare_merge(self.data_dirs):
                logging.error("åˆå¹¶å‡†å¤‡é˜¶æ®µå¤±è´¥")
                return

            # 2. åˆå¹¶æ‰€æœ‰æ•°æ®
            for sub in self.existing_dirs_list[0]:
                has_episode_file = any(self._detect_file_extension(d / sub) for d in self.data_dirs)
                logging.info("")
                if has_episode_file:
                    logging.info(f"åˆå¹¶ {sub} ä¸­çš„episodeæ–‡ä»¶...")
                    new_data_dirs = [d / sub for d in self.data_dirs]
                    suffix = self._detect_file_extension(new_data_dirs[0])
                    self._reindex_episode_files(new_data_dirs, self.output_dir_task, sub, suffix)
                else:
                    logging.info(f"åˆå¹¶ {sub} ä¸­çš„å…ƒæ•°æ®æ–‡ä»¶...")
                    new_data_dirs = [d / sub for d in self.data_dirs]
                    self._merge_jsonl_files(new_data_dirs, self.output_dir_task, "episode_index", sub)
                    self._merge_meta_info_files(new_data_dirs, self.output_dir_task, sub)

            logging.info(f"åˆå¹¶å®Œæˆï¼ç»“æœä¿å­˜è‡³: {self.output_dir_task}")
            self.fix_parquet_index_inplace(self.output_dir_task)
            return True
        except Exception as e:
            logging.error(f"åˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

    def fix_parquet_index_inplace(self, input_path: Path):
        """ç›´æ¥åœ¨åŸè·¯å¾„ä¸Šä¿®å¤ Parquet æ–‡ä»¶çš„ç´¢å¼•

        Args:
            input_path: è¾“å…¥ç›®å½•è·¯å¾„ï¼Œå¯ä»¥æ˜¯ str æˆ– pathlib.Path ç±»å‹
        """
        # ç»Ÿä¸€è½¬æ¢ä¸º Path å¯¹è±¡æ–¹ä¾¿å¤„ç†
        input_path = Path(input_path)

        if not input_path.exists():
            logging.error(f"âŒ é”™è¯¯ï¼šè¾“å…¥ç›®å½• '{input_path}' ä¸å­˜åœ¨")
            sys.exit(1)

        if not input_path.is_dir():
            logging.error(f"âŒ é”™è¯¯ï¼š'{input_path}' ä¸æ˜¯ä¸€ä¸ªç›®å½•")
            sys.exit(1)

        if not any(input_path.iterdir()):
            logging.error(f"âŒ é”™è¯¯ï¼šè¾“å…¥ç›®å½• '{input_path}' ä¸ºç©º")
            sys.exit(1)

        # æŸ¥æ‰¾æ‰€æœ‰çš„ Parquet æ–‡ä»¶
        parquet_files = []
        for file in input_path.rglob('*.parquet'):  # ä½¿ç”¨ rglob æ›¿ä»£ os.walk
            parquet_files.append(file)

        if not parquet_files:
            logging.error(f"âŒ é”™è¯¯ï¼šåœ¨ '{input_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½• Parquet æ–‡ä»¶")
            sys.exit(1)

        logging.info(f"âœ… æ‰¾åˆ° {len(parquet_files)} ä¸ª Parquet æ–‡ä»¶")
        parquet_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº

        global_frame_index = 0  # å…¨å±€ frame_index ç´¯åŠ å™¨

        for input_file in parquet_files:
            try:
                # ä»æ–‡ä»¶åæå– episode_index
                filename = input_file.name
                if not (filename.startswith('episode_') and filename.endswith('.parquet')):
                    logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®: {filename}")
                    continue

                episode_num_str = filename[8:-8]  # å»æ‰ 'episode_' å’Œ '.parquet'
                try:
                    episode_index_from_filename = int(episode_num_str)
                except ValueError:
                    logging.error(f"âŒ é”™è¯¯ï¼šæ— æ³•ä»æ–‡ä»¶å '{filename}' ä¸­è§£æ episode index")
                    continue

                # è¯»å– Parquet æ–‡ä»¶
                df = pd.read_parquet(input_file)

                # æ£€æŸ¥å¿…è¦çš„åˆ—æ˜¯å¦å­˜åœ¨
                if 'episode_index' not in df.columns:
                    logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ '{input_file}' ä¸­ç¼ºå°‘ 'episode_index' åˆ—")
                    continue
                if 'index' not in df.columns:
                    logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ '{input_file}' ä¸­ç¼ºå°‘ 'index' åˆ—")
                    continue

                # ä»»åŠ¡1: ç”¨æ–‡ä»¶åä¸­çš„ episode_index è¦†ç›–æ–‡ä»¶å†…çš„ episode_index å­—æ®µ
                original_episode_index = df['episode_index'].iloc[0] if len(df) > 0 else 0
                df['episode_index'] = episode_index_from_filename

                # ä»»åŠ¡2: ç´¯åŠ  frame_index å¹¶è¦†ç›– index å­—æ®µ
                num_frames = len(df)
                df['index'] = range(global_frame_index, global_frame_index + num_frames)
                global_frame_index += num_frames

                # å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼ŒéªŒè¯æ— è¯¯åæ›¿æ¢åŸæ–‡ä»¶
                temp_file = input_file.with_suffix('.parquet.tmp')
                df.to_parquet(temp_file, index=False)

                # æ›¿æ¢åŸæ–‡ä»¶ï¼ˆåŸå­æ“ä½œï¼Œé¿å…æŸåï¼‰
                temp_file.replace(input_file)

                logging.info(f"âœ… å¤„ç†å®Œæˆ: {input_file}")
                logging.info(
                    f"   åŸ episode_index: {original_episode_index} â†’ æ–° episode_index: {episode_index_from_filename}")
                logging.info(
                    f"   å¸§æ•°: {num_frames}, ç´¯è®¡ frame_index: {global_frame_index - num_frames} åˆ° {global_frame_index - 1}")

            except Exception as e:
                logging.error(f"âŒ å¤„ç†æ–‡ä»¶ '{input_file}' æ—¶å‡ºé”™: {e}", exc_info=True)
                temp_file = input_file.with_suffix('.parquet.tmp')
                if temp_file.exists():
                    temp_file.unlink()  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                continue

        logging.info(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆ!")
        logging.info(f"   æ€»å¤„ç†æ–‡ä»¶æ•°: {len(parquet_files)}")
        logging.info(f"   æ€»å¸§æ•°: {global_frame_index}")