import os

import time
from datetime import datetime
from ..utils.serializer import dumps, loads, dumps_str, loads_str
import signal

import asyncio
import logging
import contextlib
import importlib
import time 

from typing import List


# å¯¼å…¥TaskMessage
from .message import TaskMessage
from .task import Task
from .enums import TaskStatus
from jettask.messaging.event_pool import EventPool
from ..executor.orchestrator import ProcessOrchestrator
from ..utils import gen_task_name
from ..scheduler import TaskScheduler
from ..exceptions import TaskTimeoutError, TaskExecutionError, TaskNotFoundError
# å¯¼å…¥ç»Ÿä¸€çš„æ•°æ®åº“è¿æ¥ç®¡ç†
from jettask.db.connector import get_sync_redis_client, get_async_redis_client

# å°è¯•å¯¼å…¥ uvloop,å¦‚æœå¯ç”¨åˆ™ä½¿ç”¨å®ƒä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½
try:
    import uvloop
    asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
    UVLOOP_AVAILABLE = True
except ImportError:
    UVLOOP_AVAILABLE = False

logger = logging.getLogger('app')

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

if UVLOOP_AVAILABLE:
    logger.debug("Using uvloop for better performance")
else:
    logger.debug("uvloop not available, using default asyncio event loop")


class Jettask(object):
    # Luaè„šæœ¬ä»configæ¨¡å—å¯¼å…¥ï¼Œç»Ÿä¸€ç®¡ç†

    def __init__(self, redis_url: str = None, include: list = None, max_connections: int = None,
                 consumer_config: dict = None, tasks=None,
                 redis_prefix: str = None, scheduler_config: dict = None, pg_url: str = None,
                 task_center=None, 
                 heartbeat_interval: float = 5.0, heartbeat_timeout: float = 15.0,
                 scanner_interval: float = 2) -> None:
        self._tasks = tasks or {}
        self._task_queues = {}  # è®°å½•ä»»åŠ¡åç§°åˆ°é˜Ÿåˆ—çš„æ˜ å°„ (task_name -> queue_name)
        self.asyncio = False
        self.include = include or []

        # ä»»åŠ¡ä¸­å¿ƒç›¸å…³å±æ€§
        self.task_center = None  # å°†é€šè¿‡mount_task_centeræ–¹æ³•æŒ‚è½½æˆ–åˆå§‹åŒ–æ—¶æŒ‡å®š
        self._task_center_config = None
        self._original_redis_url = redis_url
        self._original_pg_url = pg_url

        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡
        self.redis_url = redis_url or os.environ.get('JETTASK_REDIS_URL')
        self.pg_url = pg_url or os.environ.get('JETTASK_PG_URL')
        self.max_connections = max_connections if max_connections is not None else int(os.environ.get('JETTASK_MAX_CONNECTIONS', '500'))
        self.redis_prefix = redis_prefix or os.environ.get('JETTASK_REDIS_PREFIX', 'jettask')

        # æ£€æŸ¥å¿…éœ€å‚æ•°ï¼šredis_url
        if not self.redis_url:
            raise ValueError(
                "å¿…é¡»æä¾› redis_url å‚æ•°ï¼\n\n"
                "è¯·é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ–¹å¼é…ç½®:\n"
                "  1. åˆå§‹åŒ–æ—¶ä¼ å‚:\n"
                "     app = Jettask(redis_url='redis://localhost:6379/0')\n\n"
                "  2. è®¾ç½®ç¯å¢ƒå˜é‡:\n"
                "     export JETTASK_REDIS_URL='redis://localhost:6379/0'\n\n"
                "  3. åœ¨ .env æ–‡ä»¶ä¸­é…ç½®:\n"
                "     JETTASK_REDIS_URL=redis://localhost:6379/0\n"
            )

        self.consumer_config = consumer_config or {}
        self.scheduler_config = scheduler_config or {}

        # å¿ƒè·³é…ç½®
        # æ ¡éªŒï¼šheartbeat_timeout å¿…é¡»å¤§äº heartbeat_interval çš„ 2 å€
        # å¦åˆ™ä¼šå¯¼è‡´ worker åœ¨æ­£å¸¸å¿ƒè·³æœŸé—´å°±è¢«åˆ¤å®šä¸ºè¶…æ—¶
        if heartbeat_timeout <= heartbeat_interval * 2:
            raise ValueError(
                f"heartbeat_timeout ({heartbeat_timeout}s) å¿…é¡»å¤§äº heartbeat_interval ({heartbeat_interval}s) çš„ 2 å€ï¼\n\n"
                f"å½“å‰é…ç½®ä¼šå¯¼è‡´ worker åœ¨æ­£å¸¸å¿ƒè·³é—´éš”å†…å°±è¢«åˆ¤å®šä¸ºè¶…æ—¶ã€‚\n"
                f"å»ºè®®é…ç½®ï¼šheartbeat_timeout >= heartbeat_interval * 3\n"
                f"ä¾‹å¦‚ï¼šheartbeat_interval=5.0, heartbeat_timeout=15.0"
            )
        self.heartbeat_interval = heartbeat_interval  # å¿ƒè·³å‘é€é—´éš”
        self.heartbeat_timeout = heartbeat_timeout    # å¿ƒè·³è¶…æ—¶æ—¶é—´ï¼ˆå¿…é¡» > interval * 2ï¼‰
        self.scanner_interval = scanner_interval      # Scanner æ‰«æé—´éš”

        # å¦‚æœåˆå§‹åŒ–æ—¶æä¾›äº†task_centerï¼Œç›´æ¥æŒ‚è½½
        if task_center:
            self.mount_task_center(task_center)

        # Update prefixes with the configured prefix using colon namespace
        self.STATUS_PREFIX = f"{self.redis_prefix}:STATUS:"
        self.RESULT_PREFIX = f"{self.redis_prefix}:RESULT:"
        
        # é¢„ç¼–è¯‘å¸¸ç”¨æ“ä½œï¼Œå‡å°‘è¿è¡Œæ—¶å¼€é”€
        self._loads = loads
        self._dumps = dumps
        
        # è°ƒåº¦å™¨ç›¸å…³
        self.scheduler = None
        self._scheduler_db_url = None

        self._status_prefix = self.STATUS_PREFIX
        self._result_prefix = self.RESULT_PREFIX

        # Worker çŠ¶æ€ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.worker_state_manager = None

        # Worker çŠ¶æ€æŸ¥è¯¢å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ– - éœ€è¦ Redis å®¢æˆ·ç«¯ï¼‰
        self._worker_state = None


        # åˆå§‹åŒ–æ¸…ç†çŠ¶æ€ï¼Œä½†ä¸æ³¨å†Œå¤„ç†å™¨
        self._cleanup_done = False
        self._should_exit = False
        self._worker_started = False
        self._handlers_registered = False

        # é˜Ÿåˆ—æ³¨å†Œè¡¨ï¼ˆæ‡’åŠ è½½ï¼‰
        self._registry = None
        self._registry_initialized = False

        # å­˜å‚¨å¾…æ³¨å†Œçš„é™æµé…ç½®ï¼Œå»¶è¿Ÿåˆ°start()æ–¹æ³•ä¸­æ³¨å†Œ
        self._pending_rate_limits = {}
   
    
    def _load_config_from_task_center(self):
        """ä»ä»»åŠ¡ä¸­å¿ƒåŠ è½½é…ç½®"""
        try:
            import asyncio
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­
            try:
                loop = asyncio.get_running_loop()
                # å·²åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œæ— æ³•åŒæ­¥åŠ è½½
                return False
            except RuntimeError:
                # ä¸åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œå¯ä»¥åˆ›å»ºæ–°çš„
                loop = asyncio.new_event_loop()
                if self.task_center:
                    # å¦‚æœå·²ç»åˆå§‹åŒ–ï¼Œç›´æ¥è·å–é…ç½®
                    if self.task_center._initialized:
                        config = self.task_center._config
                    else:
                        # ä½¿ç”¨å¼‚æ­¥æ¨¡å¼è¿æ¥
                        success = loop.run_until_complete(self.task_center.connect(asyncio=True))
                        if success:
                            config = self.task_center._config
                        else:
                            config = None
                else:
                    config = None
                loop.close()
            
            if config:
                # ä»»åŠ¡ä¸­å¿ƒé…ç½®ä¼˜å…ˆçº§é«˜äºæ‰‹åŠ¨é…ç½®
                redis_config = config.get('redis_config', {})
                pg_config = config.get('pg_config', {})
                # æ„å»ºRedis URL
                if redis_config:
                    redis_host = redis_config.get('host', 'localhost')
                    redis_port = redis_config.get('port', 6379)
                    redis_password = redis_config.get('password')
                    redis_db = redis_config.get('db', 0)
                    
                    if redis_password:
                        self.redis_url = f"redis://:{redis_password}@{redis_host}:{redis_port}/{redis_db}"
                    else:
                        self.redis_url = f"redis://{redis_host}:{redis_port}/{redis_db}"
                    
                    logger.debug(f"ä»ä»»åŠ¡ä¸­å¿ƒåŠ è½½Redisé…ç½®: {redis_host}:{redis_port}/{redis_db}")
                
                # æ„å»ºPostgreSQL URL
                if pg_config:
                    pg_host = pg_config.get('host', 'localhost')
                    pg_port = pg_config.get('port', 5432)
                    pg_user = pg_config.get('user', 'postgres')
                    pg_password = pg_config.get('password', '')
                    pg_database = pg_config.get('database', 'jettask')
                    
                    self.pg_url = f"postgresql://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_database}"
                    logger.debug(f"ä»ä»»åŠ¡ä¸­å¿ƒåŠ è½½PostgreSQLé…ç½®: {pg_host}:{pg_port}/{pg_database}")
                
                # ä¿å­˜é…ç½®ä¾›åç»­ä½¿ç”¨
                self._task_center_config = config
                
                # æ›´æ–°Rediså‰ç¼€ä¸ºå‘½åç©ºé—´åç§°
                if self.task_center and self.task_center.redis_prefix != "jettask":
                    self.redis_prefix = self.task_center.redis_prefix
                    # æ›´æ–°ç›¸å…³å‰ç¼€
                    self.STATUS_PREFIX = f"{self.redis_prefix}:STATUS:"
                    self.RESULT_PREFIX = f"{self.redis_prefix}:RESULT:"
                
                # æ¸…ç†å·²ç¼“å­˜çš„Redisè¿æ¥ï¼Œå¼ºåˆ¶é‡æ–°åˆ›å»º
                if hasattr(self, '_redis'):
                    delattr(self, '_redis')
                if hasattr(self, '_async_redis'):
                    delattr(self, '_async_redis')
                if hasattr(self, '_ep'):
                    delattr(self, '_ep')
                
        except Exception as e:
            import traceback
            traceback.print_exc()
            logger.warning(f"ä»ä»»åŠ¡ä¸­å¿ƒåŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨æ‰‹åŠ¨é…ç½®: {e}")
            # æ¢å¤åŸå§‹é…ç½®
            self.redis_url = self._original_redis_url
            self.pg_url = self._original_pg_url
    
    def mount_task_center(self, task_center):
        """
        æŒ‚è½½ä»»åŠ¡ä¸­å¿ƒåˆ°Jettaskåº”ç”¨
        
        å¦‚æœtask_centerå·²ç»è¿æ¥ï¼Œä¼šè‡ªåŠ¨åº”ç”¨é…ç½®åˆ°å½“å‰appã€‚
        
        Args:
            task_center: TaskCenterå®ä¾‹
            
        ä½¿ç”¨ç¤ºä¾‹ï¼š
            from jettask.core.center_client import TaskCenterClient

            # åˆ›å»ºä»»åŠ¡ä¸­å¿ƒå®¢æˆ·ç«¯ï¼ˆå¯å¤ç”¨ï¼‰
            task_center = TaskCenterClient("http://localhost:8001")
            await task_center.connect()  # åªéœ€è¿æ¥ä¸€æ¬¡
            
            # åˆ›å»ºå¤šä¸ªappå®ä¾‹ï¼Œå…±äº«åŒä¸€ä¸ªtask_center
            app1 = Jettask()
            app1.mount_task_center(task_center)  # è‡ªåŠ¨åº”ç”¨é…ç½®
            
            app2 = Jettask()
            app2.mount_task_center(task_center)  # å¤ç”¨é…ç½®
        """
        self.task_center = task_center
        
        # å¦‚æœä»»åŠ¡ä¸­å¿ƒå·²è¿æ¥ï¼Œç«‹å³åº”ç”¨æ‰€æœ‰é…ç½®
        if task_center and task_center._initialized:
            # åº”ç”¨Redisé…ç½®
            if task_center.redis_config:
                redis_url = task_center.get_redis_url()
                if redis_url:
                    self.redis_url = redis_url
                    
            # åº”ç”¨PostgreSQLé…ç½®
            if task_center.pg_config:
                pg_url = task_center.get_pg_url()
                if pg_url:
                    self.pg_url = pg_url
            
            # æ›´æ–°Rediså‰ç¼€
            self.redis_prefix = task_center.redis_prefix
            # æ›´æ–°ç›¸å…³å‰ç¼€
            self.STATUS_PREFIX = f"{self.redis_prefix}:STATUS:"
            self.RESULT_PREFIX = f"{self.redis_prefix}:RESULT:"
            self.QUEUE_PREFIX = f"{self.redis_prefix}:QUEUE:"
            self.STREAM_PREFIX = f"{self.redis_prefix}:STREAM:"
            self.TASK_PREFIX = f"{self.redis_prefix}:TASK:"
            self.SCHEDULER_PREFIX = f"{self.redis_prefix}:SCHEDULED:"
            self.LOCK_PREFIX = f"{self.redis_prefix}:LOCK:"
            
            # æ ‡è®°é…ç½®å·²åŠ è½½
            self._task_center_config = {
                'redis_config': task_center.redis_config,
                'pg_config': task_center.pg_config,
                'namespace_name': task_center.namespace_name,
                'version': task_center.version
            }
    
    
    def _setup_cleanup_handlers(self):
        """è®¾ç½®æ¸…ç†å¤„ç†å™¨"""
        # é¿å…é‡å¤æ³¨å†Œ
        if self._handlers_registered:
            return
        
        self._handlers_registered = True
        
        def signal_cleanup_handler(signum=None, frame=None):
            """ä¿¡å·å¤„ç†å™¨"""
            if self._cleanup_done:
                return
            # åªæœ‰å¯åŠ¨è¿‡workeræ‰éœ€è¦æ‰“å°æ¸…ç†ä¿¡æ¯
            if self._worker_started:
                logger.debug("Received shutdown signal, cleaning up...")
            self.cleanup()
            if signum:
                # è®¾ç½®æ ‡è®°è¡¨ç¤ºéœ€è¦é€€å‡º
                self._should_exit = True
                # å¯¹äºå¤šè¿›ç¨‹ç¯å¢ƒï¼Œä¸ç›´æ¥æ“ä½œäº‹ä»¶å¾ªç¯
                # è®©æ‰§è¡Œå™¨è‡ªå·±æ£€æµ‹é€€å‡ºæ ‡å¿—å¹¶ä¼˜é›…å…³é—­
        
        def atexit_cleanup_handler():
            """atexitå¤„ç†å™¨"""
            if self._cleanup_done:
                return
            # atexitæ—¶ä¸é‡å¤æ‰“å°æ—¥å¿—ï¼Œé™é»˜æ¸…ç†
            self.cleanup()
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        if hasattr(signal, 'SIGTERM'):
            signal.signal(signal.SIGTERM, signal_cleanup_handler)
        if hasattr(signal, 'SIGINT'):
            signal.signal(signal.SIGINT, signal_cleanup_handler)
        
        # æ³¨å†Œatexitå¤„ç†å™¨
        import atexit
        atexit.register(atexit_cleanup_handler)
    
    def cleanup(self):
        """æ¸…ç†åº”ç”¨èµ„æº"""
        if self._cleanup_done:
            return
        self._cleanup_done = True
        
        # åªæœ‰çœŸæ­£å¯åŠ¨è¿‡workeræ‰æ‰“å°æ—¥å¿—
        if self._worker_started:
   
            
            # æ¸…ç†EventPool
            if hasattr(self, 'ep') and self.ep:
                self.ep.cleanup()
            

        else:
            # å¦‚æœåªæ˜¯å®ä¾‹åŒ–ä½†æ²¡æœ‰å¯åŠ¨ï¼Œé™é»˜æ¸…ç†
            if hasattr(self, 'ep') and self.ep:
                self.ep.cleanup()


    @property
    def async_redis(self):
        """è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯ï¼ˆå…¨å±€å•ä¾‹ï¼‰"""
        # å¦‚æœé…ç½®äº†ä»»åŠ¡ä¸­å¿ƒä¸”è¿˜æœªåŠ è½½é…ç½®ï¼Œå…ˆåŠ è½½é…ç½®
        if self.task_center and self.task_center.is_enabled and not self._task_center_config:
            self._load_config_from_task_center()

        # logger.debug(f"Creating async_redis client with socket_timeout=None for redis_url={self.redis_url}")
        return get_async_redis_client(
            redis_url=self.redis_url,
            decode_responses=True,
            max_connections=self.max_connections,
            socket_timeout=None  # æ— é™ç­‰å¾…ï¼Œä¸è¶…æ—¶
        )

    @property
    def redis(self):
        """è·å–åŒæ­¥Rediså®¢æˆ·ç«¯ï¼ˆå…¨å±€å•ä¾‹ï¼‰"""
        # å¦‚æœé…ç½®äº†ä»»åŠ¡ä¸­å¿ƒä¸”è¿˜æœªåŠ è½½é…ç½®ï¼Œå…ˆåŠ è½½é…ç½®
        # if self.task_center and self.task_center.is_enabled and not self._task_center_config:
        #     self._load_config_from_task_center()

        return get_sync_redis_client(
            redis_url=self.redis_url,
            decode_responses=True,
            max_connections=self.max_connections
        )

    @property
    def binary_redis(self):
        """è·å–åŒæ­¥äºŒè¿›åˆ¶Rediså®¢æˆ·ç«¯ï¼ˆä¸è‡ªåŠ¨è§£ç ï¼Œç”¨äºè·å–msgpackæ•°æ®ï¼‰"""
        return get_sync_redis_client(
            redis_url=self.redis_url,
            decode_responses=False,
            max_connections=self.max_connections
        )

    @property
    def async_binary_redis(self):
        """è·å–å¼‚æ­¥äºŒè¿›åˆ¶Rediså®¢æˆ·ç«¯ï¼ˆä¸è‡ªåŠ¨è§£ç ï¼Œç”¨äºè·å–msgpackæ•°æ®ï¼‰"""
        return get_async_redis_client(
            redis_url=self.redis_url,
            decode_responses=False,
            max_connections=self.max_connections,
            socket_timeout=None
        )

    @property
    def worker_state(self):
        """
        è·å– WorkerState å®ä¾‹ï¼ˆå•ä¾‹ï¼Œå»¶è¿Ÿåˆå§‹åŒ–ï¼‰

        WorkerManager è´Ÿè´£ Worker çŠ¶æ€çš„æŸ¥è¯¢å’Œç®¡ç†
        """
        if self._worker_state is None:
            from jettask.worker.lifecycle import WorkerManager
            self._worker_state = WorkerManager(
                redis_client=self.redis,
                async_redis_client=self.async_redis,
                redis_prefix=self.redis_prefix
            )
            logger.debug("Initialized WorkerManager for app")
        return self._worker_state

    @property
    def ep(self):
        name = "_ep"
        if hasattr(self, name):
            ep = getattr(self, name)
        else:
            # ä¼ é€’redis_prefixåˆ°consumer_config
            consumer_config = self.consumer_config.copy() if self.consumer_config else {}
            consumer_config['redis_prefix'] = self.redis_prefix

            # åˆ›å»ºå¿…éœ€çš„ä¾èµ–
            # æ³¨æ„ï¼šä¸ä¼ å…¥ async_redis_clientï¼Œè®© EventPool è‡ªå·±æ‡’åŠ è½½åˆ›å»º
            # é¿å…åœ¨å­çº¿ç¨‹ä¸­è§¦å‘å¼‚æ­¥å®¢æˆ·ç«¯çš„åˆ›å»º

            # å¯¹äºåªç”¨äºå‘é€æ¶ˆæ¯çš„ EventPoolï¼Œä¸éœ€è¦çœŸæ­£çš„ task_event_queues
            # ä½†ä¸ºäº†æ»¡è¶³æ¥å£è¦æ±‚ï¼Œä¼ é€’ç©ºå­—å…¸
            task_event_queues = {}

            ep = EventPool(
                redis_client=self.redis,
                async_redis_client=None,  # æ‡’åŠ è½½ï¼Œé¿å…å­çº¿ç¨‹åˆ›å»º
                task_event_queues=task_event_queues,
                tasks=self._tasks,
                queue_registry=None,  # ä½¿ç”¨æ‡’åŠ è½½çš„ registry å±æ€§
                offline_recovery=self.worker_state_manager,  # offline_recovery - ä½¿ç”¨ App çš„ WorkerManager
                redis_url=self.redis_url,
                consumer_config=consumer_config,
                redis_prefix=self.redis_prefix,
                app=self
            )
            setattr(self, name, ep)
        return ep

    @property
    def registry(self):
        """è·å– QueueRegistry å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._registry_initialized and self._registry:
            return self._registry

        # æ‡’åŠ è½½åˆ›å»º QueueRegistry
        from ..messaging.registry import QueueRegistry

        self._registry = QueueRegistry(
            redis_client=self.redis,  # ä¼ å…¥åŒæ­¥å®¢æˆ·ç«¯
            async_redis_client=self.async_redis,  # ä¼ å…¥å¼‚æ­¥å®¢æˆ·ç«¯
            redis_prefix=self.redis_prefix
        )

        self._registry_initialized = True
        return self._registry

    def clear(self):
        if hasattr(self, "process"):
            delattr(self, "process")
        if hasattr(self, "_ep"):
            delattr(self, "_ep")
        if hasattr(self, "_registry"):
            self._registry = None
            self._registry_initialized = False

    def get_task_by_name(self, name: str) -> Task:
        # 1. ç›´æ¥æŸ¥æ‰¾å®Œæ•´åç§°
        task = self._tasks.get(name)
        if task:
            return task
        
        # 2. å¦‚æœæ˜¯ç®€å•åç§°ï¼ˆä¸å«.ï¼‰ï¼Œå°è¯•åŒ¹é…æ‰€æœ‰ä»¥è¯¥åç§°ç»“å°¾çš„ä»»åŠ¡
        if '.' not in name:
            for task_key, task_obj in self._tasks.items():
                # åŒ¹é… "module.function_name" å½¢å¼ï¼Œæå–å‡½æ•°åéƒ¨åˆ†
                if '.' in task_key:
                    _, func_name = task_key.rsplit('.', 1)
                    if func_name == name:
                        return task_obj
                elif task_key == name:
                    # å®Œå…¨åŒ¹é…ï¼ˆå¯èƒ½æ²¡æœ‰æ¨¡å—å‰ç¼€ï¼‰
                    return task_obj
        
        return None

    def get_task_config(self, task_name: str) -> dict:
        """
        è·å–ä»»åŠ¡é…ç½®

        Args:
            task_name: ä»»åŠ¡åç§°

        Returns:
            ä»»åŠ¡é…ç½®å­—å…¸ï¼Œå¦‚æœä»»åŠ¡ä¸å­˜åœ¨åˆ™è¿”å›None
        """
        # è·å–ä»»åŠ¡å¯¹è±¡
        task = self.get_task_by_name(task_name)
        if not task:
            return None

        # è¿”å›ä»»åŠ¡çš„é…ç½®ï¼ˆä»Taskå¯¹è±¡çš„å±æ€§ä¸­æå–ï¼‰
        return {
            'auto_ack': getattr(task, 'auto_ack', True),
            'queue': getattr(task, 'queue', None),
            'timeout': getattr(task, 'timeout', None),
            'max_retries': getattr(task, 'max_retries', 0),
            'retry_delay': getattr(task, 'retry_delay', None),
        }

    def include_module(self, modules: list):
        self.include += modules

    def _task_from_fun(
        self, fun, name=None, base=None, queue=None, bind=False, retry_config=None, rate_limit=None, auto_ack=True, **options
    ) -> Task:
        name = name or gen_task_name(fun.__name__, fun.__module__)
        base = base or Task

        # ä¸å†é™åˆ¶é˜Ÿåˆ—æ¨¡å¼ï¼Œå› ä¸ºæ¯ä¸ªtaskéƒ½æœ‰ç‹¬ç«‹çš„consumer group

        if name not in self._tasks:
            run = staticmethod(fun)
            task: Task = type(
                fun.__name__,
                (base,),
                dict(
                    {
                        "app": self,
                        "name": name,
                        "run": run,
                        "queue": queue,
                        "retry_config": retry_config,  # å­˜å‚¨é‡è¯•é…ç½®
                        "rate_limit": rate_limit,  # å­˜å‚¨é™æµé…ç½®
                        "auto_ack": auto_ack,  # å­˜å‚¨è‡ªåŠ¨ACKé…ç½®
                        "_decorated": True,
                        "__doc__": fun.__doc__,
                        "__module__": fun.__module__,
                        "__annotations__": fun.__annotations__,
                        "__wrapped__": run,
                    },
                    **options,
                ),
            )()
            task.bind_app(self)
            with contextlib.suppress(AttributeError):
                task.__qualname__ = fun.__qualname__
            self._tasks[task.name] = task

            # è®°å½•ä»»åŠ¡åˆ°é˜Ÿåˆ—çš„æ˜ å°„ (task -> queue)
            if queue:
                self._task_queues[name] = queue

            # å¦‚æœä»»åŠ¡é…ç½®äº†é™æµï¼Œæ³¨å†Œåˆ°Redisï¼›å¦åˆ™åˆ é™¤æ—§é…ç½®
            if rate_limit:
                # æ”¯æŒ int (QPS) å’Œ ConcurrencyLimit/QPSLimit å¯¹è±¡
                if isinstance(rate_limit, int) and rate_limit > 0:
                    # ç®€å•çš„ int å€¼ä½œä¸º QPS é™åˆ¶
                    self._register_rate_limit(name, rate_limit)
                elif hasattr(rate_limit, 'to_dict'):
                    # RateLimitConfig å¯¹è±¡ï¼ˆConcurrencyLimit æˆ– QPSLimitï¼‰
                    self._register_rate_limit_config(name, rate_limit)
            else:
                # æ²¡æœ‰é™æµé…ç½®ï¼Œåˆ é™¤ Redis ä¸­çš„æ—§é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                from jettask.utils.rate_limit.limiter import RateLimiterManager

                RateLimiterManager.unregister_rate_limit_config(
                    redis_client=self.redis,
                    task_name=name,
                    redis_prefix=self.redis_prefix
                )
        else:
            task = self._tasks[name]
        return task

    def _register_rate_limit(self, task_name: str, qps_limit: int):
        """å­˜å‚¨ä»»åŠ¡çš„ QPS é™æµè§„åˆ™ï¼Œå»¶è¿Ÿåˆ° start() æ—¶æ³¨å†Œåˆ° Redis"""
        from jettask.utils.rate_limit.config import QPSLimit

        # è½¬æ¢ä¸º QPSLimit é…ç½®å¯¹è±¡å¹¶å­˜å‚¨
        config = QPSLimit(qps=qps_limit)
        self._pending_rate_limits[task_name] = config
        logger.debug(f"å­˜å‚¨ä»»åŠ¡ {task_name} çš„ QPS é™æµé…ç½®ï¼Œå°†åœ¨ start() æ—¶æ³¨å†Œ")

    def _register_rate_limit_config(self, task_name: str, config):
        """å­˜å‚¨ä»»åŠ¡çš„é™æµé…ç½®å¯¹è±¡ï¼Œå»¶è¿Ÿåˆ° start() æ—¶æ³¨å†Œåˆ° Redis

        Args:
            task_name: ä»»åŠ¡åç§°
            config: RateLimitConfig å¯¹è±¡ï¼ˆConcurrencyLimit æˆ– QPSLimitï¼‰
        """
        # å­˜å‚¨é…ç½®ï¼Œå»¶è¿Ÿæ³¨å†Œ
        self._pending_rate_limits[task_name] = config
        logger.debug(f"å­˜å‚¨ä»»åŠ¡ {task_name} çš„é™æµé…ç½®ï¼Œå°†åœ¨ start() æ—¶æ³¨å†Œ")

    def _apply_pending_rate_limits(self):
        """å°†æ‰€æœ‰å¾…æ³¨å†Œçš„é™æµé…ç½®æ³¨å†Œåˆ° Redisï¼ˆåœ¨ start() æ—¶è°ƒç”¨ï¼‰"""
        if not self._pending_rate_limits:
            return

        from jettask.utils.rate_limit.limiter import RateLimiterManager

        for task_name, config in self._pending_rate_limits.items():
            try:
                RateLimiterManager.register_rate_limit_config(
                    redis_client=self.redis,
                    task_name=task_name,
                    config=config,
                    redis_prefix=self.redis_prefix
                )
                logger.debug(f"å·²æ³¨å†Œä»»åŠ¡ {task_name} çš„é™æµé…ç½®")
            except Exception as e:
                logger.error(f"æ³¨å†Œä»»åŠ¡ {task_name} çš„é™æµé…ç½®å¤±è´¥: {e}")

        logger.debug(f"å·²æ³¨å†Œ {len(self._pending_rate_limits)} ä¸ªä»»åŠ¡çš„é™æµé…ç½®")

    def _get_queues_from_tasks(self, task_names: List[str]) -> List[str]:
        """ä»ä»»åŠ¡åç§°åˆ—è¡¨è·å–å¯¹åº”çš„é˜Ÿåˆ—åˆ—è¡¨ï¼ˆä½¿ç”¨ task -> queue æ˜ å°„ï¼‰

        Args:
            task_names: ä»»åŠ¡åç§°åˆ—è¡¨

        Returns:
            é˜Ÿåˆ—åç§°åˆ—è¡¨ï¼ˆå»é‡ï¼Œä¿æŒé¡ºåºï¼‰

        Raises:
            ValueError: å¦‚æœæŸä¸ªä»»åŠ¡ä¸å­˜åœ¨æˆ–æ²¡æœ‰æŒ‡å®šé˜Ÿåˆ—
        """
        queues = []
        for task_name in task_names:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
            if task_name not in self._task_queues:
                # æä¾›æ›´å‹å¥½çš„é”™è¯¯ä¿¡æ¯
                available_tasks = list(self._task_queues.keys())
                raise ValueError(
                    f"ä»»åŠ¡ '{task_name}' ä¸å­˜åœ¨æˆ–æœªé…ç½®é˜Ÿåˆ—ã€‚\n"
                    f"å·²æ³¨å†Œçš„ä»»åŠ¡: {', '.join(available_tasks) if available_tasks else '(æ— )'}"
                )

            # ä»æ˜ å°„ä¸­è·å–é˜Ÿåˆ—
            queue = self._task_queues[task_name]

            # å»é‡
            if queue not in queues:
                queues.append(queue)

        logger.debug(f"ä» {len(task_names)} ä¸ªä»»åŠ¡è·å–åˆ° {len(queues)} ä¸ªé˜Ÿåˆ—: {queues}")
        return queues

    def get_tasks_by_queue(self, queue_name: str) -> List[str]:
        """ä»é˜Ÿåˆ—åç§°è·å–å…³è”çš„ä»»åŠ¡åˆ—è¡¨ï¼ˆåå‘æŸ¥è¯¢ï¼‰

        Args:
            queue_name: é˜Ÿåˆ—åç§°

        Returns:
            ä»»åŠ¡åç§°åˆ—è¡¨
        """
        return [task_name for task_name, queue in self._task_queues.items() if queue == queue_name]

    def task(
        self,
        name: str = None,
        queue: str = None,
        base: Task = None,
        # é‡è¯•ç›¸å…³å‚æ•°
        max_retries: int = 0,
        retry_backoff: bool = True,  # æ˜¯å¦ä½¿ç”¨æŒ‡æ•°é€€é¿
        retry_backoff_max: float = 60,  # æœ€å¤§é€€é¿æ—¶é—´ï¼ˆç§’ï¼‰
        retry_on_exceptions: tuple = None,  # å¯é‡è¯•çš„å¼‚å¸¸ç±»å‹
        # é™æµç›¸å…³å‚æ•°
        rate_limit: int = None,  # QPS é™åˆ¶ï¼ˆæ¯ç§’å…è®¸æ‰§è¡Œçš„ä»»åŠ¡æ•°ï¼‰
        # ACKç›¸å…³å‚æ•°
        auto_ack: bool = True,  # æ˜¯å¦è‡ªåŠ¨ACKï¼ˆé»˜è®¤Trueï¼‰
        *args,
        **kwargs,
    ):
        """
        ä»»åŠ¡è£…é¥°å™¨ - ç»Ÿä¸€ä½¿ç”¨ TaskRouter å†…éƒ¨å®ç°

        Args:
            name: ä»»åŠ¡åç§°
            queue: é˜Ÿåˆ—åç§°
            base: åŸºç±»
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_backoff: æ˜¯å¦ä½¿ç”¨æŒ‡æ•°é€€é¿
            retry_backoff_max: æœ€å¤§é€€é¿æ—¶é—´
            retry_on_exceptions: å¯é‡è¯•çš„å¼‚å¸¸ç±»å‹
            rate_limit: é™æµé…ç½®ï¼ˆQPSæˆ–RateLimitConfigå¯¹è±¡ï¼‰
            auto_ack: æ˜¯å¦è‡ªåŠ¨ACKï¼ˆé»˜è®¤Trueï¼‰
        """
        def _create_task_cls(fun):
            # å°†é‡è¯•é…ç½®ä¼ é€’ç»™_task_from_fun
            retry_config = None
            if max_retries > 0:
                retry_config = {
                    'max_retries': max_retries,
                    'retry_backoff': retry_backoff,
                    'retry_backoff_max': retry_backoff_max,
                }
                # å°†å¼‚å¸¸ç±»è½¬æ¢ä¸ºç±»åå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿åºåˆ—åŒ–
                if retry_on_exceptions:
                    retry_config['retry_on_exceptions'] = [
                        exc if isinstance(exc, str) else exc.__name__
                        for exc in retry_on_exceptions
                    ]

            # ç»Ÿä¸€é€šè¿‡ _task_from_fun åˆ›å»ºä»»åŠ¡ï¼ŒåŒ…å« auto_ack å‚æ•°
            return self._task_from_fun(
                fun,
                name,
                base,
                queue,
                retry_config=retry_config,
                rate_limit=rate_limit,
                auto_ack=auto_ack,  # ä¼ é€’ auto_ack
                *args,
                **kwargs
            )

        return _create_task_cls
    
    def include_router(self, router, prefix: str = None):
        """
        åŒ…å«ä¸€ä¸ªTaskRouterï¼Œå°†å…¶æ‰€æœ‰ä»»åŠ¡æ³¨å†Œåˆ°appä¸­
        
        Args:
            router: TaskRouterå®ä¾‹
            prefix: é¢å¤–çš„å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        """
        from ..task.router import TaskRouter
        
        if not isinstance(router, TaskRouter):
            raise TypeError(f"Expected TaskRouter, got {type(router)}")
        
        # è·å–routerä¸­çš„æ‰€æœ‰ä»»åŠ¡
        tasks = router.get_tasks()
        
        for task_name, task_config in tasks.items():
            # å¤åˆ¶é…ç½®ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            config = task_config.copy()
            
            # å¦‚æœæŒ‡å®šäº†é¢å¤–å‰ç¼€ï¼Œæ·»åŠ åˆ°ä»»åŠ¡åå‰é¢
            if prefix:
                if config.get('name'):
                    config['name'] = f"{prefix}.{config['name']}"
                else:
                    config['name'] = f"{prefix}.{task_name}"
            
            # è·å–ä»»åŠ¡å‡½æ•°å’Œé…ç½®
            func = config.pop('func')
            name = config.pop('name', task_name)
            queue = config.pop('queue', None)
            
            # æå–é‡è¯•ç›¸å…³å‚æ•°
            retry_config = {}
            if 'max_retries' in config:
                retry_config['max_retries'] = config.pop('max_retries', 0)
            if 'retry_delay' in config:
                retry_config['retry_backoff_max'] = config.pop('retry_delay', 60)
            
            # æ³¨å†Œä»»åŠ¡åˆ°app
            self._task_from_fun(
                func,
                name=name,
                queue=queue,
                retry_config=retry_config if retry_config else None,
                **config
            )
    
    def send_tasks(self, messages: list, asyncio: bool = False):
        """
        ç»Ÿä¸€çš„ä»»åŠ¡å‘é€æ¥å£ - æ”¯æŒåŒæ­¥å’Œå¼‚æ­¥

        Args:
            messages: TaskMessageå¯¹è±¡åˆ—è¡¨ï¼ˆæˆ–å­—å…¸åˆ—è¡¨ï¼‰
            asyncio: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰

        Returns:
            åŒæ­¥æ¨¡å¼: List[str] - ä»»åŠ¡IDåˆ—è¡¨
            å¼‚æ­¥æ¨¡å¼: è¿”å›åç¨‹ï¼Œéœ€è¦ä½¿ç”¨ await

        ä½¿ç”¨ç¤ºä¾‹:
            from jettask.core.message import TaskMessage

            # åŒæ­¥å‘é€
            msg = TaskMessage(
                queue="order_processing",
                args=(12345,),
                kwargs={"customer_id": "C001", "amount": 99.99}
            )
            task_ids = app.send_tasks([msg])

            # å¼‚æ­¥å‘é€
            task_ids = await app.send_tasks([msg], asyncio=True)

            # æ‰¹é‡å‘é€
            messages = [
                TaskMessage(queue="email", kwargs={"to": "user1@example.com"}),
                TaskMessage(queue="email", kwargs={"to": "user2@example.com"}),
                TaskMessage(queue="sms", kwargs={"phone": "123456789"}),
            ]
            task_ids = app.send_tasks(messages)

            # è·¨é¡¹ç›®å‘é€ï¼ˆä¸éœ€è¦taskå®šä¹‰ï¼‰
            messages = [
                TaskMessage(queue="remote_queue", kwargs={"data": "value"})
            ]
            task_ids = await app.send_tasks(messages, asyncio=True)
        """
        if asyncio:
            return self._send_tasks_async(messages)
        else:
            return self._send_tasks_sync(messages)

    def ack(self, ack_items: list):
        """
        æ‰¹é‡ç¡®è®¤æ¶ˆæ¯ï¼ˆACKï¼‰

        ç”¨äº auto_ack=False çš„ä»»åŠ¡ï¼Œæ‰‹åŠ¨æ‰¹é‡ç¡®è®¤æ¶ˆæ¯ã€‚
        è¿™æ˜¯åŒæ­¥æ–¹æ³•ï¼Œä¼šåœ¨åå°å¼‚æ­¥æ‰§è¡ŒACKæ“ä½œã€‚

        Args:
            ack_items: ACKé¡¹åˆ—è¡¨ï¼Œæ¯é¡¹å¯ä»¥æ˜¯ï¼š
                - (queue, event_id): ç®€å•å½¢å¼
                - (queue, event_id, group_name): å¸¦æ¶ˆè´¹è€…ç»„å
                - (queue, event_id, group_name, offset): å®Œæ•´å½¢å¼
                - dict: {'queue': ..., 'event_id': ..., 'group_name': ..., 'offset': ...}

        Example:
            from jettask import TaskRouter

            router = TaskRouter()

            @router.task(queue="batch_queue", auto_ack=False)
            async def process_batch(ctx, items):
                # æ‰¹é‡å¤„ç†
                results = []
                ack_list = []

                for item in items:
                    try:
                        result = await process_item(item)
                        results.append(result)

                        # æ”¶é›†éœ€è¦ACKçš„æ¶ˆæ¯
                        ack_list.append((
                            ctx.queue,
                            item['event_id'],
                            ctx.group_name,
                            item.get('offset')
                        ))
                    except Exception as e:
                        logger.error(f"Failed to process {item}: {e}")

                # æ‰¹é‡ç¡®è®¤æˆåŠŸå¤„ç†çš„æ¶ˆæ¯
                ctx.app.ack(ack_list)

                return results
        """
        if not ack_items:
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰ executor_coreï¼ˆWorkerè¿è¡Œæ—¶æ‰æœ‰ï¼‰
        if not hasattr(self, '_executor_core') or not self._executor_core:
            logger.warning("ACK can only be called in worker context")
            return

        # å°†ACKé¡¹æ·»åŠ åˆ°executor_coreçš„pending_acks
        for item in ack_items:
            if isinstance(item, dict):
                queue = item['queue']
                event_id = item['event_id']
                group_name = item.get('group_name')
                offset = item.get('offset')
            elif isinstance(item, (tuple, list)):
                if len(item) >= 3:
                    queue, event_id, group_name = item[0], item[1], item[2]
                    offset = item[3] if len(item) > 3 else None
                elif len(item) == 2:
                    # ğŸ”§ ä¸å†æ”¯æŒåªæœ‰ (queue, event_id) çš„ç®€åŒ–å½¢å¼
                    logger.error(
                        f"Invalid ACK item format: {item}. "
                        f"group_name is required. Use (queue, event_id, group_name) or ctx.acks([event_ids])"
                    )
                    continue
                else:
                    logger.error(f"Invalid ACK item format: {item}")
                    continue
            else:
                logger.error(f"Invalid ACK item type: {type(item)}")
                continue

            # ğŸ”§ æ£€æŸ¥ group_name æ˜¯å¦æä¾›
            if not group_name:
                logger.error(
                    f"group_name is required for ACK. item={item}. "
                    f"Use ctx.acks([event_ids]) for automatic group_name injection."
                )
                continue

            # æ·»åŠ åˆ°pending_acks
            self._executor_core.pending_acks.append((queue, event_id, group_name, offset))
            logger.debug(f"Added to pending_acks: queue={queue}, event_id={event_id}, group_name={group_name}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åˆ·æ–°
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç«‹å³åˆ·æ–°
        if len(self._executor_core.pending_acks) >= 100:
            # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡åˆ·æ–°
            import asyncio
            try:
                loop = asyncio.get_running_loop()
                asyncio.create_task(self._executor_core._flush_all_buffers())
            except RuntimeError:
                # ä¸åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œç¨åä¼šè‡ªåŠ¨åˆ·æ–°
                pass

    def _send_tasks_sync(self, messages: list):
        """åŒæ­¥å‘é€ä»»åŠ¡"""
        if not messages:
            return []

        results = []

        # æŒ‰é˜Ÿåˆ—åˆ†ç»„æ¶ˆæ¯ï¼Œä»¥ä¾¿æ‰¹é‡å¤„ç†
        queue_messages = {}
        for msg in messages:
            # æ”¯æŒTaskMessageå¯¹è±¡æˆ–å­—å…¸
            if isinstance(msg, dict):
                msg = TaskMessage.from_dict(msg)
            elif not isinstance(msg, TaskMessage):
                raise ValueError(f"Invalid message type: {type(msg)}. Expected TaskMessage or dict")

            # éªŒè¯æ¶ˆæ¯
            msg.validate()

            # ç¡®å®šå®é™…çš„é˜Ÿåˆ—åï¼ˆè€ƒè™‘ä¼˜å…ˆçº§ï¼‰
            actual_queue = msg.queue
            if msg.priority is not None:
                # å°†ä¼˜å…ˆçº§æ‹¼æ¥åˆ°é˜Ÿåˆ—ååé¢
                actual_queue = f"{msg.queue}:{msg.priority}"
                # æ›´æ–°æ¶ˆæ¯ä½“ä¸­çš„queueå­—æ®µï¼Œç¡®ä¿ä¸å®é™…å‘é€çš„stream keyä¸€è‡´
                msg.queue = actual_queue

            # æŒ‰é˜Ÿåˆ—åˆ†ç»„
            if actual_queue not in queue_messages:
                queue_messages[actual_queue] = []
            queue_messages[actual_queue].append(msg)

        # å¤„ç†æ¯ä¸ªé˜Ÿåˆ—çš„æ¶ˆæ¯
        for queue, queue_msgs in queue_messages.items():
            batch_results = self._send_batch_messages_sync(queue, queue_msgs)
            results.extend(batch_results)

        return results

    async def _send_tasks_async(self, messages: list):
        """å¼‚æ­¥å‘é€ä»»åŠ¡"""
        if not messages:
            return []

        results = []

        # æŒ‰é˜Ÿåˆ—åˆ†ç»„æ¶ˆæ¯ï¼Œä»¥ä¾¿æ‰¹é‡å¤„ç†
        queue_messages = {}
        for msg in messages:
            # æ”¯æŒTaskMessageå¯¹è±¡æˆ–å­—å…¸
            if isinstance(msg, dict):
                msg = TaskMessage.from_dict(msg)
            elif not isinstance(msg, TaskMessage):
                raise ValueError(f"Invalid message type: {type(msg)}. Expected TaskMessage or dict")

            # éªŒè¯æ¶ˆæ¯
            msg.validate()

            # ç¡®å®šå®é™…çš„é˜Ÿåˆ—åï¼ˆè€ƒè™‘ä¼˜å…ˆçº§ï¼‰
            actual_queue = msg.queue
            if msg.priority is not None:
                # å°†ä¼˜å…ˆçº§æ‹¼æ¥åˆ°é˜Ÿåˆ—ååé¢
                actual_queue = f"{msg.queue}:{msg.priority}"
                # æ›´æ–°æ¶ˆæ¯ä½“ä¸­çš„queueå­—æ®µï¼Œç¡®ä¿ä¸å®é™…å‘é€çš„stream keyä¸€è‡´
                msg.queue = actual_queue

            # æŒ‰é˜Ÿåˆ—åˆ†ç»„
            if actual_queue not in queue_messages:
                queue_messages[actual_queue] = []
            queue_messages[actual_queue].append(msg)

        # å¤„ç†æ¯ä¸ªé˜Ÿåˆ—çš„æ¶ˆæ¯
        for queue, queue_msgs in queue_messages.items():
            batch_results = await self._send_batch_messages_async(queue, queue_msgs)
            results.extend(batch_results)

        return results
    
    def _send_batch_messages_sync(self, queue: str, messages: list) -> list:
        """æ‰¹é‡å‘é€ä»»åŠ¡ï¼ˆåŒæ­¥ï¼‰

        æ³¨æ„ï¼šå»¶è¿Ÿä»»åŠ¡ç°åœ¨ä¹Ÿç›´æ¥å‘é€åˆ°æ™®é€šé˜Ÿåˆ—ï¼Œç”±TaskExecutorä¸­çš„AsyncDelayQueueå¤„ç†å»¶è¿Ÿ
        """
        from ..utils.serializer import dumps_str

        # æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŒ…æ‹¬å»¶è¿Ÿä»»åŠ¡ï¼‰éƒ½å‘é€åˆ°æ™®é€šé˜Ÿåˆ—
        all_messages = []

        for msg in messages:
            msg_dict = msg.to_dict()

            # å¤„ç†å»¶è¿Ÿä»»åŠ¡ï¼šæ·»åŠ  execute_at æ ‡è®°ï¼Œä½†ä¸å†™å…¥ DELAYED_QUEUE
            if msg.delay and msg.delay > 0:
                current_time = time.time()
                msg_dict['execute_at'] = current_time + msg.delay
                msg_dict['is_delayed'] = 1

            all_messages.append(msg_dict)

        results = []

        # æ‰¹é‡å‘é€æ‰€æœ‰ä»»åŠ¡åˆ°æ™®é€šé˜Ÿåˆ—ï¼ˆåŒ…æ‹¬å»¶è¿Ÿä»»åŠ¡ï¼‰
        if all_messages:
            batch_results = self.ep._batch_send_event_sync(
                self.ep.get_prefixed_queue_name(queue),
                [{'data': dumps_str(msg)} for msg in all_messages],
                self.ep.get_redis_client(asyncio=False, binary=True).pipeline()
            )
            results.extend(batch_results)

        return results

    async def _send_batch_messages_async(self, queue: str, messages: list) -> list:
        """æ‰¹é‡å‘é€ä»»åŠ¡ï¼ˆå¼‚æ­¥ï¼‰

        æ³¨æ„ï¼šå»¶è¿Ÿä»»åŠ¡ç°åœ¨ä¹Ÿç›´æ¥å‘é€åˆ°æ™®é€šé˜Ÿåˆ—ï¼Œç”±TaskExecutorä¸­çš„AsyncDelayQueueå¤„ç†å»¶è¿Ÿ
        """
        from ..utils.serializer import dumps_str

        # æ‰€æœ‰æ¶ˆæ¯ï¼ˆåŒ…æ‹¬å»¶è¿Ÿä»»åŠ¡ï¼‰éƒ½å‘é€åˆ°æ™®é€šé˜Ÿåˆ—
        all_messages = []

        for msg in messages:
            msg_dict = msg.to_dict()

            # å¤„ç†å»¶è¿Ÿä»»åŠ¡ï¼šæ·»åŠ  execute_at æ ‡è®°ï¼Œä½†ä¸å†™å…¥ DELAYED_QUEUE
            if msg.delay and msg.delay > 0:
                current_time = time.time()
                msg_dict['execute_at'] = current_time + msg.delay
                msg_dict['is_delayed'] = 1

            all_messages.append(msg_dict)

        results = []
        # æ‰¹é‡å‘é€æ‰€æœ‰ä»»åŠ¡åˆ°æ™®é€šé˜Ÿåˆ—ï¼ˆåŒ…æ‹¬å»¶è¿Ÿä»»åŠ¡ï¼‰
        if all_messages:
            batch_results = await self.ep._batch_send_event(
                self.ep.get_prefixed_queue_name(queue),
                [{'data': dumps_str(msg)} for msg in all_messages],
                self.ep.get_redis_client(asyncio=True, binary=True).pipeline()
            )
            results.extend(batch_results)

        return results

    def _get_task_names_from_queue(self, queue: str, task_name: str = None) -> list:
        """è·å–é˜Ÿåˆ—çš„ä»»åŠ¡ååˆ—è¡¨

        Args:
            queue: é˜Ÿåˆ—åç§°ï¼ˆå¯èƒ½åŒ…å«ä¼˜å…ˆçº§åç¼€ï¼‰
            task_name: å¯é€‰çš„ä»»åŠ¡åï¼Œå¦‚æœæä¾›åˆ™ç›´æ¥è¿”å› [task_name]

        Returns:
            ä»»åŠ¡ååˆ—è¡¨ï¼Œå¦‚æœ task_name æä¾›åˆ™è¿”å› [task_name]ï¼Œå¦åˆ™è¿”å›é˜Ÿåˆ—çš„æ‰€æœ‰ä»»åŠ¡å
        """
        if task_name is not None:
            return [task_name]

        # ä» base_queue ä¸­æå–åŸºç¡€é˜Ÿåˆ—åï¼ˆå»æ‰ä¼˜å…ˆçº§ï¼‰
        base_queue = queue.split(':')[0] if ':' in queue else queue

        # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è·å–ä»»åŠ¡å
        import asyncio
        try:
            # å°è¯•åœ¨å½“å‰äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
            loop = asyncio.get_running_loop()
            # å¦‚æœå·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªä»»åŠ¡
            task_names = asyncio.create_task(self.registry.get_task_names_by_queue(base_queue))
            # æ³¨æ„ï¼šè¿™é‡Œæ— æ³•ç­‰å¾…ï¼Œéœ€è¦è°ƒç”¨è€…å¤„ç†
            # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºè¿™æ˜¯åŒæ­¥æ–¹æ³•
            raise RuntimeError("Cannot call sync method from async context")
        except RuntimeError:
            # æ²¡æœ‰è¿è¡Œä¸­çš„äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
            task_names = asyncio.run(self.registry.get_task_names_by_queue(base_queue))

        return list(task_names) if task_names else []

    def get_result(self, event_id: str, queue: str, task_name: str = None,
                   delete: bool = False, asyncio: bool = False,
                   delayed_deletion_ex: int = None, wait: bool = False,
                   timeout: int = 300, poll_interval: float = 0.5):
        """è·å–ä»»åŠ¡æ‰§è¡Œç»“æœ

        åœ¨ä»»åŠ¡ç»„æ¶æ„ä¸‹ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½æœ‰ç‹¬ç«‹çš„æ‰§è¡Œç»“æœå­˜å‚¨ã€‚
        ç»“æœå­˜å‚¨æ ¼å¼: {redis_prefix}:TASK:{event_id}:{group_name}

        è¿™ä¸ªæ–¹æ³•æ”¯æŒå®Œå…¨è§£è€¦çš„ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼Œç”Ÿäº§è€…åªéœ€è¦çŸ¥é“ï¼š
        - event_id: å‘é€ä»»åŠ¡æ—¶è¿”å›çš„äº‹ä»¶ID
        - queue: é˜Ÿåˆ—åç§°
        - task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼Œä¸æä¾›æ—¶ä¼šè·å–è¯¥é˜Ÿåˆ—æ‰€æœ‰ä»»åŠ¡çš„ç»“æœï¼‰

        Args:
            event_id: ä»»åŠ¡äº‹ä»¶IDï¼ˆå‘é€ä»»åŠ¡æ—¶è¿”å›çš„æ¶ˆæ¯IDï¼‰
            queue: é˜Ÿåˆ—åç§°
            task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸æä¾›ï¼Œä¼šè·å–è¯¥é˜Ÿåˆ—æ‰€æœ‰ä»»åŠ¡çš„ç»“æœï¼Œè¿”å›åˆ—è¡¨
            delete: æ˜¯å¦åˆ é™¤ç»“æœï¼ˆé»˜è®¤Falseï¼‰
            asyncio: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰
            delayed_deletion_ex: å»¶è¿Ÿåˆ é™¤æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œè®¾ç½®åä¼šåœ¨æŒ‡å®šæ—¶é—´åè‡ªåŠ¨åˆ é™¤
            wait: æ˜¯å¦é˜»å¡ç­‰å¾…ç›´åˆ°ä»»åŠ¡å®Œæˆï¼ˆé»˜è®¤Falseï¼‰
            timeout: ç­‰å¾…è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300ç§’
            poll_interval: è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’

        Returns:
            å½“æŒ‡å®štask_nameæ—¶:
                åŒæ­¥æ¨¡å¼: ä»»åŠ¡ç»“æœï¼ˆå­—ç¬¦ä¸²æˆ–å­—èŠ‚ï¼‰ï¼Œå¦‚æœä»»åŠ¡æœªå®Œæˆè¿”å›None
                å¼‚æ­¥æ¨¡å¼: è¿”å›åç¨‹ï¼Œéœ€è¦ä½¿ç”¨ await
            å½“ä¸æŒ‡å®štask_nameæ—¶:
                è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å­—å…¸: [{"task_name": "xxx", "result": ..., "status": ...}, ...]

        Raises:
            TaskTimeoutError: ç­‰å¾…è¶…æ—¶
            TaskExecutionError: ä»»åŠ¡æ‰§è¡Œå¤±è´¥
            TaskNotFoundError: ä»»åŠ¡ä¸å­˜åœ¨

        Examples:
            # è·å–å•ä¸ªä»»åŠ¡ç»“æœ
            result = app.get_result("1234567890-0", "my_queue", task_name="my_task")

            # è·å–é˜Ÿåˆ—ä¸­æ‰€æœ‰ä»»åŠ¡çš„ç»“æœ
            results = app.get_result("1234567890-0", "my_queue")
            # è¿”å›: [{"task_name": "task1", "result": ..., "status": ...}, {"task_name": "task2", ...}]

            # å¼‚æ­¥è·å–æ‰€æœ‰ä»»åŠ¡ç»“æœ
            results = await app.get_result("1234567890-0", "my_queue", asyncio=True)
        """
        # åˆ¤æ–­æ˜¯å¦æŒ‡å®šäº† task_nameï¼Œå†³å®šæœ€ç»ˆè¿”å›æ ¼å¼
        return_single = task_name is not None

        # è·å–éœ€è¦æŸ¥è¯¢çš„ä»»åŠ¡ååˆ—è¡¨
        task_names = self._get_task_names_from_queue(queue, task_name)

        # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if not task_names:
            if asyncio:
                async def _return_empty_list():
                    return []
                return _return_empty_list()
            else:
                return []

        # ç»Ÿä¸€å¤„ç†ï¼šéå†æ‰€æœ‰ä»»åŠ¡è·å–ç»“æœ
        if asyncio:
            return self._get_results_async(event_id, queue, task_names, delete,
                                          delayed_deletion_ex, wait, timeout, poll_interval, return_single)
        else:
            return self._get_results_sync(event_id, queue, task_names, delete,
                                         delayed_deletion_ex, wait, timeout, poll_interval, return_single)

    def get_queue_position(self, event_id: str, queue: str, task_name: str = None, asyncio: bool = False):
        """è·å–ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­çš„æ’é˜Ÿæƒ…å†µ

        é€šè¿‡ event_id æŸ¥è¯¢ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­çš„æ’é˜Ÿä½ç½®ï¼ŒåŒ…æ‹¬ï¼š
        - è·ç¦»è¢«è¯»å–è¿˜å·®å¤šå°‘ä»»åŠ¡
        - è·ç¦»è¢«æ¶ˆè´¹è¿˜å·®å¤šå°‘ä»»åŠ¡

        Args:
            event_id: ä»»åŠ¡äº‹ä»¶IDï¼ˆå‘é€ä»»åŠ¡æ—¶è¿”å›çš„æ¶ˆæ¯IDï¼‰
            queue: é˜Ÿåˆ—åç§°
            task_name: ä»»åŠ¡åç§°ï¼ˆå¯é€‰ï¼‰ã€‚å¦‚æœä¸æä¾›ï¼Œä¼šè·å–è¯¥é˜Ÿåˆ—æ‰€æœ‰ä»»åŠ¡çš„æ’é˜Ÿæƒ…å†µ
            asyncio: æ˜¯å¦ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ˆé»˜è®¤Falseï¼‰

        Returns:
            å½“æŒ‡å®štask_nameæ—¶:
                è¿”å›å­—å…¸: {
                    "task_name": "xxx",
                    "task_offset": 12,
                    "read_offset": 14,
                    "task_ack_offset": 10,
                    "pending_read": 2,      # è·ç¦»è¢«è¯»å–è¿˜å·®2ä¸ªä»»åŠ¡
                    "pending_consume": -2   # å·²ç»è¢«æ¶ˆè´¹äº†ï¼ˆè´Ÿæ•°è¡¨ç¤ºå·²å®Œæˆï¼‰
                }
            å½“ä¸æŒ‡å®štask_nameæ—¶:
                è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸Šè¿°æ ¼å¼çš„å­—å…¸

        Note:
            æ’åä¿¡æ¯åæ˜ çš„æ˜¯ä»»åŠ¡çš„å‘é€é¡ºåºï¼ˆoffsetï¼‰ï¼Œè€Œä¸æ˜¯æ‰§è¡Œé¡ºåºã€‚
            åœ¨å¹¶å‘æ‰§è¡Œçš„åœºæ™¯ä¸‹ï¼Œæ’åé åçš„ä»»åŠ¡å¯èƒ½å…ˆæ‰§è¡Œå®Œæˆï¼Œè€Œæ’åé å‰çš„ä»»åŠ¡å¯èƒ½è¿˜åœ¨æ‰§è¡Œä¸­ã€‚

            ä¾‹å¦‚ï¼š
            - ä»»åŠ¡A (offset=10) å’Œä»»åŠ¡B (offset=15) åŒæ—¶è¢«è¯»å–
            - å¦‚æœä»»åŠ¡Bæ‰§è¡Œå¾—å¿«ï¼Œå¯èƒ½ä¼šå…ˆå®Œæˆ
            - æ­¤æ—¶ä»»åŠ¡Açš„ pending_consume å¯èƒ½ä»ä¸ºæ­£æ•°ï¼ˆè¿˜æœªæ¶ˆè´¹ç¡®è®¤ï¼‰
            - è€Œä»»åŠ¡Bçš„ pending_consume å·²ç»å˜ä¸ºè´Ÿæ•°ï¼ˆå·²å®Œæˆï¼‰

            å› æ­¤ï¼š
            - pending_read è¡¨ç¤ºæœ‰å¤šå°‘ä»»åŠ¡åœ¨ä½ ä¹‹å‰è¢«å‘é€åˆ°é˜Ÿåˆ—
            - pending_consume è¡¨ç¤ºæœ‰å¤šå°‘ä»»åŠ¡åœ¨ä½ ä¹‹å‰è¢«æ¶ˆè´¹ç¡®è®¤ï¼ˆä¸ä»£è¡¨æ‰§è¡Œé¡ºåºï¼‰
            - è´Ÿæ•°çš„ pending_consume åªè¡¨ç¤ºè¯¥ä»»åŠ¡å·²è¢«ç¡®è®¤ï¼Œä¸è¡¨ç¤ºæ‰€æœ‰å‰é¢çš„ä»»åŠ¡éƒ½å·²å®Œæˆ

        Examples:
            # è·å–å•ä¸ªä»»åŠ¡çš„æ’é˜Ÿæƒ…å†µ
            position = app.get_queue_position("1234567890-0", "my_queue", task_name="my_task")

            # è·å–é˜Ÿåˆ—ä¸­æ‰€æœ‰ä»»åŠ¡çš„æ’é˜Ÿæƒ…å†µ
            positions = app.get_queue_position("1234567890-0", "my_queue")

            # å¼‚æ­¥è·å–
            position = await app.get_queue_position("1234567890-0", "my_queue", asyncio=True)
        """
        # åˆ¤æ–­æ˜¯å¦æŒ‡å®šäº† task_nameï¼Œå†³å®šæœ€ç»ˆè¿”å›æ ¼å¼
        return_single = task_name is not None

        # è·å–éœ€è¦æŸ¥è¯¢çš„ä»»åŠ¡ååˆ—è¡¨
        task_names = self._get_task_names_from_queue(queue, task_name)

        # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨
        if not task_names:
            if asyncio:
                async def _return_empty_list():
                    return []
                return _return_empty_list()
            else:
                return []

        # ç»Ÿä¸€å¤„ç†ï¼šéå†æ‰€æœ‰ä»»åŠ¡è·å–æ’é˜Ÿæƒ…å†µ
        if asyncio:
            return self._get_queue_positions_async(event_id, queue, task_names, return_single)
        else:
            return self._get_queue_positions_sync(event_id, queue, task_names, return_single)

    def _get_queue_positions_sync(self, event_id: str, queue: str, task_names: list, return_single: bool):
        """åŒæ­¥è·å–ä»»åŠ¡æ’é˜Ÿæƒ…å†µ"""
        results = []

        # æ„å»º stream key
        prefixed_queue = f"{self.redis_prefix}:QUEUE:{queue}"

        # ä» stream ä¸­è·å–ä»»åŠ¡æ•°æ®
        try:
            # XRANGE è·å–æŒ‡å®š event_id çš„æ¶ˆæ¯
            stream_data = self.binary_redis.xrange(prefixed_queue, min=event_id, max=event_id, count=1)

            if not stream_data:
                # ä»»åŠ¡ä¸å­˜åœ¨äº stream ä¸­
                for task_name in task_names:
                    results.append({
                        "task_name": task_name,
                        "error": "Task not found in stream"
                    })
                if return_single:
                    return results[0] if results else None
                return results

            # è§£æ stream æ•°æ®
            message_id, message_data = stream_data[0]

            # è§£ç å­—æ®µ
            task_offset = None
            for key, value in message_data.items():
                if isinstance(key, bytes):
                    key = key.decode('utf-8')
                if key == 'offset':
                    if isinstance(value, bytes):
                        task_offset = int(value.decode('utf-8'))
                    else:
                        task_offset = int(value)
                    break

            if task_offset is None:
                for task_name in task_names:
                    results.append({
                        "task_name": task_name,
                        "error": "Offset not found in task data"
                    })
                if return_single:
                    return results[0] if results else None
                return results

        except Exception as e:
            for task_name in task_names:
                results.append({
                    "task_name": task_name,
                    "error": f"Failed to read from stream: {str(e)}"
                })
            if return_single:
                return results[0] if results else None
            return results

        # è·å– READ_OFFSETS å’Œ TASK_OFFSETSï¼ˆä½¿ç”¨ pipeline + HMGET ä¼˜åŒ–ï¼‰
        read_offsets_key = f"{self.redis_prefix}:READ_OFFSETS"
        task_offsets_key = f"{self.redis_prefix}:TASK_OFFSETS"

        # æå–åŸºç¡€é˜Ÿåˆ—åï¼ˆå»æ‰ä¼˜å…ˆçº§ï¼‰
        base_queue = queue.split(':')[0] if ':' in queue else queue

        # æ„å»ºéœ€è¦æŸ¥è¯¢çš„å­—æ®µåˆ—è¡¨
        offset_keys = [f"{base_queue}:{task_name}" for task_name in task_names]

        try:
            # ä½¿ç”¨ pipeline æ‰¹é‡è·å–æ‰€æœ‰éœ€è¦çš„å­—æ®µ
            pipeline = self.redis.pipeline()
            pipeline.hmget(read_offsets_key, offset_keys)
            pipeline.hmget(task_offsets_key, offset_keys)
            read_offsets_list, task_offsets_list = pipeline.execute()
        except Exception as e:
            for task_name in task_names:
                results.append({
                    "task_name": task_name,
                    "error": f"Failed to read offsets: {str(e)}"
                })
            if return_single:
                return results[0] if results else None
            return results

        # å¯¹æ¯ä¸ªä»»åŠ¡è®¡ç®—æ’é˜Ÿæƒ…å†µ
        for idx, task_name in enumerate(task_names):
            # è·å– read_offset
            read_offset = read_offsets_list[idx]
            if read_offset is not None:
                read_offset = int(read_offset)

            # è·å– task_ack_offset
            task_ack_offset = task_offsets_list[idx]
            if task_ack_offset is not None:
                task_ack_offset = int(task_ack_offset)

            # è®¡ç®—æ’é˜Ÿæƒ…å†µ
            # pending_read: æ­£æ•°è¡¨ç¤ºè¿˜å·®å¤šå°‘ä¸ªä»»åŠ¡æ‰èƒ½è¢«è¯»å–ï¼Œ0è¡¨ç¤ºåˆšå¥½è¢«è¯»å–ï¼Œè´Ÿæ•°è¡¨ç¤ºå·²è¢«è¯»å–
            pending_read = (task_offset - read_offset) if read_offset is not None else None
            # pending_consume: æ­£æ•°è¡¨ç¤ºè¿˜å·®å¤šå°‘ä¸ªä»»åŠ¡æ‰èƒ½è¢«æ¶ˆè´¹ï¼Œ0è¡¨ç¤ºåˆšå¥½è¢«æ¶ˆè´¹ï¼Œè´Ÿæ•°è¡¨ç¤ºå·²è¢«æ¶ˆè´¹
            pending_consume = (task_offset - task_ack_offset) if task_ack_offset is not None else None

            results.append({
                "task_name": task_name,
                "task_offset": task_offset,
                "read_offset": read_offset,
                "task_ack_offset": task_ack_offset,
                "pending_read": pending_read,
                "pending_consume": pending_consume
            })

        # æ ¹æ® return_single å†³å®šè¿”å›æ ¼å¼
        if return_single:
            return results[0] if results else None
        return results

    async def _get_queue_positions_async(self, event_id: str, queue: str, task_names: list, return_single: bool):
        """å¼‚æ­¥è·å–ä»»åŠ¡æ’é˜Ÿæƒ…å†µ"""
        results = []

        # æ„å»º stream key
        prefixed_queue = f"{self.redis_prefix}:QUEUE:{queue}"

        # ä» stream ä¸­è·å–ä»»åŠ¡æ•°æ®
        try:
            # XRANGE è·å–æŒ‡å®š event_id çš„æ¶ˆæ¯
            stream_data = await self.async_binary_redis.xrange(prefixed_queue, min=event_id, max=event_id, count=1)

            if not stream_data:
                # ä»»åŠ¡ä¸å­˜åœ¨äº stream ä¸­
                for task_name in task_names:
                    results.append({
                        "task_name": task_name,
                        "error": "Task not found in stream"
                    })
                if return_single:
                    return results[0] if results else None
                return results

            # è§£æ stream æ•°æ®
            message_id, message_data = stream_data[0]

            # è§£ç å­—æ®µ
            task_offset = None
            for key, value in message_data.items():
                if isinstance(key, bytes):
                    key = key.decode('utf-8')
                if key == 'offset':
                    if isinstance(value, bytes):
                        task_offset = int(value.decode('utf-8'))
                    else:
                        task_offset = int(value)
                    break

            if task_offset is None:
                for task_name in task_names:
                    results.append({
                        "task_name": task_name,
                        "error": "Offset not found in task data"
                    })
                if return_single:
                    return results[0] if results else None
                return results

        except Exception as e:
            for task_name in task_names:
                results.append({
                    "task_name": task_name,
                    "error": f"Failed to read from stream: {str(e)}"
                })
            if return_single:
                return results[0] if results else None
            return results

        # è·å– READ_OFFSETS å’Œ TASK_OFFSETSï¼ˆä½¿ç”¨ pipeline + HMGET ä¼˜åŒ–ï¼‰
        read_offsets_key = f"{self.redis_prefix}:READ_OFFSETS"
        task_offsets_key = f"{self.redis_prefix}:TASK_OFFSETS"

        # æå–åŸºç¡€é˜Ÿåˆ—åï¼ˆå»æ‰ä¼˜å…ˆçº§ï¼‰
        base_queue = queue.split(':')[0] if ':' in queue else queue

        # æ„å»ºéœ€è¦æŸ¥è¯¢çš„å­—æ®µåˆ—è¡¨
        offset_keys = [f"{base_queue}:{task_name}" for task_name in task_names]

        try:
            # ä½¿ç”¨ pipeline æ‰¹é‡è·å–æ‰€æœ‰éœ€è¦çš„å­—æ®µ
            pipeline = self.async_redis.pipeline()
            pipeline.hmget(read_offsets_key, offset_keys)
            pipeline.hmget(task_offsets_key, offset_keys)
            read_offsets_list, task_offsets_list = await pipeline.execute()
        except Exception as e:
            for task_name in task_names:
                results.append({
                    "task_name": task_name,
                    "error": f"Failed to read offsets: {str(e)}"
                })
            if return_single:
                return results[0] if results else None
            return results

        # å¯¹æ¯ä¸ªä»»åŠ¡è®¡ç®—æ’é˜Ÿæƒ…å†µ
        for idx, task_name in enumerate(task_names):
            # è·å– read_offset
            read_offset = read_offsets_list[idx]
            if read_offset is not None:
                read_offset = int(read_offset)

            # è·å– task_ack_offset
            task_ack_offset = task_offsets_list[idx]
            if task_ack_offset is not None:
                task_ack_offset = int(task_ack_offset)

            # è®¡ç®—æ’é˜Ÿæƒ…å†µ
            # pending_read: æ­£æ•°è¡¨ç¤ºè¿˜å·®å¤šå°‘ä¸ªä»»åŠ¡æ‰èƒ½è¢«è¯»å–ï¼Œ0è¡¨ç¤ºåˆšå¥½è¢«è¯»å–ï¼Œè´Ÿæ•°è¡¨ç¤ºå·²è¢«è¯»å–
            pending_read = (task_offset - read_offset) if read_offset is not None else None
            # pending_consume: æ­£æ•°è¡¨ç¤ºè¿˜å·®å¤šå°‘ä¸ªä»»åŠ¡æ‰èƒ½è¢«æ¶ˆè´¹ï¼Œ0è¡¨ç¤ºåˆšå¥½è¢«æ¶ˆè´¹ï¼Œè´Ÿæ•°è¡¨ç¤ºå·²è¢«æ¶ˆè´¹
            pending_consume = (task_offset - task_ack_offset) if task_ack_offset is not None else None

            results.append({
                "task_name": task_name,
                "task_offset": task_offset,
                "read_offset": read_offset,
                "task_ack_offset": task_ack_offset,
                "pending_read": pending_read,
                "pending_consume": pending_consume
            })

        # æ ¹æ® return_single å†³å®šè¿”å›æ ¼å¼
        if return_single:
            return results[0] if results else None
        return results

    def _build_task_key(self, task_name: str, queue: str, event_id: str):
        """æ„å»ºä»»åŠ¡çš„ key ä¿¡æ¯

        Returns:
            tuple: (group_name, full_key)
        """
        prefixed_queue = f"{self.redis_prefix}:QUEUE:{queue}"
        group_name = f"{prefixed_queue}:{task_name}"
        status_key = f"{event_id}:{group_name}"
        full_key = f"{self.redis_prefix}:TASK:{status_key}"
        return group_name, full_key

    @staticmethod
    def _decode_bytes(value):
        """è§£ç å­—èŠ‚ä¸ºå­—ç¬¦ä¸²"""
        if value and isinstance(value, bytes):
            return value.decode('utf-8')
        return value

    @staticmethod
    def _is_task_completed(status):
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆï¼ˆæˆåŠŸï¼‰"""
        return status in [TaskStatus.COMPLETED.value, TaskStatus.SUCCESS.value]

    @staticmethod
    def _is_task_failed(status):
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å¤±è´¥"""
        return status in [TaskStatus.ERROR.value, TaskStatus.FAILED.value, "ERROR", "FAILED", "error", "failed"]

    def _get_results_sync(self, event_id: str, queue: str, task_names: list,
                         delete: bool, delayed_deletion_ex: int, wait: bool,
                         timeout: int, poll_interval: float, return_single: bool):
        """åŒæ­¥è·å–ä»»åŠ¡ç»“æœï¼ˆæ”¯æŒå•ä¸ªæˆ–æ‰¹é‡ï¼‰"""
        results = []

        for task_name in task_names:
            try:
                _, full_key = self._build_task_key(task_name, queue, event_id)

                # ç»Ÿä¸€è°ƒç”¨ _get_result_syncï¼Œé€šè¿‡ wait å‚æ•°æ§åˆ¶è¡Œä¸º
                task_info = self._get_result_sync(full_key, event_id, delete, delayed_deletion_ex,
                                                  wait, timeout, poll_interval)

                # å¦‚æœä»»åŠ¡ä¸å­˜åœ¨
                if not task_info:
                    results.append({
                        "task_name": task_name,
                        "status": None,
                        "result": None
                    })
                else:
                    # æ·»åŠ  task_name åˆ°ç»“æœä¸­
                    task_info["task_name"] = task_name
                    results.append(task_info)

            except Exception as e:
                results.append({
                    "task_name": task_name,
                    "status": "ERROR",
                    "result": None,
                    "error_msg": str(e)
                })

        # æ ¹æ® return_single å†³å®šè¿”å›æ ¼å¼
        if return_single:
            return results[0] if results else None
        return results

    async def _get_results_async(self, event_id: str, queue: str, task_names: list,
                                 delete: bool, delayed_deletion_ex: int, wait: bool,
                                 timeout: int, poll_interval: float, return_single: bool):
        """å¼‚æ­¥è·å–ä»»åŠ¡ç»“æœï¼ˆæ”¯æŒå•ä¸ªæˆ–æ‰¹é‡ï¼‰"""
        results = []

        for task_name in task_names:
            try:
                _, full_key = self._build_task_key(task_name, queue, event_id)

                # ç»Ÿä¸€è°ƒç”¨ _get_result_asyncï¼Œé€šè¿‡ wait å‚æ•°æ§åˆ¶è¡Œä¸º
                task_info = await self._get_result_async(full_key, event_id, delete, delayed_deletion_ex,
                                                         wait, timeout, poll_interval)

                # å¦‚æœä»»åŠ¡ä¸å­˜åœ¨
                if not task_info:
                    results.append({
                        "task_name": task_name,
                        "status": TaskStatus.PENDING.value,
                        "result": None
                    })
                else:
                    # æ·»åŠ  task_name åˆ°ç»“æœä¸­
                    task_info["task_name"] = task_name
                    results.append(task_info)

            except Exception as e:
                results.append({
                    "task_name": task_name,
                    "status": "ERROR",
                    "result": None,
                    "error_msg": str(e)
                })

        # æ ¹æ® return_single å†³å®šè¿”å›æ ¼å¼
        if return_single:
            return results[0] if results else None
        return results

    def _get_result_sync(self, full_key: str, event_id: str, delete: bool, delayed_deletion_ex: int,
                         wait: bool = False, timeout: int = 300, poll_interval: float = 0.5):
        """åŒæ­¥è·å–ä»»åŠ¡ç»“æœï¼ˆæ”¯æŒç­‰å¾…æ¨¡å¼ï¼‰"""
        from ..exceptions import TaskTimeoutError, TaskExecutionError, TaskNotFoundError

        # ä½¿ç”¨äºŒè¿›åˆ¶å®¢æˆ·ç«¯ï¼Œä¸è‡ªåŠ¨è§£ç ï¼ˆå› ä¸º result æ˜¯ msgpack åºåˆ—åŒ–çš„ï¼‰
        client = self.binary_redis
        start_time = time.time()

        while True:
            # è·å–æ•´ä¸ª hash çš„æ‰€æœ‰å­—æ®µ
            task_data = client.hgetall(full_key)

            if not task_data:
                if wait:
                    raise TaskNotFoundError(f"Task {event_id} not found")
                return None

            # è§£ç å­—èŠ‚å­—æ®µ
            decoded_data = {}
            for key, value in task_data.items():
                # è§£ç  key
                if isinstance(key, bytes):
                    key = key.decode('utf-8')

                # è·³è¿‡å†…éƒ¨æ ‡è®°å­—æ®µ
                if key.startswith('__'):
                    continue

                # è§£ç  value - åªæœ‰ result å­—æ®µéœ€è¦ loads_str
                if isinstance(value, bytes):
                    if key == 'result':
                        try:
                            decoded_data[key] = loads_str(value)
                        except Exception:
                            decoded_data[key] = value
                    else:
                        # å…¶ä»–å­—æ®µå°è¯• UTF-8 è§£ç 
                        try:
                            decoded_data[key] = value.decode('utf-8')
                        except Exception:
                            decoded_data[key] = value
                else:
                    decoded_data[key] = value

            # å¦‚æœä¸éœ€è¦ç­‰å¾…ï¼Œå¤„ç†åˆ é™¤é€»è¾‘åç›´æ¥è¿”å›
            if not wait:
                if delayed_deletion_ex is not None:
                    client.expire(full_key, delayed_deletion_ex)
                elif delete:
                    if self.task_center and self.task_center.is_enabled:
                        client.hset(full_key, "__pending_delete", "1")
                    else:
                        client.delete(full_key)
                return decoded_data

            # éœ€è¦ç­‰å¾…ï¼šæ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            status = decoded_data.get('status')

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
            if self._is_task_completed(status):
                # ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œå¤„ç†åˆ é™¤é€»è¾‘åè¿”å›
                if delayed_deletion_ex is not None:
                    client.expire(full_key, delayed_deletion_ex)
                elif delete:
                    if self.task_center and self.task_center.is_enabled:
                        client.hset(full_key, "__pending_delete", "1")
                    else:
                        client.delete(full_key)
                return decoded_data

            elif self._is_task_failed(status):
                # ä»»åŠ¡å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                error_msg = decoded_data.get('error_msg', 'Task execution failed')
                raise TaskExecutionError(event_id, error_msg)

            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > timeout:
                raise TaskTimeoutError(f"Task {event_id} timed out after {timeout} seconds")

            # ä»»åŠ¡ä»åœ¨æ‰§è¡Œä¸­ï¼Œç­‰å¾…åé‡è¯•
            time.sleep(poll_interval)

    async def _get_result_async(self, full_key: str, event_id: str, delete: bool, delayed_deletion_ex: int,
                                wait: bool = False, timeout: int = 300, poll_interval: float = 0.5):
        """å¼‚æ­¥è·å–ä»»åŠ¡ç»“æœï¼ˆæ”¯æŒç­‰å¾…æ¨¡å¼ï¼‰"""

        # ä½¿ç”¨äºŒè¿›åˆ¶å®¢æˆ·ç«¯ï¼Œä¸è‡ªåŠ¨è§£ç ï¼ˆå› ä¸º result æ˜¯ msgpack åºåˆ—åŒ–çš„ï¼‰
        client = self.async_binary_redis
        start_time = time.time()

        while True:
            # è·å–æ•´ä¸ª hash çš„æ‰€æœ‰å­—æ®µ
            task_data = await client.hgetall(full_key)

            if not task_data:
                if wait:
                    raise TaskNotFoundError(f"Task {event_id} not found")
                return None

            # è§£ç å­—èŠ‚å­—æ®µ
            decoded_data = {}
            for key, value in task_data.items():
                # è§£ç  key
                if isinstance(key, bytes):
                    key = key.decode('utf-8')

                # è·³è¿‡å†…éƒ¨æ ‡è®°å­—æ®µ
                if key.startswith('__'):
                    continue

                # è§£ç  value - åªæœ‰ result å­—æ®µéœ€è¦ loads_str
                if isinstance(value, bytes):
                    if key == 'result':
                        try:
                            decoded_data[key] = loads_str(value)
                        except Exception:
                            decoded_data[key] = value
                    else:
                        # å…¶ä»–å­—æ®µå°è¯• UTF-8 è§£ç 
                        try:
                            decoded_data[key] = value.decode('utf-8')
                        except Exception:
                            decoded_data[key] = value
                else:
                    decoded_data[key] = value

            # å¦‚æœä¸éœ€è¦ç­‰å¾…ï¼Œå¤„ç†åˆ é™¤é€»è¾‘åç›´æ¥è¿”å›
            if not wait:
                if delayed_deletion_ex is not None:
                    await client.expire(full_key, delayed_deletion_ex)
                elif delete:
                    if self.task_center and self.task_center.is_enabled:
                        await client.hset(full_key, "__pending_delete", "1")
                    else:
                        await client.delete(full_key)
                return decoded_data

            # éœ€è¦ç­‰å¾…ï¼šæ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            status = decoded_data.get('status')

            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å®Œæˆ
            if self._is_task_completed(status):
                # ä»»åŠ¡æˆåŠŸå®Œæˆï¼Œå¤„ç†åˆ é™¤é€»è¾‘åè¿”å›
                if delayed_deletion_ex is not None:
                    await client.expire(full_key, delayed_deletion_ex)
                elif delete:
                    if self.task_center and self.task_center.is_enabled:
                        await client.hset(full_key, "__pending_delete", "1")
                    else:
                        await client.delete(full_key)
                return decoded_data

            elif self._is_task_failed(status):
                # ä»»åŠ¡å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                error_msg = decoded_data.get('error_msg', 'Task execution failed')
                raise TaskExecutionError(event_id, error_msg)

            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > timeout:
                raise TaskTimeoutError(f"Task {event_id} timed out after {timeout} seconds")

            # ç­‰å¾…åé‡è¯•
            await asyncio.sleep(poll_interval)

    def register_router(self, router, prefix: str = None):
        """
        æ³¨å†Œä»»åŠ¡è·¯ç”±å™¨
        
        Args:
            router: TaskRouterå®ä¾‹
            prefix: é¢å¤–çš„å‰ç¼€ï¼ˆå¯é€‰ï¼‰
        
        ä½¿ç”¨ç¤ºä¾‹ï¼š
            from jettask import Jettask, TaskRouter
            
            # åˆ›å»ºè·¯ç”±å™¨
            email_router = TaskRouter(prefix="email", queue="emails")
            
            @email_router.task()
            async def send_email(to: str):
                pass
            
            # æ³¨å†Œåˆ°ä¸»åº”ç”¨
            app = Jettask(redis_url="redis://localhost:6379/0")
            app.register_router(email_router)
        """
        from ..task.router import TaskRouter
        
        if not isinstance(router, TaskRouter):
            raise TypeError("router must be a TaskRouter instance")
        
        # æ³¨å†Œæ‰€æœ‰ä»»åŠ¡
        for task_name, task_config in router.get_tasks().items():
            # å¦‚æœæŒ‡å®šäº†é¢å¤–å‰ç¼€ï¼Œæ·»åŠ åˆ°ä»»åŠ¡å
            if prefix:
                if task_config.get('name'):
                    task_config['name'] = f"{prefix}.{task_config['name']}"
                task_name = f"{prefix}.{task_name}"
            
            # è·å–ä»»åŠ¡å‡½æ•°
            func = task_config.pop('func')
            name = task_config.pop('name', task_name)
            queue = task_config.pop('queue', None)
            
            # æ³¨å†Œä»»åŠ¡
            task = self._task_from_fun(func, name, None, queue, **task_config)
            logger.debug(f"Registered task: {name} (queue: {queue or self.redis_prefix})")
        
        return self

    def _mount_module(self):
        for module in self.include:
            module = importlib.import_module(module)
            for attr_name in dir(module):
                obj = getattr(module, attr_name)
                if hasattr(obj, "app"):
                    self._tasks.update(getattr(obj, "app")._tasks)

    def _validate_tasks_for_executor(self, execute_type: str, queues: List[str]):
        """éªŒè¯ä»»åŠ¡ç±»å‹æ˜¯å¦ä¸æ‰§è¡Œå™¨å…¼å®¹"""
        if execute_type in ["asyncio", "multi_asyncio"]:
            return  # AsyncIOå’ŒMultiAsyncioå¯ä»¥å¤„ç†å¼‚æ­¥ä»»åŠ¡
        
        # åªæœ‰Threadæ‰§è¡Œå™¨ä¸èƒ½å¤„ç†å¼‚æ­¥ä»»åŠ¡
        incompatible_tasks = []
        for task_name, task in self._tasks.items():
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å±äºæŒ‡å®šé˜Ÿåˆ—
            if task.queue not in queues:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¼‚æ­¥ä»»åŠ¡
            if asyncio.iscoroutinefunction(task.run):
                incompatible_tasks.append({
                    'name': task_name,
                    'queue': task.queue,
                    'type': 'async'
                })
        
        if incompatible_tasks:
            error_msg = f"\né”™è¯¯ï¼š{execute_type} æ‰§è¡Œå™¨ä¸èƒ½å¤„ç†å¼‚æ­¥ä»»åŠ¡ï¼\n"
            error_msg += "å‘ç°ä»¥ä¸‹å¼‚æ­¥ä»»åŠ¡ï¼š\n"
            for task in incompatible_tasks:
                error_msg += f"  - {task['name']} (é˜Ÿåˆ—: {task['queue']})\n"
            error_msg += f"\nè§£å†³æ–¹æ¡ˆï¼š\n"
            error_msg += f"1. ä½¿ç”¨ asyncio æˆ– process æ‰§è¡Œå™¨\n"
            error_msg += f"2. æˆ–è€…å°†è¿™äº›ä»»åŠ¡æ”¹ä¸ºåŒæ­¥å‡½æ•°ï¼ˆå»æ‰ async/awaitï¼‰\n"
            error_msg += f"3. æˆ–è€…å°†è¿™äº›ä»»åŠ¡çš„é˜Ÿåˆ—ä»ç›‘å¬åˆ—è¡¨ä¸­ç§»é™¤\n"
            raise ValueError(error_msg)



    def _create_executor(self, concurrency: int):
        """
        åˆ›å»ºè¿›ç¨‹ç¼–æ’å™¨å®ä¾‹

        Returns:
            ProcessOrchestrator å®ä¾‹
        """
        # åˆ›å»º ProcessOrchestratorï¼ˆå¤šè¿›ç¨‹ç®¡ç†å™¨ï¼‰
        orchestrator = ProcessOrchestrator(self, concurrency)

        # ä¿å­˜ orchestrator å¼•ç”¨
        self._current_executor = orchestrator

        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        def signal_handler(signum, frame):
            logger.debug(f"Main process received signal {signum}, initiating shutdown...")
            self._should_exit = True
            orchestrator.shutdown_event.set()
            raise KeyboardInterrupt()

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        return orchestrator

    async def _cleanup_worker_v3(self, heartbeat_managers: list, executor, worker_ids: list):
        """
        æ¸…ç† Worker èµ„æºï¼ˆæ–°ç‰ˆæœ¬ï¼Œæ”¯æŒå¤šä¸ªå¿ƒè·³åç¨‹ï¼‰
        """
        logger.debug("Shutting down workers...")

        # 1. åœæ­¢ Worker è¶…æ—¶æ‰«æå™¨
        # try:
        #     logger.debug("Stopping worker scanner...")
        #     self.worker_state.stop_timeout_scanner(timeout=3.0)
        # except Exception as e:
        #     logger.error(f"Error stopping worker scanner: {e}", exc_info=True)

        # 2. åœæ­¢æ‰€æœ‰å¿ƒè·³åç¨‹
        if heartbeat_managers:
            logger.debug(f"Stopping {len(heartbeat_managers)} heartbeat tasks...")
            for i, heartbeat in enumerate(heartbeat_managers):
                try:
                    worker_id = worker_ids[i][0] if i < len(worker_ids) else f"worker_{i}"
                    logger.debug(f"Stopping heartbeat task for {worker_id}...")
                    await heartbeat.stop()
                except Exception as e:
                    logger.error(f"Error stopping heartbeat #{i}: {e}", exc_info=True)

        # 3. å…³é—­æ‰§è¡Œå™¨
        if executor:
            try:
                logger.debug("Shutting down executor...")
                executor.shutdown()
            except Exception as e:
                logger.error(f"Error shutting down executor: {e}", exc_info=True)

        # 5. è°ƒç”¨é€šç”¨æ¸…ç†
        # try:
        #     self.cleanup()
        # except Exception as e:
        #     logger.error(f"Error in cleanup: {e}", exc_info=True)

        logger.debug(f"All {len(worker_ids)} workers shutdown complete")

    async def _start(self, tasks: List[str], concurrency: int = 1, prefetch_multiplier: int = 1):
        """
        å¯åŠ¨ Worker ä¸»é€»è¾‘ï¼ˆå¤šè¿›ç¨‹æ¨¡å¼ï¼Œä¸»è¿›ç¨‹è°ƒç”¨ï¼‰

        æ–°æ¶æ„æµç¨‹ï¼š
        1. ä»ä»»åŠ¡åç§°è·å–é˜Ÿåˆ—åˆ—è¡¨
        2. ç¡®ä¿ queue_registry çš„ redis å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        3. ä¸ºæ¯ä¸ªå­è¿›ç¨‹ç”Ÿæˆç‹¬ç«‹çš„ Worker ID
        4. åœ¨ä¸»è¿›ç¨‹ä¸ºæ¯ä¸ªå­è¿›ç¨‹å¯åŠ¨ç‹¬ç«‹çš„å¿ƒè·³çº¿ç¨‹
        5. åˆ›å»ºæ‰§è¡Œå™¨
        6. Fork å¹¶å¯åŠ¨å­è¿›ç¨‹ï¼Œä¼ é€’å¯¹åº”çš„ worker_id
        7. ç­‰å¾…é€€å‡ºä¿¡å·ï¼ˆé˜»å¡ï¼‰
        8. æ¸…ç†èµ„æº

        Args:
            tasks: ä»»åŠ¡åç§°åˆ—è¡¨
            concurrency: å¹¶å‘æ‰§è¡Œå™¨è¿›ç¨‹æ•°ï¼ˆå­è¿›ç¨‹æ•°é‡ï¼‰
            prefetch_multiplier: é¢„å–å€æ•°
        """
        heartbeat_managers = []
        executor = None
        worker_ids = []

        # 1. ä»ä»»åŠ¡åç§°è·å–é˜Ÿåˆ—åˆ—è¡¨
        queues = self._get_queues_from_tasks(tasks)
        logger.debug(f"Tasks {tasks} -> Queues {queues}")

        try:
            # 2. ä¸ºæ¯ä¸ªå­è¿›ç¨‹ç”Ÿæˆç‹¬ç«‹çš„ Worker ID å¹¶å¯åŠ¨å¿ƒè·³åç¨‹
            # logger.debug(f"Generating {concurrency} worker IDs and starting heartbeat tasks...")
            from jettask.worker.heartbeat import HeartbeatManager

            for i in range(concurrency):
                # ä½¿ç”¨æ–°æ–¹æ³•ï¼šç”Ÿæˆ worker_id å¹¶å¯åŠ¨å¿ƒè·³ï¼Œç­‰å¾…é¦–æ¬¡å¿ƒè·³æˆåŠŸ
                heartbeat = await HeartbeatManager.create_and_start(
                    queue_registry=self.registry,  # ä¼ é€’ queue_registry ç”¨äºå‘ç°æ–°é˜Ÿåˆ—
                    worker_manager=self.worker_state,  # ä¼ é€’ worker_manager ç”¨äºæ›´æ–° worker çŠ¶æ€
                    async_redis_client=self.async_redis,
                    redis_prefix=self.redis_prefix,
                    interval=self.heartbeat_interval,
                    heartbeat_timeout=self.heartbeat_timeout,
                    worker_state=self.worker_state
                )
                # ä»å¿ƒè·³ç®¡ç†å™¨å¯¹è±¡ä¸­è·å– worker_id å’Œ worker_key
                worker_ids.append((heartbeat.worker_id, heartbeat.worker_key))
                heartbeat_managers.append(heartbeat)
                # logger.debug(f"  Process #{i}: worker_id={heartbeat.worker_id} (heartbeat started)")

            # 3. åˆ›å»ºæ‰§è¡Œå™¨ï¼ˆåœ¨å¯åŠ¨ scanner ä¹‹å‰ï¼Œé¿å… fork å¤åˆ¶åç¨‹èµ„æºï¼‰
            executor = self._create_executor(concurrency)

            # 4. å¯åŠ¨å…¨å±€ Worker è¶…æ—¶æ‰«æå™¨ï¼ˆåœ¨ fork ä¹‹åå¯åŠ¨ï¼Œé¿å…èµ„æºå¤åˆ¶åˆ°å­è¿›ç¨‹ï¼‰
            # Scanner ä¼šä½¿ç”¨æ¯ä¸ª worker è‡ªå·±å­˜å‚¨çš„ heartbeat_timeout è¿›è¡Œè¶…æ—¶åˆ¤æ–­
            # logger.debug("Starting global worker timeout scanner...")
            # å¯åŠ¨ scannerï¼Œä½¿ç”¨é…ç½®çš„æ‰«æé—´éš”
            HeartbeatManager.start_global_timeout_scanner(
                scan_interval=self.scanner_interval,
                worker_manager=self.worker_state
            )
            # logger.debug("Global worker timeout scanner started")
         

            # 5. å¯åŠ¨ ProcessOrchestrator
            # ä¼ é€’ worker_ids åˆ—è¡¨ï¼Œæ¯ä¸ªå­è¿›ç¨‹ä½¿ç”¨å¯¹åº”çš„ worker_id
            # æ³¨æ„ï¼šexecutor.start() æ˜¯åŒæ­¥é˜»å¡è°ƒç”¨ï¼Œä½†å¿…é¡»åœ¨ä¸»çº¿ç¨‹ä¸­è¿è¡Œï¼ˆå› ä¸ºéœ€è¦è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼‰
            # å®ƒå†…éƒ¨ä½¿ç”¨ time.sleep() ä¼šé˜»å¡äº‹ä»¶å¾ªç¯ï¼Œä½†å¿ƒè·³å’Œé˜Ÿåˆ—å‘ç°ä»»åŠ¡å·²ç»åœ¨äº‹ä»¶å¾ªç¯ä¸­å¯åŠ¨
            # logger.debug(f"Starting {concurrency} executor processes...")

            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                None,  # ä½¿ç”¨é»˜è®¤çº¿ç¨‹æ± 
                executor.start,
                queues,
                tasks,
                prefetch_multiplier,
                worker_ids
            )
        

        except KeyboardInterrupt:
            logger.debug("Worker interrupted by keyboard")
        except Exception as e:
            logger.error(f"Error in worker main loop: {e}", exc_info=True)
        finally:
            # åœæ­¢å…¨å±€ scanner
            logger.debug("Stopping global worker timeout scanner...")
            await HeartbeatManager.stop_global_timeout_scanner()

            # æ¸…ç†èµ„æº
            await self._cleanup_worker_v3(heartbeat_managers, executor, worker_ids)

    def start(
        self,
        tasks: List[str],
        concurrency: int = 1,
        prefetch_multiplier: int = 1,
    ):
        """å¯åŠ¨ Workerï¼ˆä»…æ”¯æŒ multi_asyncioï¼‰

        Args:
            tasks: ä»»åŠ¡åç§°åˆ—è¡¨ï¼ˆå¿…éœ€å‚æ•°ï¼‰
            concurrency: å¹¶å‘æ•°
            prefetch_multiplier: é¢„å–å€æ•°
        """

        # æ£€æŸ¥å¿…éœ€å‚æ•°
        if not tasks:
            raise ValueError(
                "å¿…é¡»æŒ‡å®š tasks å‚æ•°ï¼\n"
                "ç¤ºä¾‹: app.start(tasks=['task1', 'task2'], concurrency=4)"
            )

        # æ ‡è®°workerå·²å¯åŠ¨
        self._worker_started = True

        # å¦‚æœé…ç½®äº†ä»»åŠ¡ä¸­å¿ƒä¸”é…ç½®å°šæœªåŠ è½½ï¼Œä»ä»»åŠ¡ä¸­å¿ƒè·å–é…ç½®
        if self.task_center and self.task_center.is_enabled and not self._task_center_config:
            self._load_config_from_task_center()

        # æ³¨å†Œæ‰€æœ‰å¾…æ³¨å†Œçš„é™æµé…ç½®åˆ° Redis
        logger.debug("æ­£åœ¨æ³¨å†Œå¾…æ³¨å†Œçš„é™æµé…ç½®...")
        self._apply_pending_rate_limits()

        # æ³¨å†Œæ¸…ç†å¤„ç†å™¨ï¼ˆåªåœ¨å¯åŠ¨workeræ—¶æ³¨å†Œï¼‰
        self._setup_cleanup_handlers()


        # ä½¿ç”¨ asyncio.run å¯åŠ¨å¼‚æ­¥çš„ _start() æ–¹æ³•
        asyncio.run(self._start(
            tasks=tasks,
            concurrency=concurrency,
            prefetch_multiplier=prefetch_multiplier,
        ))


    def get_task_info(self, event_id: str, asyncio: bool = False):
        """è·å–ä»»åŠ¡ä¿¡æ¯ï¼ˆä»TASK:hashï¼‰"""
        client = self.get_redis_client(asyncio)
        key = f"{self.redis_prefix}:TASK:{event_id}"
        if asyncio:
            return client.hgetall(key)
        else:
            return client.hgetall(key)
    
    def get_task_status(self, event_id: str, asyncio: bool = False):
        """è·å–ä»»åŠ¡çŠ¶æ€ï¼ˆä»TASK:hashçš„statuså­—æ®µï¼‰

        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•ä½¿ç”¨ç®€åŒ–çš„ key æ ¼å¼ TASK:{event_id}
        å¦‚æœéœ€è¦è·å–å¸¦ group_name çš„ä»»åŠ¡çŠ¶æ€ï¼Œè¯·ä½¿ç”¨ _get_task_status_sync æˆ– _get_task_status_async
        """
        if asyncio:
            return self._get_task_status_simple_async(event_id)
        else:
            client = self.redis
            key = f"{self.redis_prefix}:TASK:{event_id}"
            return client.hget(key, "status")

    async def _get_task_status_simple_async(self, event_id: str):
        """å¼‚æ­¥è·å–ä»»åŠ¡çŠ¶æ€ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œä¸éœ€è¦ task_name å’Œ queueï¼‰"""
        key = f"{self.redis_prefix}:TASK:{event_id}"
        return await self.async_redis.hget(key, "status")

    def set_task_status(self, event_id: str, status: str, asyncio: bool = False):
        """è®¾ç½®ä»»åŠ¡çŠ¶æ€ï¼ˆå†™å…¥TASK:hashçš„statuså­—æ®µï¼‰"""
        if asyncio:
            return self._set_task_status_async(event_id, status)
        else:
            client = self.redis
            key = f"{self.redis_prefix}:TASK:{event_id}"
            return client.hset(key, "status", status)
    
    async def _set_task_status_async(self, event_id: str, status: str):
        """å¼‚æ­¥è®¾ç½®ä»»åŠ¡çŠ¶æ€"""
        key = f"{self.redis_prefix}:TASK:{event_id}"
        return await self.async_redis.hset(key, "status", status)

    def set_task_status_by_batch(self, mapping: dict, asyncio: bool = False):
        """æ‰¹é‡è®¾ç½®ä»»åŠ¡çŠ¶æ€ï¼ˆå†™å…¥TASK:hashï¼‰"""
        if asyncio:
            return self._set_task_status_by_batch_async(mapping)
        else:
            pipeline = self.redis.pipeline()
            for event_id, status in mapping.items():
                key = f"{self.redis_prefix}:TASK:{event_id}"
                pipeline.hset(key, "status", status)
            return pipeline.execute()
    
    async def _set_task_status_by_batch_async(self, mapping: dict):
        """å¼‚æ­¥æ‰¹é‡è®¾ç½®ä»»åŠ¡çŠ¶æ€"""
        pipeline = self.async_redis.pipeline()
        for event_id, status in mapping.items():
            key = f"{self.redis_prefix}:TASK:{event_id}"
            pipeline.hset(key, "status", status)
        return await pipeline.execute()

    def del_task_status(self, event_id: str, asyncio: bool = False):
        """åˆ é™¤ä»»åŠ¡çŠ¶æ€ï¼ˆåˆ é™¤æ•´ä¸ªTASK:hashï¼‰"""
        client = self.get_redis_client(asyncio)
        key = f"{self.redis_prefix}:TASK:{event_id}"
        return client.delete(key)

    def get_redis_client(self, asyncio: bool = False):
        return self.async_redis if asyncio else self.redis

    async def get_and_delayed_deletion(self, key: str, ex: int):
        """è·å–ç»“æœå¹¶å»¶è¿Ÿåˆ é™¤ï¼ˆä»hashä¸­ï¼‰"""
        result = await self.async_redis.hget(key, "result")
        await self.async_redis.expire(key, ex)
        return result

    # ==================== å®šæ—¶ä»»åŠ¡è°ƒåº¦ç›¸å…³ ====================
    
    async def _ensure_scheduler_initialized(self, db_url: str = None):
        """ç¡®ä¿è°ƒåº¦å™¨å·²åˆå§‹åŒ–ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        if not self.scheduler:
            logger.debug("Auto-initializing scheduler...")
            # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„db_urlï¼Œç„¶åæ˜¯å®ä¾‹åŒ–æ—¶çš„pg_urlï¼Œæœ€åæ˜¯ç¯å¢ƒå˜é‡
            if not db_url:
                db_url = self.pg_url or os.environ.get('JETTASK_PG_URL')
            if not db_url:
                raise ValueError(
                    "Database URL not provided. Please provide pg_url when initializing Jettask, "
                    "or set JETTASK_PG_URL environment variable\n"
                    "Example: app = Jettask(redis_url='...', pg_url='postgresql://...')\n"
                    "Or: export JETTASK_PG_URL='postgresql://user:password@localhost:5432/jettask'"
                )

            self._scheduler_db_url = db_url

            # åˆ›å»ºè°ƒåº¦å™¨
            scheduler_config = self.scheduler_config.copy()
            scheduler_config.setdefault('scan_interval', 1.0)  # é»˜è®¤30ç§’æ‰«æä¸€æ¬¡
            scheduler_config.setdefault('batch_size', 100)
            scheduler_config.setdefault('lookahead_seconds', 3600)  # é»˜è®¤æå‰1å°æ—¶

            self.scheduler = TaskScheduler(
                app=self,
                db_url=db_url,
                **scheduler_config
            )

            logger.debug("Scheduler initialized")
    
    async def start_scheduler(self):
        """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨ï¼ˆè‡ªåŠ¨åˆå§‹åŒ–ï¼Œé˜»å¡å¼ï¼‰"""
        # è‡ªåŠ¨åˆå§‹åŒ–è°ƒåº¦å™¨
        await self._ensure_scheduler_initialized()

        try:
            # wait=True è¡¨ç¤ºé˜»å¡ç­‰å¾…è°ƒåº¦å™¨å®Œæˆ
            await self.scheduler.start(wait=True)
        except Exception as e:
            logger.error(f"Scheduler error: {e}", exc_info=True)
            raise
    
    async def register_schedules(self, schedules):
        """
        æ³¨å†Œå®šæ—¶ä»»åŠ¡ï¼ˆæ”¯æŒå•ä¸ªæˆ–æ‰¹é‡ï¼‰

        è¿™æ˜¯æ–°çš„ç»Ÿä¸€æ³¨å†Œæ–¹æ³•ï¼Œç±»ä¼¼ TaskMessage çš„è®¾è®¡æ¨¡å¼ã€‚

        Args:
            schedules: ScheduledMessage å¯¹è±¡æˆ–åˆ—è¡¨

        Returns:
            æ³¨å†Œçš„ä»»åŠ¡æ•°é‡

        Example:
            from jettask import Schedule

            # 1. å®šä¹‰å®šæ—¶ä»»åŠ¡
            schedule1 = Schedule(
                scheduler_id="notify_every_30s",
                queue="notification_queue",
                interval_seconds=30,
                kwargs={"user_id": "user_123", "message": "å®šæ—¶æé†’"}
            )

            schedule2 = Schedule(
                scheduler_id="report_cron",
                queue="report_queue",
                cron_expression="0 9 * * *",
                description="æ¯å¤©ç”ŸæˆæŠ¥å‘Š"
            )

            # 2. æ‰¹é‡æ³¨å†Œ
            count = await app.register_schedules([schedule1, schedule2])
            print(f"æ³¨å†Œäº† {count} ä¸ªå®šæ—¶ä»»åŠ¡")
        """
        from ..scheduler.definition import Schedule
        from ..db.models import ScheduledTask, TaskType

        # è‡ªåŠ¨åˆå§‹åŒ–
        await self._ensure_scheduler_initialized()

        # æ”¯æŒå•ä¸ªæˆ–åˆ—è¡¨
        if isinstance(schedules, Schedule):
            schedules = [schedules]

        if not schedules:
            return 0

        # è·å–å½“å‰å‘½åç©ºé—´
        namespace = 'default'
        if self.task_center and hasattr(self.task_center, 'namespace_name'):
            namespace = self.task_center.namespace_name
        elif self.redis_prefix and self.redis_prefix != 'jettask':
            namespace = self.redis_prefix

        # è½¬æ¢ä¸º ScheduledTask å¯¹è±¡
        tasks = []
        for schedule in schedules:
            if not isinstance(schedule, Schedule):
                raise ValueError(f"Expected Schedule, got {type(schedule)}")

            data = schedule.to_dict()
            task = ScheduledTask(
                scheduler_id=data['scheduler_id'],
                task_type=data['task_type'],  # ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²å€¼
                queue_name=data['queue'],
                namespace=namespace,
                task_args=data['task_args'],
                task_kwargs=data['task_kwargs'],
                interval_seconds=data.get('interval_seconds'),
                cron_expression=data.get('cron_expression'),
                next_run_time=data.get('next_run_time'),
                enabled=data['enabled'],
                priority=data['priority'],
                timeout=data['timeout'],
                max_retries=data['max_retries'],
                retry_delay=data['retry_delay'],
                description=data['description'],
                tags=data['tags'],
                metadata=data['metadata']
            )
            # ä¿å­˜ skip_if_exists é€‰é¡¹
            task._skip_if_exists = schedule.skip_if_exists
            tasks.append(task)

        # æ‰¹é‡æ³¨å†Œåˆ°æ•°æ®åº“
        from ..db.connector import get_pg_engine_and_factory

        _, session_factory = get_pg_engine_and_factory(
            dsn=self._scheduler_db_url,
            pool_size=10,
            max_overflow=20,
            pool_recycle=3600,
            echo=False
        )
        session = session_factory()
        try:
            registered_count = 0
            for task in tasks:
                skip_if_exists = getattr(task, '_skip_if_exists', True)

                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å­˜åœ¨
                existing_task = await ScheduledTask.get_by_scheduler_id(session, task.scheduler_id)

                if existing_task:
                    if not skip_if_exists:
                        # æ›´æ–°ç°æœ‰ä»»åŠ¡
                        task.id = existing_task.id
                        await ScheduledTask.update_task(session, task)
                        await session.commit()
                        logger.debug(f"å·²æ›´æ–°å®šæ—¶ä»»åŠ¡: {task.scheduler_id} -> {task.queue_name}")
                    else:
                        logger.debug(f"å®šæ—¶ä»»åŠ¡å·²å­˜åœ¨: {task.scheduler_id}")
                else:
                    # åˆ›å»ºæ–°ä»»åŠ¡
                    await ScheduledTask.create(session, task)
                    await session.commit()
                    registered_count += 1
                    logger.debug(f"å·²æ³¨å†Œå®šæ—¶ä»»åŠ¡: {task.scheduler_id} -> {task.queue_name}")

            return registered_count
        finally:
            await session.close()

    async def list_schedules(self, **filters):
        """
        åˆ—å‡ºå®šæ—¶ä»»åŠ¡

        Args:
            **filters: è¿‡æ»¤æ¡ä»¶ï¼ˆenabled, queue_name, task_type ç­‰ï¼‰

        Returns:
            List[ScheduledTask]: ä»»åŠ¡åˆ—è¡¨
        """
        await self._ensure_scheduler_initialized()

        from ..db.connector import get_pg_engine_and_factory
        from ..db.models import ScheduledTask

        _, session_factory = get_pg_engine_and_factory(
            dsn=self._scheduler_db_url,
            pool_size=10,
            max_overflow=20,
            pool_recycle=3600,
            echo=False
        )
        session = session_factory()
        try:
            return await ScheduledTask.list_tasks(session, **filters)
        finally:
            await session.close()

    async def remove_schedule(self, scheduler_id: str) -> bool:
        """
        ç§»é™¤å®šæ—¶ä»»åŠ¡

        Args:
            scheduler_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦

        Returns:
            bool: æ˜¯å¦æˆåŠŸç§»é™¤
        """
        await self._ensure_scheduler_initialized()

        from ..db.connector import get_pg_engine_and_factory
        from ..db.models import ScheduledTask

        _, session_factory = get_pg_engine_and_factory(
            dsn=self._scheduler_db_url,
            pool_size=10,
            max_overflow=20,
            pool_recycle=3600,
            echo=False
        )
        session = session_factory()
        try:
            task = await ScheduledTask.get_by_scheduler_id(session, scheduler_id)
            if not task:
                return False

            await ScheduledTask.delete_by_id(session, task.id)
            await session.commit()
            return True
        finally:
            await session.close()

    async def pause_schedule(self, scheduler_id: str) -> bool:
        """
        æš‚åœå®šæ—¶ä»»åŠ¡

        Args:
            scheduler_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦

        Returns:
            bool: æ˜¯å¦æˆåŠŸæš‚åœ
        """
        await self._ensure_scheduler_initialized()

        from ..db.connector import get_pg_engine_and_factory
        from ..db.models import ScheduledTask

        _, session_factory = get_pg_engine_and_factory(
            dsn=self._scheduler_db_url,
            pool_size=10,
            max_overflow=20,
            pool_recycle=3600,
            echo=False
        )
        session = session_factory()
        try:
            task = await ScheduledTask.get_by_scheduler_id(session, scheduler_id)
            if not task:
                return False

            task.enabled = False
            await ScheduledTask.update_task(session, task)
            await session.commit()
            return True
        finally:
            await session.close()

    async def resume_schedule(self, scheduler_id: str) -> bool:
        """
        æ¢å¤å®šæ—¶ä»»åŠ¡

        Args:
            scheduler_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ¢å¤
        """
        await self._ensure_scheduler_initialized()

        from ..db.connector import get_pg_engine_and_factory
        from ..db.models import ScheduledTask

        _, session_factory = get_pg_engine_and_factory(
            dsn=self._scheduler_db_url,
            pool_size=10,
            max_overflow=20,
            pool_recycle=3600,
            echo=False
        )
        session = session_factory()
        try:
            task = await ScheduledTask.get_by_scheduler_id(session, scheduler_id)
            if not task:
                return False

            task.enabled = True
            await ScheduledTask.update_task(session, task)
            await session.commit()
            return True
        finally:
            await session.close()

