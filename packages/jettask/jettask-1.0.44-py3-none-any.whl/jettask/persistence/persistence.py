"""ä»»åŠ¡æŒä¹…åŒ–æ¨¡å—

è´Ÿè´£è§£æRedis Streamæ¶ˆæ¯ï¼Œå¹¶å°†ä»»åŠ¡æ•°æ®æ‰¹é‡æ’å…¥PostgreSQLæ•°æ®åº“ã€‚
"""

import logging
from typing import Dict, List, Any
from datetime import datetime, timezone

from sqlalchemy.orm import sessionmaker
from sqlalchemy.dialects.postgresql import insert

from jettask.db.models.task import Task
from jettask.db.models.task_metrics_minute import TaskMetricsMinute
from jettask.db.models.task_runs_metrics_minute import TaskRunsMetricsMinute

logger = logging.getLogger(__name__)


class TaskPersistence:
    """ä»»åŠ¡æŒä¹…åŒ–å¤„ç†å™¨

    èŒè´£ï¼š
    - è§£æStreamæ¶ˆæ¯ä¸ºä»»åŠ¡ä¿¡æ¯
    - æ‰¹é‡æ’å…¥ä»»åŠ¡åˆ°PostgreSQLçš„tasksè¡¨
    - å¤„ç†æ’å…¥å¤±è´¥çš„é™çº§ç­–ç•¥
    """

    def __init__(
        self,
        async_session_local: sessionmaker,
        namespace_id: str,
        namespace_name: str
    ):
        """åˆå§‹åŒ–ä»»åŠ¡æŒä¹…åŒ–å¤„ç†å™¨

        Args:
            async_session_local: SQLAlchemyä¼šè¯å·¥å‚
            namespace_id: å‘½åç©ºé—´ID
            namespace_name: å‘½åç©ºé—´åç§°
        """
        self.AsyncSessionLocal = async_session_local
        self.namespace_id = namespace_id
        self.namespace_name = namespace_name


    async def batch_insert_tasks(self, tasks: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ’å…¥ä»»åŠ¡ï¼ˆå…¼å®¹ buffer.py è°ƒç”¨æ¥å£ï¼‰

        Args:
            tasks: ä»»åŠ¡è®°å½•åˆ—è¡¨

        Returns:
            å®é™…æ’å…¥çš„è®°å½•æ•°
        """
        if not tasks:
            return 0

        logger.info(f"[BATCH INSERT] æ‰¹é‡æ’å…¥ {len(tasks)} æ¡ä»»åŠ¡...")

        try:
            async with self.AsyncSessionLocal() as session:
                # å‡†å¤‡ ORM æ•°æ®
                # ğŸ”§ å…³é”®è¯´æ˜ï¼š
                # - tasks æ˜¯åˆ†åŒºè¡¨ï¼ŒæŒ‰ trigger_time åˆ†åŒºï¼Œä¸»é”®æ˜¯ (stream_id, trigger_time)
                # - trigger_time æ˜¯ä»»åŠ¡è§¦å‘æ—¶é—´ï¼ˆTIMESTAMP ç±»å‹ï¼‰
                # - created_at æ˜¯è®°å½•çš„å®é™…æ’å…¥æ—¶é—´ï¼ˆDEFAULT NOW()ï¼‰
                # - åŒä¸€ä¸ª stream_id åªä¼šæ’å…¥ä¸€æ¬¡ï¼Œä¸šåŠ¡é€»è¾‘ä¿è¯ä¸ä¼šé‡å¤
                insert_data = []
                for record in tasks:
                    # record æ˜¯ä» consumer.py ä¼ å…¥çš„æ ¼å¼
                    scheduled_task_id = record.get('scheduled_task_id')
                    trigger_time = record.get('trigger_time')

                    # å°† Unix æ—¶é—´æˆ³è½¬æ¢ä¸º datetime å¯¹è±¡
                    if isinstance(trigger_time, (int, float)):
                        trigger_time = datetime.fromtimestamp(trigger_time, timezone.utc)

                    insert_data.append({
                        'stream_id': record['stream_id'],
                        'queue': record['queue'],
                        'namespace': record['namespace'],
                        'scheduled_task_id': str(scheduled_task_id) if scheduled_task_id is not None else None,
                        'payload': record.get('payload', {}),
                        'priority': record.get('priority', 0),
                        'delay': record.get('delay', 0),
                        'trigger_time': trigger_time,
                        # created_at ç”±æ•°æ®åº“ DEFAULT NOW() è‡ªåŠ¨è®¾ç½®
                        'source': record.get('source', 'redis_stream'),
                        'task_metadata': record.get('metadata', {})
                    })

                # æ‰¹é‡æ’å…¥ - ä½¿ç”¨ PostgreSQL çš„ INSERT ON CONFLICT DO NOTHING
                # ä½¿ç”¨çº¦æŸåç§°è€Œä¸æ˜¯åˆ—å
                stmt = insert(Task).values(insert_data).on_conflict_do_nothing(
                    constraint='tasks_pkey'
                )

                await session.execute(stmt)

                # åŒæ­¥æ›´æ–°èšåˆè¡¨ï¼ˆæŒ‰åˆ†é’Ÿç²’åº¦ï¼‰
                await self._update_metrics_aggregation(session, insert_data)

                await session.commit()

                logger.info(f"[BATCH INSERT] âœ“ æˆåŠŸæ’å…¥ {len(insert_data)} æ¡ä»»åŠ¡")
                return len(insert_data)

        except Exception as e:
            logger.error(f"[BATCH INSERT] âœ— æ‰¹é‡æ’å…¥å¤±è´¥: {e}", exc_info=True)
            return 0

    async def batch_update_tasks(self, updates: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ›´æ–°ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€åˆ° task_runs è¡¨

        ä½¿ç”¨ PostgreSQL çš„ INSERT ... ON CONFLICT DO UPDATE å®ç° UPSERT æ“ä½œï¼Œ
        å¦‚æœè®°å½•å­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ã€‚

        Args:
            updates: æ›´æ–°è®°å½•åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å«ï¼š
                - stream_id: Redis Stream IDï¼ˆä¸»é”®ï¼‰
                - status: ä»»åŠ¡çŠ¶æ€
                - result: æ‰§è¡Œç»“æœ
                - error: é”™è¯¯ä¿¡æ¯
                - started_at: å¼€å§‹æ—¶é—´
                - completed_at: å®Œæˆæ—¶é—´
                - retries: é‡è¯•æ¬¡æ•°

        Returns:
            å®é™…æ›´æ–°çš„è®°å½•æ•°
        """
        if not updates:
            return 0

        # logger.info(f"[BATCH UPDATE] æ‰¹é‡æ›´æ–° {len(updates)} æ¡ä»»åŠ¡çŠ¶æ€...")
        # logger.info(f"[BATCH UPDATE] æ›´æ–°è®°å½•ç¤ºä¾‹: {updates[0] if updates else 'N/A'}")

        try:
            from sqlalchemy.dialects.postgresql import insert
            from ..db.models import TaskRun
            from ..utils.serializer import loads_str
            from datetime import datetime, timezone

            # å¯¹ç›¸åŒ stream_id çš„è®°å½•è¿›è¡Œå»é‡ï¼Œä¿ç•™æœ€æ–°çš„
            # ä½¿ç”¨å­—å…¸ï¼Œkey æ˜¯ stream_idï¼Œvalue æ˜¯è®°å½•ï¼ˆåé¢çš„ä¼šè¦†ç›–å‰é¢çš„ï¼‰
            deduplicated = {}
            for record in updates:
                stream_id = record['stream_id']
                deduplicated[stream_id] = record

            # è½¬æ¢å›åˆ—è¡¨
            unique_updates = list(deduplicated.values())

            if len(unique_updates) < len(updates):
                logger.info(
                    f"[BATCH UPDATE] å»é‡: {len(updates)} æ¡ â†’ {len(unique_updates)} æ¡ "
                    f"(åˆå¹¶äº† {len(updates) - len(unique_updates)} æ¡é‡å¤è®°å½•)"
                )

            async with self.AsyncSessionLocal() as session:
                # å‡†å¤‡ UPSERT æ•°æ®ï¼ˆç”¨äºå†™å…¥ task_runs è¡¨ï¼‰
                upsert_data = []
                # å‡†å¤‡èšåˆç»Ÿè®¡æ•°æ®ï¼ˆåŒ…å«é¢å¤–å­—æ®µç”¨äºç»Ÿè®¡ï¼‰
                aggregation_data = []

                for record in unique_updates:
                    logger.debug(f"å¤„ç†è®°å½•: {record}")
                    # è§£æ result å­—æ®µï¼ˆå¦‚æœæ˜¯åºåˆ—åŒ–çš„å­—ç¬¦ä¸²ï¼‰
                    result = record.get('result')
                    if result and isinstance(result, bytes):
                        try:
                            result = loads_str(result)
                        except Exception:
                            result = result.decode('utf-8') if isinstance(result, bytes) else result

                    # è§£æ error å­—æ®µ
                    error = record.get('error')
                    if error and isinstance(error, bytes):
                        error = error.decode('utf-8')

                    # ğŸ”§ è·å– trigger_timeï¼ˆä»»åŠ¡è§¦å‘æ—¶é—´ï¼Œä¸ä¼šå˜åŒ–ï¼‰
                    trigger_time = record.get('trigger_time')
                    if trigger_time is None:
                        # å¦‚æœæ²¡æœ‰ trigger_timeï¼Œä½¿ç”¨ started_at ä½œä¸ºåå¤‡
                        # ï¼ˆå…¼å®¹æ—§æ•°æ®ï¼Œæ–°æ•°æ®å¿…é¡»æœ‰ trigger_timeï¼‰
                        trigger_time = record.get('started_at')
                        logger.warning(f"Record missing trigger_time, using started_at as fallback: {record.get('stream_id')}")

                    # è·å–å¹¶è½¬æ¢æ—¶é—´å­—æ®µ
                    started_at = record.get('started_at')
                    completed_at = record.get('completed_at')

                    # å°† Unix æ—¶é—´æˆ³è½¬æ¢ä¸º datetime å¯¹è±¡
                    if isinstance(trigger_time, (int, float)):
                        trigger_time_dt = datetime.fromtimestamp(trigger_time, timezone.utc)
                    else:
                        trigger_time_dt = trigger_time

                    if isinstance(started_at, (int, float)):
                        started_at_dt = datetime.fromtimestamp(started_at, timezone.utc)
                    else:
                        started_at_dt = started_at

                    if isinstance(completed_at, (int, float)):
                        completed_at_dt = datetime.fromtimestamp(completed_at, timezone.utc)
                    else:
                        completed_at_dt = completed_at

                    # è®¡ç®—æ‰§è¡Œæ—¶é•¿ï¼ˆä½¿ç”¨åŸå§‹Unixæ—¶é—´æˆ³ï¼‰
                    duration = None
                    if started_at and completed_at:
                        duration = completed_at - started_at

                    # è§£æ status å­—æ®µ
                    status = record.get('status')
                    if status and isinstance(status, bytes):
                        status = status.decode('utf-8')

                    # è§£æ consumer å­—æ®µ
                    consumer = record.get('consumer')
                    if consumer and isinstance(consumer, bytes):
                        consumer = consumer.decode('utf-8')

                    # ğŸ”§ è·å– task_nameï¼ˆå·²ä» consumer æå–ï¼‰
                    task_name = record.get('task_name')

                    # task_runs è¡¨è®°å½•
                    # ğŸ”§ å…³é”®è¯´æ˜ï¼š
                    # - task_runs æ˜¯åˆ†åŒºè¡¨ï¼ŒæŒ‰ trigger_time åˆ†åŒºï¼Œä¸»é”®æ˜¯ (task_name, trigger_time, stream_id)
                    # - ä¸»é”®é¡ºåºæŒ‰ç²’åº¦ä»ç²—åˆ°ç»†ï¼štask_name > trigger_time > stream_id
                    # - task_name: ä»»åŠ¡åç§°ï¼ˆç²—ç²’åº¦ï¼‰
                    # - trigger_time: ä»»åŠ¡è§¦å‘æ—¶é—´ï¼ˆåˆ†åŒºé”®ï¼‰ï¼Œåœ¨ä»»åŠ¡åˆ›å»ºæ—¶ç¡®å®šï¼Œåç»­ä¸ä¼šå˜åŒ–ï¼ˆå³ä½¿é‡è¯•ï¼‰
                    # - stream_id: Redis Stream IDï¼ˆç»†ç²’åº¦ï¼‰
                    # - started_at: ä»»åŠ¡å®é™…å¼€å§‹æ‰§è¡Œæ—¶é—´ï¼Œå¯èƒ½å› é‡è¯•è€Œå˜åŒ–
                    # - created_at: è®°å½•çš„å®é™…æ’å…¥æ—¶é—´ï¼ˆDEFAULT NOW()ï¼‰
                    # - UPSERT èƒ½é€šè¿‡ (task_name, trigger_time, stream_id) æ­£ç¡®åŒ¹é…å·²æœ‰è®°å½•ï¼Œé¿å…é‡å¤
                    upsert_record = {
                        'task_name': task_name,
                        'trigger_time': trigger_time_dt,
                        'stream_id': record['stream_id'],
                        'status': status,
                        'result': result,
                        'error': error,
                        'started_at': started_at_dt,
                        'completed_at': completed_at_dt,
                        'retries': record.get('retries', 0),
                        'duration': duration,
                        'consumer': consumer,
                        # created_at ç”±æ•°æ®åº“ DEFAULT NOW() è‡ªåŠ¨è®¾ç½®
                        'updated_at': datetime.now(timezone.utc),
                    }
                    logger.debug(f"upsert_record: {upsert_record}")
                    upsert_data.append(upsert_record)

                    # èšåˆç»Ÿè®¡æ•°æ®ï¼ˆåŒ…å« queue, namespace, trigger_timeï¼‰
                    aggregation_record = {
                        'stream_id': record['stream_id'],
                        'task_name': task_name,
                        'status': status,
                        'started_at': started_at,
                        'completed_at': completed_at,
                        'retries': record.get('retries', 0),
                        'duration': duration,
                        # è¿™äº›å­—æ®µæ¥è‡ªåŸå§‹ recordï¼Œç”¨äºèšåˆç»Ÿè®¡
                        'queue': record.get('queue'),
                        'namespace': record.get('namespace'),
                        'trigger_time': record.get('trigger_time'),  # ç”¨äºè®¡ç®—æ—¶é—´æ¡¶
                    }
                    aggregation_data.append(aggregation_record)

                logger.info(f"[BATCH UPDATE] å‡†å¤‡å†™å…¥ {len(upsert_data)} æ¡è®°å½•")
    
                # æ‰¹é‡ UPSERT - å¦‚æœå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥
                stmt = insert(TaskRun).values(upsert_data)

                # å®šä¹‰å†²çªæ—¶çš„æ›´æ–°ç­–ç•¥
                # ä½¿ç”¨ COALESCE é¿å…ç”¨ NULL è¦†ç›–å·²æœ‰æ•°æ®
                from sqlalchemy import func
                stmt = stmt.on_conflict_do_update(
                    constraint='task_runs_pkey',  # ä¸»é”®ï¼š(task_name, trigger_time, stream_id)
                    set_={
                        # status æ€»æ˜¯æ›´æ–°ï¼ˆçŠ¶æ€å˜åŒ–ï¼‰
                        'status': stmt.excluded.status,
                        # å…¶ä»–å­—æ®µï¼šå¦‚æœæ–°å€¼ä¸æ˜¯ NULLï¼Œåˆ™æ›´æ–°ï¼›å¦åˆ™ä¿ç•™æ—§å€¼
                        'result': func.coalesce(stmt.excluded.result, TaskRun.result),
                        'error': func.coalesce(stmt.excluded.error, TaskRun.error),
                        'started_at': func.coalesce(stmt.excluded.started_at, TaskRun.started_at),
                        'completed_at': func.coalesce(stmt.excluded.completed_at, TaskRun.completed_at),
                        'retries': func.coalesce(stmt.excluded.retries, TaskRun.retries),
                        'duration': func.coalesce(stmt.excluded.duration, TaskRun.duration),
                        'consumer': func.coalesce(stmt.excluded.consumer, TaskRun.consumer),
                        'task_name': func.coalesce(stmt.excluded.task_name, TaskRun.task_name),
                        # trigger_time æ˜¯ä¸»é”®çš„ä¸€éƒ¨åˆ†ï¼Œä¸èƒ½æ›´æ–°
                        # created_at åœ¨é¦–æ¬¡æ’å…¥æ—¶ç”±æ•°æ®åº“ DEFAULT NOW() è®¾ç½®ï¼Œåç»­æ›´æ–°ä¸ä¼šæ”¹å˜
                        # updated_at æ€»æ˜¯æ›´æ–°ä¸ºå½“å‰æ—¶é—´
                        'updated_at': stmt.excluded.updated_at,
                    }
                )

                await session.execute(stmt)

                # ğŸ”§ åŒæ­¥æ›´æ–°èšåˆç»Ÿè®¡è¡¨ï¼ˆä½¿ç”¨åŒ…å« queue/namespace/trigger_time çš„æ•°æ®ï¼‰
                await self._update_task_runs_metrics_aggregation(session, aggregation_data)

                await session.commit()

                logger.info(f"[BATCH UPDATE] âœ“ æˆåŠŸæ›´æ–° {len(upsert_data)} æ¡ä»»åŠ¡çŠ¶æ€")
                return len(upsert_data)

        except Exception as e:
            logger.error(f"[BATCH UPDATE] âœ— æ‰¹é‡æ›´æ–°å¤±è´¥: {e}", exc_info=True)
            return 0

    async def _update_metrics_aggregation(self, session, tasks_data: List[Dict[str, Any]]) -> None:
        """
        æ›´æ–°ä»»åŠ¡æŒ‡æ ‡èšåˆè¡¨ï¼ˆæŒ‰åˆ†é’Ÿç²’åº¦ï¼‰

        åœ¨åŒä¸€äº‹åŠ¡ä¸­ï¼Œå°†æ–°æ’å…¥çš„ä»»åŠ¡ç»Ÿè®¡åˆ°èšåˆè¡¨ä¸­ã€‚
        ä½¿ç”¨ INSERT ON CONFLICT DO UPDATE æ¥å¤„ç†å¹¶å‘æ›´æ–°ã€‚

        Args:
            session: æ•°æ®åº“ä¼šè¯ï¼ˆåœ¨åŒä¸€äº‹åŠ¡ä¸­ï¼‰
            tasks_data: ä»»åŠ¡æ•°æ®åˆ—è¡¨
        """
        if not tasks_data:
            return

        logger.debug(f"Updating metrics aggregation for {len(tasks_data)} tasks")

        # æŒ‰ç…§ (namespace, queue, time_bucket) åˆ†ç»„ç»Ÿè®¡
        from collections import defaultdict

        metrics_map = defaultdict(int)

        for task in tasks_data:
            # è·å–ä»»åŠ¡è§¦å‘æ—¶é—´ï¼ˆç”¨äºèšåˆç»Ÿè®¡ï¼‰
            trigger_time = task.get('trigger_time')
            if not trigger_time:
                continue

            # å°† Unix æ—¶é—´æˆ³è½¬æ¢ä¸º datetime
            if isinstance(trigger_time, (int, float)):
                trigger_datetime = datetime.fromtimestamp(trigger_time, timezone.utc)
            elif isinstance(trigger_time, str):
                trigger_datetime = datetime.fromisoformat(trigger_time.replace('Z', '+00:00'))
            else:
                trigger_datetime = trigger_time

            # è®¡ç®—åˆ†é’Ÿçº§åˆ«çš„æ—¶é—´æ¡¶ï¼ˆå»æ‰ç§’å’Œå¾®ç§’ï¼‰
            time_bucket = trigger_datetime.replace(second=0, microsecond=0)

            # åˆ†ç»„é”®ï¼š(namespace, queue, time_bucket)
            key = (
                task['namespace'],
                task['queue'],
                time_bucket
            )

            metrics_map[key] += 1

        # æ‰¹é‡æ›´æ–°èšåˆè¡¨
        metrics_data = []
        for (namespace, queue, time_bucket), count in metrics_map.items():
            metrics_data.append({
                'namespace': namespace,
                'queue': queue,
                'time_bucket': time_bucket,
                'task_count': count,
                'updated_at': datetime.now(timezone.utc)
            })

        if not metrics_data:
            return

        # ä½¿ç”¨ INSERT ON CONFLICT DO UPDATE æ¥é€’å¢è®¡æ•°å™¨
        stmt = insert(TaskMetricsMinute).values(metrics_data)
        stmt = stmt.on_conflict_do_update(
            # ä¸»é”®å†²çªæ—¶æ›´æ–°
            index_elements=['namespace', 'queue', 'time_bucket'],
            # é€’å¢ task_countï¼Œæ›´æ–° updated_at
            set_={
                'task_count': TaskMetricsMinute.task_count + stmt.excluded.task_count,
                'updated_at': stmt.excluded.updated_at
            }
        )

        await session.execute(stmt)
        logger.debug(f"Updated {len(metrics_data)} metric entries in aggregation table")

    async def _update_task_runs_metrics_aggregation(
        self, session, tasks_data: List[Dict[str, Any]]
    ) -> None:
        """
        æ›´æ–°ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡èšåˆè¡¨ï¼ˆæŒ‰åˆ†é’Ÿç²’åº¦ï¼‰

        åœ¨åŒä¸€äº‹åŠ¡ä¸­ï¼Œå°†ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€ç»Ÿè®¡åˆ°èšåˆè¡¨ä¸­ã€‚
        ä½¿ç”¨ INSERT ON CONFLICT DO UPDATE æ¥å¤„ç†å¹¶å‘æ›´æ–°ã€‚

        Args:
            session: æ•°æ®åº“ä¼šè¯ï¼ˆåœ¨åŒä¸€äº‹åŠ¡ä¸­ï¼‰
            tasks_data: ä»»åŠ¡æ‰§è¡Œæ•°æ®åˆ—è¡¨ï¼ˆåŒ…å« status, duration, trigger_time, started_at ç­‰ï¼‰
        """
        if not tasks_data:
            return

        logger.debug(f"Updating task_runs metrics aggregation for {len(tasks_data)} tasks")

        from sqlalchemy import func

        # æŒ‰ç…§ (time_bucket, namespace, queue, task_name) åˆ†ç»„ç»Ÿè®¡
        # ä½¿ç”¨å­—å…¸å­˜å‚¨èšåˆæ•°æ®
        metrics_map = {}

        for task in tasks_data:
            # è·å–ä»»åŠ¡å¼€å§‹æ—¶é—´ï¼ˆç”¨äºç¡®å®šæ—¶é—´æ¡¶ï¼‰
            started_at = task.get('started_at')
            if not started_at:
                # å¦‚æœæ²¡æœ‰å¼€å§‹æ—¶é—´ï¼Œè·³è¿‡
                continue

            # è®¡ç®—åˆ†é’Ÿçº§åˆ«çš„æ—¶é—´æ¡¶
            # å°† Unix æ—¶é—´æˆ³è½¬æ¢ä¸º datetimeï¼Œç„¶åå»æ‰ç§’å’Œå¾®ç§’
            bucket_dt = datetime.fromtimestamp(started_at, tz=timezone.utc)
            time_bucket = bucket_dt.replace(second=0, microsecond=0)

            # è·å–å‘½åç©ºé—´
            namespace = task.get('namespace') or 'default'

            # è·å–é˜Ÿåˆ—åç§°
            queue = task.get('queue') or 'unknown'

            # è·å–ä»»åŠ¡åç§°
            task_name = task.get('task_name') or 'unknown'

            # åˆ†ç»„é”®ï¼š(time_bucket, namespace, queue, task_name)
            key = (time_bucket, namespace, queue, task_name)

            # åˆå§‹åŒ–è¯¥é”®çš„èšåˆæ•°æ®
            if key not in metrics_map:
                metrics_map[key] = {
                    'total_count': 0,
                    'success_count': 0,
                    'failed_count': 0,
                    'retry_count': 0,
                    'total_duration': 0.0,
                    'max_duration': None,
                    'min_duration': None,
                    'total_delay': 0.0,
                    'max_delay': None,
                    'min_delay': None,
                    'running_concurrency': 0,
                }

            metrics = metrics_map[key]

            # æ›´æ–°è®¡æ•°
            metrics['total_count'] += 1

            # æ ¹æ®çŠ¶æ€æ›´æ–°æˆåŠŸ/å¤±è´¥è®¡æ•°
            status = task.get('status')
            if status == 'success':
                metrics['success_count'] += 1
            elif status in ('failed', 'error'):
                metrics['failed_count'] += 1

            # ç´¯åŠ é‡è¯•æ¬¡æ•°
            retries = task.get('retries') or 0
            metrics['retry_count'] += retries

            # ç´¯åŠ æ‰§è¡Œæ—¶é•¿
            duration = task.get('duration')
            if duration is not None and duration > 0:
                metrics['total_duration'] += duration
                # æ›´æ–°æœ€å¤§/æœ€å°æ‰§è¡Œæ—¶é—´
                if metrics['max_duration'] is None or duration > metrics['max_duration']:
                    metrics['max_duration'] = duration
                if metrics['min_duration'] is None or duration < metrics['min_duration']:
                    metrics['min_duration'] = duration

            # è®¡ç®—æ‰§è¡Œå»¶è¿Ÿ (started_at - trigger_time)
            trigger_time = task.get('trigger_time')
            if trigger_time is not None and started_at is not None:
                delay = started_at - trigger_time
                if delay >= 0:  # åªå¤„ç†æ­£å»¶è¿Ÿ
                    metrics['total_delay'] += delay
                    # æ›´æ–°æœ€å¤§/æœ€å°å»¶è¿Ÿ
                    if metrics['max_delay'] is None or delay > metrics['max_delay']:
                        metrics['max_delay'] = delay
                    if metrics['min_delay'] is None or delay < metrics['min_delay']:
                        metrics['min_delay'] = delay

            # å¹¶å‘è®¡æ•°ï¼ˆç®€å•å®ç°ï¼šæ¯ä¸ªä»»åŠ¡åœ¨å…¶å¼€å§‹åˆ†é’Ÿå†…è®¡æ•° +1ï¼‰
            metrics['running_concurrency'] += 1

        if not metrics_map:
            return

        # æ‰¹é‡æ›´æ–°èšåˆè¡¨
        metrics_data = []
        for (time_bucket, namespace, queue, task_name), metrics in metrics_map.items():
            metrics_data.append({
                'time_bucket': time_bucket,
                'namespace': namespace,  # ğŸ”§ æ·»åŠ  namespace
                'queue': queue,  # ğŸ”§ æ·»åŠ  queue
                'task_name': task_name,
                'total_count': metrics['total_count'],
                'success_count': metrics['success_count'],
                'failed_count': metrics['failed_count'],
                'retry_count': metrics['retry_count'],
                'total_duration': metrics['total_duration'],
                'max_duration': metrics['max_duration'],
                'min_duration': metrics['min_duration'],
                'total_delay': metrics['total_delay'],
                'max_delay': metrics['max_delay'],
                'min_delay': metrics['min_delay'],
                'running_concurrency': metrics['running_concurrency'],
                'updated_at': datetime.now(timezone.utc)
            })

        if not metrics_data:
            return

        # ä½¿ç”¨ INSERT ON CONFLICT DO UPDATE
        stmt = insert(TaskRunsMetricsMinute).values(metrics_data)
        stmt = stmt.on_conflict_do_update(
            # ä¸»é”®å†²çªæ—¶æ›´æ–°ï¼ˆä¸»é”®ä¸º time_bucket, namespace, queue, task_nameï¼‰
            index_elements=['time_bucket', 'namespace', 'queue', 'task_name'],
            set_={
                # ç´¯åŠ è®¡æ•°ç±»æŒ‡æ ‡
                'total_count': TaskRunsMetricsMinute.total_count + stmt.excluded.total_count,
                'success_count': TaskRunsMetricsMinute.success_count + stmt.excluded.success_count,
                'failed_count': TaskRunsMetricsMinute.failed_count + stmt.excluded.failed_count,
                'retry_count': TaskRunsMetricsMinute.retry_count + stmt.excluded.retry_count,
                # ç´¯åŠ æ‰§è¡Œæ—¶é—´
                'total_duration': TaskRunsMetricsMinute.total_duration + stmt.excluded.total_duration,
                # æ›´æ–°æœ€å¤§/æœ€å°æ‰§è¡Œæ—¶é—´
                'max_duration': func.greatest(
                    func.coalesce(TaskRunsMetricsMinute.max_duration, stmt.excluded.max_duration),
                    stmt.excluded.max_duration
                ),
                'min_duration': func.least(
                    func.coalesce(TaskRunsMetricsMinute.min_duration, stmt.excluded.min_duration),
                    stmt.excluded.min_duration
                ),
                # ç´¯åŠ å»¶è¿Ÿ
                'total_delay': TaskRunsMetricsMinute.total_delay + stmt.excluded.total_delay,
                # æ›´æ–°æœ€å¤§/æœ€å°å»¶è¿Ÿ
                'max_delay': func.greatest(
                    func.coalesce(TaskRunsMetricsMinute.max_delay, stmt.excluded.max_delay),
                    stmt.excluded.max_delay
                ),
                'min_delay': func.least(
                    func.coalesce(TaskRunsMetricsMinute.min_delay, stmt.excluded.min_delay),
                    stmt.excluded.min_delay
                ),
                # æ›´æ–°å¹¶å‘å³°å€¼ï¼ˆå–æœ€å¤§å€¼ï¼‰
                'running_concurrency': func.greatest(
                    TaskRunsMetricsMinute.running_concurrency,
                    stmt.excluded.running_concurrency
                ),
                # æ›´æ–°æ—¶é—´æˆ³
                'updated_at': stmt.excluded.updated_at
            }
        )

        await session.execute(stmt)
        logger.debug(f"Updated {len(metrics_data)} task_runs metric entries in aggregation table")
