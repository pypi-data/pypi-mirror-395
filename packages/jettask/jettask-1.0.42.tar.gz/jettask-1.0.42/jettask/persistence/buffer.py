"""æ‰¹é‡ç¼“å†²åŒºç®¡ç†å™¨

è´Ÿè´£æ”¶é›†ä»»åŠ¡æ•°æ®å’ŒACKä¿¡æ¯ï¼Œæ‰¹é‡å†™å…¥æ•°æ®åº“å¹¶ACKã€‚
æ”¯æŒ INSERT å’Œ UPDATE ä¸¤ç§æ“ä½œç±»å‹ã€‚
"""

import time
import asyncio
import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class BatchBuffer:
    """æ‰¹é‡ç¼“å†²åŒºç®¡ç†å™¨

    è´Ÿè´£ï¼š
    1. æ”¶é›†ä»»åŠ¡æ•°æ®å’ŒACKä¿¡æ¯
    2. åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ·æ–°ï¼ˆæ‰¹é‡å¤§å°æˆ–è¶…æ—¶ï¼‰
    3. æ‰¹é‡å†™å…¥æ•°æ®åº“å¹¶ACK
    4. è‡ªåŠ¨å®šæ—¶åˆ·æ–°æœºåˆ¶
    """

    def __init__(
        self,
        max_size: int = 1000,
        max_delay: float = 5.0,
        operation_type: str = 'insert',  # 'insert' æˆ– 'update'
        redis_client_getter=None  # è·å– Redis å®¢æˆ·ç«¯çš„å‡½æ•°
    ):
        """åˆå§‹åŒ–ç¼“å†²åŒº

        Args:
            max_size: ç¼“å†²åŒºæœ€å¤§å®¹é‡ï¼ˆæ¡æ•°ï¼‰
            max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
            operation_type: æ“ä½œç±»å‹ï¼Œ'insert' æˆ– 'update'
            redis_client_getter: è·å– Redis å®¢æˆ·ç«¯çš„å¯è°ƒç”¨å¯¹è±¡ï¼ˆç”¨äºæ‰¹é‡è·å–æ•°æ®ï¼‰
        """
        self.max_size = max_size
        self.max_delay = max_delay
        self.operation_type = operation_type
        self.redis_client_getter = redis_client_getter

        # ä»»åŠ¡æ•°æ®ç¼“å†²åŒº
        self.records: List[Dict[str, Any]] = []
        self.contexts: List[Any] = []  # ä¿å­˜ TaskContext ç”¨äº ACK

        # åˆ·æ–°æ§åˆ¶
        self.last_flush_time = time.time()
        self.flush_lock = asyncio.Lock()

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_flushed = 0
        self.flush_count = 0

        # å®šæ—¶åˆ·æ–°ä»»åŠ¡
        self._auto_flush_task: Optional[asyncio.Task] = None
        self._running = False
        self._db_manager = None

    async def add(self, record: dict, context: Any = None):
        """æ·»åŠ åˆ°ç¼“å†²åŒºï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

        Args:
            record: ä»»åŠ¡æ•°æ®æˆ–æ›´æ–°æ•°æ®
            context: TaskContextï¼ˆç”¨äº ACKï¼‰
        """
        # ğŸ”§ è·³è¿‡ TASK_CHANGES é˜Ÿåˆ—çš„ INSERT æ“ä½œ
        if self.operation_type == 'insert' and record.get('queue') == 'TASK_CHANGES':
            logger.debug(f"è·³è¿‡ TASK_CHANGES é˜Ÿåˆ—çš„ INSERT æ“ä½œ: {record.get('stream_id')}")
            # ç›´æ¥ç¡®è®¤æ¶ˆæ¯ï¼Œä¸å†™å…¥æ•°æ®åº“
            if context and hasattr(context, 'ack'):
                try:
                    context.ack()
                    logger.debug(f"  âœ“ å·²ç¡®è®¤ TASK_CHANGES æ¶ˆæ¯: {record.get('stream_id')}")
                except Exception as e:
                    logger.error(f"  âœ— ç¡®è®¤ TASK_CHANGES æ¶ˆæ¯å¤±è´¥: {e}")
            return

        self.records.append(record)
        if context:
            self.contexts.append(context)

    def should_flush(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ·æ–°

        Returns:
            æ˜¯å¦éœ€è¦åˆ·æ–°
        """
        if not self.records:
            return False

        # ç¼“å†²åŒºæ»¡äº†
        if len(self.records) >= self.max_size:
            logger.debug(
                f"[{self.operation_type.upper()}] ç¼“å†²åŒºå·²æ»¡ "
                f"({len(self.records)}/{self.max_size})ï¼Œè§¦å‘åˆ·æ–°"
            )
            return True

        # è¶…æ—¶äº†
        elapsed = time.time() - self.last_flush_time
        if elapsed >= self.max_delay:
            logger.debug(
                f"[{self.operation_type.upper()}] ç¼“å†²åŒºè¶…æ—¶ "
                f"({elapsed:.1f}s >= {self.max_delay}s)ï¼Œè§¦å‘åˆ·æ–°"
            )
            return True

        return False

    async def flush(self, db_manager):
        """åˆ·æ–°ç¼“å†²åŒºåˆ°æ•°æ®åº“

        1. åŠ é”å¹¶æ‹·è´æ•°æ®ï¼Œç«‹å³æ¸…ç©ºåŸå§‹ç¼“å†²åŒºï¼ˆé¿å…æ•°æ®ä¸¢å¤±ï¼‰
        2. (UPDATEæ¨¡å¼) æ‰¹é‡ä» Redis è·å–ä»»åŠ¡æ•°æ®
        3. æ‰¹é‡å†™å…¥æ•°æ®åº“
        4. æ‰¹é‡ACKï¼ˆå¦‚æœæœ‰contextï¼‰

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨ï¼Œéœ€è¦æœ‰ batch_insert_tasks æˆ– batch_update_tasks æ–¹æ³•
        """
        # 1. åŠ é”å¹¶æ‹·è´æ•°æ®ï¼Œç«‹å³æ¸…ç©ºåŸå§‹ç¼“å†²åŒº
        if not self.records:
            return 0

        # æ‹·è´æ•°æ®
        records_to_process = self.records.copy()
        contexts_to_process = self.contexts.copy()
        count = len(records_to_process)

        # ç«‹å³æ¸…ç©ºåŸå§‹ç¼“å†²åŒºï¼Œé‡Šæ”¾é”ï¼ˆé¿å…é˜»å¡æ–°æ•°æ®çš„ addï¼‰
        self.records.clear()
        self.contexts.clear()
        self.last_flush_time = time.time()

        # 2. è§£é”åå¤„ç†æ•°æ®ï¼ˆä½¿ç”¨æ‹·è´çš„æ•°æ®ï¼‰
        start_time = time.time()

        try:
            logger.info(f"[{self.operation_type.upper()}] å¼€å§‹æ‰¹é‡åˆ·æ–° {count} æ¡è®°å½•...")

            # print(f'{records_to_process=}')
            # 3. (UPDATE æ¨¡å¼) æ‰¹é‡ä» Redis è·å–ä»»åŠ¡æ•°æ®
            if self.operation_type == 'update' and self.redis_client_getter:
                await self._batch_fetch_task_info_from_redis(records_to_process)

            # print(f'{records_to_process=}')
            # 4. æ‰¹é‡å†™å…¥æ•°æ®åº“
            if self.operation_type == 'insert':
                await db_manager.batch_insert_tasks(records_to_process)
                logger.info(f"  âœ“ æ‰¹é‡æ’å…¥ {count} æ¡ä»»åŠ¡è®°å½•")
            else:  # update
                await db_manager.batch_update_tasks(records_to_process)
                logger.info(f"  âœ“ æ‰¹é‡æ›´æ–° {count} æ¡ä»»åŠ¡çŠ¶æ€")
            # 5. æ‰¹é‡ACKï¼ˆä½¿ç”¨ TaskContext.acksï¼‰
            if contexts_to_process:
                # ğŸ”§ æŒ‰ context åˆ†ç»„ï¼ˆå› ä¸ºä¸åŒçš„ ctx å¯èƒ½æœ‰ä¸åŒçš„ group_nameï¼‰
                # ä½¿ç”¨ (queue, group_name) ä½œä¸ºåˆ†ç»„é”®
                ctx_groups = {}
                for ctx in contexts_to_process:
                    if hasattr(ctx, 'event_id') and hasattr(ctx, 'acks'):
                        # ä½¿ç”¨ (queue, group_name) ä½œä¸ºåˆ†ç»„é”®
                        group_key = (ctx.queue, ctx.group_name)
                        if group_key not in ctx_groups:
                            ctx_groups[group_key] = {'ctx': ctx, 'event_ids': []}
                        ctx_groups[group_key]['event_ids'].append(ctx.event_id)

                # ä¸ºæ¯ä¸ªåˆ†ç»„è°ƒç”¨ ctx.acks
                total_acked = 0
                for group_key, group_data in ctx_groups.items():
                    ctx = group_data['ctx']
                    event_ids = group_data['event_ids']
                    try:
                        ctx.acks(event_ids)
                        total_acked += len(event_ids)
                        logger.debug(
                            f"  âœ“ ACK {len(event_ids)} æ¡æ¶ˆæ¯ "
                            f"(queue={group_key[0]}, group={group_key[1]})"
                        )
                    except Exception as e:
                        logger.error(
                            f"  âœ— ACK å¤±è´¥ (queue={group_key[0]}, group={group_key[1]}): {e}"
                        )

                if total_acked > 0:
                    logger.info(f"  âœ“ æ‰¹é‡ç¡®è®¤ {total_acked} æ¡æ¶ˆæ¯")

            # 6. ç»Ÿè®¡
            self.total_flushed += count
            self.flush_count += 1
            elapsed = time.time() - start_time

            logger.info(
                f"[{self.operation_type.upper()}] âœ“ æ‰¹é‡åˆ·æ–°å®Œæˆ! "
                f"æœ¬æ¬¡: {count}æ¡, "
                f"è€—æ—¶: {elapsed:.3f}s, "
                f"æ€»è®¡: {self.total_flushed}æ¡ ({self.flush_count}æ¬¡åˆ·æ–°)"
            )

            return count

        except Exception as e:
            logger.error(
                f"[{self.operation_type.upper()}] âœ— æ‰¹é‡åˆ·æ–°å¤±è´¥: {e}",
                exc_info=True
            )
            # å¤±è´¥æ—¶æ•°æ®å·²ç»ä¸¢å¤±ï¼ˆå·²ä»ç¼“å†²åŒºç§»é™¤ï¼‰ï¼Œè®°å½•é”™è¯¯
            logger.error(f"  âœ— ä¸¢å¤± {count} æ¡è®°å½•")
            raise

    def get_stats(self) -> dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            'operation_type': self.operation_type,
            'current_size': len(self.records),
            'max_size': self.max_size,
            'total_flushed': self.total_flushed,
            'flush_count': self.flush_count,
            'avg_per_flush': self.total_flushed // self.flush_count if self.flush_count > 0 else 0
        }

    async def _auto_flush_loop(self):
        """è‡ªåŠ¨åˆ·æ–°å¾ªç¯

        å®šæœŸæ£€æŸ¥ç¼“å†²åŒºï¼Œå¦‚æœæ»¡è¶³åˆ·æ–°æ¡ä»¶åˆ™è‡ªåŠ¨åˆ·æ–°
        """
        logger.info(f"[{self.operation_type.upper()}] å¯åŠ¨è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡ï¼Œæ£€æŸ¥é—´éš”: {self.max_delay}s")

        while self._running:
            try:
                # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œä½¿ç”¨ max_delay ä½œä¸ºæ£€æŸ¥é—´éš”
                await asyncio.sleep(self.max_delay)

                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°
                if self.should_flush():
                    logger.debug(
                        f"[{self.operation_type.upper()}] è‡ªåŠ¨åˆ·æ–°è§¦å‘ï¼Œ"
                        f"ç¼“å†²åŒºå¤§å°: {len(self.records)}"
                    )
                    await self.flush(self._db_manager)

            except asyncio.CancelledError:
                logger.info(f"[{self.operation_type.upper()}] è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡è¢«å–æ¶ˆ")
                break
            except Exception as e:
                logger.error(
                    f"[{self.operation_type.upper()}] è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å‡ºé”™: {e}",
                    exc_info=True
                )
                # ç»§ç»­è¿è¡Œï¼Œä¸ä¸­æ–­å¾ªç¯

    async def start_auto_flush(self, db_manager):
        """å¯åŠ¨è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡ï¼ˆå¼‚æ­¥æ–¹æ³•ï¼‰

        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
        """
        if self._auto_flush_task is not None and not self._auto_flush_task.done():
            logger.warning(f"[{self.operation_type.upper()}] è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å·²åœ¨è¿è¡Œ")
            return

        self._db_manager = db_manager
        self._running = True
        self._auto_flush_task = asyncio.create_task(self._auto_flush_loop())
        logger.info(f"[{self.operation_type.upper()}] è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å·²å¯åŠ¨")

    async def stop_auto_flush(self):
        """åœæ­¢è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡"""
        self._running = False

        if self._auto_flush_task is not None and not self._auto_flush_task.done():
            self._auto_flush_task.cancel()
            try:
                await self._auto_flush_task
            except asyncio.CancelledError:
                pass
            logger.info(f"[{self.operation_type.upper()}] è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å·²åœæ­¢")

        # æœ€ååˆ·æ–°ä¸€æ¬¡ï¼Œç¡®ä¿ä¸ä¸¢æ•°æ®
        if self.records and self._db_manager:
            logger.info(f"[{self.operation_type.upper()}] æ‰§è¡Œæœ€ç»ˆåˆ·æ–°ï¼Œå‰©ä½™ {len(self.records)} æ¡è®°å½•")
            await self.flush(self._db_manager)
    async def _batch_fetch_task_info_from_redis(self, records: List[Dict[str, Any]]):
        """æ‰¹é‡ä» Redis è·å–ä»»åŠ¡ä¿¡æ¯ï¼ˆä»…ç”¨äº UPDATE æ¨¡å¼ï¼‰

        ä½¿ç”¨ pipeline æ‰¹é‡è·å–ä»»åŠ¡ä¿¡æ¯ï¼Œå¤§å¹…å‡å°‘ç½‘ç»œå¾€è¿”

        Args:
            records: è¦å¤„ç†çš„è®°å½•åˆ—è¡¨ï¼ˆä¼ å…¥çš„æ‹·è´æ•°æ®ï¼‰
        """
        if not records or not self.redis_client_getter:
            return

        # å¯¼å…¥è¾…åŠ©å‡½æ•°
        from .consumer import _parse_task_info

        redis_client = self.redis_client_getter()
        if not redis_client:
            logger.error("æ— æ³•è·å– Redis å®¢æˆ·ç«¯")
            return

        # æ”¶é›†æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ task_id
        task_ids = [record['task_id'] for record in records if 'task_id' in record]
        if not task_ids:
            return

        logger.info(f"  â³ ä½¿ç”¨ pipeline æ‰¹é‡è·å– {len(task_ids)} ä¸ªä»»åŠ¡ä¿¡æ¯...")

        try:
            # ä½¿ç”¨ pipeline æ‰¹é‡æŸ¥è¯¢
            pipeline = redis_client.pipeline()
            for task_id in task_ids:
                pipeline.hgetall(task_id)

            # æ‰§è¡Œ pipeline
            results = await pipeline.execute()

            # è§£æç»“æœå¹¶æ›´æ–° records
            valid_count = 0
            for record, task_info in zip(records, results):
                if not task_info:
                    logger.warning(f"  âš  æ— æ³•æ‰¾åˆ°ä»»åŠ¡çŠ¶æ€ä¿¡æ¯: {record.get('task_id')}")
                    continue

                # è§£æä»»åŠ¡ä¿¡æ¯
                parsed_info = _parse_task_info(task_info)
                # æ›´æ–° recordï¼Œæ·»åŠ è§£æåçš„å­—æ®µ
                record.update(parsed_info)
                valid_count += 1

            logger.info(f"  âœ“ æˆåŠŸè·å– {valid_count}/{len(task_ids)} ä¸ªä»»åŠ¡ä¿¡æ¯")

        except Exception as e:
            logger.error(f"  âœ— æ‰¹é‡è·å–ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
