"""PostgreSQL Consumer - åŸºäºé€šé…ç¬¦é˜Ÿåˆ—çš„æ–°å®ç°

å®Œå…¨æ›¿æ¢æ—§çš„ consumer.py å®ç°ï¼Œä½¿ç”¨ Jettask é€šé…ç¬¦é˜Ÿåˆ—åŠŸèƒ½ã€‚
"""

import time
import logging
from datetime import datetime, timezone

from jettask import Jettask
from jettask.core.context import TaskContext
from jettask.db.connector import get_pg_engine_and_factory, DBConfig
from .buffer import BatchBuffer
from .persistence import TaskPersistence

logger = logging.getLogger(__name__)


def _decode_redis_field(value, field_type='str'):
    """è§£æ Redis å­—æ®µå€¼ï¼ˆå¤„ç† bytes/str ç±»å‹ï¼‰

    Args:
        value: Redis è¿”å›çš„å€¼ï¼ˆå¯èƒ½æ˜¯ bytes æˆ– strï¼‰
        field_type: ç›®æ ‡ç±»å‹ ('str', 'int', 'float')

    Returns:
        è§£æåçš„å€¼ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
    """
    if not value:
        return None

    # å¦‚æœæ˜¯ bytesï¼Œå…ˆè§£ç ä¸º str
    if isinstance(value, bytes):
        try:
            value = value.decode('utf-8')
        except Exception:
            return None

    # ç±»å‹è½¬æ¢
    try:
        if field_type == 'int':
            return int(value) if value else 0
        elif field_type == 'float':
            return float(value) if value else None
        else:  # str
            return value
    except (ValueError, TypeError):
        return None


def _extract_task_name_from_consumer(consumer: str) -> str:
    """ä» consumer å­—æ®µæå– task_name

    consumer æ ¼å¼: YYDG-15b50489-9274-robust_bench2:8
    æå–é€»è¾‘: ç”¨'-'åˆ†å‰²å–æœ€åä¸€ä¸ªï¼Œå†ç”¨':'åˆ†å‰²å–ç¬¬ä¸€ä¸ª
    ç»“æœ: robust_bench2

    Args:
        consumer: consumer å­—æ®µå€¼

    Returns:
        task_nameï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
    """
    if not consumer:
        return None

    try:
        # ç”¨ '-' åˆ†å‰²ï¼Œå–æœ€åä¸€ä¸ª
        last_part = consumer.split('-')[-1]
        # ç”¨ ':' åˆ†å‰²ï¼Œå–ç¬¬ä¸€ä¸ª
        task_name = last_part.split(':')[0]
        return task_name if task_name else None
    except (IndexError, AttributeError):
        logger.warning(f"Failed to extract task_name from consumer: {consumer}")
        return None


def _parse_task_info(task_info: dict) -> dict:
    """æ‰¹é‡è§£æä»»åŠ¡ä¿¡æ¯å­—æ®µ

    Args:
        task_info: Redis hgetall è¿”å›çš„ä»»åŠ¡ä¿¡æ¯å­—å…¸

    Returns:
        è§£æåçš„å­—æ®µå­—å…¸
    """
    consumer = _decode_redis_field(task_info.get(b'consumer'), 'str')

    # # ğŸ”§ ä» consumer å­—æ®µæå– task_name
    # task_name = _extract_task_name_from_consumer(consumer)

    return {
        'retries': _decode_redis_field(task_info.get(b'retries'), 'int'),
        'trigger_time': _decode_redis_field(task_info.get(b'trigger_time_float'), 'float'),
        'started_at': _decode_redis_field(task_info.get(b'started_at'), 'float'),
        'completed_at': _decode_redis_field(task_info.get(b'completed_at'), 'float'),
        'consumer': consumer,
        'queue': _decode_redis_field(task_info.get(b'queue'), 'str'),  # ğŸ”§ æ·»åŠ  queue å­—æ®µ
        'status': _decode_redis_field(task_info.get(b'status'), 'str'),
        'result': task_info.get(b'result'),  # ä¿æŒåŸå§‹ bytes
        'error': task_info.get(b'exception') or task_info.get(b'error'),  # ä¿æŒåŸå§‹ bytes
    }


def _extract_event_id_from_task_id(task_id: str) -> str:
    """ä» task_id ä¸­æå– event_id

    task_id æ ¼å¼: prefix:TASK:event_id:queue:task_name

    Args:
        task_id: ä»»åŠ¡ ID

    Returns:
        event_id (stream_id)ï¼Œå¦‚æœæ ¼å¼æ— æ•ˆè¿”å› None
    """
    if not task_id:
        return None

    parts = task_id.split(':')
    if len(parts) >= 3:
        return parts[2]  # æå– event_id

    return None


def _extract_task_name_from_task_id(task_id: str) -> str:
    """ä» task_id ä¸­æå– task_name

    task_id æ ¼å¼: prefix:TASK:event_id:queue:task_name

    Args:
        task_id: ä»»åŠ¡ ID

    Returns:
        task_nameï¼Œå¦‚æœæ ¼å¼æ— æ•ˆè¿”å› None
    """
    if not task_id:
        return None

    parts = task_id.split(':')
    if len(parts) >= 5:
        return parts[4]  # æå– task_name

    return None


class PostgreSQLConsumer:
    """PostgreSQL Consumer - åŸºäºé€šé…ç¬¦é˜Ÿåˆ—

    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. ä½¿ç”¨ @app.task(queue='*') ç›‘å¬æ‰€æœ‰é˜Ÿåˆ—
    2. ä½¿ç”¨ @app.task(queue='TASK_CHANGES') å¤„ç†çŠ¶æ€æ›´æ–°
    3. æ‰¹é‡ INSERT å’Œ UPDATE
    4. è‡ªåŠ¨é˜Ÿåˆ—å‘ç°ï¼ˆJettask å†…ç½®ï¼‰
    """

    def __init__(
        self,
        pg_config,  # å¯ä»¥æ˜¯å­—å…¸æˆ–é…ç½®å¯¹è±¡
        redis_config,  # å¯ä»¥æ˜¯å­—å…¸æˆ–é…ç½®å¯¹è±¡
        prefix: str = "jettask",
        namespace_id: str = None,
        namespace_name: str = None,
        batch_size: int = 1000,
        flush_interval: float = 5.0
    ):
        """åˆå§‹åŒ– PG Consumer

        Args:
            pg_config: PostgreSQLé…ç½®ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
            redis_config: Redisé…ç½®ï¼ˆå­—å…¸æˆ–å¯¹è±¡ï¼‰
            prefix: Redisé”®å‰ç¼€
            node_id: èŠ‚ç‚¹IDï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä¸ä½¿ç”¨ï¼‰
            namespace_id: å‘½åç©ºé—´ID
            namespace_name: å‘½åç©ºé—´åç§°
            enable_backlog_monitor: æ˜¯å¦å¯ç”¨ç§¯å‹ç›‘æ§ï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä¸ä½¿ç”¨ï¼‰
            backlog_monitor_interval: ç§¯å‹ç›‘æ§é—´éš”ï¼ˆå…¼å®¹æ—§æ¥å£ï¼Œä¸ä½¿ç”¨ï¼‰
            batch_size: æ‰¹é‡å¤§å°
            flush_interval: åˆ·æ–°é—´éš”ï¼ˆç§’ï¼‰
        """
        self.pg_config = pg_config
        self.redis_config = redis_config
        self.redis_prefix = prefix
        self.namespace_id = namespace_id
        self.namespace_name = namespace_name or "default"

        # æ„å»º Redis URLï¼ˆå…¼å®¹å­—å…¸å’Œå¯¹è±¡ä¸¤ç§æ ¼å¼ï¼‰
        if isinstance(redis_config, dict):
            # å­—å…¸æ ¼å¼ - ä¼˜å…ˆä½¿ç”¨ 'url' å­—æ®µ
            redis_url = redis_config.get('url') or redis_config.get('redis_url')
            if not redis_url:
                # ä»ç‹¬ç«‹å­—æ®µæ„å»º
                password = redis_config.get('password', '')
                host = redis_config.get('host', 'localhost')
                port = redis_config.get('port', 6379)
                db = redis_config.get('db', 0)
                redis_url = f"redis://"
                if password:
                    redis_url += f":{password}@"
                redis_url += f"{host}:{port}/{db}"
        else:
            # å¯¹è±¡æ ¼å¼
            redis_url = f"redis://"
            if hasattr(redis_config, 'password') and redis_config.password:
                redis_url += f":{redis_config.password}@"
            redis_url += f"{redis_config.host}:{redis_config.port}/{redis_config.db}"

        self.redis_url = redis_url
        logger.debug(f"æ„å»º Redis URL: {redis_url}")

        # æ•°æ®åº“å¼•æ“å’Œä¼šè¯ï¼ˆå°†åœ¨ start æ—¶åˆå§‹åŒ–ï¼‰
        self.async_engine = None
        self.AsyncSessionLocal = None
        self.db_manager = None

        # åˆ›å»º Jettask åº”ç”¨
        self.app = Jettask(
            redis_url=redis_url,
            redis_prefix=prefix
        )

        # åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„æ‰¹é‡ç¼“å†²åŒº
        # 1. INSERT ç¼“å†²åŒºï¼ˆç”¨äºæ–°ä»»åŠ¡æŒä¹…åŒ–ï¼‰
        self.insert_buffer = BatchBuffer(
            max_size=batch_size,
            max_delay=flush_interval,
            operation_type='insert'
        )

        # 2. UPDATE ç¼“å†²åŒºï¼ˆç”¨äºä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼‰
        self.update_buffer = BatchBuffer(
            max_size=batch_size // 2,  # çŠ¶æ€æ›´æ–°é€šå¸¸æ›´é¢‘ç¹ï¼Œç”¨è¾ƒå°çš„æ‰¹æ¬¡
            max_delay=flush_interval,
            operation_type='update',
            redis_client_getter=lambda: self.app.async_binary_redis  # æ‰¹é‡è·å–ä»»åŠ¡ä¿¡æ¯
        )

        # æ³¨å†Œä»»åŠ¡
        self._register_tasks()

        # è¿è¡Œæ§åˆ¶
        self._running = False

        # auto flush å¯åŠ¨æ ‡å¿—ï¼ˆåœ¨ worker è¿›ç¨‹ä¸­æ‡’åŠ è½½å¯åŠ¨ï¼‰
        self._auto_flush_started = False

    async def _ensure_auto_flush_started(self):
        """ç¡®ä¿ auto flush åœ¨ worker è¿›ç¨‹ä¸­å¯åŠ¨ï¼ˆåªå¯åŠ¨ä¸€æ¬¡ï¼‰"""
        if not self._auto_flush_started:
            logger.info("[Workerè¿›ç¨‹] å¯åŠ¨ç¼“å†²åŒºè‡ªåŠ¨åˆ·æ–°ä»»åŠ¡...")
            await self.insert_buffer.start_auto_flush(self.db_manager)
            await self.update_buffer.start_auto_flush(self.db_manager)
            self._auto_flush_started = True
            logger.info("[Workerè¿›ç¨‹] âœ“ ç¼“å†²åŒºè‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å·²å¯åŠ¨")

    def _register_tasks(self):
        """æ³¨å†Œä»»åŠ¡å¤„ç†å™¨"""
        # åˆ›å»ºé—­åŒ…å‡½æ•°æ¥è®¿é—®å®ä¾‹å±æ€§
        consumer = self  # æ•è· self å¼•ç”¨

        @self.app.task(queue='*', auto_ack=False, name=f'{self.namespace_name}._handle_persist_task')
        async def _handle_persist_task(ctx: TaskContext, *args, **kwargs):
            # print(f'{args=} {kwargs=}')
            return await consumer._do_handle_persist_task(ctx, *args, **kwargs)

        @self.app.task(queue='TASK_CHANGES', auto_ack=False, name=f'{self.namespace_name}._handle_status_update')
        async def _handle_status_update(ctx: TaskContext, **kwargs):
            # print(f'{kwargs=}')

            return await consumer._do_handle_status_update(ctx, **kwargs)

    async def _do_handle_persist_task(self, ctx: TaskContext, *args, **kwargs):
        """å¤„ç†ä»»åŠ¡æŒä¹…åŒ–ï¼ˆINSERTï¼‰

        ä½¿ç”¨é€šé…ç¬¦ queue='*' ç›‘å¬æ‰€æœ‰é˜Ÿåˆ—
        Jettask ä¼šè‡ªåŠ¨å‘ç°æ–°é˜Ÿåˆ—å¹¶å¼€å§‹æ¶ˆè´¹

        Args:
            ctx: Jettask è‡ªåŠ¨æ³¨å…¥çš„ä»»åŠ¡ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« queue, event_id ç­‰ï¼‰
            **kwargs: ä»»åŠ¡çš„åŸå§‹æ•°æ®å­—æ®µ
        """
        # ğŸ”§ ç¡®ä¿ auto flush åœ¨ worker è¿›ç¨‹ä¸­å¯åŠ¨ï¼ˆæ‡’åŠ è½½ï¼‰
        await self._ensure_auto_flush_started()

        # æ·»åŠ å…³é”®æ—¥å¿—ï¼Œç¡®è®¤æ–¹æ³•è¢«è°ƒç”¨
        # logger.info(f"[æŒä¹…åŒ–ä»»åŠ¡] æ”¶åˆ°æ¶ˆæ¯ - é˜Ÿåˆ—: {ctx.queue}, Stream ID: {ctx.event_id}, task_name: {kwargs.get('task_name')}, metadata: {ctx.metadata}")

        # è·³è¿‡ TASK_CHANGES é˜Ÿåˆ—ï¼ˆç”±å¦ä¸€ä¸ªä»»åŠ¡å¤„ç†ï¼‰
        if ctx.queue == f'TASK_CHANGES':
            logger.debug(f"[æŒä¹…åŒ–ä»»åŠ¡] è·³è¿‡ TASK_CHANGES é˜Ÿåˆ—: {ctx.event_id}")
            ctx.acks([ctx.event_id])
            return

        try:

            # ğŸ”§ ä» ctx.metadata ä¸­æå–å…ƒæ•°æ®
            metadata = ctx.metadata or {}

            trigger_time = metadata.get('trigger_time', time.time())
            if isinstance(trigger_time, (str, bytes)):
                trigger_time = float(trigger_time)

            priority = metadata.get('priority', 0)
            if priority and isinstance(priority, (str, bytes)):
                priority = int(priority)
            elif priority is None:
                priority = 0

            # æå– delay å‚æ•°
            delay = metadata.get('delay', 0)
            if delay and isinstance(delay, (str, bytes)):
                delay = float(delay)
            elif delay is None:
                delay = 0

            scheduled_task_id = metadata.get('scheduled_task_id')

            payload = {
                'args': args,
                'kwargs': kwargs,
            }

            # ğŸ”§ å…³é”®è¯´æ˜ï¼š
            # - tasks æ˜¯åˆ†åŒºè¡¨ï¼ŒæŒ‰ trigger_time åˆ†åŒºï¼Œä¸»é”®æ˜¯ (stream_id, trigger_time)
            # - trigger_time åœ¨ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸä¸­ä¸ä¼šå˜åŒ–ï¼Œç¡®ä¿ UPSERT èƒ½æ­£ç¡®åŒ¹é…å·²æœ‰è®°å½•
            # - created_at ä½¿ç”¨æ•°æ®åº“é»˜è®¤å€¼ï¼ˆNOW()ï¼‰ï¼Œè¡¨ç¤ºè®°å½•çœŸæ­£çš„åˆ›å»ºæ—¶é—´
            # - ä¸è¦åœ¨ record ä¸­è®¾ç½® created_atï¼Œè®©æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆ
            record = {
                'stream_id': ctx.event_id,
                'queue': ctx.queue.replace(f'{self.redis_prefix}:QUEUE:', ''),
                'payload': payload,
                'priority': priority,
                'delay': delay,
                # created_at ä¸è®¾ç½®ï¼Œä½¿ç”¨æ•°æ®åº“é»˜è®¤å€¼
                'trigger_time': trigger_time,  # ç›´æ¥å­˜å‚¨ Unix æ—¶é—´æˆ³
                'scheduled_task_id': scheduled_task_id,
                'namespace': self.namespace_name,
                'source': 'scheduler' if scheduled_task_id else 'redis_stream',
            }

            # æ·»åŠ åˆ°ç¼“å†²åŒºï¼ˆä¸ç«‹å³å¤„ç†ï¼Œä¸ç«‹å³ ACKï¼‰
            await self.insert_buffer.add(record, ctx)
            logger.debug(f"[æŒä¹…åŒ–ä»»åŠ¡] å·²æ·»åŠ åˆ°ç¼“å†²åŒºï¼Œå½“å‰å¤§å°: {len(self.insert_buffer.records)}/{self.insert_buffer.max_size}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæ‰¹é‡å¤§å°æˆ–è¶…æ—¶ï¼‰
            if self.insert_buffer.should_flush():
                logger.info(f"[æŒä¹…åŒ–ä»»åŠ¡] è§¦å‘åˆ·æ–°ï¼Œç¼“å†²åŒºå¤§å°: {len(self.insert_buffer.records)}")
                await self.insert_buffer.flush(self.db_manager)

            # åŒæ—¶æ£€æŸ¥ UPDATE ç¼“å†²åŒºæ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆåˆ©ç”¨è¿™æ¬¡æœºä¼šï¼‰
            if self.update_buffer.should_flush():
                await self.update_buffer.flush(self.db_manager)

        except Exception as e:
            logger.error(f"æŒä¹…åŒ–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            # å‡ºé”™ä¹Ÿè¦ ACKï¼Œé¿å…æ¶ˆæ¯å †ç§¯
            ctx.acks([ctx.event_id])

    async def _do_handle_status_update(self, ctx: TaskContext, **kwargs):
        """å¤„ç†ä»»åŠ¡çŠ¶æ€æ›´æ–°ï¼ˆUPDATEï¼‰

        æ¶ˆè´¹ TASK_CHANGES é˜Ÿåˆ—ï¼Œæ‰¹é‡æ›´æ–°æ•°æ®åº“ä¸­çš„ä»»åŠ¡çŠ¶æ€

        Args:
            ctx: Jettask è‡ªåŠ¨æ³¨å…¥çš„ä»»åŠ¡ä¸Šä¸‹æ–‡
            **kwargs: ä»»åŠ¡çš„åŸå§‹æ•°æ®å­—æ®µï¼ˆåŒ…å« task_idï¼‰
        """
        # æ·»åŠ å…³é”®æ—¥å¿—ï¼Œç¡®è®¤æ–¹æ³•è¢«è°ƒç”¨
        # logger.info(f"[çŠ¶æ€æ›´æ–°] æ”¶åˆ°æ¶ˆæ¯ - é˜Ÿåˆ—: {ctx.queue}, Stream ID: {ctx.event_id}, kwargs: {kwargs}")

        # ğŸ”§ ç¡®ä¿ auto flush åœ¨ worker è¿›ç¨‹ä¸­å¯åŠ¨ï¼ˆæ‡’åŠ è½½ï¼‰
        await self._ensure_auto_flush_started()

        try:
            # ä»æ¶ˆæ¯ä¸­è·å– task_id
            task_id = kwargs.get('task_id')
            if not task_id:
                logger.warning(f"TASK_CHANGES æ¶ˆæ¯ç¼ºå°‘ task_id: {ctx.event_id}")
                ctx.acks([ctx.event_id])
                return

            # ä» task_id ä¸­æå– event_id (stream_id) å’Œ task_name
            event_id = _extract_event_id_from_task_id(task_id)
            if not event_id:
                logger.error(f"æ— æ•ˆçš„ task_id æ ¼å¼: {task_id}")
                ctx.acks([ctx.event_id])
                return

            task_name = _extract_task_name_from_task_id(task_id)

            # print(f'{task_id=} {event_id=} {task_name=}')
            # åªä¿å­˜ task_id å’Œ task_nameï¼Œå»¶è¿Ÿåˆ°æ‰¹é‡åˆ·æ–°æ—¶å†è·å–ä»»åŠ¡ä¿¡æ¯
            update_record = {
                'task_id': task_id,
                'stream_id': event_id,
                'task_name': task_name,
                'namespace': self.namespace_name,  # ğŸ”§ æ·»åŠ  namespace å­—æ®µ
            }

            # æ·»åŠ åˆ°çŠ¶æ€æ›´æ–°ç¼“å†²åŒº
            await self.update_buffer.add(update_record, ctx)
            logger.debug(f"[çŠ¶æ€æ›´æ–°] å·²æ·»åŠ åˆ°ç¼“å†²åŒºï¼Œå½“å‰å¤§å°: {len(self.update_buffer.records)}/{self.update_buffer.max_size}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆæ‰¹é‡å¤§å°æˆ–è¶…æ—¶ï¼‰
            if self.update_buffer.should_flush():
                logger.info(f"[çŠ¶æ€æ›´æ–°] è§¦å‘åˆ·æ–°ï¼Œç¼“å†²åŒºå¤§å°: {len(self.update_buffer.records)}")
                await self.update_buffer.flush(self.db_manager)

            # åŒæ—¶æ£€æŸ¥ INSERT ç¼“å†²åŒºæ˜¯å¦éœ€è¦åˆ·æ–°ï¼ˆåˆ©ç”¨è¿™æ¬¡æœºä¼šï¼‰
            if self.insert_buffer.should_flush():
                await self.insert_buffer.flush(self.db_manager)

        except Exception as e:
            logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}", exc_info=True)
            # å‡ºé”™ä¹Ÿè¦ ACK
            ctx.acks([ctx.event_id])

    async def start(self, concurrency: int = 4, prefetch_multiplier: int = 1):
        """å¯åŠ¨ Consumer

        Args:
            concurrency: å¹¶å‘æ•°
        """
        logger.info(f"Starting PostgreSQL consumer (wildcard queue mode)")
        logger.info(f"Namespace: {self.namespace_name} ({self.namespace_id or 'N/A'})")

        # 1. ä½¿ç”¨ connector.py ç»Ÿä¸€ç®¡ç†æ•°æ®åº“è¿æ¥
        # è§£æ PostgreSQL é…ç½®ä¸ºæ ‡å‡† DSN
        dsn = DBConfig.parse_pg_config(self.pg_config)

        # ä½¿ç”¨å…¨å±€å•ä¾‹å¼•æ“å’Œä¼šè¯å·¥å‚
        self.async_engine, self.AsyncSessionLocal = get_pg_engine_and_factory(
            dsn,
            pool_size=50,
            max_overflow=20,
            pool_pre_ping=True,
            pool_recycle=300,
            echo=False
        )

        logger.debug(f"ä½¿ç”¨å…¨å±€ PostgreSQL è¿æ¥æ± : {dsn[:50]}...")

        # 2. åˆå§‹åŒ–ä»»åŠ¡æŒä¹…åŒ–ç®¡ç†å™¨
        self.db_manager = TaskPersistence(
            async_session_local=self.AsyncSessionLocal,
            namespace_id=self.namespace_id,
            namespace_name=self.namespace_name
        )

        # 3. è®¾ç½®è¿è¡ŒçŠ¶æ€
        self._running = True

        # 4. æ³¨æ„ï¼šä¸åœ¨ä¸»è¿›ç¨‹ä¸­å¯åŠ¨ auto flush
        # auto flush ä¼šåœ¨ worker å­è¿›ç¨‹ä¸­æ‡’åŠ è½½å¯åŠ¨ï¼ˆé¦–æ¬¡ä»»åŠ¡æ‰§è¡Œæ—¶ï¼‰
        # è¿™æ ·é¿å…è¿›ç¨‹éš”ç¦»é—®é¢˜ï¼ˆä¸»è¿›ç¨‹çš„ asyncio.Task æ— æ³•åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œï¼‰

        # 5. åœ¨å¯åŠ¨ worker å‰æ‰§è¡Œä¸€äº›åŒæ­¥çš„å‡†å¤‡å·¥ä½œï¼ˆä» app.start() ä¸­æå–ï¼‰
        # æ ‡è®° worker å·²å¯åŠ¨
        self.app._worker_started = True

        # å¦‚æœé…ç½®äº†ä»»åŠ¡ä¸­å¿ƒä¸”é…ç½®å°šæœªåŠ è½½ï¼Œä»ä»»åŠ¡ä¸­å¿ƒè·å–é…ç½®
        if self.app.task_center and self.app.task_center.is_enabled and not self.app._task_center_config:
            self.app._load_config_from_task_center()

        # æ³¨å†Œæ‰€æœ‰å¾…æ³¨å†Œçš„é™æµé…ç½®åˆ° Redis
        logger.info("æ­£åœ¨æ³¨å†Œå¾…æ³¨å†Œçš„é™æµé…ç½®...")
        self.app._apply_pending_rate_limits()

        # æ³¨å†Œæ¸…ç†å¤„ç†å™¨ï¼ˆåªåœ¨å¯åŠ¨workeræ—¶æ³¨å†Œï¼‰
        self.app._setup_cleanup_handlers()

        # å¯åŠ¨ Workerï¼ˆä½¿ç”¨é€šé…ç¬¦é˜Ÿåˆ—ï¼‰
        logger.info("=" * 60)
        logger.info(f"å¯åŠ¨ PG Consumer (é€šé…ç¬¦é˜Ÿåˆ—æ¨¡å¼)")
        logger.info("=" * 60)
        logger.info(f"å‘½åç©ºé—´: {self.namespace_name} ({self.namespace_id or 'N/A'})")
        logger.info(f"ç›‘å¬é˜Ÿåˆ—: * (æ‰€æœ‰é˜Ÿåˆ—) + TASK_CHANGES (çŠ¶æ€æ›´æ–°)")
        logger.info(f"INSERT æ‰¹é‡: {self.insert_buffer.max_size} æ¡")
        logger.info(f"UPDATE æ‰¹é‡: {self.update_buffer.max_size} æ¡")
        logger.info(f"åˆ·æ–°é—´éš”: {self.insert_buffer.max_delay} ç§’")
        logger.info(f"å¹¶å‘æ•°: {concurrency}")
        logger.info("=" * 60)

        try:
            # å¯åŠ¨ Worker
            # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨ _start() è€Œä¸æ˜¯ start()ï¼Œå› ä¸ºï¼š
            # - app.start() æ˜¯åŒæ­¥æ–¹æ³•ï¼Œå†…éƒ¨ä½¿ç”¨ asyncio.run()
            # - app._start() æ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œå¯ä»¥åœ¨å·²æœ‰çš„äº‹ä»¶å¾ªç¯ä¸­ä½¿ç”¨ await
            # - æˆ‘ä»¬çš„ consumer.start() æ˜¯å¼‚æ­¥çš„ï¼Œæ‰€ä»¥å¿…é¡»è°ƒç”¨ _start()

            # è·å–å·²æ³¨å†Œä»»åŠ¡çš„åç§°
            task_names = list(self.app._tasks.keys())
            logger.info(f"å·²æ³¨å†Œçš„ä»»åŠ¡: {task_names}")

            await self.app._start(
                tasks=task_names,  # ğŸ¯ å…³é”®ï¼šä¼ é€’ä»»åŠ¡åç§°åˆ—è¡¨
                concurrency=concurrency,
                prefetch_multiplier=prefetch_multiplier
            )
        finally:
            await self.stop()

    async def stop(self):
        """åœæ­¢ Consumer"""
        logger.info("åœæ­¢ PG Consumer...")
        self._running = False

        # åœæ­¢è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡ï¼ˆä¼šè‡ªåŠ¨æ‰§è¡Œæœ€åä¸€æ¬¡åˆ·æ–°ï¼‰
        try:
            await self.insert_buffer.stop_auto_flush()
            await self.update_buffer.stop_auto_flush()
            logger.info("âœ“ ç¼“å†²åŒºè‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢è‡ªåŠ¨åˆ·æ–°ä»»åŠ¡å¤±è´¥: {e}")

        # æ³¨æ„ï¼šä¸å…³é—­æ•°æ®åº“å¼•æ“ï¼Œå› ä¸ºå®ƒæ˜¯å…¨å±€å•ä¾‹ï¼Œç”± connector.py ç®¡ç†
        # å¤šä¸ª consumer å®ä¾‹å¯èƒ½å…±äº«åŒä¸€ä¸ªå¼•æ“

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        insert_stats = self.insert_buffer.get_stats()
        update_stats = self.update_buffer.get_stats()

        logger.info("=" * 60)
        logger.info("PG Consumer ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 60)
        logger.info(f"INSERT: æ€»è®¡ {insert_stats['total_flushed']} æ¡, "
                   f"åˆ·æ–° {insert_stats['flush_count']} æ¬¡, "
                   f"å¹³å‡ {insert_stats['avg_per_flush']} æ¡/æ¬¡")
        logger.info(f"UPDATE: æ€»è®¡ {update_stats['total_flushed']} æ¡, "
                   f"åˆ·æ–° {update_stats['flush_count']} æ¬¡, "
                   f"å¹³å‡ {update_stats['avg_per_flush']} æ¡/æ¬¡")
        logger.info("=" * 60)

        logger.info("PG Consumer å·²åœæ­¢")
