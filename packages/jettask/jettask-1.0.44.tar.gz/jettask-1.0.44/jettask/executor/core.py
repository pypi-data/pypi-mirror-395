"""
æ‰§è¡Œå™¨æ ¸å¿ƒé€»è¾‘

ä»AsyncioExecutoræå–çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
èŒè´£:
1. ä»»åŠ¡æ‰§è¡Œ
2. Pipelineç®¡ç†
3. é™æµæ§åˆ¶
4. ç»Ÿè®¡æ”¶é›†
"""

import asyncio
import logging
import time
import os
from enum import Enum
from typing import Dict, Optional, TYPE_CHECKING
from collections import defaultdict, deque

from ..utils.traceback_filter import filter_framework_traceback
from ..utils.task_logger import TaskContextManager, configure_task_logging
from ..utils.serializer import dumps_str
from ..exceptions import RetryableError
from ..core.enums import TaskStatus
from ..utils.rate_limit.limiter import RateLimiterManager, ConcurrencyRateLimiter
from ..config.lua_scripts import LUA_SCRIPT_BATCH_SEND_EVENT
if TYPE_CHECKING:
    from ..core.app import Jettask

logger = logging.getLogger('app')


class ExecutionMode(Enum):
    """æ‰§è¡Œæ¨¡å¼"""
    SINGLE_PROCESS = "single_process"  # å•è¿›ç¨‹æ¨¡å¼
    MULTI_PROCESS = "multi_process"    # å¤šè¿›ç¨‹æ¨¡å¼
    AUTO = "auto"                      # è‡ªåŠ¨é€‰æ‹©

# Luaè„šæœ¬ï¼šåŸå­åœ°æ›´æ–°Redis hashä¸­çš„æœ€å¤§å€¼
UPDATE_MAX_OFFSET_LUA = """
local hash_key = KEYS[1]
local field = KEYS[2]
local new_value = tonumber(ARGV[1])

local current = redis.call('HGET', hash_key, field)
if current == false or tonumber(current) < new_value then
    redis.call('HSET', hash_key, field, new_value)
    return 1
else
    return 0
end
"""


class ExecutorCore:
    """
    æ‰§è¡Œå™¨æ ¸å¿ƒé€»è¾‘

    ä»AsyncioExecutoræå–çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
    èŒè´£:
    1. ä»»åŠ¡æ‰§è¡Œ
    2. Pipelineç®¡ç†
    3. é™æµæ§åˆ¶
    4. ç»Ÿè®¡æ”¶é›†
    """

    @staticmethod
    def _extract_trigger_time_from_event_id(event_id: str) -> float:
        """
        ä» Redis Stream çš„ event_id ä¸­æå–æ—¶é—´æˆ³

        Args:
            event_id: Redis Stream æ¶ˆæ¯ IDï¼Œæ ¼å¼ä¸º "{milliseconds_timestamp}-{sequence}"
                     ä¾‹å¦‚: "1761568186806-0"

        Returns:
            float: ç§’çº§æ—¶é—´æˆ³
        """
        try:
            # Redis Stream ID æ ¼å¼: {timestamp_ms}-{sequence}
            timestamp_ms = event_id.split('-')[0]
            return float(timestamp_ms) / 1000.0  # è½¬æ¢ä¸ºç§’
        except (ValueError, IndexError, AttributeError):
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å½“å‰æ—¶é—´
            logger.warning(f"Failed to extract timestamp from event_id: {event_id}")
            return time.time()

    def __init__(self, app: "Jettask", task_name: str, concurrency: int = 100):
        """
        åˆå§‹åŒ–æ‰§è¡Œå™¨æ ¸å¿ƒ

        Args:
            app: Applicationå®ä¾‹
            task_name: ä»»åŠ¡åç§°
            concurrency: å¹¶å‘æ•°
        """
        self.app = app
        self.task_name = task_name
        self.concurrency = concurrency

        # Pipelineé…ç½®
        self.pipeline_config = {
            'ack': {'max_batch': 1000, 'max_delay': 0.05},
            'task_info': {'max_batch': 2000, 'max_delay': 0.1},
            'status': {'max_batch': 1000, 'max_delay': 0.15},
            'data': {'max_batch': 1000, 'max_delay': 0.15},
            'stats': {'max_batch': 5000, 'max_delay': 0.2}
        }

        # Pipelineç¼“å†²åŒº
        self.pending_acks = []
        self.status_updates = []
        self.data_updates = []
        self.task_info_updates = {}
        self.stats_updates = []

        # Pipelineæ—¶é—´è·Ÿè¸ª
        self.last_pipeline_flush = {
            'ack': time.time(),
            'task_info': time.time(),
            'status': time.time(),
            'data': time.time(),
            'stats': time.time()
        }

        # æ€§èƒ½è®¡æ•°
        self.batch_counter = 0
        self.pipeline_operation_count = 0

        # å‰ç¼€å’Œç¼“å­˜
        self.prefix = self.app.ep.redis_prefix or 'jettask'
        self._status_prefix = self.app._status_prefix
        self._result_prefix = self.app._result_prefix
        self._prefixed_queue_cache = {}

        # Pending countç¼“å­˜
        self.pending_cache = {}
        self.pending_cache_expire = 0

        # é™æµå™¨ç®¡ç†å™¨
        self.rate_limiter_manager = None

        # é…ç½®ä»»åŠ¡æ—¥å¿—
        log_format = os.environ.get('JETTASK_LOG_FORMAT', 'text').lower()
        if log_format == 'json':
            configure_task_logging(format='json')
        else:
            format_string = os.environ.get('JETTASK_LOG_FORMAT_STRING')
            if format_string:
                configure_task_logging(format='text', format_string=format_string)

        logger.debug(f"ExecutorCore initialized for task {task_name}")

    def _get_prefixed_queue_cached(self, queue: str) -> str:
        """ç¼“å­˜é˜Ÿåˆ—åç§°ä»¥é¿å…é‡å¤å­—ç¬¦ä¸²æ‹¼æ¥"""
        if queue not in self._prefixed_queue_cache:
            self._prefixed_queue_cache[queue] = self.app.ep.get_prefixed_queue_name(queue)
        return self._prefixed_queue_cache[queue]

    async def get_pending_count_cached(self, queue: str) -> int:
        """è·å–ç¼“å­˜çš„pendingè®¡æ•°"""
        current_time = time.time()

        if (current_time - self.pending_cache_expire > 30 or
            queue not in self.pending_cache):
            try:
                pending_info = await self.app.ep.async_redis_client.xpending(queue, queue)
                self.pending_cache[queue] = pending_info.get("pending", 0)
                self.pending_cache_expire = current_time
            except Exception:
                self.pending_cache[queue] = 0

        return self.pending_cache.get(queue, 0)

    async def _quick_ack(self, queue: str, event_id: str, group_name: str = None,
                        offset: int = None):
        """å¿«é€ŸACK with unified pipeline management"""
        group_name = group_name or queue
        self.pending_acks.append((queue, event_id, group_name, offset))
        current_time = time.time()

        ack_config = self.pipeline_config['ack']
        time_since_flush = current_time - self.last_pipeline_flush['ack']

        should_flush = (
            len(self.pending_acks) >= ack_config['max_batch'] or
            (len(self.pending_acks) >= 50 and time_since_flush >= ack_config['max_delay'])
        )

        if should_flush:
            await self._flush_all_buffers()

    async def _flush_all_buffers(self):
        """ç»Ÿä¸€Pipelineåˆ·æ–°"""
        pipeline = self.app.ep.async_redis_client.pipeline()
        operations_count = 0

        # 1. å¤„ç†ACKæ“ä½œ
        if self.pending_acks:
            acks_by_queue_group = defaultdict(lambda: defaultdict(list))
            max_offsets = {}

            for item in self.pending_acks:
                if len(item) == 4:
                    queue, event_id, group_name, offset = item
                elif len(item) == 3:
                    queue, event_id, group_name = item
                    offset = None
                else:
                    queue, event_id = item
                    group_name = queue
                    offset = None

                prefixed_queue = self._get_prefixed_queue_cached(queue)
                acks_by_queue_group[prefixed_queue][group_name].append(event_id)

                if group_name and offset is not None:
                    key = (queue, group_name)
                    if key not in max_offsets or offset > max_offsets[key]:
                        max_offsets[key] = offset

            # å¤„ç†offsetæ›´æ–°
            if max_offsets:
                task_offset_key = f"{self.prefix}:TASK_OFFSETS"
                for (queue, group_name), offset in max_offsets.items():
                    # ä» group_name ä¸­æå– task_nameï¼ˆæœ€åä¸€æ®µï¼‰
                    task_name = group_name.split(':')[-1]
                    # æ„å»º fieldï¼šé˜Ÿåˆ—åï¼ˆå«ä¼˜å…ˆçº§ï¼‰+ ä»»åŠ¡å
                    # ä¾‹å¦‚ï¼šrobust_bench2:8:benchmark_task
                    task_field = f"{queue}:{task_name}"
                    pipeline.eval(UPDATE_MAX_OFFSET_LUA, 2, task_offset_key, task_field, offset)
                    operations_count += 1

            # æ‰§è¡Œstream ACK
            for prefixed_queue, groups in acks_by_queue_group.items():
                for group_name, event_ids in groups.items():
                    stream_key = prefixed_queue.encode() if isinstance(prefixed_queue, str) else prefixed_queue
                    group_key = group_name.encode() if isinstance(group_name, str) else group_name
                    batch_bytes = [b.encode() if isinstance(b, str) else b for b in event_ids]
                    # print(f'{stream_key=} {group_key=} {batch_bytes=}')
                    pipeline.xack(stream_key, group_key, *batch_bytes)
                    operations_count += 1

            self.pending_acks.clear()
        # 2. å¤„ç†ä»»åŠ¡ä¿¡æ¯æ›´æ–°
        task_change_events = []
        if self.task_info_updates:
            for event_key, updates in self.task_info_updates.items():
                if event_key.endswith('_handle_status_update') or \
                    event_key.endswith('_handle_persist_task'):
                    continue  # è·³è¿‡æ— æ•ˆçš„event_key
                key = f"{self.prefix}:TASK:{event_key}"
                key_bytes = key.encode() 
                if updates:
                    encoded_updates = {k.encode(): v.encode() if isinstance(v, str) else v
                                     for k, v in updates.items()}
                    pipeline.hset(key_bytes, mapping=encoded_updates)
                    pipeline.expire(key_bytes, 3600)
                    operations_count += 2
                    task_change_events.append(key)

            # å‘é€å˜æ›´äº‹ä»¶ï¼ˆä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿ offset æ›´æ–°å’Œ REGISTRY æ³¨å†Œï¼‰
            if task_change_events:
                self._send_task_changes_with_offset(task_change_events, pipeline)

            self.task_info_updates.clear()

        # 3. å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self, 'stats_updates') and self.stats_updates:
            # for stat_op in self.stats_updates:
            #     if 'queue' in stat_op and 'field' in stat_op:
            #         stats_key = f"{self.prefix}:STATS:{stat_op['queue']}".encode()
            #         field = stat_op['field'].encode() if isinstance(stat_op['field'], str) else stat_op['field']
            #         pipeline.hincrby(stats_key, field, stat_op.get('value', 1))
            #         operations_count += 1
            self.stats_updates.clear()

        # æ‰§è¡Œpipeline
        if operations_count > 0:
            try:
                results = await pipeline.execute()

                if isinstance(results, Exception):
                    logger.error(f"Pipeline execution error: {results}")
                else:
                    for i, result in enumerate(results):
                        if isinstance(result, Exception):
                            logger.error(f"Pipeline operation {i} error: {result}")

                logger.debug(f"Unified pipeline executed {operations_count} operations")
                self.pipeline_operation_count += operations_count

            except Exception as e:
                import traceback
                logger.error(f"Pipeline flush error: {traceback.format_exc()}")

        # æ›´æ–°åˆ·æ–°æ—¶é—´
        current_time = time.time()
        for key in self.last_pipeline_flush:
            self.last_pipeline_flush[key] = current_time

    async def _collect_stats_async(self, queue: str, success: bool,
                                   processing_time: float, total_latency: float):
        """é«˜æ€§èƒ½å¼‚æ­¥ç»Ÿè®¡æ”¶é›†"""
        try:
            if hasattr(self.app, 'consumer_manager') and self.app.consumer_manager:
                if hasattr(self, 'stats_updates'):
                    self.stats_updates.append({
                        'queue': queue,
                        'field': 'success_count' if success else 'error_count',
                        'value': 1
                    })
                    self.stats_updates.append({
                        'queue': queue,
                        'field': 'total_processing_time',
                        'value': int(processing_time * 1000)
                    })

                    if len(self.stats_updates) >= self.pipeline_config['stats']['max_batch']:
                        asyncio.create_task(self._flush_all_buffers())
        except Exception:
            pass

    async def execute_task(self, event_id: str, event_data: dict, queue: str,
                          routing: dict = None, consumer: str = None,
                          group_name: str = None, **kwargs):
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡

        è¿™æ˜¯ä»AsyncioExecutor.logic()æå–çš„æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
        """
        status = "success"
        exception = None
        error_msg = None
        ret = None
        task = None
        args = ()
        kwargs_inner = {}

        status_key = f"{event_id}:{group_name}"
        task_name = event_data.get("_task_name") or event_data.get("name")

        if not task_name:
            logger.error(f"No _task_name in event_data for event {event_id}")
            return

        # è®¾ç½®ä»»åŠ¡æ—¥å¿—ä¸Šä¸‹æ–‡
        async with TaskContextManager(
            event_id=event_id,
            task_name=task_name,
            queue=queue,
            worker_id=consumer
        ):
            trigger_time_float = self._extract_trigger_time_from_event_id(event_id)
            
            try:
                # åˆå§‹åŒ–æ‰§è¡Œæ—¶é—´ï¼ˆåœ¨ finally å—ä¸­éœ€è¦ç”¨åˆ°ï¼‰
                execution_start_time = time.time()

                # æ£€æŸ¥æ¢å¤æ¶ˆæ¯
                if kwargs.get('_recovery'):
                    logger.debug(f"Processing recovered message {event_id}")

                # æ£€æŸ¥å»¶è¿Ÿä»»åŠ¡
                if event_data.get('is_delayed') and 'execute_at' in event_data:
                    execute_at = float(event_data['execute_at'])
                    if execute_at > time.time():
                        logger.debug(f"Task {event_id} delayed until {execute_at}")
                        return

                # è·å–é‡è¯•é…ç½®

                # è·å–ä»»åŠ¡
                task = self.app.get_task_by_name(task_name)
                retry_config = task.retry_config or {}
                max_retries = retry_config.get('max_retries', 0)
                if not task:
                    exception = f"{task_name=} {queue=} æœªç»‘å®šä»»ä½•task"
                    logger.error(exception)

                    offset = self._extract_offset(event_data)
                    await self._quick_ack(queue, event_id, group_name, offset)

                    current_time = time.time()
                    # ğŸ”§ ä» event_id æå–è§¦å‘æ—¶é—´
                    duration = current_time - trigger_time_float

                    self.task_info_updates[status_key] = {
                        "status": TaskStatus.ERROR.value,
                        "exception": exception,
                        "started_at": str(execution_start_time),
                        "completed_at": str(current_time),
                        "duration": str(duration),
                        "consumer": consumer,
                        "trigger_time_float": str(trigger_time_float),
                        "queue": str(queue)
                    }
                    await self._flush_all_buffers()
                    return

                self.pedding_count = await self.get_pending_count_cached(queue)

                # è·å–å‚æ•°
                args = event_data.get("args", ()) or ()
                kwargs_inner = event_data.get("kwargs", {}) or {}

                # ğŸ”§ ä» event_id æå–è§¦å‘æ—¶é—´

                # ğŸ”§ æå–å…ƒæ•°æ®ï¼ˆä¸æ”¾å…¥ kwargs_innerï¼‰
                scheduled_task_id = event_data.get('scheduled_task_id')
                priority = event_data.get('priority')
                delay = event_data.get('delay')

                # æ„å»º metadata å­—å…¸
                metadata = {
                    'priority': priority,
                    'delay': delay,
                    'trigger_time': trigger_time_float,
                    'scheduled_task_id': scheduled_task_id,
                    'group_name': group_name,
                    'queue': queue,
                }

                # å¤„ç†ç‰¹æ®Šäº‹ä»¶ç±»å‹
                if "event_type" in event_data and "customer_data" in event_data:
                    args = (event_data["event_type"], event_data["customer_data"])
                    extra_kwargs = {k: v for k, v in event_data.items()
                                  if k not in ["event_type", "customer_data", "_broadcast",
                                             "_target_tasks", "_timestamp", "trigger_time",
                                             "name", "_task_name"]}
                    kwargs_inner.update(extra_kwargs)

                # æ‰§è¡Œon_before
                result = task.on_before(
                    event_id=event_id,
                    pedding_count=self.pedding_count,
                    args=args,
                    kwargs=kwargs_inner,
                )
                if asyncio.iscoroutine(result):
                    result = await result

                if result and result.reject:
                    self.task_info_updates[status_key] = {
                        "status": TaskStatus.REJECTED.value,
                        "consumer": consumer,
                        "started_at": str(execution_start_time),
                        "completed_at": str(time.time()),
                        "error_msg": "Task rejected by on_before",
                        "trigger_time_float": str(trigger_time_float),
                        "queue": str(queue)
                    }
                    await self._flush_all_buffers()
                    return

                # æ ‡è®°ä»»åŠ¡å¼€å§‹ï¼ˆexecution_start_time å·²åœ¨ try å—å¼€å§‹æ—¶åˆå§‹åŒ–ï¼‰
                self.task_info_updates[status_key] = {
                    "status": TaskStatus.RUNNING.value,
                    "consumer": consumer,
                    "started_at": str(execution_start_time),
                    "trigger_time_float": str(trigger_time_float),
                    "queue": str(queue)
                }

                # é‡è¯•å¾ªç¯
                current_retry = 0
                last_exception = None

                while current_retry <= max_retries:
                    try:
                        if current_retry > 0:
                            logger.debug(f"Retry attempt {current_retry}/{max_retries} for task {event_id}")

                        # ğŸ”§ è¿‡æ»¤æ‰€æœ‰å•ä¸‹åˆ’çº¿å’ŒåŒä¸‹åˆ’çº¿å¼€å¤´çš„å†…éƒ¨å‚æ•°
                        # å…ƒæ•°æ®é€šè¿‡ä¸“ç”¨å‚æ•°ä¼ é€’ï¼Œä¸æ±¡æŸ“ä»»åŠ¡çš„ kwargs
                        clean_kwargs = {k: v for k, v in kwargs_inner.items()
                                      if not k.startswith('_')}

                        # print(f'{queue=} {args=} {kwargs_inner=}')
                        task_result = task(
                            event_id,
                            trigger_time_float,         # ğŸ”§ ä½¿ç”¨ä» event_id æå–çš„æ—¶é—´æˆ³
                            queue,
                            group_name,           # ğŸ”§ ä½œä¸ºç‹¬ç«‹å‚æ•°ä¼ é€’
                            scheduled_task_id,    # ğŸ”§ ä½œä¸ºç‹¬ç«‹å‚æ•°ä¼ é€’
                            metadata,             # ğŸ”§ ä¼ é€’å…ƒæ•°æ®
                            *args,
                            **clean_kwargs
                        )
                        if asyncio.iscoroutine(task_result):
                            ret = await task_result
                        else:
                            ret = task_result

                        result = task.on_success(
                            event_id=event_id,
                            args=args,
                            kwargs=clean_kwargs,
                            result=ret,
                        )
                        if asyncio.iscoroutine(result):
                            await result

                        # ä»»åŠ¡æˆåŠŸ,ACKæ¶ˆæ¯ï¼ˆæ£€æŸ¥auto_acké…ç½®ï¼‰
                        task_config = self.app.get_task_config(task_name)
                        auto_ack = task_config.get('auto_ack', True) if task_config else True
                        if auto_ack:
                            offset = self._extract_offset(event_data)
                            await self._quick_ack(queue, event_id, group_name, offset)

                        break

                    except SystemExit:
                        logger.debug('Task interrupted by system exit, leaving message pending for recovery')
                        status = TaskStatus.ERROR.value
                        exception = "System exit"
                        error_msg = "Task interrupted by shutdown"
                        # ä¸ ACK è¢«ä¸­æ–­çš„ä»»åŠ¡ï¼Œè®©å…¶ä»– worker æ¢å¤
                        # offset = self._extract_offset(event_data)
                        # await self._quick_ack(queue, event_id, group_name, offset)
                        break

                    except Exception as e:
                        last_exception = e
                        current_retry += 1

                        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç»§ç»­é‡è¯•
                        should_retry = False
                        if current_retry <= max_retries:
                            retry_on_exceptions = retry_config.get('retry_on_exceptions')

                            if retry_on_exceptions:
                                exc_type_name = type(e).__name__
                                should_retry = exc_type_name in retry_on_exceptions
                            else:
                                should_retry = True

                        if should_retry:
                            # è®¡ç®—é‡è¯•å»¶è¿Ÿ
                            if isinstance(e, RetryableError) and e.retry_after is not None:
                                delay = e.retry_after
                            else:
                                retry_backoff = retry_config.get('retry_backoff', True)
                                if retry_backoff:
                                    base_delay = 1.0
                                    delay = min(base_delay * (2 ** (current_retry - 1)),
                                              retry_config.get('retry_backoff_max', 60))
                                else:
                                    delay = 1.0

                            logger.info(f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥ (å°è¯• {current_retry}/{max_retries}): {str(e)}ï¼Œå°†åœ¨ {delay:.2f}s åé‡è¯•')
                            await asyncio.sleep(delay)
                            continue
                        else:
                            # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°æˆ–ä¸æ»¡è¶³é‡è¯•æ¡ä»¶
                            # å¦‚æœé…ç½®äº†é‡è¯•ï¼Œæ˜¾ç¤ºé‡è¯•æ¬¡æ•°ï¼›å¦åˆ™ç›´æ¥æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                            if max_retries > 0:
                                logger.error(f'ä»»åŠ¡åœ¨ {current_retry-1} æ¬¡é‡è¯•åå¤±è´¥: {str(e)}')
                            else:
                                logger.error(f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}')

                            status = TaskStatus.ERROR.value
                            exception = filter_framework_traceback()
                            error_msg = str(e)
                            logger.error(exception)

                            offset = self._extract_offset(event_data)
                            await self._quick_ack(queue, event_id, group_name, offset)
                            break

            finally:
                # è®¡ç®—å®Œæˆæ—¶é—´
                completed_at = time.time()
                execution_time = max(0, completed_at - execution_start_time)
                total_latency = max(0, completed_at - trigger_time_float)

                # æ”¶é›†ç»Ÿè®¡
                # await self._collect_stats_async(
                #     queue=queue,
                #     success=(status == "success"),
                #     processing_time=execution_time,
                #     total_latency=total_latency
                # )

                # æ›´æ–°ä»»åŠ¡ä¿¡æ¯
                task_info = {
                    "completed_at": str(completed_at),
                    "execution_time": execution_time,
                    "duration": total_latency,
                    "consumer": consumer,
                    'status': status,
                    'retries': current_retry,
                    "trigger_time_float": str(trigger_time_float),
                    "queue": str(queue)
                }

                if ret:
                    task_info["result"] = ret if isinstance(ret, str) else dumps_str(ret)

                if exception:
                    task_info["exception"] = exception
                if error_msg:
                    task_info["error_msg"] = error_msg

                if status_key in self.task_info_updates:
                    self.task_info_updates[status_key].update(task_info)
                else:
                    self.task_info_updates[status_key] = task_info

                # è°ƒç”¨on_end
                if task:
                    if 'clean_kwargs' not in locals():
                        clean_kwargs = {k: v for k, v in kwargs_inner.items()
                                      if not k.startswith('_') and not k.startswith('__')}

                    result = task.on_end(
                        event_id=event_id,
                        args=args,
                        kwargs=clean_kwargs,
                        result=ret,
                        pedding_count=self.pedding_count,
                    )
                    if asyncio.iscoroutine(result):
                        await result

                # å¤„ç†routing
                if routing:
                    agg_key = routing.get("agg_key")
                    routing_key = routing.get("routing_key")
                    if routing_key and agg_key:
                        if queue in self.app.ep.solo_running_state and routing_key in self.app.ep.solo_running_state[queue]:
                            self.app.ep.solo_running_state[queue][routing_key] -= 1
                    try:
                        if result and result.urgent_retry:
                            self.app.ep.solo_urgent_retry[routing_key] = True
                    except:
                        pass
                    if result and result.delay:
                        self.app.ep.task_scheduler[queue][routing_key] = time.time() + result.delay

                self.batch_counter -= 1

    def _extract_offset(self, event_data: dict) -> Optional[int]:
        """ä»event_dataä¸­æå–offset"""
        if isinstance(event_data, dict):
            offset = event_data.get('offset')
            if offset is not None:
                try:
                    return int(offset)
                except (ValueError, TypeError):
                    pass
        return None

    def _send_task_changes_with_offset(self, task_ids: list, pipeline):
        """å‘é€ TASK_CHANGES æ¶ˆæ¯å¹¶è‡ªåŠ¨æ›´æ–° offset

        ä½¿ç”¨ Lua è„šæœ¬ç¡®ä¿ï¼š
        1. è‡ªåŠ¨é€’å¢ offset
        2. æ³¨å†Œé˜Ÿåˆ—åˆ° REGISTRY:QUEUES
        3. æ¶ˆæ¯ä¸­åŒ…å« offset å­—æ®µ

        Args:
            task_ids: ä»»åŠ¡ ID åˆ—è¡¨
            pipeline: Redis pipeline
        """
        from jettask.utils.serializer import dumps_str

        # å‡†å¤‡Luaè„šæœ¬å‚æ•°
        stream_key = f"{self.prefix}:QUEUE:TASK_CHANGES".encode()
        lua_args = [self.prefix.encode() if isinstance(self.prefix, str) else self.prefix]

        # ä¸ºæ¯ä¸ª task_id æ„å»ºæ¶ˆæ¯
        for task_id in task_ids:
            message_data = {'kwargs': {'task_id': task_id}}
            data = dumps_str(message_data)
            lua_args.append(data)

        # ä½¿ç”¨ pipeline æ‰§è¡Œ Lua è„šæœ¬
        pipeline.eval(
            LUA_SCRIPT_BATCH_SEND_EVENT,
            1,  # 1ä¸ªKEY
            stream_key,  # KEY[1]: stream key
            *lua_args  # ARGV: prefix, data1, data2, ...
        )
        logger.debug(f"å·²æ·»åŠ  {len(task_ids)} æ¡ TASK_CHANGES æ¶ˆæ¯åˆ° pipeline")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.debug("ExecutorCore cleaning up...")

        # åœæ­¢é™æµå™¨ï¼ˆé‡Šæ”¾æ‰€æœ‰æŒæœ‰çš„é”ï¼‰
        if self.rate_limiter_manager:
            try:
                await self.rate_limiter_manager.stop_all()
                logger.debug("Rate limiter manager stopped and locks released")
            except Exception as e:
                logger.error(f"Error stopping rate limiter manager: {e}")

        # åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº
        try:
            await asyncio.wait_for(self._flush_all_buffers(), timeout=0.5)
            logger.debug("Buffers flushed successfully")
        except asyncio.TimeoutError:
            logger.warning("Buffer flush timeout")
        except Exception as e:
            logger.error(f"Error flushing buffers: {e}")



__all__ = ['ExecutorCore', 'ExecutionMode', 'UPDATE_MAX_OFFSET_LUA']
