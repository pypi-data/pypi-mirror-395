{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit test data\n",
    "\n",
    "This directory contains very small, toy, data sets that are used\n",
    "for unit tests.\n",
    "\n",
    "## Object catalog: small_sky\n",
    "\n",
    "This \"object catalog\" is 131 randomly generated radec values.\n",
    "\n",
    "- All radec positions are in the Healpix pixel order 0, pixel 11.\n",
    "- IDs are integers from 700-831.\n",
    "\n",
    "The following are imports and paths that are used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import lsdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "import pyarrow.parquet as pq\n",
    "from dask.distributed import Client\n",
    "from hats.io.file_io import remove_directory\n",
    "\n",
    "from hats_import import pipeline_with_client, ImportArguments\n",
    "\n",
    "tmp_path = tempfile.TemporaryDirectory()\n",
    "tmp_dir = tmp_path.name\n",
    "\n",
    "hats_import_dir = \".\"\n",
    "client = Client(n_workers=1, threads_per_worker=1, local_directory=tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky\n",
    "\n",
    "This \"object catalog\" is 131 randomly generated radec values.\n",
    "\n",
    "- All radec positions are in the Healpix pixel order 0, pixel 11.\n",
    "- IDs are integers from 700-831.\n",
    "\n",
    "This catalog was generated with the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_object_catalog\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        highest_healpix_order=5,\n",
    "        output_artifact_name=\"small_sky_object_catalog\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "        addl_hats_properties={\"hats_cols_default\": [\"ra\", \"dec\", \"id\"]},\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source catalog\n",
    "\n",
    "### small_sky_source\n",
    "\n",
    "This \"source catalog\" is 131 detections at each of the 131 objects\n",
    "in the \"small_sky\" catalog. These have a random magnitude, MJD, and\n",
    "band (selected from ugrizy). The full script that generated the values\n",
    "can be found [here](https://github.com/delucchi-cmu/hipscripts/blob/main/twiddling/small_sky_source.py)\n",
    "\n",
    "The catalog was generated with the following snippet, using raw data\n",
    "from the `hats-import` file.\n",
    "\n",
    "NB: `pixel_threshold=3000` is set just to make sure that we're generating\n",
    "a handful of files at various healpix orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_source_catalog\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky_source\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        ra_column=\"source_ra\",\n",
    "        dec_column=\"source_dec\",\n",
    "        catalog_type=\"source\",\n",
    "        highest_healpix_order=5,\n",
    "        pixel_threshold=3000,\n",
    "        drop_empty_siblings=False,\n",
    "        output_artifact_name=\"small_sky_source_catalog\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small_sky_source_npix_dir\n",
    "\n",
    "Similar to small_sky_source but with LEAF pixel directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_source_npix_dir_catalog\")\n",
    "with tempfile.TemporaryDirectory() as pipeline_tmp:\n",
    "    args = ImportArguments(\n",
    "        input_path=Path(hats_import_dir) / \"small_sky_source\",\n",
    "        output_path=\".\",\n",
    "        file_reader=\"csv\",\n",
    "        ra_column=\"source_ra\",\n",
    "        dec_column=\"source_dec\",\n",
    "        catalog_type=\"source\",\n",
    "        highest_healpix_order=5,\n",
    "        pixel_threshold=3000,\n",
    "        drop_empty_siblings=False,\n",
    "        output_artifact_name=\"small_sky_source_npix_dir_catalog\",\n",
    "        tmp_dir=pipeline_tmp,\n",
    "        npix_suffix=\"/\",  # Directory suffix\n",
    "    )\n",
    "    pipeline_with_client(args, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested catalog: small_sky_nested_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_sky_object = lsdb.read_hats(\"small_sky_object_catalog\", columns=\"all\")\n",
    "small_sky_source = lsdb.read_hats(\"small_sky_source_catalog\")\n",
    "small_sky_nested = small_sky_object.join_nested(\n",
    "    small_sky_source, left_on=\"id\", right_on=\"object_id\", nested_column_name=\"lc\"\n",
    ")\n",
    "lsdb.io.to_hats(\n",
    "    small_sky_nested,\n",
    "    base_catalog_path=\"small_sky_nested_catalog\",\n",
    "    catalog_name=\"small_sky_nested_catalog\",\n",
    "    histogram_order=5,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association catalog: small_sky_object_source_association\n",
    "\n",
    "This catalog contains a mapping between small_sky objects and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(\"./small_sky_object_source_association\")\n",
    "\n",
    "association = small_sky_object.crossmatch(small_sky_source, suffixes=(\"_obj\", \"_src\"), radius_arcsec=3600)\n",
    "\n",
    "lsdb.io.to_association(\n",
    "    association[[\"id_obj\", \"object_id_src\", \"_dist_arcsec\"]],\n",
    "    base_catalog_path=\"small_sky_object_source_association\",\n",
    "    catalog_name=\"small_sky_object_source_association\",\n",
    "    primary_catalog_dir=\"small_sky_object_catalog\",\n",
    "    primary_column_association=\"id_obj\",\n",
    "    primary_id_column=\"id\",\n",
    "    join_catalog_dir=\"small_sky_source_catalog\",\n",
    "    join_column_association=\"object_id_src\",\n",
    "    join_id_column=\"object_id\",\n",
    "    separation_column=\"_dist_arcsec\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "tmp_path.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Malformed Catalogs: bad_schemas and wrong_files_and_rows\n",
    "\n",
    "These datasets are designed to fail verification tests.\n",
    "They are generated by mangling `small_sky_object_catalog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input data that will be used to generate the malformed catalogs.\n",
    "input_dataset_path = Path(hats_import_dir) / \"small_sky_object_catalog\" / \"dataset\"\n",
    "input_ds = pds.parquet_dataset(input_dataset_path / \"_metadata\")\n",
    "\n",
    "# Unit tests expect the Npix=11 data file\n",
    "input_frag = next(frag for frag in input_ds.get_fragments() if frag.path.endswith(\"Npix=11.parquet\"))\n",
    "frag_key = Path(input_frag.path).relative_to(input_dataset_path)\n",
    "input_tbl = input_frag.to_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_write_metadata(output_dataset_path: Path, schema: pa.Schema | None = None) -> None:\n",
    "    schema = schema or input_tbl.schema\n",
    "    dataset = pds.dataset(output_dataset_path)\n",
    "    metadata_collector = []\n",
    "    for frag in dataset.get_fragments():\n",
    "        frag.ensure_complete_metadata()\n",
    "        frag.metadata.set_file_path(str(Path(frag.path).relative_to(output_dataset_path)))\n",
    "        metadata_collector.append(frag.metadata)\n",
    "    pq.write_metadata(\n",
    "        schema=schema, where=output_dataset_path / \"_metadata\", metadata_collector=metadata_collector\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bad_schemas\n",
    "\n",
    "This dataset is designed to fail all schema verification tests.\n",
    "\n",
    "```\n",
    "bad_schemas/\n",
    "|- dataset/\n",
    "    |- _common_metadata.import_truth        # mimics schema provided by user upon import\n",
    "    |- _common_metadata                     # wrong types\n",
    "    |- _metadata                            # wrong file-level metadata\n",
    "    |- Norder=0/Dir=0/\n",
    "        |- Npix=11.parquet                  # direct copy of input\n",
    "        |- Npix=11.extra_column.parquet     # extra column\n",
    "        |- Npix=11.missing_column.parquet   # missing column\n",
    "        |- Npix=11.wrong_dtypes.parquet     # wrong types\n",
    "        |- Npix=11.wrong_metadata.parquet   # wrong metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset_path = Path(\".\") / \"bad_schemas\" / \"dataset\"\n",
    "remove_directory(output_dataset_path)\n",
    "\n",
    "# Existing files may result in unexpected metadata output.\n",
    "if output_dataset_path.parent.exists() and any(output_dataset_path.parent.iterdir()):\n",
    "    raise FileExistsError(\"bad_schemas directory exists and is not empty. Remove it and try again.\")\n",
    "\n",
    "# We will create the following files using input_frag\n",
    "ffrag_out = output_dataset_path / frag_key\n",
    "fextra_col = ffrag_out.with_suffix(\".extra_column.parquet\")\n",
    "fmissing_col = ffrag_out.with_suffix(\".missing_column.parquet\")\n",
    "fwrong_types = ffrag_out.with_suffix(\".wrong_dtypes.parquet\")\n",
    "fwrong_metadata = ffrag_out.with_suffix(\".wrong_metadata.parquet\")\n",
    "\n",
    "ffrag_out.parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a direct copy of input_frag for all files that will be recorded in the _metadata file\n",
    "for file_out in [ffrag_out, fmissing_col, fextra_col, fwrong_types]:\n",
    "    shutil.copy(input_frag.path, file_out)\n",
    "\n",
    "# Write a _metadata that has the correct schema except for file-level metadata\n",
    "metadata = input_tbl.schema.metadata or {}\n",
    "metadata.update({b\"extra key\": b\"extra value\"})\n",
    "collect_and_write_metadata(output_dataset_path, schema=input_tbl.schema.with_metadata(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write new data files using incorrect schemas.\n",
    "\n",
    "# Drop a column\n",
    "pq.write_table(input_tbl.drop_columns(\"dec_error\"), fmissing_col)\n",
    "\n",
    "# Add an extra column\n",
    "extra_col = pa.array(range(len(input_tbl)))\n",
    "extra_col_tbl = input_tbl.add_column(5, pa.field(\"extra\", pa.int64()), extra_col)\n",
    "pq.write_table(extra_col_tbl, fextra_col)\n",
    "\n",
    "# Mangle file-level metadata\n",
    "wrong_metadata = {\"bad key\": \"bad value\"}\n",
    "pq.write_table(input_tbl.replace_schema_metadata(wrong_metadata), fwrong_metadata)\n",
    "\n",
    "# Change some types\n",
    "wrong_dtypes_fields = [\n",
    "    fld if not fld.name.startswith(\"ra\") else fld.with_type(pa.float16()) for fld in input_tbl.schema\n",
    "]\n",
    "wrong_dtypes_schema = pa.schema(wrong_dtypes_fields, metadata=input_tbl.schema.metadata)\n",
    "pq.write_table(input_tbl.cast(wrong_dtypes_schema), fwrong_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a _common_metadata with the wrong dtypes.\n",
    "pq.write_metadata(schema=wrong_dtypes_schema, where=output_dataset_path / \"_common_metadata\")\n",
    "\n",
    "# Write a _common_metadata with the correct schema but no hats columns.\n",
    "# This mimics a schema that could have been passed as 'use_schema_file' upon import.\n",
    "fimport_schema = (output_dataset_path / \"_common_metadata\").with_suffix(\".import_truth\")\n",
    "hats_cols = [\"_healpix_29\", \"Norder\", \"Dir\", \"Npix\"]\n",
    "import_schema = pa.schema([fld for fld in input_tbl.schema if fld.name not in hats_cols])\n",
    "pq.write_metadata(schema=import_schema, where=fimport_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrong_files_and_rows\n",
    "\n",
    "This dataset is designed to fail the following verification tests:\n",
    "\n",
    "- Files listed in metadata match files on disk.\n",
    "- Row counts in metadata match row counts on disk and (if provided) user-supplied truth.\n",
    "- `hats.io.validation.is_valid_catalog`\n",
    "\n",
    "```\n",
    "wrong_files_and_rows/\n",
    "|- properties                               # direct copy of input\n",
    "|- dataset/\n",
    "    |- _common_metadata                     # direct copy of input\n",
    "    |- _metadata                            # missing file 'Npix=11.extra_file.parquet'\n",
    "    |- Norder=0/Dir=0/\n",
    "        |- Npix=11.parquet                  # direct copy of input\n",
    "        |- Npix=11.extra_file.parquet       # added after _metadata generated\n",
    "        |- Npix=11.extra_rows.parquet       # rows appended after _metadata generated\n",
    "        |- (Npix=11.missing_file.parquet)   # dropped after _metadata generated\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset_path = Path(\".\") / \"wrong_files_and_rows\" / \"dataset\"\n",
    "remove_directory(output_dataset_path.parent)\n",
    "\n",
    "# Existing files may result in unexpected metadata output.\n",
    "if output_dataset_path.parent.exists() and any(output_dataset_path.parent.iterdir()):\n",
    "    raise FileExistsError(\"wrong_files_and_rows directory exists and is not empty. Remove it and try again.\")\n",
    "\n",
    "# We will create the following files using input_frag\n",
    "ffrag_out = output_dataset_path / frag_key\n",
    "fmissing_file = ffrag_out.with_suffix(\".missing_file.parquet\")\n",
    "fextra_file = ffrag_out.with_suffix(\".extra_file.parquet\")\n",
    "fextra_rows = ffrag_out.with_suffix(\".extra_rows.parquet\")\n",
    "\n",
    "ffrag_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy metadata files that we will not alter\n",
    "shutil.copy(input_dataset_path.parent / \"properties\", output_dataset_path.parent / \"properties\")\n",
    "shutil.copy(input_dataset_path / \"_common_metadata\", output_dataset_path / \"_common_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a direct copy of input_frag for all files that will be recorded in the _metadata file\n",
    "for file_out in [ffrag_out, fmissing_file, fextra_rows]:\n",
    "    shutil.copy(input_frag.path, file_out)\n",
    "\n",
    "# Write _metadata\n",
    "collect_and_write_metadata(output_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mangle the dataset.\n",
    "\n",
    "# Add a file\n",
    "shutil.copy(input_frag.path, fextra_file)\n",
    "\n",
    "# Remove a file\n",
    "fmissing_file.unlink()\n",
    "\n",
    "# Add rows to an existing file\n",
    "new_tbl = pa.concat_tables([input_tbl, input_tbl.take([1, 2, 3, 4])])\n",
    "pq.write_table(new_tbl, fextra_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
