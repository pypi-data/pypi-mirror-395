Metadata-Version: 2.1
Name: rag-opt
Version: 0.1.9
Summary: RAG workflow Optimizer based on Bayesian Optimization
Author-Email: abdelrahman abounida <abdelrahmanaboneda@gmail.com>
License: MIT
Requires-Python: <3.13,>=3.11
Requires-Dist: langchain-community>=0.3.21
Requires-Dist: langchain>=0.3.27
Requires-Dist: unstructured>=0.18.14
Requires-Dist: loguru>=0.7.3
Requires-Dist: langchain-text-splitters>=0.3.11
Requires-Dist: tiktoken>=0.11.0
Requires-Dist: qdrant-client>=1.12.1
Requires-Dist: langchain-chroma>=0.2.2
Requires-Dist: faiss-cpu>=1.11.0.post1
Requires-Dist: numpy>=1.26.4
Requires-Dist: langchain-qdrant>=0.2.0
Requires-Dist: flashrank>=0.2.10
Requires-Dist: rank-bm25>=0.2.2
Requires-Dist: fastmobo==0.1.6
Requires-Dist: langsmith>=0.3.45
Requires-Dist: langchain-unstructured>=0.1.6
Requires-Dist: langchain-core>=0.3.76
Requires-Dist: openai>=1.107.2
Requires-Dist: aiohttp>=3.12.15
Requires-Dist: msoffcrypto-tool>=5.4.2
Requires-Dist: langchain-openai>=0.3.34
Requires-Dist: ragatouille>=0.0.9.post2
Requires-Dist: pinecone>=7.3.0
Requires-Dist: langchain-pinecone>=0.2.12
Requires-Dist: pinecone-client>=6.0.0
Description-Content-Type: text/markdown

<p align="center">
  <img src="/assets/light2.svg" alt="RAGOpt Logo" width="1000"/>
</p>

<p align="center">
  RAGOpt eliminates manual hyperparameter tuning in your RAG pipelines with Bayesian optimization ðŸš€
</p>

<p align="center">
  <a href="https://github.com/your-repo/publish"><img src="https://img.shields.io/badge/Publish-passing-brightgreen" alt="Publish passing"></a>
  <a href="https://pypi.org/project/your-package/"><img src="https://img.shields.io/badge/release-v0.1.5-orange" alt="PyPI version"></a>
  <a href="#"><img src="https://img.shields.io/badge/Python-3.11-blue" alt="Python 3.11"></a>
  <a href="LICENSE"><img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="MIT License"></a>
</p>

---

- **Documentation**: https://ragopt.aboneda.com
- **Colab Notebook**: https://colab.research.google.com/drive/1hrfAHCfm3x0Ov-amCEHpptMiyoqC-McE

---

**RAGOpt** is a Python framework to optimize Retrieval-Augmented Generation (RAG) pipelines. It eliminates manual hyperparameter tuning using Bayesian optimization, automatically finding the best configuration for your dataset and use case.

## Key Features

- Optimizes 20+ RAG hyperparameters including chunk size, overlap, embedding strategies, and LLM selection.
- Flexible with any LangChain-compatible model or provider.
- Partially Opinionated - Smart defaults with full flexibility for customization
- Generates Pareto-optimal configurations for your specific data.
- Comprehensive Metrics - Quality (precision, recall, faithfulness), performance (latency, cost), and safety (toxicity, bias)

---

## Installation

```bash
pip install rag-opt
```

## ðŸ”¥ Quick Start

### 1. Generate Training Questions

```python
from langchain.chat_models import init_chat_model
from rag_opt.rag import DatasetGenerator

# Initialize LLM
llm = init_chat_model(
    model="gpt-3.5-turbo",
    model_provider="openai",
    api_key="sk-***"
)

# Generate Q&A pairs from your documents
data_gen = DatasetGenerator(llm, dataset_path="./data")
dataset = data_gen.generate(10)
dataset.to_json("./rag_dataset.json")
```

### 2. Define Your Search Space

Create `rag_config.yaml`:

```yaml
chunk_size:
  bounds: [512, 1024]
  dtype: int

max_tokens:
  bounds: [256, 512]
  dtype: int

chunk_overlap:
  bounds: [0, 200]
  dtype: int

temperature:
  bounds: [0.0, 1.0]
  dtype: float

search_type:
  choices: ["similarity", "mmr", "hybrid"]

vector_store:
  choices:
    faiss: {}
    pinecone:
      api_key: "YOUR_API_KEY"
      index_name: "your-index"

embedding:
  choices:
    openai:
      api_key: "YOUR_API_KEY"
      models:
        - "text-embedding-3-large"
        - "text-embedding-ada-002"
    huggingface:
      models:
        - "all-MiniLM-L6-v2"

llm:
  choices:
    openai:
      api_key: "YOUR_API_KEY"
      models:
        - "gpt-4o"
        - "gpt-3.5-turbo"

k:
  bounds: [1, 10]
  dtype: int

use_reranker: false
```

### 3. Run Optimization

```python
from rag_opt.dataset import TrainDataset
from rag_opt.optimizer import Optimizer

# Load dataset
train_dataset = TrainDataset.from_json("rag_dataset.json")

# Initialize optimizer
optimizer = Optimizer(
    train_dataset=train_dataset,
    config_path="rag_config.yaml",
    verbose=True
)

# Find optimal configuration
best_config = optimizer.optimize(n_trials=3, best_one=True)
best_config.to_json()
```

### 4. Get Your Optimized Config

Output example:

```yaml
{
"chunk_size": 500
"max_tokens": 100
"chunk_overlap": 200
"search_type": "hybrid"
"k": "1"
"temperature": 1.0
"embedding":
  "provider": "openai"
  "model": "text-embedding-3-large"
"llm":
  "provider": "openai"
  "model": "gpt-4o"
"vector_store":
  "provider": "faiss"
"use_reranker": "true"
}
```

## How It Works

1. **Dataset Generation** - Create synthetic Q&A pairs from your documents
2. **Search Space Definition** - Configure which parameters to optimize
3. **Bayesian Optimization** - Intelligently sample and evaluate configurations
4. **Multi-Metric Evaluation** - Assess quality, performance, and safety
5. **Pareto-Optimal Results** - Get the best configurations for your priorities

## RAGOpt vs Alternatives

- **AutoRAG**: Uses Bayesian optimization instead of grid search like AutoRAG
- **Ragas**: Flexible evaluation framework, not rigidâ€”bring your own metrics
- **Manual Tuning**: Systematic, data-driven approach saves time and improves results

## License

This project is licensed under the terms of the MIT license.
