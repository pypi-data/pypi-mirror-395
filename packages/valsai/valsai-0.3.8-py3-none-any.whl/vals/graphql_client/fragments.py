# Generated by ariadne-codegen
# Source: vals/graphql/

from datetime import datetime
from typing import Any, List, Optional

from pydantic import Field

from .base_model import BaseModel
from .enums import EvaluationStatus, RunStatus


class ExampleOutputTypeFields(BaseModel):
    type: str
    text: str


class ConditionalCheckOutputTypeFields(BaseModel):
    operator: str
    criteria: str


class CheckModifierOutputTypeFields(BaseModel):
    optional: bool
    severity: Optional[float]
    display_metrics: bool = Field(alias="displayMetrics")
    examples: List["CheckModifierOutputTypeFieldsExamples"]
    extractor: Optional[str]
    conditional: Optional["CheckModifierOutputTypeFieldsConditional"]
    category: Optional[str]


class CheckModifierOutputTypeFieldsExamples(ExampleOutputTypeFields):
    pass


class CheckModifierOutputTypeFieldsConditional(ConditionalCheckOutputTypeFields):
    pass


class CheckOutputTypeFields(BaseModel):
    operator: str
    criteria: str
    modifiers: "CheckOutputTypeFieldsModifiers"


class CheckOutputTypeFieldsModifiers(CheckModifierOutputTypeFields):
    pass


class MetadataOutputTypeFields(BaseModel):
    in_tokens: int = Field(alias="inTokens")
    out_tokens: int = Field(alias="outTokens")
    duration_seconds: float = Field(alias="durationSeconds")


class QuestionAnswerPairFragment(BaseModel):
    id: Any
    input_under_test: str = Field(alias="inputUnderTest")
    llm_output: str = Field(alias="llmOutput")
    context: Any
    output_context: Any = Field(alias="outputContext")
    metadata: "QuestionAnswerPairFragmentMetadata"
    file_ids: List[str] = Field(alias="fileIds")
    local_evals: List["QuestionAnswerPairFragmentLocalEvals"] = Field(
        alias="localEvals"
    )
    test: "QuestionAnswerPairFragmentTest"
    status: EvaluationStatus


class QuestionAnswerPairFragmentMetadata(BaseModel):
    in_tokens: int = Field(alias="inTokens")
    out_tokens: int = Field(alias="outTokens")
    duration_seconds: float = Field(alias="durationSeconds")


class QuestionAnswerPairFragmentLocalEvals(BaseModel):
    id: str
    score: float
    feedback: str
    created_at: datetime = Field(alias="createdAt")


class QuestionAnswerPairFragmentTest(BaseModel):
    id: str


class ResultJsonElementTypeFields(BaseModel):
    operator: str
    criteria: str
    modifiers: "ResultJsonElementTypeFieldsModifiers"
    auto_eval: float = Field(alias="autoEval")
    feedback: str
    eval_cont: float = Field(alias="evalCont")
    is_global: bool = Field(alias="isGlobal")
    severity: float
    custom_operator: Optional["ResultJsonElementTypeFieldsCustomOperator"] = Field(
        alias="customOperator"
    )


class ResultJsonElementTypeFieldsModifiers(CheckModifierOutputTypeFields):
    pass


class ResultJsonElementTypeFieldsCustomOperator(BaseModel):
    rubric: Optional[List["ResultJsonElementTypeFieldsCustomOperatorRubric"]]


class ResultJsonElementTypeFieldsCustomOperatorRubric(BaseModel):
    label: str
    score: int


class RunFragment(BaseModel):
    qa_set: "RunFragmentQaSet" = Field(alias="qaSet")
    run_id: str = Field(alias="runId")
    run_review: Optional["RunFragmentRunReview"] = Field(alias="runReview")
    status: RunStatus
    text_summary: str = Field(alias="textSummary")
    timestamp: datetime
    completed_at: Optional[datetime] = Field(alias="completedAt")
    archived: bool
    name: str
    error_message: str = Field(alias="errorMessage")
    average_duration: float = Field(alias="averageDuration")
    average_tokens_in: float = Field(alias="averageTokensIn")
    average_tokens_out: float = Field(alias="averageTokensOut")
    parameters: "RunFragmentParameters"
    pass_rate: Optional["RunFragmentPassRate"] = Field(alias="passRate")
    success_rate: Optional["RunFragmentSuccessRate"] = Field(alias="successRate")
    custom_metrics: List["RunFragmentCustomMetrics"] = Field(alias="customMetrics")
    test_suite: "RunFragmentTestSuite" = Field(alias="testSuite")
    project: "RunFragmentProject"


class RunFragmentQaSet(BaseModel):
    id: Any


class RunFragmentRunReview(BaseModel):
    id: Any


class RunFragmentParameters(BaseModel):
    eval_model: str = Field(alias="evalModel")
    maximum_threads: int = Field(alias="maximumThreads")
    run_confidence_evaluation: bool = Field(alias="runConfidenceEvaluation")
    heavyweight_factor: int = Field(alias="heavyweightFactor")
    create_text_summary: bool = Field(alias="createTextSummary")
    model_under_test: str = Field(alias="modelUnderTest")
    temperature: Optional[float]
    max_output_tokens: int = Field(alias="maxOutputTokens")
    system_prompt: str = Field(alias="systemPrompt")
    custom_parameters: Any = Field(alias="customParameters")


class RunFragmentPassRate(BaseModel):
    value: float
    error: float


class RunFragmentSuccessRate(BaseModel):
    value: float
    error: float


class RunFragmentCustomMetrics(BaseModel):
    metric_id: str = Field(alias="metricId")
    name: str
    pass_rate: float = Field(alias="passRate")
    error: Optional[str]


class RunFragmentTestSuite(BaseModel):
    id: str
    title: str


class RunFragmentProject(BaseModel):
    id: Any
    slug: str


class TestFragment(BaseModel):
    id: str
    input_under_test: str = Field(alias="inputUnderTest")
    context: Any
    tags: List[str]
    cross_version_id: str = Field(alias="crossVersionId")
    golden_output: str = Field(alias="goldenOutput")
    file_ids: List[str] = Field(alias="fileIds")
    checks: List["TestFragmentChecks"]
    test_suite: "TestFragmentTestSuite" = Field(alias="testSuite")


class TestFragmentChecks(BaseModel):
    operator: str
    criteria: str
    modifiers: "TestFragmentChecksModifiers"


class TestFragmentChecksModifiers(BaseModel):
    optional: bool
    severity: Optional[float]
    examples: List["TestFragmentChecksModifiersExamples"]
    extractor: Optional[str]
    conditional: Optional["TestFragmentChecksModifiersConditional"]
    category: Optional[str]
    display_metrics: bool = Field(alias="displayMetrics")


class TestFragmentChecksModifiersExamples(BaseModel):
    type: str
    text: str


class TestFragmentChecksModifiersConditional(BaseModel):
    operator: str
    criteria: str


class TestFragmentTestSuite(BaseModel):
    id: str


class TestResultFields(BaseModel):
    id: Any
    llm_output: str = Field(alias="llmOutput")
    result_json: List["TestResultFieldsResultJson"] = Field(alias="resultJson")
    qa_pair: "TestResultFieldsQaPair" = Field(alias="qaPair")
    test: "TestResultFieldsTest"
    metadata: "TestResultFieldsMetadata"


class TestResultFieldsResultJson(ResultJsonElementTypeFields):
    pass


class TestResultFieldsQaPair(BaseModel):
    context: Any
    output_context: Any = Field(alias="outputContext")
    error_message: str = Field(alias="errorMessage")


class TestResultFieldsTest(TestFragment):
    pass


class TestResultFieldsMetadata(MetadataOutputTypeFields):
    pass


ExampleOutputTypeFields.model_rebuild()
ConditionalCheckOutputTypeFields.model_rebuild()
CheckModifierOutputTypeFields.model_rebuild()
CheckOutputTypeFields.model_rebuild()
MetadataOutputTypeFields.model_rebuild()
QuestionAnswerPairFragment.model_rebuild()
ResultJsonElementTypeFields.model_rebuild()
RunFragment.model_rebuild()
TestFragment.model_rebuild()
TestResultFields.model_rebuild()
