{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vals import Suite, Test, Check, RunParameters, RunStatus, Run\n",
    "from vals.sdk.auth import configure_credentials\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "vals_api_key = os.getenv(\"VALS_API_KEY\") or \"\"\n",
    "\n",
    "print(\"Sourced vals api key=%s\" % vals_api_key)\n",
    "print(\"Sourced vals env=%s\" % os.getenv(\"VALS_ENV\"))\n",
    "\n",
    "configure_credentials(vals_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cba5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Default functionality for running a test suite. Goes over\n",
    "- Pulling a test suite from an id\n",
    "- Create a test suite from scratch\n",
    "- Starting a run from a test suite\n",
    "- Waiting for a run to complete\n",
    "- Fetching the results of a run\n",
    "- Fetching the test results from a run\n",
    "- Fetching the qa pairs from a run\n",
    "- Exporting the results to either a json or csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda151b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull a test suite\n",
    "SUITE_ID = \"xxxx-xxx-42d4-a341-4b233fb88205\"\n",
    "suite = await Suite.from_id(SUITE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or create a test suite\n",
    "suite = Suite(\n",
    "    title=\"Test Suite\",\n",
    "    global_checks=[Check(operator=\"grammar\")],\n",
    "    tests=[\n",
    "        Test(\n",
    "            input_under_test=\"What is QSBS?\",\n",
    "            checks=[Check(operator=\"equals\", criteria=\"QSBS\")],\n",
    "        ),\n",
    "        Test(\n",
    "            input_under_test=\"What is an 83 election?\",\n",
    "            checks=[Check(operator=\"equals\", criteria=\"QSBS\")],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "await suite.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db81b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the parameters you would like to use for the run\n",
    "parameters = RunParameters(\n",
    "    parallelism=3,\n",
    "    max_output_tokens=2048,\n",
    "    custom_parameters={\n",
    "        \"top_p\": 0.5,\n",
    "        \"text\": {\"verbosity\": \"low\"},\n",
    "        \"reasoning\": {\"effort\": \"low\", \"summary\": \"auto\"},\n",
    "    },\n",
    ")\n",
    "\n",
    "# Model thats going to be under test\n",
    "model = \"openai/gpt-5-mini-2025-08-07\"\n",
    "\n",
    "run = await suite.run(model=model, parameters=parameters)\n",
    "\n",
    "# Wait for the run to complete before returning the results\n",
    "completed_status = await run.wait_for_run_completion()\n",
    "\n",
    "if completed_status == RunStatus.SUCCESS:\n",
    "    print(\"Run completed successfully. Status=%s\" % completed_status)\n",
    "else:\n",
    "    print(\"Run failed. Status=%s\" % completed_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacd23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling the run using the id is supported\n",
    "RUN_ID = \"xxxx-xxx-4709-924e-64d3ed837774\"\n",
    "run = await Run.from_id(RUN_ID)\n",
    "run_status = run.status\n",
    "\n",
    "\n",
    "print(\"Run status: %s\" % run_status)\n",
    "print(\"Can also visit the run result page directly at %s\" % run.url)\n",
    "\n",
    "\n",
    "print(\"===Averaged metadata inside of the run===\")\n",
    "print(\"Average duration: %s\" % run.average_duration)\n",
    "print(\"Average input tokens: %s\" % run.average_input_tokens)\n",
    "print(\"Average output tokens: %s\" % run.average_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd08450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the test results from the run\n",
    "test_results = await run.test_results\n",
    "\n",
    "for test_result in test_results[:2]:\n",
    "    print(\n",
    "        \"Question=%s, Answer=%s\"\n",
    "        % (test_result.input_under_test[0:50], test_result.llm_output[0:100])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the results by tags, operators and string\n",
    "tags_to_filter = [\"tag1\", \"tag2\"]\n",
    "operators_to_filter = [\"INCLUDES_ANY\"]\n",
    "search_string = \"What is burden shifting under Title VII?\"\n",
    "\n",
    "filtered_test_results = await run.fetch_test_results(\n",
    "    operators=operators_to_filter, tags=tags_to_filter, search=search_string\n",
    ")\n",
    "\n",
    "print(\"Fetched %s test results\" % len(filtered_test_results))\n",
    "print(filtered_test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306dc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also retrieve just the question answer pairs\n",
    "qa_pairs = await run.qa_pairs\n",
    "\n",
    "for qa_pair in qa_pairs[:2]:\n",
    "    print(\n",
    "        \"Question=%s, Answer=%s\"\n",
    "        % (qa_pair.input_under_test[0:50], qa_pair.llm_output[0:100])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067c6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supports two file format types\n",
    "os.makedirs(\"data_files\", exist_ok=True)\n",
    "\n",
    "# Json\n",
    "json_data = await run.fetch_json()\n",
    "with open(\"data_files/run.json\", \"w\") as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "# Csv\n",
    "run_result_df, test_results_df = await run.fetch_csv()\n",
    "combined = (\n",
    "    run_result_df.to_csv(index=False) + \"\\n\" + test_results_df.to_csv(index=False)\n",
    ")\n",
    "\n",
    "with open(\"data_files/run.csv\", \"w\") as f:\n",
    "    f.write(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9119b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List runs using search params\n",
    "\n",
    "SUITE_ID = \"xxxx-xxx-42d4-a341-4b233fb88205\"\n",
    "status = RunStatus.SUCCESS\n",
    "model_under_test = \"openai/gpt-4o-mini\"\n",
    "\n",
    "results = await Run.list_runs(\n",
    "    suite_id=SUITE_ID,\n",
    "    model_under_test=model_under_test,\n",
    "    status=status,\n",
    ")\n",
    "\n",
    "for result in results[:10]:\n",
    "    print(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extra functionality for runs, can use these when the default options are not sufficient\n",
    "\n",
    "- Custom model functions\n",
    "- Starting a run from question answer pairs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b31bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vals.sdk.types import QuestionAnswerPair\n",
    "from typing import Any\n",
    "from io import BytesIO\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605cc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to need to create a method that when provided an input will return a response from a model\n",
    "# Do not need all these fields but showing you everything you can pass in\n",
    "async def query_model(\n",
    "    input: str, files: dict[str, BytesIO], context: dict[str, Any]\n",
    ") -> str:\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    assert openai_api_key is not None, \"OPENAI_API_KEY is not set\"\n",
    "\n",
    "    openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": input}],\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    if not content:\n",
    "        raise ValueError(\"No response from model\")\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom model function that will be used to run the suite\n",
    "async def custom_model(\n",
    "    input: str, files: dict[str, BytesIO], context: dict[str, Any]\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Custom model function instantiating a openai client that will be used to generate a response\n",
    "\n",
    "    You can use the context or files provided inside of the test you created earlier to generate a response\n",
    "    \"\"\"\n",
    "\n",
    "    # Use that query_model function we created earlier to generate a response\n",
    "    return await query_model(input, files, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the suite, specifying custom operators inside the checks if needed\n",
    "suite = Suite(\n",
    "    title=\"Test Suite with custom operators\",\n",
    "    tests=[\n",
    "        Test(\n",
    "            input_under_test=\"What is QSBS?\",\n",
    "            checks=[Check(operator=\"equals\", criteria=\"QSBS\")],\n",
    "            context={\n",
    "                \"message_history\": [\n",
    "                    {\"role\": \"user\", \"content\": \"What is QSBS?\"},\n",
    "                    {\"role\": \"assistant\", \"content\": \"QSBS is a company.\"},\n",
    "                ]\n",
    "            },\n",
    "            files_under_test=[\"../data_files/postmoney_safe.docx\"],\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "await suite.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc78c325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in the custom model function we created earlier\n",
    "run = await suite.run(\n",
    "    model=custom_model,\n",
    "    wait_for_completion=True,\n",
    "    parameters=RunParameters(parallelism=3),\n",
    ")\n",
    "\n",
    "print(f\"Run URL: {run.url}\")\n",
    "print(f\"Pass rate: {run.pass_rate}\")\n",
    "\n",
    "for test_result in await run.test_results:\n",
    "    print(\n",
    "        f\"Question={test_result.input_under_test[0:50]}, Answer={test_result.llm_output[0:100]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also start a run with previous data by constructing a question answer pair\n",
    "# This also supports metadata, output context, and more\n",
    "qa_pairs = [\n",
    "    QuestionAnswerPair(\n",
    "        input_under_test=\"What is QSBS?\",\n",
    "        llm_output=\"QSBS is a company.\",\n",
    "        # If there are duplicated questions with unique files, provide the file ids here to match the question answer pair to the test\n",
    "    )\n",
    "]\n",
    "\n",
    "# model name can be arbitrary, evaluation will be done inside of the platform using the question and answer pair set that you uploaded\n",
    "run = await suite.run(\n",
    "    model=qa_pairs, model_name=\"gpt-4o-mini\", wait_for_completion=True\n",
    ")\n",
    "\n",
    "print(f\"Run URL: {run.url}\")\n",
    "print(f\"Pass rate: {run.pass_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
