# This file is automatically generated by pyo3_stub_gen
# ruff: noqa: E501, F401

import builtins
import datetime
import typing

import numpy
import numpy.typing

@typing.final
class Adam:
    r"""
    Adaptive Moment Estimation (Adam) gradient-based optimiser.
    """
    def __new__(cls) -> Adam:
        r"""
        Create an Adam optimiser with library defaults.
        """
    def with_max_iter(self, max_iter: builtins.int) -> Adam:
        r"""
        Limit the maximum number of optimisation iterations.
        """
    def with_threshold(self, threshold: builtins.float) -> Adam:
        r"""
        Set the stopping threshold on the gradient norm.
        """
    def with_step_size(self, step_size: builtins.float) -> Adam:
        r"""
        Configure the base learning rate / step size.
        """
    def with_betas(self, beta1: builtins.float, beta2: builtins.float) -> Adam:
        r"""
        Override the exponential decay rates for the first and second moments.
        """
    def with_eps(self, eps: builtins.float) -> Adam:
        r"""
        Override the numerical stability constant added to the denominator.
        """
    def with_patience(self, patience_seconds: builtins.float) -> Adam:
        r"""
        Abort the run once the patience window has elapsed.
        """
    def run(
        self, problem: Problem, initial: typing.Sequence[builtins.float]
    ) -> OptimisationResults:
        r"""
        Optimise the given problem using Adam starting from the provided point.
        """

@typing.final
class CMAES:
    r"""
    Covariance Matrix Adaptation Evolution Strategy optimiser.
    """
    def __new__(cls) -> CMAES:
        r"""
        Create a CMA-ES optimiser with library defaults.
        """
    def with_max_iter(self, max_iter: builtins.int) -> CMAES:
        r"""
        Limit the number of iterations/generations before termination.
        """
    def with_threshold(self, threshold: builtins.float) -> CMAES:
        r"""
        Set the stopping threshold on the best objective value.
        """
    def with_sigma0(self, sigma0: builtins.float) -> CMAES:
        r"""
        Set the initial global step-size (standard deviation).
        """
    def with_patience(self, patience_seconds: builtins.float) -> CMAES:
        r"""
        Abort the run if no improvement occurs for the given wall-clock duration.
        """
    def with_population_size(self, population_size: builtins.int) -> CMAES:
        r"""
        Specify the number of offspring evaluated per generation.
        """
    def with_seed(self, seed: builtins.int) -> CMAES:
        r"""
        Initialise the internal RNG for reproducible runs.
        """
    def run(
        self, problem: Problem, initial: typing.Sequence[builtins.float]
    ) -> OptimisationResults:
        r"""
        Optimise the given problem starting from the provided mean vector.
        """

@typing.final
class CostMetric:
    @property
    def name(self) -> builtins.str:
        r"""
        Name of the cost metric.
        """
    def __repr__(self) -> builtins.str: ...

@typing.final
class DiffsolBuilder:
    r"""
    Differential equation solver builder.
    """
    def __new__(cls) -> DiffsolBuilder:
        r"""
        Create an empty differential solver builder.
        """
    def __copy__(self) -> DiffsolBuilder: ...
    def __deepcopy__(self, _memo: dict) -> DiffsolBuilder: ...
    def with_diffsl(self, dsl: builtins.str) -> DiffsolBuilder:
        r"""
        Register the DiffSL program describing the system dynamics.
        """
    def remove_diffsl(self) -> DiffsolBuilder:
        r"""
        Remove any registered DiffSL program.
        """
    def with_data(self, data: numpy.typing.NDArray[numpy.float64]) -> DiffsolBuilder:
        r"""
        Attach observed data used to fit the differential equation.

        The first column must contain the time samples (t_span) and the remaining
        columns the observed trajectories.
        """
    def remove_data(self) -> DiffsolBuilder:
        r"""
        Remove any previously attached data along with its time span.
        """
    def with_backend(self, backend: builtins.str) -> DiffsolBuilder:
        r"""
        Choose whether to use dense or sparse diffusion solvers.
        """
    def with_parallel(
        self, parallel: builtins.bool | None = None
    ) -> DiffsolBuilder:
        r"""
        Opt into parallel proposal generation when supported by the backend.
        """
    def with_config(
        self, config: typing.Mapping[builtins.str, builtins.float]
    ) -> DiffsolBuilder: ...
    def with_rtol(self, rtol: builtins.float) -> DiffsolBuilder:
        r"""
        Adjust the relative integration tolerance.
        """
    def with_atol(self, atol: builtins.float) -> DiffsolBuilder:
        r"""
        Adjust the absolute integration tolerance.
        """
    def with_parameter(
        self,
        name: builtins.str,
        initial_value: builtins.float,
        bounds: tuple[builtins.float, builtins.float] | None = None,
    ) -> DiffsolBuilder:
        r"""
        Register a named optimisation variable in the order it appears in vectors.
        """
    def clear_parameters(self) -> DiffsolBuilder:
        r"""
        Remove previously provided parameter defaults.
        """
    def with_cost(self, cost: CostMetric) -> DiffsolBuilder:
        r"""
        Select the error metric used to compare simulated and observed data.
        """
    def remove_cost(self) -> DiffsolBuilder:
        r"""
        Reset the cost metric to the default sum of squared errors.
        """
    def with_optimiser(self, optimiser: NelderMead | CMAES | Adam) -> DiffsolBuilder:
        r"""
        Configure the default optimiser used when `Problem.optimize` omits one.
        """
    def build(self) -> Problem:
        r"""
        Create a `Problem` representing the differential solver model.
        """

@typing.final
class NelderMead:
    r"""
    Classic simplex-based direct search optimiser.
    """
    def __new__(cls) -> NelderMead:
        r"""
        Create a Nelder-Mead optimiser with default coefficients.
        """
    def with_max_iter(self, max_iter: builtins.int) -> NelderMead:
        r"""
        Limit the number of simplex iterations.
        """
    def with_threshold(self, threshold: builtins.float) -> NelderMead:
        r"""
        Set the stopping threshold on simplex size or objective reduction.
        """
    def with_position_tolerance(self, tolerance: builtins.float) -> NelderMead:
        r"""
        Stop once simplex vertices fall within the supplied positional tolerance.
        """
    def with_max_evaluations(self, max_evaluations: builtins.int) -> NelderMead:
        r"""
        Abort after evaluating the objective `max_evaluations` times.
        """
    def with_coefficients(
        self,
        alpha: builtins.float,
        gamma: builtins.float,
        rho: builtins.float,
        sigma: builtins.float,
    ) -> NelderMead:
        r"""
        Override the reflection, expansion, contraction, and shrink coefficients.
        """
    def with_patience(self, patience_seconds: builtins.float) -> NelderMead:
        r"""
        Abort if the objective fails to improve within the allotted time.
        """
    def run(
        self, problem: Problem, initial: typing.Sequence[builtins.float]
    ) -> OptimisationResults:
        r"""
        Optimise the given problem starting from the provided initial simplex centre.
        """

@typing.final
class OptimisationResults:
    r"""
    Container for optimiser outputs and diagnostic metadata.
    """
    @property
    def x(self) -> builtins.list[builtins.float]:
        r"""
        Decision vector corresponding to the best-found objective value.
        """
    @property
    def fun(self) -> builtins.float:
        r"""
        Objective value evaluated at `x`.
        """
    @property
    def nit(self) -> builtins.int:
        r"""
        Number of iterations performed by the optimiser.
        """
    @property
    def nfev(self) -> builtins.int:
        r"""
        Total number of objective function evaluations.
        """
    @property
    def time(self) -> datetime.timedelta:
        r"""
        Total number of objective function evaluations.
        """
    @property
    def success(self) -> builtins.bool:
        r"""
        Whether the run satisfied its convergence criteria.
        """
    @property
    def message(self) -> builtins.str:
        r"""
        Human-readable status message summarising the termination state.
        """
    @property
    def termination_reason(self) -> builtins.str:
        r"""
        Structured termination flag describing why the run ended.
        """
    @property
    def final_simplex(self) -> builtins.list[builtins.list[builtins.float]]:
        r"""
        Simplex vertices at termination, when provided by the optimiser.
        """
    @property
    def final_simplex_values(self) -> builtins.list[builtins.float]:
        r"""
        Objective values corresponding to `final_simplex`.
        """
    @property
    def covariance(
        self,
    ) -> builtins.list[builtins.list[builtins.float]] | None:
        r"""
        Estimated covariance of the search distribution, if available.
        """
    def __repr__(self) -> builtins.str:
        r"""
        Render a concise summary of the optimisation outcome.
        """

@typing.final
class Problem:
    r"""
    Executable optimisation problem wrapping the Chronopt core implementation.
    """
    def evaluate(self, x: typing.Sequence[builtins.float]) -> builtins.float:
        r"""
        Evaluate the configured objective function at `x`.
        """
    def evaluate_gradient(
        self, x: typing.Sequence[builtins.float]
    ) -> builtins.list[builtins.float] | None:
        r"""
        Evaluate the gradient of the objective function at `x` if available.
        """
    def optimize(
        self,
        initial: typing.Sequence[builtins.float] | None = None,
        optimiser: NelderMead | CMAES | Adam | None = None,
    ) -> OptimisationResults:
        r"""
        Solve the problem starting from `initial` using the supplied optimiser.
        """
    def get_config(self, key: builtins.str) -> builtins.float | None:
        r"""
        Return the numeric configuration value stored under `key` if present.
        """
    def dimension(self) -> builtins.int:
        r"""
        Return the number of parameters the problem expects.
        """
    def parameters(
        self,
    ) -> builtins.list[
        tuple[
            builtins.str,
            builtins.float,
            tuple[builtins.float, builtins.float] | None,
        ]
    ]: ...
    def default_parameters(self) -> builtins.list[builtins.float]:
        r"""
        Return the default parameter vector implied by the builder.
        """
    def config(self) -> builtins.dict[builtins.str, builtins.float]:
        r"""
        Return a copy of the problem configuration dictionary.
        """

@typing.final
class ScalarBuilder:
    r"""
    High-level builder for optimisation `Problem` instances exposed to Python.
    """
    def __new__(cls) -> ScalarBuilder:
        r"""
        Create an empty builder with no objective, parameters, or default optimiser.
        """
    def with_optimiser(self, optimiser: NelderMead | CMAES | Adam) -> ScalarBuilder:
        r"""
        Configure the default optimiser used when `Problem.optimize` omits one.
        """
    def with_callable(self, obj: typing.Any) -> ScalarBuilder:
        r"""
        Attach the objective function callable executed during optimisation.
        """
    def with_gradient(self, obj: typing.Any) -> ScalarBuilder:
        r"""
        Attach the gradient callable returning derivatives of the objective.
        """
    def with_parameter(
        self,
        name: builtins.str,
        initial_value: builtins.float,
        bounds: tuple[builtins.float, builtins.float] | None = None,
    ) -> ScalarBuilder:
        r"""
        Register a named optimisation variable in the order it appears in vectors.
        """
    def build(self) -> Problem:
        r"""
        Finalize the builder into an executable `Problem`.
        """

@typing.final
class VectorBuilder:
    r"""
    Time-series problem builder for vector-valued objectives.
    """
    def __new__(cls) -> VectorBuilder:
        r"""
        Create an empty vector problem builder.
        """
    def with_objective(self, objective: typing.Any) -> VectorBuilder:
        r"""
        Register a callable that produces predictions matching the data shape.

        The callable should accept a parameter vector and return a numpy array
        of the same shape as the observed data.
        """
    def with_data(self, data: numpy.typing.NDArray[numpy.float64]) -> VectorBuilder:
        r"""
        Attach observed data used to fit the model.

        The data should be a 1D numpy array. The shape will be inferred
        from the data length.
        """
    def with_config(self, key: builtins.str, value: builtins.float) -> VectorBuilder:
        r"""
        Stores an optimisation configuration value keyed by name.
        """
    def with_parameter(
        self,
        name: builtins.str,
        initial_value: builtins.float,
        bounds: tuple[builtins.float, builtins.float] | None = None,
    ) -> VectorBuilder:
        r"""
        Register a named optimisation variable in the order it appears in vectors.
        """
    def clear_parameters(self) -> VectorBuilder:
        r"""
        Remove previously provided parameter defaults.
        """
    def with_cost(self, cost: CostMetric) -> VectorBuilder:
        r"""
        Select the error metric used to compare predictions and observed data.
        """
    def remove_cost(self) -> VectorBuilder:
        r"""
        Reset the cost metric to the default sum of squared errors.
        """
    def with_optimiser(self, optimiser: NelderMead | CMAES | Adam) -> VectorBuilder:
        r"""
        Configure the default optimiser used when `Problem.optimize` omits one.
        """
    def build(self) -> Problem:
        r"""
        Create a `Problem` representing the vector optimisation model.
        """

def GaussianNLL(
    variance: builtins.float, weight: builtins.float = 1.0
) -> CostMetric: ...
def RMSE(weight: builtins.float = 1.0) -> CostMetric: ...
def SSE(weight: builtins.float = 1.0) -> CostMetric: ...
def builder_factory_py() -> ScalarBuilder:
    r"""
    Return a convenience factory for creating `Builder` instances.
    """
