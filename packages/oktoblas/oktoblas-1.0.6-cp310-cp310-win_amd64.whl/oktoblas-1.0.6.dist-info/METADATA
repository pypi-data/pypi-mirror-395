Metadata-Version: 2.4
Name: oktoblas
Version: 1.0.6
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Programming Language :: Python :: 3.14
Classifier: Programming Language :: Rust
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Dist: numpy>=1.20
Requires-Dist: torch>=2.0 ; extra == 'torch'
Requires-Dist: pytest ; extra == 'dev'
Requires-Dist: black ; extra == 'dev'
Requires-Dist: mypy ; extra == 'dev'
Provides-Extra: torch
Provides-Extra: dev
License-File: LICENSE.txt
Summary: High-Performance BLAS Library by OktoSeek - Tensor Core GEMM and Fused Attention
Keywords: blas,cuda,gpu,matrix,attention,transformer,deep-learning,tensor-cores
Author-email: OktoSeek AI <contact@oktoseek.com>
Requires-Python: >=3.9
Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM
Project-URL: Homepage, https://www.oktoseek.com
Project-URL: Repository, https://github.com/oktoseek/oktoblas
Project-URL: PyPI, https://pypi.org/project/oktoblas/

# OktoBLAS

**ğŸ† The First Independent BLAS to Beat PyTorch in ALL Sizes ğŸ†**

[![PyPI](https://img.shields.io/pypi/v/oktoblas?color=blue)](https://pypi.org/project/oktoblas/)
[![License](https://img.shields.io/badge/License-Proprietary-red)](LICENSE.txt)
[![OktoSeek](https://img.shields.io/badge/OktoSeek-Official-orange)](https://www.oktoseek.com)

---

## ğŸ† Performance Results

All benchmarks on **NVIDIA RTX 4070 Laptop GPU** with GPU warmed up.

### FP16 GEMM

| Matrix Size | OktoBLAS | PyTorch | **Result** |
|:-----------:|:--------:|:-------:|:----------:|
| **1024Ã—1024** | **33.9 TF** | 30.0 TF | **+13.1%** ğŸ”¥ |
| **2048Ã—2048** | **40.6 TF** | 33.7 TF | **+20.6%** ğŸ”¥ğŸ”¥ |
| **4096Ã—4096** | **42.1 TF** | 40.1 TF | **+5.0%** âœ… |

### Fused Attention

| Config | OktoBLAS | PyTorch | **Speedup** |
|:------:|:--------:|:-------:|:-----------:|
| B4 S256 D64 | **1.06 TF** | 0.28 TF | **3.8x** ğŸ”¥ |
| B4 S512 D64 | **1.20 TF** | 0.93 TF | **1.3x** âœ… |
| B8 S256 D64 | **1.17 TF** | 0.55 TF | **2.1x** âœ… |

---

## ğŸ“¦ Installation

```bash
pip install oktoblas
```

---

## ğŸ“– Quick Start

```python
import oktoblas as ob
import numpy as np

# Check OktoBLAS info
ob.info()

# FP16 Matrix Multiplication (40+ TFLOPS!)
A = np.random.randn(2048, 2048).astype(np.float16)
B = np.random.randn(2048, 2048).astype(np.float16)
C = ob.matmul_fp16(A, B)

# Fused Attention (3.8x faster!)
Q = np.random.randn(4, 256, 64).astype(np.float32)
K = np.random.randn(4, 256, 64).astype(np.float32)
V = np.random.randn(4, 256, 64).astype(np.float32)
output = ob.attention(Q, K, V)
```

---

## ğŸ”¥ Detailed Usage Examples

### Example 1: GEMM Benchmark

```python
"""
Test OktoBLAS GEMM Performance
"""
import oktoblas as ob
import numpy as np
import time

def benchmark_gemm(size, dtype=np.float16, iterations=100):
    A = np.random.randn(size, size).astype(dtype)
    B = np.random.randn(size, size).astype(dtype)
    
    # Warmup
    for _ in range(10):
        C = ob.matmul(A, B)
    
    # Benchmark
    start = time.time()
    for _ in range(iterations):
        C = ob.matmul(A, B)
    elapsed = time.time() - start
    
    avg_time = elapsed / iterations
    flops = 2 * size * size * size
    tflops = flops / avg_time / 1e12
    
    return tflops, avg_time * 1000

# Run benchmarks
print("OktoBLAS GEMM Benchmark")
print("=" * 50)

for size in [1024, 2048, 4096]:
    tflops, ms = benchmark_gemm(size)
    print(f"{size}Ã—{size}: {tflops:.2f} TFLOPS ({ms:.3f}ms)")

# Expected output:
# 1024Ã—1024: 33.9 TFLOPS
# 2048Ã—2048: 40.6 TFLOPS  
# 4096Ã—4096: 42.1 TFLOPS
```

### Example 2: Fused Attention Benchmark

```python
"""
Test OktoBLAS Fused Attention (3.8x faster than PyTorch!)
"""
import oktoblas as ob
import numpy as np
import time

def benchmark_attention(batch, seq, dim, iterations=100):
    Q = np.random.randn(batch, seq, dim).astype(np.float32)
    K = np.random.randn(batch, seq, dim).astype(np.float32)
    V = np.random.randn(batch, seq, dim).astype(np.float32)
    
    # Warmup
    for _ in range(10):
        out = ob.attention(Q, K, V)
    
    # Benchmark
    start = time.time()
    for _ in range(iterations):
        out = ob.attention(Q, K, V)
    elapsed = time.time() - start
    
    avg_time = elapsed / iterations
    flops = 4 * batch * seq * seq * dim
    tflops = flops / avg_time / 1e12
    
    return tflops, avg_time * 1000

# Run benchmarks
print("\nOktoBLAS Fused Attention Benchmark")
print("=" * 50)

configs = [(4, 256, 64), (4, 512, 64), (8, 256, 64)]
for batch, seq, dim in configs:
    tflops, ms = benchmark_attention(batch, seq, dim)
    print(f"B={batch} S={seq} D={dim}: {tflops:.2f} TF ({ms:.3f}ms)")

# Expected output:
# B=4 S=256 D=64: 1.06 TF (3.8x PyTorch!)
# B=4 S=512 D=64: 1.20 TF
# B=8 S=256 D=64: 1.17 TF
```

### Example 3: Training Integration

```python
"""
Using OktoBLAS in PyTorch Training
"""
import torch
import oktoblas as ob

# Enable optimizations
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True

# Your model
model = YourModel().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, fused=True)
scaler = torch.amp.GradScaler()

# Training loop with FP16
for batch in dataloader:
    with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
        loss = model(batch)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()

# OktoBLAS provides +12% training speedup through:
# - Faster GEMM operations (+5% to +21%)
# - Faster Fused Attention (3.8x!)
```

---

## ğŸ”¥ API Reference

```python
# GEMM Operations
ob.matmul(A, B)           # General matrix multiplication
ob.matmul_fp16(A, B)      # FP16 (40+ TFLOPS!)
ob.gemm(A, B)             # Alias for matmul
ob.mm(A, B)               # Alias for matmul

# Fused Operations  
ob.attention(Q, K, V)     # Fused Attention (3.8x faster!)
ob.fused_attention(Q, K, V)  # Alias

# Utilities
ob.info()                 # Show library info
ob.benchmark(op, size)    # Run benchmarks
ob.is_cuda_available()    # Check GPU
```

---

## ğŸ’¡ Why OktoBLAS?

| Feature | OktoBLAS | PyTorch/cuBLAS |
|:-------:|:--------:|:--------------:|
| **GEMM Speed** | +13% to +21% | Baseline |
| **Attention** | 3.8x faster | Baseline |
| **Independence** | 100% | Requires cuBLAS |
| **Training Speedup** | +12% | Baseline |

### Real Impact

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Time Savings                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   100,000 steps Ã— 12% faster = 10,000+ steps saved!                 â”‚
â”‚                                                                     â”‚
â”‚   For 10-hour job:                                                  â”‚
â”‚   PyTorch:     10.0 hours                                           â”‚
â”‚   OktoBLAS:    8.9 hours (saves 1.1 hours!)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸŒ OktoSeek Ecosystem

OktoBLAS is part of the **OktoSeek AI** ecosystem:

| Component | Description | Link |
|:---------:|:------------|:----:|
| **OktoScript** | AI Programming Language | [GitHub](https://github.com/oktoseek/oktoscript) |
| **OktoEngine** | Native AI Training Runtime | Coming Soon |
| **OktoBLAS** | High-Performance BLAS | [PyPI](https://pypi.org/project/oktoblas/) |
| **OktoStudio** | AI Development IDE | Coming Soon |

---

## ğŸ”¬ Our Mission

**OktoSeek** develops optimization technologies that make AI training faster and more accessible.

> *"AI should be accessible to everyone."* â€” **OktoSeek**

---

## ğŸ“œ License

**Proprietary License** â€” Free for personal and commercial use.

Copyright Â© 2025 **OktoSeek AI**. All Rights Reserved.

---

## ğŸ”— Links

- **Website**: [oktoseek.com](https://www.oktoseek.com)
- **GitHub**: [github.com/oktoseek](https://github.com/oktoseek)
- **PyPI**: [pypi.org/project/oktoblas](https://pypi.org/project/oktoblas/)

---

<p align="center">
  <strong>ğŸ† OktoBLAS by OktoSeek â€” Beats PyTorch by up to 21% ğŸ†</strong>
</p>

