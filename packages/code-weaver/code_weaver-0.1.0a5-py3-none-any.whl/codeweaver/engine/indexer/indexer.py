# sourcery skip: no-complex-if-expressions
# SPDX-FileCopyrightText: 2025 Knitli Inc.
# SPDX-FileContributor: Adam Poulemanos <adam@knit.li>
#
# SPDX-License-Identifier: MIT OR Apache-2.0

"""The indexer service for managing and querying indexed data.

The indexer orchestrates file discovery, chunking, embedding generation, and storage
in vector databases. It supports checkpointing for resuming interrupted indexing
operations, and integrates with CodeWeaver's provider registry for embedding and
vector store services.

It is the backend service that powers CodeWeaver's code search and retrieval capabilities.
"""

from __future__ import annotations

import asyncio
import logging
import signal
import time

from pathlib import Path
from typing import TYPE_CHECKING, Annotated, Any, Protocol, TypedDict

import rignore

from pydantic import DirectoryPath, NonNegativeFloat, NonNegativeInt, PrivateAttr
from watchfiles import Change

from codeweaver.common.logging import log_to_client_or_fallback
from codeweaver.common.statistics import SessionStatistics, get_session_statistics
from codeweaver.common.utils.git import set_relative_path
from codeweaver.config.chunker import ChunkerSettings
from codeweaver.config.settings import Unset
from codeweaver.core.discovery import DiscoveredFile
from codeweaver.core.language import ConfigLanguage, SemanticSearchLanguage
from codeweaver.core.stores import BlakeStore, get_blake_hash, make_blake_store
from codeweaver.core.types.dictview import DictView
from codeweaver.core.types.models import BasedModel
from codeweaver.engine.chunking_service import ChunkingService
from codeweaver.engine.indexer.checkpoint import CheckpointManager, IndexingCheckpoint
from codeweaver.engine.indexer.manifest import FileManifestManager, IndexFileManifest
from codeweaver.engine.indexer.progress import IndexingStats
from codeweaver.engine.watcher.types import FileChange
from codeweaver.exceptions import IndexingError, ProviderError


logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from typing import Any

    from codeweaver.config.providers import (
        EmbeddingProviderSettings,
        SparseEmbeddingProviderSettings,
        VectorStoreProviderSettings,
    )
    from codeweaver.config.types import CodeWeaverSettingsDict
    from codeweaver.core.chunks import CodeChunk
    from codeweaver.providers.embedding.providers.base import EmbeddingProvider


class ProgressCallback(Protocol):
    """Protocol for progress callback functions.

    Supports both the legacy phase-transition style and granular updates.
    """

    def __call__(
        self, phase: str, current: int, total: int, *, extra: dict[str, Any] | None = None
    ) -> None:
        """Report progress update.

        Args:
            phase: Current phase (discovery, chunking, embedding, indexing)
            current: Current progress count
            total: Total items to process
            extra: Optional extra data (e.g., chunks_created for chunking phase)
        """
        ...


class UserProviderSelectionDict(TypedDict):
    """User-selected provider configuration dictionary."""

    embedding: DictView[EmbeddingProviderSettings[Any]] | None
    sparse_embedding: DictView[SparseEmbeddingProviderSettings[Any]] | None
    vector_store: DictView[VectorStoreProviderSettings] | None


_user_config: None | UserProviderSelectionDict = None


def _get_user_provider_config() -> UserProviderSelectionDict:
    from codeweaver.common.registry.provider import get_provider_config_for
    from codeweaver.providers.provider import ProviderKind

    global _user_config
    if _user_config is None:
        _user_config = UserProviderSelectionDict(
            embedding=get_provider_config_for(ProviderKind.EMBEDDING),
            sparse_embedding=get_provider_config_for(ProviderKind.SPARSE_EMBEDDING),
            vector_store=get_provider_config_for(ProviderKind.VECTOR_STORE),
        )
    return _user_config


def _get_embedding_instance(*, sparse: bool = False) -> EmbeddingProvider[Any] | None:
    """Get embedding provider instance using new registry API."""
    from codeweaver.common.registry import get_provider_registry

    kind = "sparse_embedding" if sparse else "embedding"
    registry = get_provider_registry()

    if provider_enum := registry.get_provider_enum_for(kind):
        return registry.get_provider_instance(provider_enum, kind, singleton=True)
    return None


def _get_vector_store_instance() -> Any | None:
    """Get vector store provider instance using registry API."""
    from codeweaver.common.registry import get_provider_registry

    registry = get_provider_registry()
    if provider_enum := registry.get_provider_enum_for("vector_store"):
        return registry.get_provider_instance(provider_enum, "vector_store", singleton=True)
    return None


def _get_chunking_service() -> ChunkingService:
    """Stub function to get chunking service instance."""
    from codeweaver.config.settings import get_settings
    from codeweaver.engine.chunker import ChunkGovernor
    from codeweaver.engine.chunking_service import ChunkingService

    chunk_settings = get_settings().chunker
    governor = ChunkGovernor.from_settings(
        ChunkerSettings() if isinstance(chunk_settings, Unset) else chunk_settings
    )
    return ChunkingService(governor=governor)


class Indexer(BasedModel):
    """Main indexer class. Wraps a DiscoveredFilestore and chunkers."""

    _store: Annotated[BlakeStore[DiscoveredFile] | None, PrivateAttr()] = None
    _walker_settings: Annotated[dict[str, Any] | None, PrivateAttr()] = None
    _project_path: Annotated[DirectoryPath | None, PrivateAttr()] = None
    _checkpoint_manager: Annotated[CheckpointManager | None, PrivateAttr()] = None
    _checkpoint: Annotated[IndexingCheckpoint | None, PrivateAttr()] = None
    _manifest_manager: Annotated[FileManifestManager | None, PrivateAttr()] = None
    _file_manifest: Annotated[IndexFileManifest | None, PrivateAttr()] = None
    _manifest_lock: Annotated[asyncio.Lock | None, PrivateAttr()] = None
    _deleted_files: Annotated[list[Path], PrivateAttr()] = PrivateAttr(default_factory=list)
    _session_statistics: Annotated[SessionStatistics, PrivateAttr()] = PrivateAttr(
        default_factory=get_session_statistics
    )
    _last_checkpoint_time: Annotated[NonNegativeFloat, PrivateAttr()] = 0.0
    _files_since_checkpoint: Annotated[NonNegativeInt, PrivateAttr()] = 0
    _failover_manager: Annotated[Any | None, PrivateAttr()] = None  # VectorStoreFailoverManager
    _duplicate_dense_count: Annotated[NonNegativeInt, PrivateAttr()] = 0
    _duplicate_sparse_count: Annotated[NonNegativeInt, PrivateAttr()] = 0

    def __init__(
        self,
        walker: rignore.Walker | None = None,
        store: BlakeStore[DiscoveredFile] | None = None,
        chunking_service: Any | None = None,  # ChunkingService type
        *,
        auto_initialize_providers: bool = True,
        project_path: Path | None = None,
        walker_settings: dict[str, Any] | None = None,
    ) -> None:
        """Initialize the Indexer with optional pipeline components.

        Args:
            walker: rignore walker for file discovery (deprecated, use walker_settings)
            store: Store for discovered file metadata
            chunking_service: Service for chunking files (optional)
            auto_initialize_providers: Auto-initialize providers from global registry
            project_path: Project path for checkpoint management (preferred)
            walker_settings: Settings dict for creating rignore.Walker instances
        """
        from codeweaver.common.utils.git import get_project_path

        self._project_path = (
            project_path
            if project_path and not isinstance(project_path, Unset) and project_path.exists()
            else get_project_path()
        )
        # Store walker settings for creating fresh walkers
        # If walker is provided but not settings, extract settings from project_path
        if walker_settings is not None:
            self._walker_settings = walker_settings
        elif walker is not None:
            # Legacy: walker provided directly - store minimal settings
            # Note: This means we can only recreate with default settings
            if project_path is not None:
                self._walker_settings = {"path": str(project_path)}
            else:
                # Can't recreate without project_path - store None
                self._walker_settings = None
                logger.warning(
                    "Walker provided without walker_settings or project_path - "
                    "cannot recreate walker if exhausted"
                )
        elif project_path is not None:
            # Auto-create walker settings from project_path
            self._walker_settings = {"path": str(project_path)}
            logger.debug("Auto-created walker settings for project_path: %s", project_path)
        else:
            self._walker_settings = None

        from codeweaver.core.discovery import DiscoveredFile

        self._store = store or make_blake_store(value_type=DiscoveredFile)
        self._chunking_service = chunking_service or _get_chunking_service()
        self._stats = IndexingStats()
        from codeweaver.common.statistics import get_session_statistics

        self._session_statistics = get_session_statistics()
        # Pipeline provider Fields (initialized lazily on first use)
        self._embedding_provider: Any | None = None
        self._sparse_provider: Any | None = None
        self._vector_store: Any | None = None
        self._providers_initialized: bool = False

        # Initialize checkpoint manager
        self._checkpoint_manager = CheckpointManager(project_path=self._project_path)

        self._checkpoint = None
        self._last_checkpoint_time = time.time()
        self._files_since_checkpoint = 0
        self._shutdown_requested = False
        self._original_sigterm_handler = None
        self._original_sigint_handler = None

        # Initialize file manifest manager
        self._manifest_manager = FileManifestManager(project_path=self._project_path)

        self._file_manifest = None
        self._manifest_lock = None  # Initialize as None, created lazily in async context

        # Note: Provider initialization is now deferred to first async operation
        # auto_initialize_providers parameter is deprecated but kept for compatibility
        if auto_initialize_providers:
            logger.debug(
                "auto_initialize_providers=True: providers will be initialized on first async operation"
            )

        # Register signal handlers for graceful shutdown
        self._register_signal_handlers()

        logger.info("Indexer initialized")
        logger.info("Using project path: %s", self._checkpoint_manager.project_path)
        logger.debug("Providers will be initialized lazily on first use")

    def _register_signal_handlers(self) -> None:
        """Register signal handlers for graceful shutdown with checkpoint saving.

        Note: Signal handlers are intentionally minimal and thread-safe.
        They only set the shutdown flag - actual cleanup happens in async context.
        """

        def handle_shutdown_signal(signum: int, frame: Any) -> None:
            """Handle shutdown signal by setting shutdown flag.

            This is intentionally minimal to avoid blocking in signal handler.
            Checkpoint saving happens in the finally block of prime_index.
            """
            signal_name = signal.Signals(signum).name
            logger.info("Received %s signal, requesting shutdown...", signal_name)
            self._shutdown_requested = True

            # Don't save checkpoint here - it may block
            # Checkpoint is saved in finally blocks of async methods

        # Store original handlers and install new ones
        try:
            self._original_sigterm_handler = signal.signal(signal.SIGTERM, handle_shutdown_signal)
            self._original_sigint_handler = signal.signal(signal.SIGINT, handle_shutdown_signal)
            logger.debug("Signal handlers registered for graceful shutdown")
        except (ValueError, OSError) as e:
            # Signal handling may not be available in all contexts (e.g., threads)
            logger.debug("Could not register signal handlers: %s", e)

    def _cleanup_signal_handlers(self) -> None:
        """Restore original signal handlers."""
        try:
            if self._original_sigterm_handler is not None:
                _ = signal.signal(signal.SIGTERM, self._original_sigterm_handler)
            if self._original_sigint_handler is not None:
                _ = signal.signal(signal.SIGINT, self._original_sigint_handler)
            logger.debug("Signal handlers restored")
        except (ValueError, OSError) as e:
            logger.debug("Could not restore signal handlers: %s", e)

    def _reset_duplicate_counts(self) -> None:
        """Reset duplicate embedding counters."""
        self._duplicate_dense_count = 0
        self._duplicate_sparse_count = 0

    def _log_duplicate_summary(self, status_display: Any | None = None) -> None:
        """Log summary of duplicate embeddings.

        Args:
            status_display: Optional StatusDisplay instance for clean user-facing output
        """
        total_duplicates = self._duplicate_dense_count + self._duplicate_sparse_count
        if total_duplicates > 0:
            if status_display:
                status_display.print_warning(f"{total_duplicates} chunks already indexed, skipped")
            elif logger.isEnabledFor(logging.INFO):
                logger.info(
                    "%d chunks already indexed (dense: %d, sparse: %d), skipped",
                    total_duplicates,
                    self._duplicate_dense_count,
                    self._duplicate_sparse_count,
                )

    async def _initialize_providers_async(self) -> None:
        """Initialize pipeline providers asynchronously from global registry.

        This is idempotent and can be safely called multiple times.
        Providers that fail to initialize will be set to None with appropriate logging.
        """
        if self._providers_initialized:
            return

        # Initialize embedding provider (dense)
        try:
            self._embedding_provider = _get_embedding_instance(sparse=False)
            logger.debug(
                "Initialized embedding provider: %s", type(self._embedding_provider).__name__
            )
        except Exception as e:
            logger.warning("Could not initialize embedding provider: %s", e)
            self._embedding_provider = None

        # Initialize sparse embedding provider
        try:
            self._sparse_provider = _get_embedding_instance(sparse=True)
            logger.debug("Initialized sparse provider: %s", type(self._sparse_provider).__name__)
        except Exception as e:
            logger.debug("Could not initialize sparse embedding provider: %s", e)
            self._sparse_provider = None

        # Warn if no embedding providers available
        if not self._embedding_provider and not self._sparse_provider:
            logger.warning(
                "⚠️  No embedding providers initialized - indexing will proceed without embeddings"
            )

        # Initialize vector store with failover support
        try:
            from codeweaver.engine.failover import VectorStoreFailoverManager

            # Get primary vector store instance
            primary_store = _get_vector_store_instance()
            if primary_store:
                await primary_store._initialize()

            # Create and initialize failover manager
            self._failover_manager = VectorStoreFailoverManager()
            await self._failover_manager.initialize(
                primary_store=primary_store,
                project_path=self._checkpoint_manager.project_path,
                indexer=self,
            )

            # Use the active store (initially primary, switches to backup on failure)
            self._vector_store = self._failover_manager.active_store

            if self._vector_store:
                logger.info(
                    "Vector store initialized with backup failover support: %s",
                    type(self._vector_store).__name__,
                )
            else:
                logger.debug("No vector store available (primary failed to initialize)")

        except Exception as e:
            # Provide specific guidance for common connection errors
            error_msg = str(e).lower()
            if any(
                indicator in error_msg
                for indicator in [
                    "illegal request line",
                    "connection refused",
                    "connect error",
                    "cannot connect",
                    "connection error",
                    "failed to connect",
                ]
            ):
                # Log at debug level - health checks will display this to users
                logger.debug(
                    "Failed to connect to PRIMARY Qdrant vector store. "
                    "Please verify:\n"
                    "  - Qdrant is running (default: http://localhost:6333 for HTTP, :6334 for gRPC)\n"
                    "  - The configured URL matches your Qdrant instance\n"
                    "  - Check firewall/network settings if using remote Qdrant\n"
                    "  Original error: %s",
                    e,
                )
            elif "timeout" in error_msg or "timed out" in error_msg:
                logger.warning(
                    "Qdrant connection timed out. Please verify:\n"
                    "  - Qdrant server is responsive\n"
                    "  - Network latency is acceptable\n"
                    "  - Consider increasing timeout settings\n"
                )
            elif "unauthorized" in error_msg or "authentication" in error_msg:
                logger.warning(
                    "Qdrant authentication failed. Please verify:\n"
                    "  - API key is correctly configured\n"
                    "  - Authentication credentials are valid\n"
                )
            else:
                logger.warning("Could not initialize vector store.", exc_info=True)

            self._vector_store = None
            self._failover_manager = None

        # Ensure chunking service is initialized
        self._chunking_service = self._chunking_service or _get_chunking_service()

        self._providers_initialized = True

    def _get_current_embedding_models(self) -> dict[str, str | None]:
        """Get current embedding model configuration.

        Returns:
            Dictionary with current dense and sparse model info
        """
        result: dict[str, str | None] = {
            "dense_provider": None,
            "dense_model": None,
            "sparse_provider": None,
            "sparse_model": None,
        }

        # Get dense embedding provider info
        if self._embedding_provider:
            provider_name = type(self._embedding_provider).__name__.replace("Provider", "").lower()
            result["dense_provider"] = provider_name
            # Try to get model name from provider - use model_name property if available
            if hasattr(self._embedding_provider, "model_name"):
                result["dense_model"] = str(self._embedding_provider.model_name)
            elif hasattr(self._embedding_provider, "model"):
                result["dense_model"] = str(self._embedding_provider.model)

        # Get sparse embedding provider info
        if self._sparse_provider:
            provider_name = type(self._sparse_provider).__name__.replace("Provider", "").lower()
            result["sparse_provider"] = provider_name
            # Try to get model name from provider - use model_name property if available
            if hasattr(self._sparse_provider, "model_name"):
                result["sparse_model"] = str(self._sparse_provider.model_name)
            elif hasattr(self._sparse_provider, "model"):
                result["sparse_model"] = str(self._sparse_provider.model)

        return result

    async def _index_file(self, path: Path, context: Any = None) -> None:
        """Execute full pipeline for a single file: discover → chunk → embed → index.

        Args:
            path: Path to the file to index
            context: Optional FastMCP context for structured logging
        """
        # Ensure manifest lock is initialized in async context
        if self._manifest_lock is None:
            self._manifest_lock = asyncio.Lock()

        try:
            # Delete old chunks if file is being reindexed (already exists in manifest)
            # This prevents stale embeddings from accumulating in the vector store
            if self._file_manifest and self._vector_store:
                relative_path = set_relative_path(path)
                if relative_path and self._file_manifest.has_file(relative_path):
                    try:
                        await self._vector_store.delete_by_file(path)
                        logger.debug("Deleted old chunks for reindexed file: %s", relative_path)
                    except Exception:
                        logger.warning(
                            "Failed to delete old chunks for: %s", relative_path, exc_info=True
                        )

            # 1. Discover and store file metadata
            self._last_indexing_phase = "discovery"
            discovered_file = DiscoveredFile.from_path(path)
            if not discovered_file or not discovered_file.is_text:
                logger.debug("Skipping non-text file: %s", path)
                return

            self._store.set(discovered_file.file_hash, discovered_file)
            self._stats.files_discovered += 1

            # Track file discovery in session statistics
            self._session_statistics.add_file_from_discovered(discovered_file, "processed")

            await log_to_client_or_fallback(
                context,
                "debug",
                {
                    "msg": "File discovered",
                    "extra": {
                        "phase": "discovery",
                        "file_path": str(path),
                        "file_size": discovered_file.size,
                        "file_language": discovered_file.ext_kind.language.variable
                        if discovered_file.ext_kind
                        and isinstance(
                            discovered_file.ext_kind.language,
                            SemanticSearchLanguage | ConfigLanguage,
                        )
                        else str(discovered_file.ext_kind.language)
                        if discovered_file.ext_kind
                        else "unknown",
                        "total_discovered": self._stats.files_discovered,
                    },
                },
            )

            # 2. Chunk via ChunkingService (if available)
            self._last_indexing_phase = "chunking"
            if not self._chunking_service:
                logger.warning("No chunking service configured, skipping file: %s", path)
                return

            chunks = self._chunking_service.chunk_file(discovered_file)
            self._stats.chunks_created += len(chunks)

            # Track chunk creation in session statistics
            for chunk in chunks:
                self._session_statistics.add_chunk_from_codechunk(chunk, "processed")

            await log_to_client_or_fallback(
                context,
                "debug",
                {
                    "msg": "File chunked",
                    "extra": {
                        "phase": "chunking",
                        "file_path": str(path),
                        "chunks_created": len(chunks),
                        "total_chunks": self._stats.chunks_created,
                    },
                },
            )

            # 3. Embed chunks (if embedding providers available)
            self._last_indexing_phase = "embedding"
            if self._embedding_provider or self._sparse_provider:
                await self._embed_chunks(chunks)
                self._stats.chunks_embedded += len(chunks)

                # Log summary of duplicate embeddings if any were skipped
                if self._duplicate_dense_count > 0 or self._duplicate_sparse_count > 0:
                    logger.info(
                        "Skipped %d chunks with existing embeddings (dense: %d, sparse: %d)",
                        self._duplicate_dense_count + self._duplicate_sparse_count,
                        self._duplicate_dense_count,
                        self._duplicate_sparse_count,
                    )

                await log_to_client_or_fallback(
                    context,
                    "debug",
                    {
                        "msg": "Chunks embedded",
                        "extra": {
                            "phase": "embedding",
                            "file_path": str(path),
                            "chunks_embedded": len(chunks),
                            "total_embedded": self._stats.chunks_embedded,
                            "dense_provider": type(self._embedding_provider).__name__
                            if self._embedding_provider
                            else None,
                            "sparse_provider": type(self._sparse_provider).__name__
                            if self._sparse_provider
                            else None,
                        },
                    },
                )
            else:
                await log_to_client_or_fallback(
                    context,
                    "warning",
                    {
                        "msg": "No embedding providers configured",
                        "extra": {
                            "phase": "embedding",
                            "file_path": str(path),
                            "action": "skipped",
                        },
                    },
                )

            # 4. Retrieve updated chunks from registry (single source of truth!)
            from codeweaver.providers.embedding.registry import get_embedding_registry

            registry = get_embedding_registry()
            updated_chunks = [
                registry[chunk.chunk_id].chunk for chunk in chunks if chunk.chunk_id in registry
            ]

            # If no chunks were embedded, use original chunks
            if not updated_chunks:
                logger.debug("No embedded chunks, using original chunks for: %s", path)
                updated_chunks = chunks

            # 5. Index to vector store (if available)
            self._last_indexing_phase = "storage"
            if self._vector_store:
                is_backup = (
                    self._failover_manager.is_failover_active if self._failover_manager else False
                )
                await self._vector_store.upsert(updated_chunks, for_backup=is_backup)
                self._stats.chunks_indexed += len(updated_chunks)

                await log_to_client_or_fallback(
                    context,
                    "debug",
                    {
                        "msg": "Chunks indexed to vector store",
                        "extra": {
                            "phase": "storage",
                            "file_path": str(path),
                            "chunks_indexed": len(updated_chunks),
                            "total_indexed": self._stats.chunks_indexed,
                            "vector_store": type(self._vector_store).__name__,
                        },
                    },
                )
            else:
                await log_to_client_or_fallback(
                    context,
                    "warning",
                    {
                        "msg": "No vector store configured",
                        "extra": {"phase": "storage", "file_path": str(path), "action": "skipped"},
                    },
                )

            self._stats.files_processed += 1

            # Track successful indexing in session statistics
            self._session_statistics.add_file_from_discovered(discovered_file, "indexed")

            # Track for backup sync via failover manager
            if self._failover_manager:
                self._failover_manager.record_file_indexed(discovered_file)

            # 6. Update file manifest with successful indexing
            # Only update if all critical operations succeeded and we have chunks
            if self._file_manifest and updated_chunks and self._manifest_lock:
                chunk_ids = [str(chunk.chunk_id) for chunk in updated_chunks]
                if relative_path := set_relative_path(path):
                    try:
                        # Get current embedding model info
                        model_info = self._get_current_embedding_models()

                        async with self._manifest_lock:
                            self._file_manifest.add_file(
                                path=relative_path,
                                content_hash=discovered_file.file_hash,
                                chunk_ids=chunk_ids,
                                dense_embedding_provider=model_info["dense_provider"],
                                dense_embedding_model=model_info["dense_model"],
                                sparse_embedding_provider=model_info["sparse_provider"],
                                sparse_embedding_model=model_info["sparse_model"],
                                has_dense_embeddings=bool(self._embedding_provider),
                                has_sparse_embeddings=bool(self._sparse_provider),
                            )
                        logger.debug(
                            "Updated manifest for file: %s (%d chunks)",
                            relative_path,
                            len(chunk_ids),
                        )
                    except ValueError as e:
                        logger.warning("Failed to add file to manifest: %s - %s", relative_path, e)

            await log_to_client_or_fallback(
                context,
                "info",
                {
                    "msg": "File processing complete",
                    "extra": {
                        "file_path": str(path),
                        "chunks_created": len(chunks),
                        "files_processed": self._stats.files_processed,
                        "total_files": self._stats.files_discovered,
                        "progress_pct": round(
                            (self._stats.files_processed / self._stats.files_discovered * 100), 1
                        )
                        if self._stats.files_discovered > 0
                        else 0,
                    },
                },
            )

        except Exception as e:
            # Determine phase where error occurred based on progress
            phase = "discovery"
            if hasattr(self, "_last_indexing_phase"):
                phase = self._last_indexing_phase

            logger.warning("Failed to index file %s in phase '%s'", path, phase, exc_info=True)
            self._stats.add_error(path, e, phase)

            await log_to_client_or_fallback(
                context,
                "error",
                {
                    "msg": "File indexing failed",
                    "extra": {
                        "file_path": str(path),
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "phase": phase,
                        "total_errors": self._stats.total_errors(),
                    },
                },
            )

    def _telemetry_keys(self) -> None:
        return None

    async def _embed_chunks(self, chunks: list[Any]) -> None:
        """Embed chunks with both dense and sparse providers.

        Args:
            chunks: List of CodeChunk objects to embed
        """
        if not chunks:
            return

        # Get embedding registry to check which chunks already have embeddings
        from codeweaver.providers.embedding.registry import get_embedding_registry

        registry = get_embedding_registry()

        # Dense embeddings
        if self._embedding_provider:
            try:
                if chunks_needing_dense := [
                    chunk
                    for chunk in chunks
                    if chunk.chunk_id not in registry or not registry[chunk.chunk_id].has_dense
                ]:
                    await self._embedding_provider.embed_documents(chunks_needing_dense)
                    logger.debug(
                        "Generated dense embeddings for %d chunks", len(chunks_needing_dense)
                    )
                else:
                    logger.debug(
                        "All %d chunks already have dense embeddings, skipping", len(chunks)
                    )
            except ValueError as e:
                # Handle duplicate embedding errors gracefully
                if "already set" in str(e):
                    # Increment counter silently - this is normal deduplication behavior
                    self._duplicate_dense_count += 1
                else:
                    raise
            except Exception:
                logger.warning("Dense embedding failed", exc_info=True)

        # Sparse embeddings
        if self._sparse_provider:
            try:
                if chunks_needing_sparse := [
                    chunk
                    for chunk in chunks
                    if chunk.chunk_id not in registry or not registry[chunk.chunk_id].has_sparse
                ]:
                    await self._sparse_provider.embed_documents(chunks_needing_sparse)
                    logger.debug(
                        "Generated sparse embeddings for %d chunks", len(chunks_needing_sparse)
                    )
                else:
                    logger.debug(
                        "All %d chunks already have sparse embeddings, skipping", len(chunks)
                    )
            except ValueError as e:
                # Handle duplicate embedding errors gracefully
                if "already set" in str(e):
                    # Increment counter silently - this is normal deduplication behavior
                    self._duplicate_sparse_count += 1
                else:
                    raise
            except Exception:
                logger.warning("Sparse embedding failed", exc_info=True)

    async def _delete_file(self, path: Path) -> None:
        """Remove file from store and vector store.

        Args:
            path: Path to the file to remove
        """
        # Ensure manifest lock is initialized in async context
        if self._manifest_lock is None:
            self._manifest_lock = asyncio.Lock()

        try:
            if removed := self._remove_path(path):
                logger.debug("Removed %d entries from store for: %s", removed, path)

            # Remove from vector store
            if self._vector_store:
                try:
                    await self._vector_store.delete_by_file(path)
                    logger.debug("Removed chunks from vector store for: %s", path)
                except Exception:
                    logger.warning("Failed to remove from vector store", exc_info=True)

            # Remove from file manifest (use relative path)
            if (
                self._file_manifest
                and self._manifest_lock
                and (relative_path := set_relative_path(path))
            ):
                try:
                    async with self._manifest_lock:
                        entry = self._file_manifest.remove_file(relative_path)
                    if entry:
                        logger.debug(
                            "Removed file from manifest: %s (%d chunks)",
                            relative_path,
                            entry["chunk_count"],
                        )
                except ValueError as e:
                    logger.warning("Failed to remove file from manifest: %s - %s", relative_path, e)

            # Track deletion for backup sync via failover manager
            if self._failover_manager:
                self._failover_manager.record_file_deleted(path)
        except Exception:
            logger.warning("Failed to delete file %s", path, exc_info=True)

    async def _cleanup_deleted_files(self) -> None:
        """Clean up files that were deleted from the repository.

        Removes chunks from vector store and entries from manifest.
        """
        if not self._deleted_files:
            return

        logger.info("Cleaning up %d deleted files", len(self._deleted_files))

        for path in self._deleted_files:
            await self._delete_file(path)

        self._deleted_files.clear()
        logger.info("Deleted file cleanup complete")

    async def index(self, change: FileChange) -> None:
        """Index a single file based on a watchfiles change event.

        Executes full pipeline: file → chunks → embeddings → vector store.
        Handles added, modified, and deleted file events.
        """
        try:
            change_type, raw_path = change
        except Exception:
            logger.warning("Invalid FileChange tuple received: %r", change, exc_info=True)
            return

        path = Path(raw_path)

        match change_type:
            case Change.added | Change.modified:
                # Skip non-files quickly
                if not path.exists() or not path.is_file():
                    return
                # Execute full pipeline
                await self._index_file(path)

            case Change.deleted:
                # Remove from store and vector store
                await self._delete_file(path)

            case _:
                logger.debug("Unhandled change type %s for %s", change_type, path)

    # ---- public helpers ----
    def _load_file_manifest(self) -> bool:
        """Load file manifest for incremental indexing.

        Returns:
            True if manifest was loaded successfully
        """
        if not self._manifest_manager:
            logger.debug("No manifest manager configured")
            return False

        if manifest := self._manifest_manager.load():
            # Validate that loaded manifest matches current project path
            if manifest.project_path.resolve() != self._manifest_manager.project_path.resolve():
                logger.warning(
                    "Loaded manifest project path mismatch (expected %s, got %s). Creating new manifest.",
                    self._manifest_manager.project_path,
                    manifest.project_path,
                )
                self._file_manifest = self._manifest_manager.create_new()
                return False

            self._file_manifest = manifest
            logger.info(
                "File manifest loaded: %d files, %d chunks",
                manifest.total_files,
                manifest.total_chunks,
            )
            return True
        # Create new manifest
        self._file_manifest = self._manifest_manager.create_new()
        logger.info("Created new file manifest")
        return False

    def _save_file_manifest(self) -> bool:
        """Save current file manifest to disk.

        Returns:
            True if save was successful, False otherwise
        """
        if not self._manifest_manager or not self._file_manifest:
            logger.warning("No manifest manager or manifest to save")
            return False

        return self._manifest_manager.save(self._file_manifest)

    async def _update_manifest_for_batch(
        self, discovered_files: list[DiscoveredFile], all_chunks: list[CodeChunk]
    ) -> None:
        """Update file manifest after batch indexing.

        Groups chunks by file and updates manifest with chunk IDs for each file.

        Args:
            discovered_files: List of files that were indexed
            all_chunks: All chunks created from those files
        """
        if not self._file_manifest or not self._manifest_manager:
            logger.debug("No manifest to update")
            return

        # Ensure manifest lock is initialized in async context
        if self._manifest_lock is None:
            self._manifest_lock = asyncio.Lock()

        # Group chunks by file path
        from collections import defaultdict

        chunks_by_file: dict[Path, list[str]] = defaultdict(list)
        for chunk in all_chunks:
            if chunk.file_path:
                chunks_by_file[chunk.file_path].append(str(chunk.chunk_id))

        # Update manifest for each file
        for discovered_file in discovered_files:
            chunk_ids = chunks_by_file.get(discovered_file.path, [])
            if not chunk_ids:
                logger.debug("No chunks found for file: %s", discovered_file.path)
                continue

            if relative_path := set_relative_path(discovered_file.path):
                try:
                    # Get current embedding model info
                    model_info = self._get_current_embedding_models()

                    async with self._manifest_lock:
                        self._file_manifest.add_file(
                            path=relative_path,
                            content_hash=discovered_file.file_hash,
                            chunk_ids=chunk_ids,
                            dense_embedding_provider=model_info["dense_provider"],
                            dense_embedding_model=model_info["dense_model"],
                            sparse_embedding_provider=model_info["sparse_provider"],
                            sparse_embedding_model=model_info["sparse_model"],
                            has_dense_embeddings=bool(self._embedding_provider),
                            has_sparse_embeddings=bool(self._sparse_provider),
                        )
                    logger.debug(
                        "Updated manifest for file: %s (%d chunks)", relative_path, len(chunk_ids)
                    )
                except ValueError as e:
                    logger.warning("Failed to add file to manifest: %s - %s", relative_path, e)

    def _try_restore_from_checkpoint(self, *, force_reindex: bool) -> bool:
        """Attempt to restore indexing state from checkpoint.

        Args:
            force_reindex: If True, skip restoration

        Returns:
            True if successfully restored, False otherwise
        """
        if force_reindex:
            return False

        try:
            if self.load_checkpoint():
                logger.info("Resuming from checkpoint")
                return True
        except Exception as e:
            logger.debug("Could not restore from persistence: %s", e)
        return False

    def _discover_files_to_index(
        self, progress_callback: ProgressCallback | None = None
    ) -> list[Path]:
        """Discover files to index using the configured walker settings.

        Creates a fresh walker each time to avoid generator exhaustion issues.
        With incremental indexing, only returns files that are new or modified.

        Args:
            progress_callback: Optional callback for progress updates during discovery

        Returns:
            List of file paths to index
        """
        if not self._walker_settings:
            logger.warning("No walker settings configured, cannot prime index")
            return []

        all_files: list[Path] = []
        try:
            # Create a fresh walker each time - walkers are generators and get exhausted
            walker = rignore.Walker(**self._walker_settings)
            file_count = 0
            for p in walker:
                if p and p.is_file():
                    all_files.append(p)
                    file_count += 1
                    # Report progress every 50 files during discovery
                    # Use 0 as total to indicate indeterminate progress
                    if progress_callback and file_count % 50 == 0:
                        progress_callback("discovery", file_count, 0)
        except Exception:
            logger.warning("Failure during file discovery", exc_info=True)
            return []

        if not all_files:
            logger.info("No files found to index")
            return []

        # If no manifest, index all files
        if not self._file_manifest:
            logger.info("No file manifest - will index all %d discovered files", len(all_files))
            self._stats.files_discovered = len(all_files)
            return all_files

        # Filter to only new or modified files
        files_to_index: list[Path] = []
        unchanged_count = 0
        model_changed_count = 0
        total_to_check = len(all_files)

        # Get current embedding model configuration for comparison
        current_models = self._get_current_embedding_models()

        for idx, path in enumerate(all_files):
            try:
                # Compute current hash (path is absolute from walker)
                current_hash = get_blake_hash(path.read_bytes())

                # Convert to relative path for manifest lookup
                relative_path = set_relative_path(path)
                if not relative_path:
                    # If can't convert to relative, treat as new file
                    files_to_index.append(path)
                    continue

                try:
                    # Check if file needs reindexing (content or embedding model changed)
                    needs_reindex, reason = self._file_manifest.file_needs_reindexing(
                        relative_path,
                        current_hash,
                        current_dense_provider=current_models["dense_provider"],
                        current_dense_model=current_models["dense_model"],
                        current_sparse_provider=current_models["sparse_provider"],
                        current_sparse_model=current_models["sparse_model"],
                    )

                    if needs_reindex:
                        files_to_index.append(path)
                        if "model_changed" in reason:
                            model_changed_count += 1
                            logger.debug(
                                "File needs reindexing due to %s: %s", reason, relative_path
                            )
                    else:
                        unchanged_count += 1
                except ValueError as e:
                    # Invalid path in manifest operations
                    logger.warning("Invalid path %s: %s, will index it", relative_path, e)
                    files_to_index.append(path)
            except Exception:
                logger.warning("Error checking file %s, will index it", path, exc_info=True)
                files_to_index.append(path)

            # Report progress every 100 files during filtering
            if progress_callback and (idx + 1) % 100 == 0:
                progress_callback("discovery", idx + 1, total_to_check)

        # Detect deleted files (in manifest but not on disk)
        # Convert all discovered files to relative paths for comparison
        manifest_files = self._file_manifest.get_all_file_paths()
        all_files_relative = {set_relative_path(p) for p in all_files if set_relative_path(p)}
        deleted_files = manifest_files - all_files_relative

        if deleted_files:
            logger.info("Detected %d deleted files to clean up", len(deleted_files))
            # Schedule cleanup (will be done in separate phase)
            # Convert relative paths from manifest to absolute paths for cleanup
            if self._project_path:
                self._deleted_files = [self._project_path / rel_path for rel_path in deleted_files]
            else:
                logger.warning("No project root set, cannot resolve deleted file paths")
                self._deleted_files = []

        logger.info(
            "Incremental indexing: %d new/modified, %d unchanged, %d deleted%s",
            len(files_to_index),
            unchanged_count,
            len(deleted_files) if deleted_files else 0,
            f", {model_changed_count} due to embedding model changes"
            if model_changed_count > 0
            else "",
        )

        self._stats.files_discovered = len(files_to_index)
        return files_to_index

    async def _perform_batch_indexing_async(
        self, files_to_index: list[Path], progress_callback: ProgressCallback | None
    ) -> None:
        """Execute batch-through-pipeline indexing for discovered files.

        Processes files in batches of 50, each batch flowing through the complete
        pipeline (check → chunk → embed → index) before starting the next batch.
        This provides faster time-to-usable results and lower memory usage.

        Args:
            files_to_index: List of files to process
            progress_callback: Optional callback for granular progress updates
        """
        import math

        batch_size = 50
        total_batches = math.ceil(len(files_to_index) / batch_size)

        logger.info(
            "Starting batch-through-pipeline indexing: %d files in %d batches",
            len(files_to_index),
            total_batches,
        )

        # Signal batch processing start
        if progress_callback:
            progress_callback(
                "batch_start",
                0,
                total_batches,
                extra={"total_files": len(files_to_index), "batch_size": batch_size},
            )

        for batch_num in range(1, total_batches + 1):
            start_idx = (batch_num - 1) * batch_size
            end_idx = min(start_idx + batch_size, len(files_to_index))
            batch = files_to_index[start_idx:end_idx]

            logger.debug(
                "Processing batch %d/%d: %d files (indices %d-%d)",
                batch_num,
                total_batches,
                len(batch),
                start_idx,
                end_idx - 1,
            )

            # Signal batch start
            if progress_callback:
                progress_callback(
                    "batch_start", batch_num, total_batches, extra={"files_in_batch": len(batch)}
                )

            try:
                await self._index_files_batch(batch, progress_callback)
            except Exception:
                logger.warning(
                    "Failure during batch %d/%d indexing", batch_num, total_batches, exc_info=True
                )

            # Signal batch complete
            if progress_callback:
                progress_callback("batch_complete", batch_num, total_batches)

            # Checkpoint after each batch
            if self._checkpoint_manager:
                self.save_checkpoint()
                logger.debug("Checkpoint saved after batch %d/%d", batch_num, total_batches)

            # Yield to event loop between file batches to keep server responsive
            await asyncio.sleep(0)

        logger.info("Batch-through-pipeline indexing complete: %d batches processed", total_batches)

    def _finalize_indexing(self) -> None:
        """Log final statistics, save checkpoint and manifest, and cleanup."""
        logger.info(
            "Indexing complete: %d files processed, %d chunks created, %d indexed, %d errors in %.2fs (%.2f files/sec)",
            self._stats.files_processed,
            self._stats.chunks_created,
            self._stats.chunks_indexed,
            self._stats.total_errors(),
            self._stats.elapsed_time(),
            self._stats.processing_rate(),
        )

        # Log error summary if there were errors
        if self._stats.total_errors() > 0:
            self._log_error_summary()
        # Save file manifest
        self._save_file_manifest()

        # Save final checkpoint
        self.save_checkpoint()
        logger.info("Final checkpoint saved")

        # Clean up checkpoint file on successful completion
        if self._checkpoint_manager and self._stats.total_errors() == 0:
            self._checkpoint_manager.delete()
            logger.info("Checkpoint file deleted after successful completion")

    def _log_error_summary(self):
        error_summary = self._stats.get_error_summary()
        logger.warning(
            "⚠️  Indexing completed with errors: %d total errors", error_summary["total_errors"]
        )
        logger.warning("Errors by phase: %s", error_summary["by_phase"])
        logger.warning("Errors by type: %s", error_summary["by_type"])

        # Log first 2 errors at WARNING for visibility, rest at DEBUG
        for i, error in enumerate(self._stats.structured_errors[:5]):
            log_func = logger.warning if i < 2 else logger.debug
            log_func(
                "Error %d/%d: %s in %s - %s: %s",
                i + 1,
                error_summary["total_errors"],
                error["file_path"],
                error["phase"],
                error["error_type"],
                error["error_message"],
            )
        if error_summary["total_errors"] > 5:
            logger.debug("... and %d more errors", error_summary["total_errors"] - 5)

    async def prime_index(
        self,
        *,
        force_reindex: bool = False,
        progress_callback: ProgressCallback | None = None,
        status_display: Any | None = None,
    ) -> int:
        """Perform an initial indexing pass using the configured rignore walker.

        Enhanced with persistence support, incremental indexing, and batch processing.

        Args:
            force_reindex: If True, skip persistence checks and reindex everything
            progress_callback: Optional callback for granular progress updates
            status_display: Optional StatusDisplay instance for clean user-facing output

        Returns:
            Number of files indexed
        """
        # Initialize providers asynchronously (idempotent)
        await self._initialize_providers_async()

        # Load file manifest for incremental indexing (unless force_reindex)
        if not force_reindex:
            self._load_file_manifest()
        elif self._manifest_manager:
            self._file_manifest = self._manifest_manager.create_new()
            logger.info("Force reindex - created new file manifest")

        # Try to restore from checkpoint (unless force_reindex)
        if self._try_restore_from_checkpoint(force_reindex=force_reindex):
            # Note: In current version, we still reindex discovered files
            # Full resumption would require storing processed file list in checkpoint
            pass

        # Reset stats for new indexing run
        self._stats = IndexingStats()
        self._deleted_files = []
        self._reset_duplicate_counts()

        # Discover files to index (with incremental filtering if manifest exists)
        files_to_index = self._discover_files_to_index(progress_callback)

        # Clean up deleted files first (before indexing new/modified files)
        if self._deleted_files:
            try:
                await self._cleanup_deleted_files()
            except Exception:
                logger.warning("Failed to clean up deleted files", exc_info=True)

        if not files_to_index:
            logger.info("No files to index (all up to date)")
            self._finalize_indexing()
            return 0

        # Report discovery phase complete
        if progress_callback:
            progress_callback(
                "discovery", self._stats.files_discovered, self._stats.files_discovered
            )

        # Index files in batch
        await self._perform_batch_indexing_async(files_to_index, progress_callback)

        # Perform automatic reconciliation: detect and fix missing embeddings
        # Initialize tracking variables for exception handling context
        dense_file_count = 0
        sparse_file_count = 0
        current_models: dict[str, str | None] = {}

        if (
            not force_reindex
            and self._vector_store
            and (self._embedding_provider or self._sparse_provider)
        ):
            try:
                logger.info("Checking for missing embeddings in vector store...")

                # Get current embedding configuration
                current_models = self._get_current_embedding_models()

                # Check if any files need embeddings
                files_needing = self._file_manifest.get_files_needing_embeddings(
                    current_dense_provider=current_models["dense_provider"],
                    current_dense_model=current_models["dense_model"],
                    current_sparse_provider=current_models["sparse_provider"],
                    current_sparse_model=current_models["sparse_model"],
                )

                needs_dense = bool(files_needing.get("dense_only") and self._embedding_provider)
                needs_sparse = bool(files_needing.get("sparse_only") and self._sparse_provider)

                if needs_dense or needs_sparse:
                    dense_file_count = len(files_needing.get("dense_only", []))
                    sparse_file_count = len(files_needing.get("sparse_only", []))
                    if needs_dense:
                        logger.info(
                            "Found %d files needing dense embeddings",
                            dense_file_count,
                        )
                    if needs_sparse:
                        logger.info(
                            "Found %d files needing sparse embeddings",
                            sparse_file_count,
                        )

                    logger.info("Starting automatic reconciliation...")
                    reconciliation_result = await self.add_missing_embeddings_to_existing_chunks(
                        add_dense=needs_dense, add_sparse=needs_sparse
                    )

                    if reconciliation_result["chunks_updated"] > 0:
                        logger.info(
                            "Reconciliation complete: updated %d chunks across %d files",
                            reconciliation_result["chunks_updated"],
                            reconciliation_result["files_processed"],
                        )
                    else:
                        logger.debug("Reconciliation complete: no chunks needed updating")
            except (ProviderError, IndexingError) as e:
                # Provider or indexing errors are expected failure modes
                logger.warning(
                    "Automatic reconciliation failed: %s (collection=%s, dense_files=%d, sparse_files=%d, dense_provider=%s, sparse_provider=%s)",
                    str(e),
                    self._vector_store.collection if self._vector_store else "unknown",
                    dense_file_count,
                    sparse_file_count,
                    current_models.get("dense_provider", "none"),
                    current_models.get("sparse_provider", "none"),
                    exc_info=True,
                )
            except (ConnectionError, TimeoutError, OSError) as e:
                # Network/IO errors that may be transient
                logger.warning(
                    "Automatic reconciliation failed due to connection/IO error: %s (collection=%s)",
                    str(e),
                    self._vector_store.collection if self._vector_store else "unknown",
                    exc_info=True,
                )

        # Finalize and report
        self._finalize_indexing()

        # Log duplicate summary
        self._log_duplicate_summary(status_display)

        return self._stats.files_processed

    @classmethod
    def from_settings(cls, settings: DictView[CodeWeaverSettingsDict] | None = None) -> Indexer:
        """Create an Indexer instance from settings (sync version).

        Note: This method cannot set inc_exc patterns asynchronously.
        Use from_settings_async() for proper async initialization, or
        manually configure the walker's inc_exc patterns after creation.

        Args:
            settings: Optional settings dictionary view

        Returns:
            Configured Indexer instance (may need async initialization via prime_index)
        """
        from codeweaver.config.indexer import DefaultIndexerSettings, IndexerSettings
        from codeweaver.config.settings import get_settings_map

        settings_map = settings or get_settings_map()
        indexer_data = settings_map["indexer"]

        # Handle different types of indexer_data
        if isinstance(indexer_data, Unset):
            index_settings = IndexerSettings.model_validate(DefaultIndexerSettings)
        elif isinstance(indexer_data, IndexerSettings):
            # Use the existing IndexerSettings instance directly
            index_settings = indexer_data
        else:
            # If it's a dict or something else, try to validate it
            index_settings = IndexerSettings.model_validate(
                DefaultIndexerSettings | indexer_data
                if isinstance(indexer_data, dict)
                else DefaultIndexerSettings
            )

        # Note: inc_exc setting is skipped in sync version
        # The walker will be created with default settings
        # For proper inc_exc patterns, use from_settings_async()
        if not index_settings.inc_exc_set:
            logger.debug(
                "inc_exc patterns not set (async operation required). "
                "Use from_settings_async() for full initialization."
            )

        walker_settings = index_settings.to_settings()
        return cls(walker_settings=walker_settings, project_path=settings_map["project_path"])

    @classmethod
    async def from_settings_async(
        cls, settings: DictView[CodeWeaverSettingsDict] | None = None
    ) -> Indexer:
        """Create an Indexer instance from settings with full async initialization.

        This method properly awaits all async operations including inc_exc pattern setting.
        Recommended over from_settings() for production use.

        Args:
            settings: Optional settings dictionary view

        Returns:
            Fully initialized Indexer instance
        """
        from codeweaver.common.utils.git import get_project_path
        from codeweaver.config.indexer import DefaultIndexerSettings, IndexerSettings
        from codeweaver.config.settings import get_settings_map

        settings_map = settings or get_settings_map()
        indexer_data = settings_map["indexer"]

        # Handle different types of indexer_data
        if isinstance(indexer_data, Unset):
            index_settings = IndexerSettings.model_validate(DefaultIndexerSettings)
        elif isinstance(indexer_data, IndexerSettings):
            # Use the existing IndexerSettings instance directly
            index_settings = indexer_data
        else:
            # If it's a dict or something else, try to validate it
            index_settings = IndexerSettings.model_validate(
                DefaultIndexerSettings | indexer_data
                if isinstance(indexer_data, dict)
                else DefaultIndexerSettings
            )

        # Properly await inc_exc initialization
        if not index_settings.inc_exc_set:
            project_path_value = (
                get_project_path()
                if isinstance(settings_map["project_path"], Unset)
                else settings_map["project_path"]
            )
            await index_settings.set_inc_exc(project_path_value)
            logger.debug("inc_exc patterns initialized for project: %s", project_path_value)

        walker_settings = index_settings.to_settings()
        indexer = cls(walker_settings=walker_settings, project_path=settings_map["project_path"])

        # Initialize providers asynchronously
        await indexer._initialize_providers_async()

        return indexer

    def _discover_files_for_batch(self, files: list[Path]) -> list[DiscoveredFile]:
        """Convert file paths to DiscoveredFile objects.

        Args:
            files: List of file paths to discover

        Returns:
            List of valid DiscoveredFile objects
        """
        discovered_files: list[DiscoveredFile] = []
        for path in files:
            try:
                discovered_file = DiscoveredFile.from_path(path)
                if discovered_file and discovered_file.is_text:
                    discovered_files.append(discovered_file)
                    self._store.set(discovered_file.file_hash, discovered_file)
            except Exception:
                logger.warning("Failed to discover file %s", path, exc_info=True)
                self._stats.files_with_errors.append(path)
        return discovered_files

    def _chunk_discovered_files(
        self,
        discovered_files: list[DiscoveredFile],
        progress_callback: ProgressCallback | None = None,
    ) -> list[CodeChunk]:
        """Chunk discovered files using the chunking service.

        Args:
            discovered_files: List of discovered files to chunk
            progress_callback: Optional callback for progress updates

        Returns:
            List of code chunks created from the files
        """
        if not self._chunking_service:
            self._chunking_service = _get_chunking_service()
        all_chunks: list[CodeChunk] = []
        total_files = len(discovered_files)

        for files_chunked, (file_path, chunks) in enumerate(
            self._chunking_service.chunk_files(discovered_files), start=1
        ):
            all_chunks.extend(chunks)
            self._stats.chunks_created += len(chunks)
            logger.debug("Chunked %s: %d chunks", file_path, len(chunks))

            # Report progress for each file chunked
            if progress_callback:
                progress_callback(
                    "chunking",
                    files_chunked,
                    total_files,
                    extra={"chunks_created": len(all_chunks)},
                )

        return all_chunks

    async def _embed_chunks_in_batches(
        self,
        chunks: list[CodeChunk],
        batch_size: int = 100,
        progress_callback: ProgressCallback | None = None,
    ) -> None:
        """Embed chunks in batches with separate dense/sparse progress reporting.

        Args:
            chunks: List of chunks to embed
            batch_size: Number of chunks per batch
            progress_callback: Optional callback for progress updates
        """
        from codeweaver.common.utils.procs import low_priority
        from codeweaver.providers.embedding.registry import get_embedding_registry

        total_chunks = len(chunks)
        registry = get_embedding_registry()

        # Track dense and sparse separately
        dense_embedded = 0
        sparse_embedded = 0

        # Run embedding at low priority to avoid starving the system
        with low_priority():
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i : i + batch_size]

                # Dense embeddings
                if self._embedding_provider:
                    try:
                        if chunks_needing_dense := [
                            chunk
                            for chunk in batch
                            if chunk.chunk_id not in registry
                            or not registry[chunk.chunk_id].has_dense
                        ]:
                            await self._embedding_provider.embed_documents(chunks_needing_dense)
                            dense_embedded += len(chunks_needing_dense)
                            logger.debug(
                                "Dense embedded batch %d-%d (%d chunks)",
                                i,
                                i + len(batch),
                                len(chunks_needing_dense),
                            )
                        else:
                            dense_embedded += len(batch)
                    except ValueError as e:
                        if "already set" not in str(e):
                            raise
                        self._duplicate_dense_count += 1
                        dense_embedded += len(batch)
                    except Exception:
                        logger.warning(
                            "Dense embedding failed for batch %d-%d",
                            i,
                            i + len(batch),
                            exc_info=True,
                        )
                        dense_embedded += len(batch)  # Still count as processed

                    # Report dense embedding progress (always, even after exceptions)
                    if progress_callback:
                        progress_callback("dense_embedding", dense_embedded, total_chunks)

                # Sparse embeddings
                if self._sparse_provider:
                    try:
                        if chunks_needing_sparse := [
                            chunk
                            for chunk in batch
                            if chunk.chunk_id not in registry
                            or not registry[chunk.chunk_id].has_sparse
                        ]:
                            await self._sparse_provider.embed_documents(chunks_needing_sparse)
                            sparse_embedded += len(chunks_needing_sparse)
                            logger.debug(
                                "Sparse embedded batch %d-%d (%d chunks)",
                                i,
                                i + len(batch),
                                len(chunks_needing_sparse),
                            )
                        else:
                            sparse_embedded += len(batch)
                    except ValueError as e:
                        if "already set" not in str(e):
                            raise
                        self._duplicate_sparse_count += 1
                        sparse_embedded += len(batch)
                    except Exception:
                        logger.warning(
                            "Sparse embedding failed for batch %d-%d",
                            i,
                            i + len(batch),
                            exc_info=True,
                        )
                        sparse_embedded += len(batch)  # Still count as processed

                    # Report sparse embedding progress (always, even after exceptions)
                    if progress_callback:
                        progress_callback("sparse_embedding", sparse_embedded, total_chunks)

                # Explicitly yield to event loop between batches to prevent blocking
                # other async tasks (HTTP server, management server, etc.)
                await asyncio.sleep(0)

                # Update overall stats
                self._stats.chunks_embedded += len(batch)
                logger.debug("Embedded batch %d-%d (%d chunks)", i, i + len(batch), len(batch))

    def _retrieve_embedded_chunks(self, chunks: list[CodeChunk]) -> list[CodeChunk]:
        """Retrieve embedded chunks from registry, falling back to originals if needed.

        Args:
            chunks: Original chunks to look up in registry

        Returns:
            Updated chunks from registry, or original chunks if none found
        """
        from codeweaver.providers.embedding.registry import get_embedding_registry

        registry = get_embedding_registry()
        updated_chunks = [
            registry[chunk.chunk_id].chunk for chunk in chunks if chunk.chunk_id in registry
        ]

        if not updated_chunks:
            logger.info(
                "No chunks found in embedding registry. This typically means all chunks were filtered as duplicates during deduplication. "
                "Using original chunks for indexing. Total chunks: %d, Registry size: %d",
                len(chunks),
                len(registry),
            )
            return chunks

        if len(updated_chunks) < len(chunks):
            logger.info(
                "Retrieved %d/%d chunks from registry. %d chunks were deduplicated.",
                len(updated_chunks),
                len(chunks),
                len(chunks) - len(updated_chunks),
            )

        return updated_chunks

    async def _index_chunks_to_store(self, chunks: list[CodeChunk]) -> None:
        """Index chunks to the vector store.

        Args:
            chunks: List of chunks to index
        """
        if not self._vector_store:
            # Log at debug level - this was already shown during health checks
            logger.debug(
                "No vector store configured, skipping vector store upsert. "
                "Chunking will continue, but chunks will not be indexed for semantic search."
            )
            return

        try:
            is_backup = (
                self._failover_manager.is_failover_active if self._failover_manager else False
            )
            await self._vector_store.upsert(chunks, for_backup=is_backup)
            self._stats.chunks_indexed += len(chunks)
            logger.info(
                "Indexed %d chunks to vector store (total: %d)",
                len(chunks),
                self._stats.chunks_indexed,
            )
        except Exception:
            logger.warning("Failed to index to vector store", exc_info=True)

    async def _phase_embed_and_index(
        self, all_chunks: list[CodeChunk], progress_callback: ProgressCallback | None, context: Any
    ) -> None:
        """Execute embedding and indexing phases if providers are initialized."""
        if not (self._embedding_provider or self._sparse_provider or self._vector_store):
            await log_to_client_or_fallback(
                context,
                "debug",
                {
                    "msg": "Skipping embedding and indexing phases",
                    "extra": {"reason": "no_providers_initialized"},
                },
            )
            return

        # Phase 3: Embed chunks in batches
        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Starting embedding phase",
                "extra": {
                    "phase": "embedding",
                    "chunks_to_embed": len(all_chunks),
                    "dense_provider": type(self._embedding_provider).__name__
                    if self._embedding_provider
                    else None,
                    "sparse_provider": type(self._sparse_provider).__name__
                    if self._sparse_provider
                    else None,
                },
            },
        )

        await self._embed_chunks_in_batches(all_chunks, progress_callback=progress_callback)

        # Get registry to check actual embeddings created
        from codeweaver.providers.embedding.registry import get_embedding_registry

        registry = get_embedding_registry()
        embedded_count = len(registry)
        dedup_count = len(all_chunks) - embedded_count if embedded_count < len(all_chunks) else 0

        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Embedding complete",
                "extra": {
                    "phase": "embedding",
                    "chunks_submitted": len(all_chunks),
                    "chunks_embedded": embedded_count,
                    "chunks_deduplicated": dedup_count,
                    "registry_size": len(registry),
                },
            },
        )

        # Note: Progress callbacks for dense/sparse are already fired in _embed_chunks_in_batches

        # Phase 4: Retrieve embedded chunks from registry
        updated_chunks = self._retrieve_embedded_chunks(all_chunks)

        # Phase 5: Index to vector store
        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Starting vector store indexing",
                "extra": {
                    "phase": "storage",
                    "chunks_to_index": len(updated_chunks),
                    "vector_store": type(self._vector_store).__name__
                    if self._vector_store
                    else None,
                },
            },
        )

        await self._index_chunks_to_store(updated_chunks)

        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Vector store indexing complete",
                "extra": {"phase": "storage", "chunks_indexed": self._stats.chunks_indexed},
            },
        )

        if progress_callback:
            progress_callback("indexing", self._stats.chunks_indexed, len(updated_chunks))

    async def _phase_discovery(
        self, files: list[Path], progress_callback: ProgressCallback | None, context: Any
    ) -> list[DiscoveredFile]:
        """Execute discovery phase and return discovered files."""
        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Starting batch indexing",
                "extra": {
                    "phase": "discovery",
                    "batch_size": len(files),
                    "total_discovered": self._stats.files_discovered,
                },
            },
        )

        discovered_files = self._discover_files_for_batch(files)

        if progress_callback:
            progress_callback("checking", len(discovered_files), len(files))

        return discovered_files

    async def _phase_chunking(
        self,
        discovered_files: list[DiscoveredFile],
        progress_callback: ProgressCallback | None,
        context: Any,
    ) -> list[CodeChunk]:
        """Execute chunking phase and return chunks."""
        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Discovery complete, starting chunking",
                "extra": {
                    "phase": "chunking",
                    "files_discovered": len(discovered_files),
                    "languages": list({
                        (
                            f.ext_kind.language.value
                            if hasattr(f.ext_kind.language, "value")
                            else str(f.ext_kind.language)
                        )
                        for f in discovered_files
                        if f.ext_kind
                    }),
                },
            },
        )

        all_chunks = self._chunk_discovered_files(discovered_files, progress_callback)

        await log_to_client_or_fallback(
            context,
            "info",
            {
                "msg": "Chunking complete",
                "extra": {
                    "phase": "chunking",
                    "chunks_created": len(all_chunks),
                    "files_chunked": len(discovered_files),
                    "avg_chunks_per_file": round(len(all_chunks) / len(discovered_files), 1),
                },
            },
        )

        if progress_callback:
            progress_callback(
                "chunking",
                len(discovered_files),
                len(discovered_files),
                extra={"chunks_created": len(all_chunks)},
            )

        return all_chunks

    async def _index_files_batch(
        self,
        files: list[Path],
        progress_callback: ProgressCallback | None = None,
        context: Any = None,
    ) -> None:
        """Index multiple files in batch using the chunking service.

        Args:
            files: List of file paths to index
            progress_callback: Optional callback for granular progress updates
            context: Optional FastMCP context for structured logging
        """
        if not files:
            return

        # Check for shutdown request
        if self._shutdown_requested:
            await log_to_client_or_fallback(
                context,
                "info",
                {
                    "msg": "Shutdown requested",
                    "extra": {"action": "stopping_batch_indexing", "files_remaining": len(files)},
                },
            )
            return

        if not self._chunking_service:
            await log_to_client_or_fallback(
                context,
                "warning",
                {
                    "msg": "No chunking service configured",
                    "extra": {"action": "cannot_batch_index", "files_count": len(files)},
                },
            )
            return

        # Phase 1: Discover files
        discovered_files = await self._phase_discovery(files, progress_callback, context)
        if not discovered_files:
            await log_to_client_or_fallback(
                context,
                "info",
                {
                    "msg": "No valid files to index",
                    "extra": {"phase": "discovery", "files_attempted": len(files)},
                },
            )
            return

        # Phase 1.5: Delete old chunks for files being reindexed
        # This prevents stale embeddings from accumulating in the vector store
        if self._file_manifest and self._vector_store:
            files_deleted = 0
            for file_path in files:
                relative_path = set_relative_path(file_path)
                if relative_path and self._file_manifest.has_file(relative_path):
                    try:
                        await self._vector_store.delete_by_file(file_path)
                        files_deleted += 1
                    except Exception:
                        logger.warning(
                            "Failed to delete old chunks for: %s", relative_path, exc_info=True
                        )
            if files_deleted > 0:
                logger.debug("Deleted old chunks for %d reindexed files in batch", files_deleted)

        # Phase 2: Chunk files
        all_chunks = await self._phase_chunking(discovered_files, progress_callback, context)
        if not all_chunks:
            await log_to_client_or_fallback(
                context,
                "info",
                {
                    "msg": "No chunks created",
                    "extra": {"phase": "chunking", "files_processed": len(discovered_files)},
                },
            )
            return

        # Phase 3-5: Embed and index
        await self._phase_embed_and_index(all_chunks, progress_callback, context)

        # Update file manifest for batch indexing
        await self._update_manifest_for_batch(discovered_files, all_chunks)

        # Update stats with successful file count
        self._stats.files_processed += len(discovered_files)
        self._files_since_checkpoint += len(discovered_files)

        # Save checkpoint if threshold reached
        if self._should_checkpoint():
            self.save_checkpoint()
            logger.info(
                "Checkpoint saved at %d/%d files processed",
                self._stats.files_processed,
                self._stats.files_discovered,
            )

    def _should_checkpoint(self) -> bool:
        """Check if checkpoint should be saved based on frequency criteria.

        Returns:
            True if checkpoint should be saved (every 100 files or every 5 minutes)
        """
        # Check file count threshold
        if self._files_since_checkpoint >= 100:
            return True

        # Check time threshold (300 seconds = 5 minutes)
        elapsed_time = time.time() - self._last_checkpoint_time
        return elapsed_time >= 300

    @property
    def stats(self) -> IndexingStats:
        """Get current indexing statistics."""
        return self._stats

    @property
    def session_statistics(self) -> SessionStatistics:
        """Get session statistics for comprehensive tracking."""
        return self._session_statistics

    def _remove_path(self, path: Path) -> int:
        """Remove a path from the store.

        Returns:
            Number of entries removed
        """
        to_delete: list[Any] = []

        # Get project root for resolving relative paths
        project_path = self._project_path

        for key, discovered_file in list(self._store.items()):
            try:
                # Try samefile first (handles symlinks and different path representations)
                if discovered_file.path.samefile(path):
                    to_delete.append(key)
            except OSError:
                # If either file doesn't exist, fall back to path comparison
                # This happens when files are deleted, which is the typical case for _remove_path
                try:
                    # Resolve paths for comparison
                    # discovered_file.path is *always* relative to project path
                    discovered_abs = (project_path / discovered_file.path).resolve()

                    path_abs = (project_path / path).resolve()

                    if discovered_abs == path_abs:
                        to_delete.append(key)
                except Exception:
                    # defensive: malformed entry shouldn't break cleanup
                    logger.warning("Error checking stored item for deletion", exc_info=True)
                    continue
            except Exception:
                # defensive: malformed entry shouldn't break cleanup
                logger.warning("Error checking stored item for deletion", exc_info=True)
                continue
        for key in to_delete:
            self._store.delete(key)
        return len(to_delete)

    @staticmethod
    def keep_alive(alive_time: float = 5000) -> None:
        """A long-lived no-op function suitable as the run target for arun_process.

        We keep the child process alive so arun_process can signal and restart it,
        but all indexing happens in the callback on the main process.
        """
        from time import sleep

        try:
            while True:
                sleep(alive_time)
        except KeyboardInterrupt:
            # allow graceful stop
            return

    async def initialize_from_vector_store(self) -> None:
        """Query vector store for indexed files on cold start.

        - Query vector store for all indexed chunks
        - Reconstruct file metadata store from chunk payloads
        - Populate self._store with DiscoveredFile objects
        """
        logger.debug("Persistence from vector store not yet implemented")

    def save_state(self, state_path: Path) -> None:
        """Save current indexer state to a file."""

    def save_checkpoint(self, checkpoint_path: DirectoryPath | None = None) -> None:
        """Save indexing state to checkpoint file.

        Saves current indexing progress including:
        - Files discovered/processed/indexed counts
        - Chunks created/embedded/indexed counts
        - Error list with file paths
        - Settings hash for invalidation detection
        - File manifest status for incremental indexing

        Args:
            checkpoint_path: Optional custom checkpoint file path (primarily for testing)
        """
        if not self._checkpoint_manager:
            logger.warning("No checkpoint manager configured")
            return

        # Compute settings hash

        settings_hash = self._checkpoint_manager.compute_settings_hash(
            self._checkpoint_manager.get_relevant_settings()
        )

        # Create or update checkpoint
        if not self._checkpoint:
            self._checkpoint = IndexingCheckpoint(
                project_path=self._checkpoint_manager.get_relevant_settings()["project_path"],
                settings_hash=settings_hash,
            )

        # Type narrowing for checkpoint - guaranteed to be IndexingCheckpoint at this point
        checkpoint = self._checkpoint
        assert isinstance(checkpoint, IndexingCheckpoint), "Checkpoint must be initialized"  # noqa: S101

        # Update checkpoint with current stats
        if not isinstance(self._stats, IndexingStats):
            self._stats = IndexingStats()
        checkpoint.files_discovered = self._stats.files_discovered
        checkpoint.files_embedding_complete = self._stats.files_processed
        checkpoint.files_indexed = self._stats.files_processed
        checkpoint.chunks_created = self._stats.chunks_created
        checkpoint.chunks_embedded = self._stats.chunks_embedded
        checkpoint.chunks_indexed = self._stats.chunks_indexed
        checkpoint.files_with_errors = [str(p) for p in self._stats.files_with_errors]
        checkpoint.settings_hash = settings_hash

        # Update manifest info
        if self._file_manifest:
            checkpoint.has_file_manifest = True
            checkpoint.manifest_file_count = self._file_manifest.total_files
        else:
            checkpoint.has_file_manifest = False
            checkpoint.manifest_file_count = 0

        # Save to disk
        self._checkpoint_manager.save(checkpoint)
        self._last_checkpoint_time = time.time()
        self._files_since_checkpoint = 0

    def _construct_checkpoint_fingerprint(self) -> str:
        """Construct a fingerprint hash of current settings for checkpoint validation."""
        if not self._checkpoint_manager:
            raise RuntimeError("No checkpoint manager configured")
        return self._checkpoint_manager.compute_settings_hash(
            self._checkpoint_manager.get_relevant_settings()
        )

    def load_checkpoint(self, _checkpoint_path: Path | None = None) -> bool:
        """Load indexing state from checkpoint file.

        Loads checkpoint if available and valid:
        - Verifies settings hash matches current config
        - Skips if checkpoint >24 hours old
        - Restores stats for progress tracking

        Args:
            _checkpoint_path: Optional custom checkpoint file path (primarily for testing)

        Returns:
            True if checkpoint was loaded successfully and is valid for resumption
        """
        if not self._checkpoint_manager:
            logger.warning("No checkpoint manager configured")
            return False

        # Load checkpoint from disk
        checkpoint = self._checkpoint_manager.load()
        if not checkpoint:
            return False

        current_settings_hash = self._construct_checkpoint_fingerprint()

        if not self._checkpoint_manager.should_resume(
            checkpoint, current_settings_hash, max_age_hours=24
        ):
            logger.info("Checkpoint cannot be used for resumption, will reindex from scratch")
            return False

        # Restore stats from checkpoint
        self._stats.files_discovered = checkpoint.files_discovered
        self._stats.files_processed = checkpoint.files_embedding_complete
        self._stats.chunks_created = checkpoint.chunks_created
        self._stats.chunks_embedded = checkpoint.chunks_embedded
        self._stats.chunks_indexed = checkpoint.chunks_indexed
        type(self._stats).files_with_errors = [  # ty: ignore[invalid-assignment]
            path
            for p in checkpoint.files_with_errors
            if p and isinstance(p, str) and (path := Path(p)).exists()
        ]

        self._checkpoint = checkpoint
        logger.info(
            "Checkpoint loaded successfully: %d/%d files processed, %d chunks created",
            checkpoint.files_embedding_complete,
            checkpoint.files_discovered,
            checkpoint.chunks_created,
        )
        return True

    async def validate_manifest_with_vector_store(self) -> dict[str, Any]:
        """Validate that chunks in manifest exist in vector store and detect orphans.

        Returns:
            Dictionary with validation results including:
            - total_chunks: Total chunks in manifest
            - missing_chunks: Number of chunks not found in vector store
            - missing_chunk_ids: List of missing chunk IDs
            - files_with_missing_chunks: List of file paths with missing chunks
            - orphaned_chunks: Number of chunks in store but not in manifest
            - orphaned_chunk_ids: List of orphaned chunk IDs (stale embeddings)
        """
        if not self._file_manifest:
            return {
                "error": "No manifest loaded",
                "total_chunks": 0,
                "missing_chunks": 0,
                "missing_chunk_ids": [],
                "files_with_missing_chunks": [],
                "orphaned_chunks": 0,
                "orphaned_chunk_ids": [],
            }

        if not self._vector_store:
            return {
                "error": "No vector store configured",
                "total_chunks": self._file_manifest.total_chunks,
                "missing_chunks": 0,
                "missing_chunk_ids": [],
                "files_with_missing_chunks": [],
                "orphaned_chunks": 0,
                "orphaned_chunk_ids": [],
            }

        from qdrant_client.models import UUID

        # Get all chunk IDs from manifest
        manifest_chunk_ids = self._file_manifest.get_all_chunk_ids()

        logger.info(
            "Validating %d chunks from manifest against vector store", len(manifest_chunk_ids)
        )

        # Try to retrieve chunks from vector store
        missing_chunk_ids: list[str] = []
        orphaned_chunk_ids: list[str] = []
        files_with_missing: set[str] = set()

        # Batch retrieve chunks (Qdrant supports retrieving multiple points)
        try:
            # Convert string UUIDs to Qdrant UUID format
            point_ids = [UUID(chunk_id) for chunk_id in manifest_chunk_ids]

            # Retrieve points from vector store
            collection_name = self._vector_store.collection
            if not collection_name:
                return {
                    "error": "No collection name configured",
                    "total_chunks": len(manifest_chunk_ids),
                    "missing_chunks": 0,
                    "missing_chunk_ids": [],
                    "files_with_missing_chunks": [],
                    "orphaned_chunks": 0,
                    "orphaned_chunk_ids": [],
                }

            # Retrieve points in batches to avoid Qdrant limits
            retrieved = []
            for i in range(0, len(point_ids), 1000):
                batch_ids = point_ids[i : i + 1000]
                batch_retrieved = await self._vector_store.client.retrieve(
                    collection_name=collection_name,
                    ids=batch_ids,
                    with_payload=False,
                    with_vectors=False,
                )
                retrieved.extend(batch_retrieved)

            # Check which chunks are missing from store
            retrieved_ids = {str(point.id) for point in retrieved}
            missing_chunk_ids = [cid for cid in manifest_chunk_ids if cid not in retrieved_ids]

            # Find files with missing chunks
            if missing_chunk_ids:
                for path_str, entry in self._file_manifest.files.items():
                    chunk_ids = entry["chunk_ids"]
                    if any(cid in missing_chunk_ids for cid in chunk_ids):
                        files_with_missing.add(path_str)

            # Detect orphaned chunks (in store but not in manifest)
            # Use HasIdCondition filter to efficiently find points NOT in manifest
            from qdrant_client.models import Filter, HasIdCondition

            orphan_filter = Filter(
                must_not=[HasIdCondition(has_id=point_ids)]  # Reuse point_ids from earlier
            )

            # Scroll with filter to get only orphaned points
            orphaned_chunk_ids = []
            offset = None
            while True:
                scroll_result = await self._vector_store.client.scroll(
                    collection_name=collection_name,
                    scroll_filter=orphan_filter,
                    limit=1000,
                    offset=offset,
                    with_payload=False,
                    with_vectors=False,
                )
                points, next_offset = scroll_result
                if not points:
                    break
                orphaned_chunk_ids.extend(str(point.id) for point in points)
                if next_offset is None:
                    break
                offset = next_offset

            logger.info(
                "Validation complete: %d/%d manifest chunks found, %d missing, %d orphaned",
                len(retrieved_ids),
                len(manifest_chunk_ids),
                len(missing_chunk_ids),
                len(orphaned_chunk_ids),
            )

        except Exception as e:
            logger.warning("Failed to validate chunks against vector store: %s", e, exc_info=True)
            return {
                "error": f"Validation failed: {e}",
                "total_chunks": len(manifest_chunk_ids),
                "missing_chunks": 0,
                "missing_chunk_ids": [],
                "files_with_missing_chunks": [],
                "orphaned_chunks": 0,
                "orphaned_chunk_ids": [],
            }

        return {
            "total_chunks": len(manifest_chunk_ids),
            "missing_chunks": len(missing_chunk_ids),
            "missing_chunk_ids": missing_chunk_ids[:100],  # Limit to first 100 for logging
            "files_with_missing_chunks": sorted(files_with_missing),
            "orphaned_chunks": len(orphaned_chunk_ids),
            "orphaned_chunk_ids": orphaned_chunk_ids[:100],  # Limit to first 100 for logging
        }

    async def add_missing_embeddings_to_existing_chunks(
        self, *, add_dense: bool = False, add_sparse: bool = False
    ) -> dict[str, Any]:
        """Add missing embedding types to existing chunks without reprocessing files.

        This performs selective reindexing by adding sparse or dense embeddings
        to chunks that already exist in the vector store but lack certain embedding types.

        Args:
            add_dense: Whether to add dense embeddings to chunks that don't have them
            add_sparse: Whether to add sparse embeddings to chunks that don't have them

        Returns:
            Dictionary with operation results including:
            - files_processed: Number of files processed
            - chunks_updated: Number of chunks updated
            - errors: List of errors encountered
        """
        if not self._file_manifest:
            return {"error": "No manifest loaded", "files_processed": 0, "chunks_updated": 0}

        if not self._vector_store:
            return {
                "error": "No vector store configured",
                "files_processed": 0,
                "chunks_updated": 0,
            }

        if not (add_dense or add_sparse):
            return {
                "error": "Must specify add_dense or add_sparse",
                "files_processed": 0,
                "chunks_updated": 0,
            }

        if add_dense and not self._embedding_provider:
            return {
                "error": "Dense embedding provider is not configured",
                "files_processed": 0,
                "chunks_updated": 0,
            }
        if add_sparse and not self._sparse_provider:
            return {
                "error": "Sparse embedding provider is not configured",
                "files_processed": 0,
                "chunks_updated": 0,
            }
        # Get current embedding configuration
        current_models = self._get_current_embedding_models()

        # Find files needing embeddings
        files_needing = self._file_manifest.get_files_needing_embeddings(
            current_dense_provider=current_models["dense_provider"],
            current_dense_model=current_models["dense_model"],
            current_sparse_provider=current_models["sparse_provider"],
            current_sparse_model=current_models["sparse_model"],
        )

        files_to_process: list[Path] = []
        if add_dense and self._embedding_provider:
            files_to_process.extend(files_needing["dense_only"])
            logger.info("Found %d files needing dense embeddings", len(files_needing["dense_only"]))

        if add_sparse and self._sparse_provider:
            files_to_process.extend(files_needing["sparse_only"])
            logger.info(
                "Found %d files needing sparse embeddings", len(files_needing["sparse_only"])
            )

        # Deduplicate files to avoid double processing
        files_to_process = list(set(files_to_process))
        if not files_to_process:
            logger.info("No files need embedding updates")
            return {"files_processed": 0, "chunks_updated": 0, "errors": []}

        # Process each file: retrieve chunks, add embeddings, update vector store
        from qdrant_client.models import UUID

        files_processed = 0
        chunks_updated = 0
        errors: list[str] = []

        for file_path in files_to_process:
            try:
                # Get chunk IDs for this file from manifest
                chunk_ids = self._file_manifest.get_chunk_ids_for_file(file_path)
                if not chunk_ids:
                    continue

                # Retrieve points from vector store to get payloads
                point_ids = [UUID(cid) for cid in chunk_ids]
                collection_name = self._vector_store.collection
                if not collection_name:
                    errors.append(f"{file_path}: No collection name")
                    continue

                retrieved = await self._vector_store.client.retrieve(
                    collection_name=collection_name,
                    ids=point_ids,
                    with_payload=True,
                    with_vectors=True,  # Need to check what vectors already exist
                )

                # For each retrieved point, generate the missing embedding
                updates: list[tuple[str, dict[str, list[float]]]] = []
                for point in retrieved:
                    # Reconstruct CodeChunk from payload (simplified - may need adjustment)
                    payload = point.payload
                    if not payload:
                        continue

                    # Check which vector types already exist in this point
                    # Handle various vector representations from Qdrant:
                    # - dict[str, list[float]]: Named vectors
                    # - Mapping (e.g., NamedVectors): Mapping-like objects with vector names
                    # - list[float]: Single unnamed vector
                    # - None: No vectors
                    from collections.abc import Mapping

                    existing_vectors = point.vector if hasattr(point, "vector") else {}
                    if isinstance(existing_vectors, Mapping):
                        # Already a mapping (dict or NamedVectors), use keys as-is
                        existing_vector_names = set(existing_vectors.keys())
                    elif existing_vectors:
                        # Single unnamed vector (list), treat as dense with empty string key
                        existing_vector_names = {""}
                    else:
                        existing_vector_names = set()

                    # Generate missing embeddings
                    vectors_to_add: dict[str, list[float]] = {}

                    # Only add dense if requested AND not already present
                    if (
                        add_dense
                        and self._embedding_provider
                        and "" not in existing_vector_names  # Check if dense vector missing
                        and (chunk_text := payload.get("text", ""))
                    ):
                        # Use embedding provider to generate dense embedding
                        dense_emb = await self._embedding_provider.embed_document([chunk_text])
                        if dense_emb and len(dense_emb) > 0:
                            # Use empty string for default dense vector
                            vectors_to_add[""] = dense_emb[0]

                    # Only add sparse if requested AND not already present
                    if (
                        add_sparse
                        and self._sparse_provider
                        and "sparse" not in existing_vector_names  # Check if sparse vector missing
                        and (chunk_text := payload.get("text", ""))
                    ):
                        sparse_emb = await self._sparse_provider.embed_document([chunk_text])
                        if sparse_emb and len(sparse_emb) > 0:
                            # Sparse vector name
                            vectors_to_add["sparse"] = sparse_emb[0]

                    if vectors_to_add:
                        updates.append((str(point.id), vectors_to_add))

                # Update vectors in vector store using update_vectors
                if updates:
                    try:
                        # Batch update: collect all point IDs and vectors
                        batch_points = [UUID(point_id) for point_id, _ in updates]
                        batch_vectors = [vectors for _, vectors in updates]
                        await self._vector_store.client.update_vectors(
                            collection_name=collection_name,
                            points=batch_points,
                            vectors=batch_vectors,
                        )
                        chunks_updated += len(updates)

                        # Only update manifest if vector store update succeeded
                        if self._manifest_lock:
                            async with self._manifest_lock:
                                if (relative_path := set_relative_path(file_path)) and (
                                    entry := self._file_manifest.get_file(relative_path)
                                ):
                                    # Update the entry to reflect new embeddings
                                    self._file_manifest.add_file(
                                        path=relative_path,
                                        content_hash=entry["content_hash"],
                                        chunk_ids=entry["chunk_ids"],
                                        dense_embedding_provider=current_models["dense_provider"]
                                        if add_dense
                                        else entry.get("dense_embedding_provider"),
                                        dense_embedding_model=current_models["dense_model"]
                                        if add_dense
                                        else entry.get("dense_embedding_model"),
                                        sparse_embedding_provider=current_models["sparse_provider"]
                                        if add_sparse
                                        else entry.get("sparse_embedding_provider"),
                                        sparse_embedding_model=current_models["sparse_model"]
                                        if add_sparse
                                        else entry.get("sparse_embedding_model"),
                                        has_dense_embeddings=True
                                        if add_dense
                                        else entry.get("has_dense_embeddings", False),
                                        has_sparse_embeddings=True
                                        if add_sparse
                                        else entry.get("has_sparse_embeddings", False),
                                    )
                        else:
                            logger.warning(
                                "Manifest update skipped for file %s because manifest lock is None. This may lead to inconsistent state.",
                                file_path,
                            )

                        files_processed += 1
                        # Save manifest after each successful file processing
                        self._save_file_manifest()

                    except Exception as update_error:
                        logger.warning(
                            "Failed to batch update vectors for file %s: %s",
                            file_path,
                            update_error,
                        )
                        errors.append(f"{file_path}: {update_error}")
                        # Skip this file - don't update manifest or increment counter
            except Exception as e:
                error_msg = f"{file_path}: {e}"
                logger.warning(
                    "Problem adding embeddings to file %s: %s", file_path, e, exc_info=True
                )
                errors.append(error_msg)

        # Final manifest save (optional, for completeness)
        self._save_file_manifest()

        logger.info(
            "Selective reindexing complete: %d files processed, %d chunks updated, %d errors",
            files_processed,
            chunks_updated,
            len(errors),
        )

        return {
            "files_processed": files_processed,
            "chunks_updated": chunks_updated,
            "errors": errors[:10],  # Limit errors in response
            "total_errors": len(errors),
        }


__all__ = ("Indexer",)
