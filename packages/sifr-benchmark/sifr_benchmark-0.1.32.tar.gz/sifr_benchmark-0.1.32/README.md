# SiFR Benchmark

**How well do AI agents understand web UI?**

Benchmark comparing SiFR vs HTML vs AXTree vs Screenshots across complex websites.

> âš ï¸ **This is an example run, not a definitive study.** The benchmark is fully reproducible â€” run it yourself on your sites, your models, your use cases. We show our results; you verify on yours.

## Results

Tested on 10 high-complexity sites: Amazon, YouTube, Reddit, eBay, Walmart, Airbnb, Yelp, IMDB, ESPN, GitHub.

All formats tested with **equal 400KB token budget** for fair comparison.

| Format | Accuracy | Tokens (avg) | 
|--------|----------|--------------|
| **SiFR** | **71.7%** | 102K |
| Screenshot | 27.0% | 38K |
| Raw HTML | 11.4% | 122K |
| AXTree | 1.5% | 6K |

**SiFR is 2.7x more accurate than screenshots and 6.3x more accurate than raw HTML.**

### Per-Site Breakdown

| Site | SiFR | Screenshot | HTML | AXTree |
|------|------|------------|------|--------|
| GitHub | ğŸ† **100%** | 0% | â€” | 0% |
| YouTube | ğŸ† **100%** | 64% | 0% | 0% |
| Amazon | ğŸ† **85.7%** | 22.9% | â€” | 0% |
| Walmart | ğŸ† **85.7%** | 13.3% | 11.4% | 0% |
| Reddit | ğŸ† **83.3%** | 36% | â€” | 0% |
| Yelp | ğŸ† **62.5%** | 57.1% | â€” | 0% |
| ESPN | ğŸ† **57.1%** | 11.4% | 22.9% | 0% |
| IMDB | ğŸ† **50%** | 16% | â€” | 16.7% |
| eBay | ğŸ† **28.6%** | 26.7% | 11.4% | 0% |

SiFR wins on **9 out of 9 sites** where it ran successfully.

## What is SiFR?

**Structured Interface Format for Representation** â€” a compact format optimized for LLM understanding of web UI.

```yaml
a015:
  tag: a
  text: "Add to Cart"
  box: [500, 300, 120, 40]
  attrs: {href: "/cart/add", class: "btn-primary"}
  salience: high
```

Key advantages:
- **Actionable IDs**: Every element gets a unique ID (`a015`, `btn003`)
- **Salience scoring**: High/medium/low importance ranking
- **Structured for LLMs**: Optimized for "find element â†’ take action" tasks
- **Model-agnostic**: Works with any LLM that can read text

## Installation

```bash
pip install sifr-benchmark
```

### Prerequisites

1. **Element-to-LLM Chrome Extension** â€” captures pages in SiFR format
   - Load unpacked from `element-to-llm-chrome/`

2. **API Keys**
   ```bash
   export OPENAI_API_KEY=sk-...
   export ANTHROPIC_API_KEY=sk-ant-...  # optional
   ```

3. **Playwright** (for automated capture)
   ```bash
   playwright install chromium
   ```

## Quick Start

### Full Benchmark

Capture â†’ Generate Ground Truth â†’ Test â€” all in one command:

```bash
sifr-bench full-benchmark-e2llm https://www.amazon.com https://www.youtube.com \
  -e /path/to/element-to-llm-extension \
  -s 400
```

Options:
- `-e, --extension` â€” Path to E2LLM extension (required)
- `-s, --target-size` â€” Token budget in KB for ALL formats (default: 400)
- `-m, --models` â€” Models to test (default: gpt-4o-mini)
- `-v, --verbose` â€” Show detailed output

### Other Commands

```bash
# List all benchmark runs
sifr-bench list-runs

# Compare multiple runs
sifr-bench compare benchmark_runs/run_1 benchmark_runs/run_2

# Validate SiFR files
sifr-bench validate examples/

# Show help
sifr-bench info
```

## How It Works

### 1. Capture

The extension captures 4 formats simultaneously:
- **SiFR** â€” Structured format with salience scoring
- **HTML** â€” Raw rendered DOM (`outerHTML`)
- **AXTree** â€” Playwright accessibility tree
- **Screenshot** â€” Full-page PNG

### 2. Ground Truth Generation

GPT-4o Vision analyzes screenshot + SiFR to generate agent tasks:
- **Click**: "Click the Sign In button" â†’ `a003`
- **Input**: "Enter search query" â†’ `input001`
- **Locate**: "Find the main heading" â†’ `h1001`

### 3. Benchmark

Each format tested with same token budget, same model, same prompts:

```
Task: "Click on the shopping cart icon"
Expected: a015

SiFR response: a015 âœ“
HTML response: none âœ—
Screenshot response: cart icon (no ID) âœ—
```

## Methodology Notes

> **Run it yourself.** This benchmark exists so you can test on your own sites and models. Our results are one data point â€” your results on your use case matter more.

- **Equal token budget**: All formats truncated to same size (400KB default). Fair comparison.
  
- **Ground truth is auto-generated**: GPT-4o Vision creates tasks. For production, consider human verification.

- **AXTree 0% is a real finding**: Many agent frameworks use accessibility trees. This shows why that's problematic.

- **7 tasks per site**: Practical, not academic. When did you last need 2000 clicks on one page?

## Why Raw HTML Fails

```
Amazon HTML: 909KB original
After truncation: 400KB (loses 56% of content)
Result: 0% accuracy â€” critical elements gone

Amazon SiFR: 613KB original  
After truncation: 400KB (loses 35% of content)
Result: 85.7% accuracy â€” structure survives
```

HTML is verbose. When you truncate it, you lose random chunks. SiFR is pre-compressed with salience scoring â€” important elements survive truncation.

## Output Format

```
        Benchmark Results: Combined (10 sites)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Format     â”ƒ Accuracy â”ƒ  Tokens â”ƒ  Latency â”ƒ Status â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ sifr       â”‚    71.7% â”‚ 101,683 â”‚ 30,221ms â”‚   âœ…   â”‚
â”‚ screenshot â”‚    27.0% â”‚  38,074 â”‚  7,942ms â”‚   âš ï¸   â”‚
â”‚ html_raw   â”‚    11.4% â”‚ 122,190 â”‚ 35,901ms â”‚   âš ï¸   â”‚
â”‚ axtree     â”‚     1.5% â”‚   6,044 â”‚  2,034ms â”‚   âš ï¸   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Status:
- âœ… Success (accuracy â‰¥ 50%)
- âš ï¸ Warning (accuracy < 50%)
- âŒ Failed (accuracy = 0%)

## Run Directory Structure

Each benchmark creates an isolated run:

```
benchmark_runs/run_20251206_210357/
â”œâ”€â”€ captures/
â”‚   â”œâ”€â”€ sifr/*.sifr
â”‚   â”œâ”€â”€ html/*.html
â”‚   â”œâ”€â”€ axtree/*.json
â”‚   â””â”€â”€ screenshots/*.png
â”œâ”€â”€ ground-truth/*.json
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw_results.json
â”‚   â””â”€â”€ summary.json
â””â”€â”€ run_meta.json
```

## Tested Models

Default: gpt-4o-mini

The benchmark supports any OpenAI or Anthropic model. Run with different models:

```bash
sifr-bench full-benchmark-e2llm ... -m gpt-4o
sifr-bench full-benchmark-e2llm ... -m claude-sonnet
```

## Contributing

- **Add test sites**: Run benchmark on more URLs
- **Improve ground truth**: Manual verification of tasks
- **New models**: Add support in `models.py`
- **Bug reports**: Open an issue

## Citation

```bibtex
@misc{sifr2025,
  title={SiFR: Structured Interface Format for AI Web Agents},
  author={SiFR Contributors},
  year={2025},
  url={https://github.com/Alechko375/sifr-benchmark}
}
```

## License

MIT
