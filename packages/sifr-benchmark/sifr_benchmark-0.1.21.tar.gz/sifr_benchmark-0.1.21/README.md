# SiFR Benchmark

**How well do AI agents understand web UI?**

Benchmark comparing SiFR vs HTML vs AXTree vs Screenshots across 10 complex websites.

## Results

Tested on 10 high-complexity sites: Amazon, YouTube, Reddit, eBay, Walmart, Airbnb, Yelp, IMDB, ESPN, GitHub.

| Format | Accuracy | Tokens (avg) | Latency | 
|--------|----------|--------------|---------|
| **SiFR** | **64.6%** | 25,512 | 7.5s |
| Screenshot | 21.5% | 37,765 | 8.0s |
| Raw HTML | 4.7% | 32,879 | 8.3s |
| AXTree | 3.0% | 5,289 | 1.9s |

**SiFR is 3x more accurate than screenshots and 14x more accurate than raw HTML.**

### Per-Site Breakdown

| Site | SiFR | Screenshot | HTML | AXTree |
|------|------|------------|------|--------|
| GitHub | ğŸ† **100%** | 0% | 0% | 0% |
| YouTube | ğŸ† **100%** | 53.3% | 0% | 0% |
| Walmart | ğŸ† **85.7%** | 30% | 11.4% | 0% |
| Reddit | ğŸ† **83.3%** | 0% | 0% | 0% |
| eBay | ğŸ† **71.4%** | 13.3% | 0% | 14.3% |
| Amazon | ğŸ† **66.7%** | 25.7% | 0% | 0% |
| Airbnb | ğŸ† **57.1%** | 0% | 34.3% | 0% |
| Yelp | ğŸ¤ 50% | 50% | 0% | 12.5% |
| ESPN | ğŸ† **42.9%** | 0% | 0% | 0% |
| IMDB | 0% | ğŸ† **45%** | 0% | 0% |

SiFR wins on **9 out of 10 sites**.

## What is SiFR?

**Structured Interface Format for Representation** â€” a compact format optimized for LLM understanding of web UI.

```yaml
a015:
  tag: a
  text: "Add to Cart"
  box: [500, 300, 120, 40]
  attrs: {href: "/cart/add", class: "btn-primary"}
  salience: high
```

Key advantages:
- **Compact**: 10-20x smaller than raw HTML
- **Actionable IDs**: Every element has a unique ID (`a015`, `btn003`)
- **Salience scoring**: High/medium/low importance ranking
- **LLM-native**: Structured for AI comprehension

## Installation

```bash
pip install sifr-benchmark
```

### Prerequisites

1. **Element-to-LLM Chrome Extension** â€” captures pages in SiFR format
   - [Chrome Web Store](https://chromewebstore.google.com/detail/element-to-llm-dom-captur/oofdfeinchhgnhlikkfdfcldbpcjcgnj)
   - Or load unpacked from `element-to-llm-chrome/`

2. **API Keys**
   ```bash
   export OPENAI_API_KEY=sk-...
   export ANTHROPIC_API_KEY=sk-ant-...  # optional
   ```

3. **Playwright** (for automated capture)
   ```bash
   playwright install chromium
   ```

## Quick Start

### Full Benchmark (Recommended)

Capture â†’ Generate Ground Truth â†’ Test â€” all in one command:

```bash
sifr-bench full-benchmark-e2llm https://www.amazon.com https://www.youtube.com \
  -e /path/to/element-to-llm-extension \
  -s 400
```

Options:
- `-e, --extension` â€” Path to E2LLM extension (required)
- `-s, --target-size` â€” SiFR budget in KB (default: 100, max: 380)
- `-m, --models` â€” Models to test (default: gpt-4o-mini)
- `-v, --verbose` â€” Show detailed output

### Other Commands

```bash
# List all benchmark runs
sifr-bench list-runs

# Compare multiple runs
sifr-bench compare benchmark_runs/run_1 benchmark_runs/run_2

# Validate SiFR files
sifr-bench validate examples/

# Show help
sifr-bench info
```

## How It Works

### 1. Capture (E2LLM Extension)

The extension captures 4 formats simultaneously:
- **SiFR** â€” Structured format with salience scoring
- **HTML** â€” Raw rendered DOM (`outerHTML`)
- **AXTree** â€” Playwright accessibility tree
- **Screenshot** â€” Full-page PNG

### 2. Ground Truth Generation

GPT-4o Vision analyzes the screenshot + SiFR to generate tasks:
- **Click tasks**: "Click the Sign In button" â†’ `a003`
- **Input tasks**: "Enter search query" â†’ `input001`
- **Locate tasks**: "Find the main heading" â†’ `h1001`

### 3. Benchmark

Each format is tested against the same ground truth:
```
Question: "Click on the shopping cart icon"
Expected: a015
SiFR response: a015 âœ“
HTML response: none âœ—
```

## Output Format

```
        Benchmark Results: Combined (10 sites)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ Format     â”ƒ Accuracy â”ƒ Tokens â”ƒ Latency â”ƒ Status â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ sifr       â”‚    64.6% â”‚ 25,512 â”‚  7,511msâ”‚   âœ…   â”‚
â”‚ screenshot â”‚    21.5% â”‚ 37,765 â”‚  8,039msâ”‚   âš ï¸   â”‚
â”‚ html_raw   â”‚     4.7% â”‚ 32,879 â”‚  8,332msâ”‚   âš ï¸   â”‚
â”‚ axtree     â”‚     3.0% â”‚  5,289 â”‚  1,876msâ”‚   âš ï¸   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Status icons:
- âœ… Success (accuracy â‰¥ 50%)
- âš ï¸ Warning (accuracy < 50%)
- âŒ Failed (accuracy = 0%)

## Run Directory Structure

Each benchmark creates an isolated run:

```
benchmark_runs/run_20251206_182941/
â”œâ”€â”€ captures/
â”‚   â”œâ”€â”€ sifr/*.sifr
â”‚   â”œâ”€â”€ html/*.html
â”‚   â”œâ”€â”€ axtree/*.json
â”‚   â””â”€â”€ screenshots/*.png
â”œâ”€â”€ ground-truth/*.json
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ raw_results.json
â”‚   â””â”€â”€ summary.json
â””â”€â”€ run_meta.json
```

## Key Findings

1. **SiFR dominates complex sites** â€” 100% on GitHub/YouTube, 85%+ on Walmart/Reddit
2. **Screenshots struggle with dense UI** â€” Can't reliably identify elements
3. **Raw HTML is unusable** â€” Too large, no semantic structure for LLMs
4. **AXTree IDs don't match** â€” Own ID scheme incompatible with ground truth

### Why IMDB Failed?

IMDB has the largest DOM (706KB SiFR, 2171KB HTML). Truncation to 97KB removes critical elements. This highlights the need for smarter budgeting in the E2LLM extension.

## Tested Models

- GPT-4o-mini (default)
- GPT-4o
- Claude 3.5 Sonnet
- Claude 3 Haiku

## Contributing

- **Add test sites**: Run benchmark on more URLs
- **Improve ground truth**: Manual verification of tasks
- **New models**: Add support in `models.py`

## Citation

```bibtex
@misc{sifr2025,
  title={SiFR: Structured Interface Format for AI Web Agents},
  author={SiFR Contributors},
  year={2025},
  url={https://github.com/Alechko375/sifr-benchmark}
}
```

## License

MIT
