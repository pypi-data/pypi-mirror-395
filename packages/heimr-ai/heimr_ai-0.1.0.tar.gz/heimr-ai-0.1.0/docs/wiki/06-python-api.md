# Python API

[← Back to Index](../WIKI.md)

Heimr can be used programmatically as a Python library, giving you full control over the analysis pipeline.

---

## Installation

```bash
pip install heimr-ai
```

---

## Basic Usage

```python
from heimr import Analyzer

# Configuration dictionary
config = {
    'prometheus': 'http://localhost:9090',
    'loki': 'http://localhost:3100',
    'explain': True 
}

# Initialize with a load test file
analyzer = Analyzer(
    file_path="results.jtl",
    config=config,
    llm_model="llama3.1:8b"
)

# Run analysis
# Returns an AnalysisResult object
result = analyzer.analyze()

# Access results
print(result.status)           # "PASSED" or "FAILED"
print(result.kpi)              # KPI metrics dict
print(result.llm_explanation)  # AI-generated explanation
```

---

## Configuration Options

### Analyzer Constructor

```python
analyzer = Analyzer(
    # Required
    file_path: str,              # Path to load test results
    
    # Optional Configuration Dictionary
    # Keys: 'prometheus', 'loki', 'tempo', 'explain'
    config: Dict[str, Any] = None,
    
    # LLM Overrides
    llm_url: str = "http://localhost:11434/v1",
    llm_model: str = "llama3.1:8b",
    no_llm: bool = False,        # Skip AI analysis
)
```

### Environment Variables

For cloud LLMs, set these before creating the Analyzer:

```python
import os

# OpenAI
os.environ["OPENAI_API_KEY"] = "sk-..."

# Anthropic
os.environ["ANTHROPIC_API_KEY"] = "sk-..."
```

---

## AnalysisResult Object

The `analyze()` method returns an `AnalysisResult` dataclass with the following attributes:

| Attribute | Type | Description |
|-----------|------|-------------|
| `status` | `str` | "PASSED" or "FAILED" |
| `failure_signals` | `list[str]` | Reasons for failure (e.g., "Error Rate: 5.0%") |
| `kpi` | `dict` | Performance KPIs (throughput, latency p50/p95/p99, errors) |
| `stats` | `dict` | Legacy stats dictionary used by some components |
| `anomalies` | `DataFrame` | Pandas DataFrame of detected anomalies |
| `anomaly_summary` | `dict` | Summary of anomalies (count, avg_latency) |
| `prom_metrics` | `dict` | Raw Prometheus metrics |
| `loki_logs` | `list` | Relevant log entries from Loki |
| `tempo_traces` | `list` | Slow traces from Tempo |
| `llm_explanation` | `str` | Full markdown explanation generated by LLM |
| `df` | `DataFrame` | The raw parsed load test data |

---

## Streaming LLM Output

You can stream the LLM generation in real-time by providing a callback:

```python
def print_chunk(chunk):
    print(chunk, end="", flush=True)

result = analyzer.analyze(stream_callback=print_chunk)
```

---

## Integration Example: CI/CD

```python
import sys
from heimr import Analyzer

analyzer = Analyzer(
    file_path="results.json",
    no_llm=True  # Fast mode for CI
)

result = analyzer.analyze()

# Check Status
if result.status == "FAILED":
    print("❌ Analysis Failed!")
    print("Reasons:", ", ".join(result.failure_signals))
    sys.exit(1)

# Custom Checks on KPI
p99 = result.kpi['latency']['p99']
if p99 > 500:
    print(f"❌ P99 latency {p99}ms exceeds 500ms!")
    sys.exit(1)

print("✅ All checks passed")
sys.exit(0)
```

---

## Error Handling

```python
from heimr import Analyzer

try:
    analyzer = Analyzer(file_path="results.jtl")
    result = analyzer.analyze()
except FileNotFoundError:
    print("File not found")
except ValueError as e:
    print(f"Invalid file format: {e}")
except Exception as e:
    print(f"Analysis failed: {e}")
```
