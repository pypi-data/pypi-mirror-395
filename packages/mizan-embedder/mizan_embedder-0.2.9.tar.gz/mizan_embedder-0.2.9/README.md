# mizan-embedder

**Mizan-optimized Embedding Models for AI, Search, and RAG.**  
`mizan-embedder` is the official embedding-model library in the **Mizan ecosystem**, designed to create *scale-aware*, *noise-resistant*, and *proportionally accurate* embeddings trained using the **Mizan Balance Function**.

> **Proposed & Developed By:**  
> **Ahsan Shaokat** â€” Computer Scientist & AI/ML Researcher  
> Inventor of the **Mizan Balance Function** (2025)

---

# ğŸŒŸ Overview

Modern embedding systems (MiniLM, MPNet, E5, etc.) use **cosine similarity**, which:

- âŒ Ignores magnitude  
- âŒ Fails with noisy or multi-scale embeddings  
- âŒ Produces unstable rankings in RAG  
- âŒ Forces L2-normalization (losing information)  

**Mizan-Embedder fixes this** by training models specifically for:

- âœ” **Mizan similarity** (scale-aware)  
- âœ” **Proportional contrastive learning**  
- âœ” **Chunk-length stable retrieval**  
- âœ” **Large document embeddings**  
- âœ” **Multimodal (text + images)**  

This library enables you to build **your own embedding models**, optimized for the **mizan_vector** search engine.

---

# ğŸ“¦ Features

### ğŸ§  **MizanEmbeddingModel-v1**
- Transformer backbone (DistilBERT, MiniLM, BERT, or any HF model)
- Projection head to target embedding dimension (e.g., 384)
- Supports `mean`, `cls`, and `max` pooling
- Optional L2 normalization (usually disabled for Mizan)

### ğŸ§° **Utilities Included**
- **Dataset utilities** for contrastive text pairs  
- **Collate function** for fast tokenization  
- **Inference wrapper** (`MizanTextEncoderWrapper`)  
- **Example training script** (`train_text_contrastive.py`)

### ğŸ”Œ **Integrates Seamlessly With:**
- `mizan_vector` (Memory store + Postgres pgvector)
- `mizan-rag` (retrieval pipelines)
- Any Python ML workflow

---

# ğŸ“ Project Structure

mizan-embedder/
â”‚
â”œâ”€â”€ mizan_embedder/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ model.py # MizanEmbeddingModel + inference wrapper
â”‚ â”œâ”€â”€ data.py # Dataset + collate functions
â”‚
â”œâ”€â”€ train_text_contrastive.py # Example training script
â”œâ”€â”€ pyproject.toml # PyPI-ready config
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE


---

# âš™ï¸ Installation

From local repo:

```bash
pip install -e .
ğŸ§± Architecture
ğŸ”¹ MizanEmbeddingModel
A transformer-based encoder with:

Backbone (HuggingFace model)

Projection layer â†’ [hidden_size] â†’ [embedding_dim]

Pooling (mean, cls, max)

Normalization (optional)

Diagram:

mathematica
Input Text â†’ Tokenizer â†’ Transformer Backbone â†’ Pooling â†’ Projection â†’ Embedding
ğŸ”¹ Why Projection?
To unify embedding dimensions across:

text models

code models

multimodal models

future Mizan models

ğŸš€ Usage
ğŸ”¹ Load the encoder

from mizan_embedder.model import MizanTextEncoderWrapper

encoder = MizanTextEncoderWrapper(
    backbone_name="distilbert-base-uncased",
    emb_dim=384,
    pooling="mean",
    normalize=False,  # Mizan works best without normalization
)

vector = encoder.encode_one("Mizan is a scale-aware similarity function.")
print(vector.shape)
ğŸ§ª Training Your First Mizan Encoder
Use the provided script:

python train_text_contrastive.py
This script:

Loads text pairs

Tokenizes them

Trains with MizanContrastiveLoss

Prints loss per epoch

Example Training Code (simplified)

from mizan_embedder.model import MizanEmbeddingModel
from mizan-vector.losses import MizanContrastiveLoss

model = MizanEmbeddingModel(
    backbone_name="distilbert-base-uncased",
    emb_dim=384,
    pooling="mean",
)

loss_fn = MizanContrastiveLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

for enc1, enc2, labels in loader:
    emb1 = model(**enc1)
    emb2 = model(**enc2)

    loss = loss_fn(emb1, emb2, labels)
    loss.backward()
    optimizer.step()
ğŸ” Contrastive Dataset Format
Your dataset should consist of (text1, text2, label) pairs:

Label = 1 â†’ similar

Label = 0 â†’ not similar

Example:

pairs = [
    ("what is mizan?", "mizan is a scale-aware similarity function", 1),
    ("who invented mizan?", "Ahsan Shaokat proposed the Mizan Balance Function", 1),
    ("cosine similarity", "apples are fruit", 0),
]
Dataset loader handles this automatically.

ğŸ¤– Inference: Encoding Many Sentences

texts = [
    "Mizan is scale-aware.",
    "Cosine ignores magnitude.",
    "Apples are fruit.",
]

embs = encoder.encode(texts)
print(embs.shape)  # e.g. torch.Size([3, 384])
ğŸ”— Integrating With mizan-vector
Example: full semantic search pipeline

from mizan-vector import MizanMemoryStore
from mizan_embedder.model import MizanTextEncoderWrapper

encoder = MizanTextEncoderWrapper()
store = MizanMemoryStore(dim=384)

docs = [
    "Mizan Balance Function is scale-aware.",
    "Cosine similarity uses only angle.",
    "Ahsan Shaokat invented Mizan.",
]

embs = encoder.encode(docs)

for doc, emb in zip(docs, embs):
    store.add_document(content=doc, embedding=emb.tolist())

query = "who created the mizan function?"
q_emb = encoder.encode_one(query).tolist()

results = store.search(q_emb, top_k=3, metric="mizan")

for r in results:
    print(r.score, r.content)
ğŸ”¥ Why Use Mizan-Based Embeddings?
Problem in Cosine Models	Mizan Solution
Loses magnitude info	Keeps scale meaningfully
Sensitive to noise/outliers	Proportional + stable
Long chunks score lower	Corrects length bias
Normalized embeddings only	No normalization needed
RAG retrieval unstable	Stable across chunk sizes
Cosine â‰  semantic meaning	Mizan captures proportional similarity

Mizan-optimized embeddings simply behave more naturally for real-world retrieval.

ğŸ—ºï¸ Roadmap
Next Versions:
âœ” MizanTextEncoder-base-384
STS/NLI-trained

Released in mizan-models

âœ” MizanCodeEncoder-base
CodeBERT-based

Code â†” docstring training

âœ” MizanMultimodalEncoder-v1
CLIP-based

Image â†” text contrastive training

âœ” mizan-rag
Full retrieval pipeline (chunking â†’ embedding â†’ storing â†’ LLM answering)

ğŸ“œ License
MIT License
Â© 2025 Ahsan Shaokat

ğŸ™Œ Acknowledgements
Special thanks to:

HuggingFace transformers

pgvector open-source community

PyTorch developers