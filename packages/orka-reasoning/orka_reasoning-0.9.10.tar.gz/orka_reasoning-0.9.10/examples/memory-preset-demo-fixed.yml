orchestrator:
  id: multi-perspective-analysis
  strategy: sequential
  agents:
    - memory-read_0
    - openai-answer_2
    - fork_3
    - join_9
    - openai-binary_10
    - router_11
    - final_summary
  queue: orka:generated
  memory_preset: semantic

agents:
  # Step 1: Check memory for past context
  - id: memory-read_0
    type: memory
    prompt: Retrieve any stored memories about how the subject '{{ get_input() }}' was classified or understood in the past. Return "NONE" if nothing matches.
    config:
      operation: read
    namespace: fact_validator

  # Step 2: Initial analysis with memory context
  - id: openai-answer_2
    type: local_llm
    prompt: Given previous context {{ get_agent_response('memory-read_0') }}, provide an initial detailed answer to {{ get_input() }}. Be comprehensive and informative.
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7

  # Step 3: Fork into parallel analysis paths
  - id: fork_3
    type: fork
    targets:
      - - openai-binary_4
        - openai-classification_5
        - openai-answer_6
      - - openai-answer_7
        - failover_11

  # Fork Branch 1: Validation + Classification + Alternative perspective
  - id: openai-binary_4
    type: local_llm
    prompt: Does the question {{ get_input() }} require factual validation? Answer with exactly 'true' or 'false' only.
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.1

  - id: openai-classification_5
    type: local_llm
    prompt: "Classify the domain of the question {{ get_input() }}. Choose exactly one from: science, history, technology, geography, culture, general. Answer with only the domain name."
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.2

  - id: openai-answer_6
    type: local_llm
    prompt: "Provide an alternative perspective or deeper insight into the question {{ get_input() }} considering domain: {{ get_agent_response('openai-classification_5') }}. Be creative and thoughtful."
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.8

  # Fork Branch 2: Summary + Web search
  - id: openai-answer_7
    type: local_llm
    prompt: Provide a concise summary for the question {{ get_input() }}. Be clear and to the point.
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.5

  - id: failover_11
    type: failover
    children:
      - id: duckduckgo_12
        type: duckduckgo
        prompt: "{{ get_input() }}"
      - id: duckduckgo_13
        type: duckduckgo
        prompt: "{{ get_input() }}"

  # Step 4: Join results from parallel branches
  - id: join_9
    type: join
    group: fork_3

  # Step 5: Check if information is coherent
  - id: openai-binary_10
    type: local_llm
    prompt: "Is the provided information coherent and complete based on outputs: {{ get_agent_response('join_9') }}? Answer with exactly 'true' or 'false' only."
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.1

  # Step 6: Route based on coherence check
  - id: router_11
    type: router
    params:
      decision_key: openai-binary_10
      routing_map:
        "true":
          - openai-answer_14
        "false":
          - openai-answer_15

  # Router path: Coherent information
  - id: openai-answer_14
    type: local_llm
    prompt: Given confirmed coherent inputs {{ get_agent_response('join_9') }}, provide a polished final response to {{ get_input() }}. Be professional and comprehensive.
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.6

  # Router path: Incomplete information
  - id: openai-answer_15
    type: local_llm
    prompt: Given identified gaps in coherence or completeness in {{ get_agent_response('join_9') }}, clarify or complete the information to fully answer {{ get_input() }}. Fill in the missing pieces.
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7

  # Step 7: Final summary with all context
  - id: final_summary
    type: local_llm
    prompt: |-
      Based on the question "{{ get_input() }}", provide a comprehensive response using the available information:

      {% set memory_info = get_agent_response('memory-read_0') %}
      {% if memory_info %}
      Retrieved information: {{ memory_info }}
      {% endif %}

      {% set analysis = get_agent_response('openai-answer_2') %}
      {% if analysis %}
      Analysis: {{ analysis }}
      {% endif %}

      {% set validation = get_agent_response('openai-binary_4') %}
      {% set classification = get_agent_response('openai-classification_5') %}
      {% set alternative = get_agent_response('openai-answer_6') %}
      {% set coherence = get_agent_response('openai-binary_10') %}
      {% set final_answer = safe_get_response('openai-answer_14', get_agent_response('openai-answer_15')) %}

      {% if final_answer %}
      Final answer: {{ final_answer }}
      {% endif %}

      Synthesize all available information to provide the most accurate and helpful response to the user's question.
    model: llama3.2:3b
    model_url: http://localhost:11434/api/generate
    provider: ollama
    temperature: 0.7

  # Step 8: Store final result in memory
  - id: memory-write_final
    type: memory
    prompt: "{{ safe_get_response('openai-answer_14', get_agent_response('openai-answer_15')) }}"
    config:
      operation: write
    namespace: fact_validator
    metadata:
      source: "{{ 'openai-answer_14' if get_agent_response('openai-answer_14') else 'openai-answer_15' }}"
      result: "{{ safe_get_response('openai-answer_14', get_agent_response('openai-answer_15')) }}"
      category: stored
    key_template: "{{ get_input() }}"
