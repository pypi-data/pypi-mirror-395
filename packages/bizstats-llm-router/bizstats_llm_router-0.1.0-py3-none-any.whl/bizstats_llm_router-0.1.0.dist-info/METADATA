Metadata-Version: 2.4
Name: bizstats-llm-router
Version: 0.1.0
Summary: Multi-provider LLM routing with intelligent failover, cost optimization, and CrewAI integration
Project-URL: Homepage, https://github.com/absolut-e/bizstats-llm-router
Project-URL: Documentation, https://github.com/absolut-e/bizstats-llm-router#readme
Project-URL: Repository, https://github.com/absolut-e/bizstats-llm-router
Project-URL: Issues, https://github.com/absolut-e/bizstats-llm-router/issues
Author-email: "Absolut-e Data Com Inc." <account@absolut-e.com>
License: Proprietary - Copyright (c) 2025 Absolut-e Data Com Inc. and BizStats.AI. All rights reserved.
Keywords: ai,anthropic,azure,bedrock,crewai,failover,google,llm,machine-learning,multi-provider,ollama,openai,routing
Classifier: Development Status :: 4 - Beta
Classifier: Framework :: AsyncIO
Classifier: Intended Audience :: Developers
Classifier: License :: Other/Proprietary License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Typing :: Typed
Requires-Python: >=3.11
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: httpx>=0.26.0
Requires-Dist: pydantic-settings>=2.0.0
Requires-Dist: pydantic>=2.0.0
Requires-Dist: tenacity>=8.2.0
Provides-Extra: all
Requires-Dist: anthropic>=0.18.0; extra == 'all'
Requires-Dist: crewai>=0.186.0; extra == 'all'
Requires-Dist: google-generativeai>=0.4.0; extra == 'all'
Requires-Dist: litellm>=1.0.0; extra == 'all'
Requires-Dist: ollama>=0.1.0; extra == 'all'
Requires-Dist: openai>=1.0.0; extra == 'all'
Requires-Dist: redis>=5.0.0; extra == 'all'
Provides-Extra: anthropic
Requires-Dist: anthropic>=0.18.0; extra == 'anthropic'
Provides-Extra: crewai
Requires-Dist: crewai>=0.186.0; extra == 'crewai'
Requires-Dist: litellm>=1.0.0; extra == 'crewai'
Provides-Extra: dev
Requires-Dist: black>=24.0.0; extra == 'dev'
Requires-Dist: mypy>=1.8.0; extra == 'dev'
Requires-Dist: pytest-asyncio>=0.23.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.1.0; extra == 'dev'
Requires-Dist: pytest-mock>=3.12.0; extra == 'dev'
Requires-Dist: pytest>=8.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: google
Requires-Dist: google-generativeai>=0.4.0; extra == 'google'
Provides-Extra: ollama
Requires-Dist: ollama>=0.1.0; extra == 'ollama'
Provides-Extra: openai
Requires-Dist: openai>=1.0.0; extra == 'openai'
Provides-Extra: redis
Requires-Dist: redis>=5.0.0; extra == 'redis'
Provides-Extra: test
Requires-Dist: aioresponses>=0.7.0; extra == 'test'
Requires-Dist: pytest-asyncio>=0.23.0; extra == 'test'
Requires-Dist: pytest-cov>=4.1.0; extra == 'test'
Requires-Dist: pytest-mock>=3.12.0; extra == 'test'
Requires-Dist: pytest>=8.0.0; extra == 'test'
Requires-Dist: respx>=0.20.0; extra == 'test'
Description-Content-Type: text/markdown

# @bizstats/llm-router

**Multi-provider LLM routing with intelligent failover, cost optimization, and CrewAI integration.**

Copyright (c) 2025 Absolut-e Data Com Inc. and BizStats.AI. All rights reserved.

## Features

- **Multi-Provider Support**: OpenAI, Anthropic, Google Gemini, Ollama, Azure OpenAI, AWS Bedrock
- **Intelligent Routing**: Request routing based on capabilities, cost, privacy, and custom rules
- **Automatic Failover**: Seamless fallback to backup providers when primary fails
- **Response Caching**: In-memory and Redis caching for cost optimization
- **Rate Limiting**: Token bucket rate limiting per provider
- **Cost Tracking**: Budget management with alerts
- **Streaming Support**: Full async streaming for all providers
- **Type Safety**: Full type hints and Pydantic models

## Installation

```bash
# Basic installation
pip install bizstats-llm-router

# With specific providers
pip install bizstats-llm-router[openai]
pip install bizstats-llm-router[anthropic]
pip install bizstats-llm-router[google]
pip install bizstats-llm-router[ollama]

# With all providers
pip install bizstats-llm-router[all]

# With caching support
pip install bizstats-llm-router[redis]
```

## Quick Start

```python
import asyncio
from bizstats_llm_router import (
    LLMRouter,
    LLMMessage,
    LLMRouterSettings,
)
from bizstats_llm_router.adapters.openai import create_openai_adapter
from bizstats_llm_router.adapters.ollama import create_ollama_adapter

async def main():
    # Create router
    router = LLMRouter()

    # Register providers
    openai_adapter = create_openai_adapter("gpt-4", api_key="your-key")
    router.register_adapter("gpt4", openai_adapter)

    ollama_adapter = create_ollama_adapter("gemma2:9b")
    router.register_adapter("gemma", ollama_adapter)

    # Initialize
    await router.initialize()

    # Send request
    response = await router.chat([
        LLMMessage(role="system", content="You are a helpful assistant."),
        LLMMessage(role="user", content="Hello!"),
    ])

    print(f"Response: {response.content}")
    print(f"Tokens: {response.tokens_used}")
    print(f"Cost: ${response.cost:.6f}")

    # Cleanup
    await router.cleanup()

asyncio.run(main())
```

## Streaming

```python
async def stream_example():
    router = LLMRouter()
    # ... setup adapters ...

    async for chunk in router.stream_chat([
        LLMMessage(role="user", content="Tell me a story")
    ]):
        print(chunk.content, end="", flush=True)
        if chunk.is_final:
            print(f"\n\nFinished: {chunk.finish_reason}")
```

## Intelligent Routing

```python
from bizstats_llm_router import (
    LLMRouterSettings,
    RoutingConfig,
    SelectionStrategy,
)

# Configure routing
settings = LLMRouterSettings(
    routing=RoutingConfig(
        selection_strategy=SelectionStrategy.CHEAPEST,
        enable_failover=True,
        max_failover_attempts=3,
        routing_rules={
            "code": ["gpt4", "claude"],
            "general": ["gemma", "gpt35"],
        },
    ),
)

router = LLMRouter(settings=settings)

# Route by request type
response = await router.chat(
    messages,
    request_type="code",  # Will prefer gpt4 or claude
)

# Route by requirements
response = await router.chat(
    messages,
    requirements={
        "privacy_level": "high",  # Prefer local models
        "capabilities": ["vision"],  # Must support vision
        "max_cost_per_1k": 0.01,  # Cost constraint
    },
)
```

## Caching

```python
from bizstats_llm_router import CacheConfig

settings = LLMRouterSettings(
    cache=CacheConfig(
        enabled=True,
        backend="memory",  # or "redis"
        ttl_seconds=3600,
        max_size=1000,
    ),
)

router = LLMRouter(settings=settings)
```

## Rate Limiting

```python
from bizstats_llm_router import RateLimitConfig

settings = LLMRouterSettings(
    rate_limit=RateLimitConfig(
        enabled=True,
        requests_per_minute=60,
        tokens_per_minute=100000,
        burst_size=10,
        per_provider_limits={
            "openai": {"requests_per_minute": 30},
        },
    ),
)
```

## Cost Tracking

```python
from bizstats_llm_router import CostTrackingConfig

settings = LLMRouterSettings(
    cost_tracking=CostTrackingConfig(
        enabled=True,
        budget_limit_daily=10.0,
        budget_limit_monthly=200.0,
        alert_threshold_percent=80.0,
        track_per_user=True,
    ),
)

router = LLMRouter(settings=settings)

# Get cost summary
metrics = router.get_metrics()
print(f"Total cost: ${metrics['costs']['total_cost']:.4f}")
```

## Supported Providers

| Provider | Models | Features |
|----------|--------|----------|
| **OpenAI** | GPT-4, GPT-4o, GPT-3.5 | Chat, Streaming, Tools, Vision |
| **Anthropic** | Claude 3 Opus/Sonnet/Haiku | Chat, Streaming, Tools, Vision |
| **Google** | Gemini 1.5/2.0 | Chat, Streaming, Vision |
| **Ollama** | Llama, Gemma, etc. | Chat, Streaming, Local |
| **Azure OpenAI** | GPT-4, GPT-3.5 | Chat, Streaming, Enterprise |
| **AWS Bedrock** | Claude, Llama | Chat, Streaming, Enterprise |

## API Reference

### LLMRouter

Main router class for LLM requests.

```python
router = LLMRouter(settings=None, registry=None)
await router.initialize()
await router.chat(messages, **options)
async for chunk in router.stream_chat(messages, **options):
    ...
await router.cleanup()
```

### LLMMessage

Standardized message format.

```python
message = LLMMessage(
    role="user",  # "system", "user", "assistant"
    content="Hello!",
    metadata={},  # Optional metadata
)
```

### LLMResponse

Standardized response format.

```python
response.content      # Response text
response.model        # Model name
response.provider     # Provider name
response.tokens_used  # Total tokens
response.cost         # Cost in USD
response.response_time  # Time in seconds
```

## Development

```bash
# Install with dev dependencies
cd packages/llm-router
uv sync --extra dev --extra test

# Run tests
uv run pytest tests/ -v --cov=src/bizstats_llm_router

# Format code
uv run black src tests
uv run ruff check src tests
```

## License

Proprietary - Copyright (c) 2025 Absolut-e Data Com Inc. and BizStats.AI

All rights reserved. This software is proprietary and confidential.
Unauthorized copying, distribution, or use is strictly prohibited.

## Contact

- **Website**: https://bizstats.ai
- **Email**: account@absolut-e.com
