<p align="center">
  <img src="https://raw.githubusercontent.com/Lexsi-Labs/DLBacktrace/dlb_v2/assets/images/dlb_logo.png"
       alt="DLB Logo" width="300px"/>
</p>


<h2 align="center">DLBacktrace: Model-Agnostic Explainability for Any Deep Learning Model</h2>

<p align="center">
  <a href="https://python.org"><img src="https://img.shields.io/badge/Python-3.8%2B-blue.svg"></a>
  <a href="https://pytorch.org"><img src="https://img.shields.io/badge/PyTorch-2.6%2B-red.svg"></a>
  <a href="https://dlbacktrace.lexsi.ai/"><img src="https://img.shields.io/badge/docs-available-green.svg"></a>
  <a href="LICENSE.md"><img src="https://img.shields.io/badge/license-Lexsi%20Labs%20Source%20Available-orange"></a>
</p>

<p align="center">
  <i>Making AI Transparent and Explainable ‚Äî Developed by <a href="https://lexsi.ai/">Lexsi Labs</a></i> üöÄ
</p>

---

## Overview

DLBacktrace is a model-agnostic explainability framework developed by *Lexsi Labs*. It provides comprehensive layerwise importance values (relevance scores) and model tracing capabilities across a wide range of model architectures ‚Äî including Transformers, Large Language Models (LLMs), Mixture-of-Experts (MoEs), and more ‚Äî as well as diverse task types such as Tabular, Vision, and Text. The framework is designed for robust and efficient execution on both CPU and GPU environments.

## Key Features

### Core Capabilities
- **üîç Deep Model Interpretability:** Gain comprehensive insights into your AI models using advanced relevance propagation algorithms
- **üéØ Multi-Task Support:** Binary/Multi-class classification, object detection, segmentation, and text generation
- **üèóÔ∏è Architecture Agnostic:** Support for Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformer, and custom architectures including Mixture-of-Experts (MoEs)
- **‚ö° High Performance:** Optimized execution engine with CUDA acceleration and deterministic tracing
- **üîß Robust Operations:** Full support for negative indexing and complex tensor operations
- **üìä Comprehensive Tracing:** Layer-wise activation and relevance analysis with detailed execution tracking
- **üõ°Ô∏è Production Ready:** Deterministic execution environment with comprehensive error handling

### Advanced Features
- **üöÄ High-Level Pipeline Interface:** Simplified API for text/image classification and generation with automatic model loading and configuration
- **üé≤ DLB Auto Sampler:** Advanced text generation with multiple decoding strategies ‚Äî including greedy decoding, beam search (deterministic), and stochastic sampling methods such as temperature, top-k, and top-p ‚Äî along with token-level relevance tracking
- **üß† Mixture-of-Experts (MoEs) Support:** Built-in support for MoEs architectures (JetMoE, OLMoE, Qwen3-MoE, GPT-Oss) with expert-level relevance analysis
- **üå°Ô∏è Temperature Scaling:** Control generation diversity and model confidence with flexible temperature parameters
- **üîÑ Enhanced Execution Engine:** Critical fixes for RoBERTa, LLaMA, and other transformer models 

---

## ‚ö° Performance Improvements in DLB v2

DLB v2 introduces **major architectural upgrades** to the explainability engine ‚Äî resulting in *orders of magnitude faster performance* compared to v1.

> üìò **Note:** All benchmarks below were conducted on the **LLaMA-3.2‚Äì3B model** using the MMLU dataset on an NVIDIA RTX 4090.

---

### üìä Benchmark Summary

| Metric | DLB v1 | DLB v2 | Improvement |
|:-------|:-------|:-------|:-------------|
| ‚è±Ô∏è **Explainability Time** | Up to 29,000 s | üïí 20 s | üî• **1000√ó faster** |
| üöÄ **Throughput** | ~0.03 tokens/s | üß© 75 tokens/s | ‚ö° **2000√ó higher** |
| üìà **Scalability** | Degrades sharply | Scales linearly | ‚úÖ **Stable & predictable** |

---

### üìà Performance Graphs

| Metric | Comparison Plot |
|:--------|:----------------|
| üïí **Total Time vs Sequence Length** | <img src="https://raw.githubusercontent.com/Lexsi-Labs/DLBacktrace/dlb_v2/assets/images/total_dlb_time_comparison.png" width="500"/> |
| üöÄ **Token Throughput** | <img src="https://raw.githubusercontent.com/Lexsi-Labs/DLBacktrace/dlb_v2/assets/images/throughput_comparison.png" width="500"/> |
| ‚öôÔ∏è **Speedup (v2 / v1)** | <img src="https://raw.githubusercontent.com/Lexsi-Labs/DLBacktrace/dlb_v2/assets/images/speedup_comparison.png" width="500"/> |

---

> üí° **DLB v2** achieves up to **1000√ó speedup** using fused GPU kernels.

## Installation

### From Source (Recommended)

```bash
pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126
pip install dl-backtrace
```

### Requirements

- Python 3.8+
- PyTorch 2.6+ (with CUDA 12.6 support recommended)
- Additional dependencies: transformers, matplotlib, seaborn, graphviz, joblib, zstandard

See `requirements.txt` for the complete list of dependencies.

### Hugging Face Setup

For accessing models from Hugging Face Hub (required for BERT, RoBERTa, LLaMA, etc.):

```bash
# Install Hugging Face CLI
pip install huggingface_hub

# Login to Hugging Face (required for gated models)
huggingface-cli login
```

You'll need a Hugging Face account and access token. Get your token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).

## Quick Start

### PyTorch Models (Recommended)

```python
import torch
import torch.nn as nn
from dl_backtrace.pytorch_backtrace import DLBacktrace

# Define your model
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.linear(x)

# Initialize model and DLBacktrace
model = MyModel()
x = torch.randn(1, 10)  # Example input

# Create DLBacktrace instance
dlb = DLBacktrace(
    model=model,
    input_for_graph=(x,),
    device='cuda',    # 'cpu',
    verbose=False
)

# Get layer-wise outputs
node_io = dlb.predict(x)

# Calculate relevance propagation
relevance = dlb.evaluation(
    mode="default",
    multiplier=100.0,
    task="binary-classification"
)
```

## Advanced Features

### üöÄ High-Level Pipeline Interface
Simplified API for common ML tasks with automatic model loading and configuration:

```python
from dl_backtrace.pytorch_backtrace import DLBacktrace
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained("textattack/bert-base-uncased-SST-2")
tokenizer = AutoTokenizer.from_pretrained("textattack/bert-base-uncased-SST-2")

# Prepare input
text = "This movie is fantastic!"
tokens = tokenizer(text, return_tensors="pt")
input_ids = tokens["input_ids"]
attention_mask = tokens["attention_mask"]

# Initialize DLBacktrace
dlb = DLBacktrace(
    model,
    (input_ids, attention_mask),
    device='cuda',  # or 'cpu'
    verbose=False
)

# Run text classification with run_task() - ONE CALL!
results = dlb.run_task(
    task="text-classification",  # or "auto" for automatic detection
    inputs={'input_ids': input_ids, 'attention_mask': attention_mask},
    debug=False
)

# Access predictions and relevance
predictions = results['predictions']
relevance = results['relevance']
print(f"Predicted class: {predictions.argmax(axis=-1)}")
print(f"Token relevance shape: {relevance['input_ids'].shape}")
```

### üé≤ DLB Auto Sampler
Advanced text generation with multiple sampling strategies and token-level relevance tracking:

```python
from dl_backtrace.pytorch_backtrace.dlbacktrace.core.dlb_auto_sampler import DLBAutoSampler

sampler = DLBAutoSampler(model, tokenizer)

# Greedy sampling
output = sampler.generate("Prompt", strategy="greedy", max_length=50)

# Top-k and Top-p sampling
output = sampler.generate("Prompt", strategy="top_k", top_k=50, temperature=0.8)
output = sampler.generate("Prompt", strategy="top_p", top_p=0.9, temperature=0.8)

# Beam search
output = sampler.generate("Prompt", strategy="beam_search", num_beams=5)

# Access token-level relevance
print(output['relevance_scores'])
```

### üß† Mixture-of-Experts (MoEs) Support
Built-in support for MoE architectures with expert-level relevance analysis:

```python
from dl_backtrace.moe_pytorch_backtrace import Backtrace
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load MoE model (GPT-OSS, JetMoE, OLMoE, Qwen3-MoE)
model = AutoModelForCausalLM.from_pretrained("openai/gpt-oss-20b")
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b")

# Initialize MoE Backtrace
bt = Backtrace(
    model=model,
    model_type='gpt_oss',  # or 'jetmoe', 'olmoe', 'qwen'
    device='cpu'  # or 'cuda'
)

# Prepare input
prompt = "What is the capital of France?"
tokens = tokenizer(prompt, return_tensors="pt")
input_ids = tokens["input_ids"]
attention_mask = tokens["attention_mask"]

# Run generation with run_task() - ONE CALL!
results = bt.run_task(
    task="generation",
    inputs={'input_ids': input_ids, 'attention_mask': attention_mask},
    tokenizer=tokenizer,
    max_new_tokens=10,
    return_relevance=True,
    return_scores=True,
    debug=False
)

# Access generated text and relevance
generated_text = tokenizer.decode(results['generated_ids'][0], skip_special_tokens=True)
print(f"Generated: {generated_text}")

# Get expert-level relevance
expert_relevance = bt.all_layer_expert_relevance
print(f"Expert routing across {len(expert_relevance)} layers")
```

### üå°Ô∏è Temperature Scaling
Control generation diversity and model confidence:

```python
from dl_backtrace.pytorch_backtrace.dlbacktrace import DLBacktrace

dlb = DLBacktrace(model, input_for_graph=(input_tensor,))

# Apply temperature scaling for generation
output = dlb.generate_with_temperature(
    input_ids,
    temperature=0.7,  # Lower = more focused, Higher = more diverse
    max_length=100
)
```

### ‚ö° Execution Engines
DLBacktrace provides optimized execution engines:

#### ExecutionEngineNoCache 
- **Memory-efficient**: Runs entirely in RAM for faster execution
- **Enhanced Operations**: Supports 100+ PyTorch operations with robust error handling
- **Recent Improvements**: Critical fixes for transformer models (RoBERTa, LLaMA, BERT)

### üõ°Ô∏è Deterministic Execution Environment
DLBacktrace automatically sets up a deterministic environment for consistent results:
- ‚úÖ CUDA memory management and synchronization
- ‚úÖ Deterministic algorithms and cuDNN settings
- ‚úÖ Random seed control and environment variables
- ‚úÖ Warning suppression for cleaner output

### üîß Robust Tensor Operations
Full support for PyTorch's negative indexing and complex operations:
- ‚úÖ `transpose(-1, -2)`, `permute([-1, -2, 0])`
- ‚úÖ `unsqueeze(-1)`, `squeeze(-1)`
- ‚úÖ `slice(dim=-1, ...)`, `cat(tensors, dim=-1)`
- ‚úÖ `index_select(dim=-1, ...)`

### Evaluation Parameters

| Parameter    | Description | Values |
|--------------|-------------|--------|
| `mode`       | Evaluation algorithm mode | `default`, `contrastive` |
| `multiplier` | Starting relevance at output layer | Float (default: 100.0) |
| `scaler`     | Relevance scaling factor | Float (default: 1.0) |
| `thresholding` | Pixel selection threshold for segmentation | Float (default: 0.5) |
| `task`       | Model task type | `binary-classification`, `multi-class classification` |

## Example Notebooks

| Name        | Task        | Link                          |
|-------------|-------------|-------------------------------|
| Custom Tabular Model | Binary Classification | [Colab Link](https://colab.research.google.com/drive/1TqgeeBqQ1G9UGWfHV0MUloCccalpsRCh?usp=sharing)|
| VGG Model | Multi-Class Classification | [Colab Link](https://colab.research.google.com/drive/1iJJZ0ApWHltTjnbGRKhJDrHlKTlm1koD?usp=sharing) |
| ResNet Model | Multi-Class Classification | [Colab Link](https://colab.research.google.com/drive/1mpo--AD8vNqm6Y05rb46Yzjx6VhLwXZh?usp=sharing) |
| ViT Model | Multi-Class Classification | [Colab Link](https://colab.research.google.com/drive/1BhzIw7Pf9-g1tqndaijwZ5FLaDUpjBaR?usp=sharing) |
| DenseNet Model | Multi-Class Classification | [Colab Link](https://colab.research.google.com/drive/1CE2XBBGd5VSQuipJTyyRcb7mu5RSG6K5?usp=sharing) |
| EfficientNet Model | Multi-Class Classification | [Colab Link](https://colab.research.google.com/drive/1O-MyvIKWoADG2RrF43p2k8mUYpF9N_8m?usp=sharing) |
| MobileNet Model | Multi-Class Classification | [Colab Link](https://colab.research.google.com/drive/1BzsID9U3HndLrh67nPWRw_bm7UWLwLOH?usp=sharing) |
| BERT-Base Model | Sentiment Classification | [Colab Link](https://colab.research.google.com/drive/1ANZPjaAxl2oF2WHj23f87AR9-ZDIMBm9?usp=sharing) |
| ALBERT Model | Sentiment Classification | [Colab Link](https://colab.research.google.com/drive/1RuAW0FgtWqKkdVbc97VXf9z4oDXmA1ms?usp=sharing) |
| RoBERTa Model | Sentiment Classification | [Colab Link](https://colab.research.google.com/drive/1Nw6lTSQKJvGU9JBZeXnA7EXboU7mE282?usp=sharing) |
| DistilBERT Model | Sentiment Classification | [Colab Link](https://colab.research.google.com/drive/13_hqUC2vaJWfF-UheggHJU5RmWS5A2u3?usp=sharing) |
| Electra Model | Sentiment Classification | [Colab Link](https://colab.research.google.com/drive/1sht3uLej8g-4hMtaHm7VwwUuGAmAqpH_?usp=sharing) |
| XLNeT Model | Sentiment Classification | [Colab Link](https://colab.research.google.com/drive/1ZmVusCPgeXLGnbt7NzM-3SJuiGRgTzBa?usp=sharing) |
| LLaMA-3.2-1B Model | Text Generation | [Colab Link](https://colab.research.google.com/drive/1i_CKoCfKdY4fcWyFdzuc_0e868jux12h?usp=sharing) |
| LLaMA-3.2-3B Model | Text Generation | [Colab Link](https://colab.research.google.com/drive/1ki8kcc4ez8-kdvdlhtoq7Sed9v5hiaNs?usp=sharing) |

For more detailed examples and use cases, check out our documentation.

## Supported Layers

### PyTorch

**Core Operations:**
- [x] **Linear (Fully Connected) Layer**
- [x] **Convolutional Layer** (Conv2D)
- [x] **Reshape & Flatten Layers**
- [x] **Pooling Layers** (AdaptiveAvgPool2d, MaxPool2d, AvgPool2d, AdaptiveMaxPool2d)
- [x] **1D Pooling Layers** (AvgPool1d, MaxPool1d, AdaptiveAvgPool1d, AdaptiveMaxPool1d)
- [x] **Concatenate & Add Layers**
- [x] **LSTM Layer**
- [x] **Dropout Layer**
- [x] **Embedding Layer**

**Advanced Operations:**
- [x] **Tensor Manipulation** (transpose, permute, unsqueeze, squeeze, slice, cat, index_select)
- [x] **Negative Indexing Support** (all operations support PyTorch's negative indexing)
- [x] **Layer Normalization**
- [x] **Batch Normalization**
- [x] **View & Reshape Operations**

## Testing & Validation

### Supported Models
DLBacktrace has been extensively tested with:
- **Vision Models**: ResNet, VGG, DenseNet, EfficientNet, MobileNet, ViT
- **NLP Models**: BERT, ALBERT, RoBERTa, DistilBERT, ELECTRA, XLNet, LLaMA-3.2, Qwen
- **MoE Models**: JetMoE, OLMoE (Open Language Model with Experts), Qwen3-MoE, GPT-OSS
- **Tasks**: Classification, Object Detection, Segmentation, Text Generation, Expert-Level Analysis

## Getting Started

If you're new to DLBacktrace:

1. **üìñ Read the Documentation**: [https://dlbacktrace.lexsi.ai/](https://dlbacktrace.lexsi.ai/)
2. **üöÄ Try the Quick Start**: See examples above for PyTorch models
3. **üíª Explore Notebooks**: Check out our comprehensive example notebooks for various use cases
4. **üß™ Run Tests**: Validate your installation with the benchmark scripts

For advanced features like the Pipeline Interface, Auto Sampler, MoE models, and Temperature Scaling, refer to the full documentation.

## Contributing

We welcome contributions from the community! Please follow our contribution guidelines and submit pull requests for any improvements.

## License

This project is licensed under a custom License - see the [LICENSE](LICENSE.md) file for details.

## Recent Updates & New Features

### Latest Release (2025)

**New Features:**
- üöÄ **High-Level Pipeline Interface**: Simplified API for text/image classification and generation
- üé≤ **DLB Auto Sampler**: Advanced text generation with multiple sampling strategies
- üß† **MoE Model Support**: Built-in support for Mixture of Experts architectures (JetMoE, OLMoE, Qwen3-MoE, GPT-OSS)
- üå°Ô∏è **Temperature Scaling**: Flexible control over generation diversity and model confidence

**Critical Fixes & Improvements:**
- üîß Enhanced execution engine with robust handling of complex tensor operations
- ‚ö° Deterministic environment setup for consistent, reproducible results
- üõ°Ô∏è Comprehensive error handling for production use
- üö® Critical fixes for transformer models (RoBERTa, LLaMA, BERT)
- üß† Smart attention detection for bidirectional vs causal attention
- üíæ Memory optimization and improved OOM error handling

## Contact

For any inquiries, support, or collaboration opportunities:

- **Email**: [support@lexsi.ai](mailto:support@lexsi.ai)
- **Website**: [https://lexsi.ai/](https://lexsi.ai/)
- **GitHub Issues**: [https://github.com/Lexsi-Labs/DLBacktrace/issues](https://github.com/Lexsi-Labs/DLBacktrace/issues)
- **Documentation**: [https://dlbacktrace.lexsi.ai/](https://dlbacktrace.lexsi.ai/)

## Citations
```
@misc{sankarapu2024dlbacktracemodelagnosticexplainability,
      title={DLBacktrace: A Model Agnostic Explainability for any Deep Learning Models}, 
      author={Vinay Kumar Sankarapu and Chintan Chitroda and Yashwardhan Rathore and Neeraj Kumar Singh and Pratinav Seth},
      year={2024},
      eprint={2411.12643},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.12643}, 
}
```
---

<p align="center"> <b>DLBacktrace</b> ‚Äî Bridging Performance and Explainability üîç<br> <a href="https://lexsi.ai/">Lexsi Labs</a> | <a href="mailto:support@lexsi.ai">support@lexsi.ai</a> </p> 
