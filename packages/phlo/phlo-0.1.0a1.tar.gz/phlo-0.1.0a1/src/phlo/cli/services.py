"""
Phlo Services Management

Manages Docker infrastructure for Phlo projects.
Creates .phlo/ directory in user projects with docker-compose configuration.
"""

import subprocess
import sys
from pathlib import Path
from typing import Optional

import click
import yaml

PHLO_CONFIG_FILE = "phlo.yaml"

PHLO_CONFIG_TEMPLATE = """# Phlo Project Configuration
name: {name}
description: "{description}"

# Smart defaults are used for infrastructure (9 services: dagster, postgres, minio, etc.)
# Override only what you need. For example:
#
# infrastructure:
#   services:
#     postgres:
#       host: custom-host  # Override postgres host
#   container_naming_pattern: "{{project}}_{{service}}"  # Custom naming
"""


def check_docker_running() -> bool:
    """Check if Docker daemon is running."""
    try:
        result = subprocess.run(
            ["docker", "info"],
            capture_output=True,
            timeout=10,
        )
        return result.returncode == 0
    except (FileNotFoundError, subprocess.TimeoutExpired):
        return False


def require_docker():
    """Exit with helpful message if Docker is not running."""
    if not check_docker_running():
        click.echo("Error: Docker is not running.", err=True)
        click.echo("", err=True)
        click.echo("Please start Docker Desktop and try again.", err=True)
        click.echo("Download: https://docs.docker.com/get-docker/", err=True)
        sys.exit(1)


def get_project_config() -> dict:
    """Load project configuration from phlo.yaml.

    Returns default config if file doesn't exist.
    """
    config_path = Path.cwd() / PHLO_CONFIG_FILE
    if config_path.exists():
        with open(config_path) as f:
            return yaml.safe_load(f) or {}

    # Default: derive name from current directory
    return {
        "name": Path.cwd().name.lower().replace(" ", "-").replace("_", "-"),
        "description": "Phlo data lakehouse",
    }


def get_project_name() -> str:
    """Get the project name for Docker Compose."""
    config = get_project_config()
    return config.get("name", Path.cwd().name.lower().replace(" ", "-").replace("_", "-"))


def find_dagster_container(project_name: str) -> str:
    """Find the running Dagster webserver container for the project.

    Checks phlo.yaml infrastructure config first, then falls back to dynamic discovery.
    """
    from phlo.infrastructure import get_container_name

    configured_name = get_container_name("dagster_webserver", project_name)

    if configured_name:
        result = subprocess.run(
            ["docker", "ps", "--filter", f"name={configured_name}", "--format", "{{.Names}}"],
            capture_output=True,
            text=True,
        )
        if result.stdout.strip():
            return configured_name

    default_name = f"{project_name}-dagster-webserver-1"
    result = subprocess.run(
        ["docker", "ps", "--filter", f"name={default_name}", "--format", "{{.Names}}"],
        capture_output=True,
        text=True,
    )

    if result.stdout.strip():
        return default_name

    result = subprocess.run(
        [
            "docker",
            "ps",
            "--filter",
            f"name={project_name}.*dagster.*webserver",
            "--format",
            "{{.Names}}",
        ],
        capture_output=True,
        text=True,
    )

    containers = result.stdout.strip().split("\n")
    if containers and containers[0]:
        return containers[0]

    raise RuntimeError(
        f"Could not find running Dagster webserver container for project '{project_name}'. "
        f"Expected container name: {configured_name or default_name}"
    )


def _init_nessie_branches(project_name: str) -> None:
    """Initialize Nessie branches (main, dev) if they don't exist.

    Creates the branch structure needed for Write-Audit-Publish pattern:
    - main: production data (validated, published to BI)
    - dev: development/feature work (isolated transforms)
    """
    import json
    import time

    container_name = f"{project_name}-nessie-1"
    nessie_url = "http://localhost:19120"

    # Wait for Nessie to be ready
    for _ in range(30):
        try:
            result = subprocess.run(
                [
                    "docker",
                    "exec",
                    container_name,
                    "curl",
                    "-s",
                    f"{nessie_url}/api/v1/trees",
                ],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode == 0 and "references" in result.stdout:
                break
        except Exception:
            pass
        time.sleep(1)
    else:
        click.echo("Warning: Nessie not ready, skipping branch initialization", err=True)
        return

    # Get existing branches
    try:
        result = subprocess.run(
            [
                "docker",
                "exec",
                container_name,
                "curl",
                "-s",
                f"{nessie_url}/api/v1/trees",
            ],
            capture_output=True,
            text=True,
            timeout=10,
        )
        data = json.loads(result.stdout)
        existing = {r["name"] for r in data.get("references", [])}
    except Exception as e:
        click.echo(f"Warning: Could not check Nessie branches: {e}", err=True)
        return

    # Create dev branch from main if it doesn't exist
    if "dev" not in existing and "main" in existing:
        click.echo("Creating Nessie 'dev' branch from 'main'...")
        try:
            # Get main branch hash
            result = subprocess.run(
                [
                    "docker",
                    "exec",
                    container_name,
                    "curl",
                    "-s",
                    f"{nessie_url}/api/v1/trees/tree/main",
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )
            main_data = json.loads(result.stdout)
            main_hash = main_data.get("hash", "")

            if main_hash:
                # Create dev branch
                result = subprocess.run(
                    [
                        "docker",
                        "exec",
                        container_name,
                        "curl",
                        "-s",
                        "-X",
                        "POST",
                        f"{nessie_url}/api/v1/trees/tree",
                        "-H",
                        "Content-Type: application/json",
                        "-d",
                        json.dumps({"type": "BRANCH", "name": "dev", "hash": main_hash}),
                    ],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
                if "dev" in result.stdout:
                    click.echo("Created Nessie 'dev' branch.")
                else:
                    click.echo(
                        f"Warning: Could not create dev branch: {result.stdout}",
                        err=True,
                    )
        except Exception as e:
            click.echo(f"Warning: Could not create dev branch: {e}", err=True)
    elif "dev" in existing:
        click.echo("Nessie branches ready (main, dev).")


def _run_dbt_compile(project_name: str) -> None:
    """Run dbt deps + compile to generate manifest.json for Dagster.

    This runs after services start to ensure Dagster can discover dbt models.
    """
    import time

    # Check if dbt project exists
    dbt_project = Path.cwd() / "transforms" / "dbt"
    if not (dbt_project / "dbt_project.yml").exists():
        return  # No dbt project, skip

    click.echo("")
    click.echo("Compiling dbt models...")

    # Wait for services to be ready
    time.sleep(5)

    container_name = find_dagster_container(project_name)

    try:
        # Run dbt deps
        result = subprocess.run(
            [
                "docker",
                "exec",
                container_name,
                "bash",
                "-c",
                "cd /app/transforms/dbt && dbt deps --profiles-dir profiles",
            ],
            capture_output=True,
            text=True,
            timeout=60,
        )
        if result.returncode != 0:
            click.echo(f"Warning: dbt deps failed: {result.stderr}", err=True)

        # Run dbt compile
        result = subprocess.run(
            [
                "docker",
                "exec",
                container_name,
                "bash",
                "-c",
                "cd /app/transforms/dbt && dbt compile --profiles-dir profiles --target dev",
            ],
            capture_output=True,
            text=True,
            timeout=120,
        )
        if result.returncode == 0:
            click.echo("dbt models compiled successfully.")
            click.echo("Restarting Dagster to pick up dbt manifest...")
            subprocess.run(
                [
                    "docker",
                    "restart",
                    container_name,
                    f"{project_name}-dagster-daemon-1",
                ],
                capture_output=True,
                timeout=30,
            )
        else:
            click.echo(f"Warning: dbt compile failed: {result.stderr}", err=True)
            click.echo("You may need to run 'dbt compile' manually.", err=True)
    except subprocess.TimeoutExpired:
        click.echo("Warning: dbt compile timed out.", err=True)
    except Exception as e:
        click.echo(f"Warning: Could not compile dbt: {e}", err=True)


DOCKER_COMPOSE_CONTENT = """# Phlo Infrastructure Stack
# Generated by: phlo services init
# Complete data lakehouse platform with orchestration and BI

services:
  # --- Storage Layer ---
  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - ./volumes/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-phlo}"]
      interval: 10s
      timeout: 5s
      retries: 10

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
    command: ["server", "/data", "--console-address", ":9001"]
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    volumes:
      - ./volumes/minio:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10

  minio-setup:
    image: minio/mc:latest
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
    entrypoint: >
      /bin/sh -c "
      sleep 5 &&
      mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD} &&
      mc mb --ignore-existing myminio/lake &&
      mc mb --ignore-existing myminio/lake/warehouse &&
      mc mb --ignore-existing myminio/lake/stage &&
      echo 'Buckets created successfully'
      "
    depends_on:
      minio:
        condition: service_healthy
    restart: "no"

  # --- Iceberg Catalog ---
  nessie:
    image: ghcr.io/projectnessie/nessie:${NESSIE_VERSION:-0.99.0}
    restart: unless-stopped
    environment:
      NESSIE_VERSION_STORE_TYPE: JDBC
      QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres:5432/${POSTGRES_DB:-phlo}
      QUARKUS_DATASOURCE_USERNAME: ${POSTGRES_USER:-phlo}
      QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      nessie.catalog.default-warehouse: warehouse
      nessie.catalog.warehouses.warehouse.location: s3://lake/warehouse
      nessie.catalog.service.s3.default-options.endpoint: http://minio:9000/
      nessie.catalog.service.s3.default-options.path-style-access: "true"
      nessie.catalog.service.s3.default-options.region: us-east-1
      nessie.catalog.service.s3.default-options.access-key: urn:nessie-secret:quarkus:nessie.catalog.secrets.access-key
      nessie.catalog.secrets.access-key.name: ${MINIO_ROOT_USER:-minio}
      nessie.catalog.secrets.access-key.secret: ${MINIO_ROOT_PASSWORD:-minio123}
    ports:
      - "${NESSIE_PORT:-19120}:19120"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v1/config"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # --- Query Engine ---
  trino:
    image: trinodb/trino:${TRINO_VERSION:-467}
    restart: unless-stopped
    ports:
      - "${TRINO_PORT:-8080}:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog:ro
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      nessie:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 45s

  # --- Orchestration ---
  dagster-webserver:
    image: phlo/dagster:${DAGSTER_VERSION:-latest}
    build:
      context: .
      dockerfile: dagster/Dockerfile
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    restart: unless-stopped
    environment:
      DAGSTER_HOME: /opt/dagster
      PYTHONPATH: /opt/dagster:/app
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
      AWS_REGION: us-east-1
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_S3_USE_SSL: "false"
      AWS_S3_URL_STYLE: "path"
      NESSIE_HOST: nessie
      NESSIE_PORT: 19120
      TRINO_HOST: trino
      TRINO_PORT: 8080
      TRINO_CATALOG: iceberg
      ICEBERG_WAREHOUSE_PATH: s3://lake/warehouse
      ICEBERG_STAGING_PATH: s3://lake/stage
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      WORKFLOWS_PATH: /app/workflows
      CASCADE_HOST_PLATFORM: ${CASCADE_HOST_PLATFORM:-$$(uname -s)}
    command: ["dagster-webserver", "-h", "0.0.0.0", "-p", "3000", "-w", "/opt/dagster/workspace.yaml"]
    ports:
      - "${DAGSTER_PORT:-3000}:3000"
    volumes:
      - ./dagster:/opt/dagster
      - ../workflows:/app/workflows:ro
      - ../transforms:/app/transforms:ro
      - ../tests:/app/tests:ro
    depends_on:
      minio:
        condition: service_healthy
      minio-setup:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      nessie:
        condition: service_healthy
      trino:
        condition: service_healthy

  dagster-daemon:
    image: phlo/dagster:${DAGSTER_VERSION:-latest}
    build:
      context: .
      dockerfile: dagster/Dockerfile
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    restart: unless-stopped
    environment:
      DAGSTER_HOME: /opt/dagster
      PYTHONPATH: /opt/dagster:/app
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
      AWS_REGION: us-east-1
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_S3_USE_SSL: "false"
      AWS_S3_URL_STYLE: "path"
      NESSIE_HOST: nessie
      NESSIE_PORT: 19120
      TRINO_HOST: trino
      TRINO_PORT: 8080
      TRINO_CATALOG: iceberg
      ICEBERG_WAREHOUSE_PATH: s3://lake/warehouse
      ICEBERG_STAGING_PATH: s3://lake/stage
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      WORKFLOWS_PATH: /app/workflows
      CASCADE_HOST_PLATFORM: ${CASCADE_HOST_PLATFORM:-$$(uname -s)}
    command: ["dagster-daemon", "run", "-w", "/opt/dagster/workspace.yaml"]
    volumes:
      - ./dagster:/opt/dagster
      - ../workflows:/app/workflows:ro
      - ../transforms:/app/transforms:ro
      - ../tests:/app/tests:ro
    depends_on:
      dagster-webserver:
        condition: service_started

  # --- Business Intelligence ---
  superset:
    image: apache/superset:${SUPERSET_VERSION:-4.0.0}
    restart: unless-stopped
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-phlo-superset-secret-change-me}
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
      SUPERSET_ADMIN_USER: ${SUPERSET_ADMIN_USER:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL:-admin@example.com}
    command: >
      /bin/sh -c "
      superset db upgrade &&
      superset fab create-admin --username $${SUPERSET_ADMIN_USER} --firstname Admin --lastname User --email $${SUPERSET_ADMIN_EMAIL} --password $${SUPERSET_ADMIN_PASSWORD} || true &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads
      "
    ports:
      - "${SUPERSET_PORT:-8088}:8088"
    volumes:
      - ./volumes/superset:/app/superset_home
    depends_on:
      postgres:
        condition: service_healthy
      trino:
        condition: service_healthy

  # --- Database Admin ---
  pgweb:
    image: sosedoff/pgweb
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-phlo}:${POSTGRES_PASSWORD:-phlo}@postgres:5432/${POSTGRES_DB:-phlo}?sslmode=disable
    ports:
      - "${PGWEB_PORT:-8081}:8081"
    depends_on:
      postgres:
        condition: service_healthy

  # --- Observability (optional, use --profile observability) ---
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-v3.1.0}
    restart: unless-stopped
    profiles: ["observability", "all"]
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
      - "--storage.tsdb.retention.time=30d"
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./prometheus:/etc/prometheus:ro
      - ./volumes/prometheus:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5

  loki:
    image: grafana/loki:${LOKI_VERSION:-3.2.1}
    restart: unless-stopped
    profiles: ["observability", "all"]
    command: -config.file=/etc/loki/loki-config.yml
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - ./loki:/etc/loki:ro
      - ./volumes/loki:/loki
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-11.3.1}
    restart: unless-stopped
    profiles: ["observability", "all"]
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
    ports:
      - "${GRAFANA_PORT:-3003}:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./volumes/grafana:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- API Layer (optional, use --profile api) ---
  postgrest:
    image: postgrest/postgrest:${POSTGREST_VERSION:-v12.2.3}
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      PGRST_DB_URI: postgresql://${POSTGRES_USER:-phlo}:${POSTGRES_PASSWORD:-phlo}@postgres:5432/${POSTGRES_DB:-phlo}
      PGRST_DB_SCHEMAS: api,public
      PGRST_DB_ANON_ROLE: ${POSTGRES_USER:-phlo}
      PGRST_SERVER_HOST: 0.0.0.0
      PGRST_SERVER_PORT: 3000
      PGRST_OPENAPI_SERVER_PROXY_URI: http://localhost:${POSTGREST_PORT:-3002}
    ports:
      - "${POSTGREST_PORT:-3002}:3000"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/"]
      interval: 10s
      timeout: 5s
      retries: 5

  hasura:
    image: hasura/graphql-engine:${HASURA_VERSION:-v2.46.0}
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      HASURA_GRAPHQL_DATABASE_URL: postgresql://${POSTGRES_USER:-phlo}:${POSTGRES_PASSWORD:-phlo}@postgres:5432/${POSTGRES_DB:-phlo}
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_ADMIN_SECRET: ${HASURA_ADMIN_SECRET:-phlo-hasura-admin-secret}
      HASURA_GRAPHQL_UNAUTHORIZED_ROLE: anonymous
    ports:
      - "${HASURA_PORT:-8080}:8080"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
"""

ENV_CONTENT = """# Phlo Infrastructure Configuration
# Generated by: phlo services init

# PostgreSQL
POSTGRES_USER=phlo
POSTGRES_PASSWORD=phlo
POSTGRES_DB=phlo
POSTGRES_PORT=5432

# MinIO (S3-compatible storage)
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=minio123
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001

# Nessie (Iceberg catalog)
NESSIE_VERSION=0.99.0
NESSIE_PORT=19120

# Trino (Query engine)
TRINO_VERSION=467
TRINO_PORT=8080

# Dagster (Orchestration)
DAGSTER_PORT=3000

# Host Platform Detection (auto-detected via uname -s)
# Override if needed:
#   CASCADE_HOST_PLATFORM=Darwin  # for in-process executor (more stable on macOS)
#   CASCADE_HOST_PLATFORM=Linux   # for multiprocess executor (better performance)

# GitHub token for private repo access (required for phlo from private GitHub repo)
# Create at: https://github.com/settings/tokens
GITHUB_TOKEN=

# Superset (BI)
SUPERSET_VERSION=4.0.0
SUPERSET_PORT=8088
SUPERSET_SECRET_KEY=phlo-superset-secret-change-me
SUPERSET_ADMIN_USER=admin
SUPERSET_ADMIN_PASSWORD=admin
SUPERSET_ADMIN_EMAIL=admin@example.com

# pgweb (Database admin)
PGWEB_PORT=8081

# Observability (optional, use --profile observability)
PROMETHEUS_VERSION=v3.1.0
PROMETHEUS_PORT=9090
LOKI_VERSION=3.2.1
LOKI_PORT=3100
GRAFANA_VERSION=11.3.1
GRAFANA_PORT=3003
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# API Layer (optional, use --profile api)
POSTGREST_VERSION=v12.2.3
POSTGREST_PORT=3002
HASURA_VERSION=v2.46.0
HASURA_PORT=8080
HASURA_ADMIN_SECRET=phlo-hasura-admin-secret

# Iceberg configuration (used by Phlo)
ICEBERG_WAREHOUSE_PATH=s3://lake/warehouse
ICEBERG_STAGING_PATH=s3://lake/stage
"""

GITIGNORE_CONTENT = """# Phlo infrastructure files
.env
volumes/
"""

# Trino catalog configuration for Iceberg
TRINO_ICEBERG_PROPERTIES = """connector.name=iceberg
iceberg.catalog.type=rest
iceberg.rest-catalog.uri=http://nessie:19120/iceberg
iceberg.rest-catalog.warehouse=warehouse
fs.native-s3.enabled=true
s3.endpoint=http://minio:9000
s3.path-style-access=true
s3.region=us-east-1
"""

# Dagster Dockerfile
DAGSTER_DOCKERFILE = """FROM python:3.11-slim

WORKDIR /opt/dagster

ARG GITHUB_TOKEN

# Install system dependencies and uv
RUN apt-get update && apt-get install -y --no-install-recommends \\
    curl \\
    git \\
    && rm -rf /var/lib/apt/lists/* \\
    && pip install uv

# Install phlo from GitHub (uses token if provided for private repos)
RUN if [ -n "$GITHUB_TOKEN" ]; then \\
        uv pip install --system "phlo @ git+https://${GITHUB_TOKEN}@github.com/iamgp/phlo.git" dagster-postgres pyiceberg[s3] trino; \\
    else \\
        uv pip install --system "phlo @ git+https://github.com/iamgp/phlo.git" dagster-postgres pyiceberg[s3] trino; \\
    fi

# Copy workspace configuration
COPY dagster/workspace.yaml /opt/dagster/workspace.yaml
COPY dagster/dagster.yaml /opt/dagster/dagster.yaml

EXPOSE 3000

CMD ["dagster-webserver", "-h", "0.0.0.0", "-p", "3000"]
"""

# Dagster workspace.yaml
DAGSTER_WORKSPACE_YAML = """load_from:
  - python_module:
      module_name: phlo.framework.definitions
      working_directory: /app
"""

# Dagster dagster.yaml
DAGSTER_YAML = """storage:
  postgres:
    postgres_db:
      hostname:
        env: POSTGRES_HOST
      port:
        env: POSTGRES_PORT
      username:
        env: POSTGRES_USER
      password:
        env: POSTGRES_PASSWORD
      db_name:
        env: POSTGRES_DB

run_coordinator:
  module: dagster.core.run_coordinator
  class: QueuedRunCoordinator

run_launcher:
  module: dagster.core.launcher
  class: DefaultRunLauncher

sensors:
  use_threads: true
  num_workers: 4

schedules:
  use_threads: true
  num_workers: 4
"""

# Dev mode Dockerfile - installs deps but not phlo (mounted via volume)
DAGSTER_DOCKERFILE_DEV = """FROM python:3.11-slim

WORKDIR /opt/dagster

# Install system dependencies and uv
RUN apt-get update && apt-get install -y --no-install-recommends \\
    curl \\
    git \\
    && rm -rf /var/lib/apt/lists/* \\
    && pip install uv

# Install all phlo dependencies (but not phlo itself - that's mounted)
RUN uv pip install --system \\
    dagster \\
    dagster-webserver \\
    dagster-postgres \\
    dagster-dbt \\
    dagster-pandera \\
    dagster-aws \\
    dbt-core \\
    dbt-trino \\
    dbt-postgres \\
    "pyiceberg[s3fs,pyarrow]" \\
    s3fs \\
    trino \\
    pandas \\
    pandera \\
    "dlt[parquet]" \\
    pydantic-settings \\
    requests \\
    tenacity \\
    pyarrow \\
    click \\
    rich

# Copy workspace configuration
COPY dagster/workspace.yaml /opt/dagster/workspace.yaml
COPY dagster/dagster.yaml /opt/dagster/dagster.yaml

EXPOSE 3000

CMD ["dagster-webserver", "-h", "0.0.0.0", "-p", "3000"]
"""

# Dev mode docker-compose override - mounts local phlo source
DOCKER_COMPOSE_DEV_CONTENT = """# Development mode override - mounts local phlo source
# Auto-generated by: phlo services start --dev
#
# This file is used automatically when --dev flag is passed.
# It mounts local phlo source into containers for instant iteration.

services:
  dagster-webserver:
    build:
      dockerfile: dagster/Dockerfile.dev
    volumes:
      - ./dagster:/opt/dagster
      - ../workflows:/app/workflows:ro
      - ../transforms:/app/transforms
      - ../tests:/app/tests:ro
      # Mount local phlo source (path set via PHLO_DEV_SOURCE_PATH env var)
      - ${PHLO_DEV_SOURCE_PATH}/src/phlo:/opt/phlo-src/phlo:ro
    environment:
      PYTHONPATH: /opt/dagster:/app:/opt/phlo-src

  dagster-daemon:
    build:
      dockerfile: dagster/Dockerfile.dev
    volumes:
      - ./dagster:/opt/dagster
      - ../workflows:/app/workflows:ro
      - ../transforms:/app/transforms
      - ../tests:/app/tests:ro
      # Mount local phlo source
      - ${PHLO_DEV_SOURCE_PATH}/src/phlo:/opt/phlo-src/phlo:ro
    environment:
      PYTHONPATH: /opt/dagster:/app:/opt/phlo-src
"""

# Prometheus configuration
PROMETHEUS_CONFIG = """global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'dagster'
    static_configs:
      - targets: ['dagster-webserver:3000']
    metrics_path: /metrics

  - job_name: 'trino'
    static_configs:
      - targets: ['trino:8080']
"""

# Loki configuration
LOKI_CONFIG = """auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  instance_addr: 127.0.0.1
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: tsdb
      object_store: filesystem
      schema: v13
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

analytics:
  reporting_enabled: false
"""

# Grafana datasources provisioning
GRAFANA_DATASOURCES = """apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
"""


def get_phlo_dir() -> Path:
    """Get the .phlo directory path in current project."""
    return Path.cwd() / ".phlo"


def ensure_phlo_dir() -> Path:
    """Ensure .phlo directory exists with required files."""
    phlo_dir = get_phlo_dir()

    if not phlo_dir.exists():
        click.echo("Error: .phlo directory not found.", err=True)
        click.echo("Run 'phlo services init' first.", err=True)
        sys.exit(1)

    return phlo_dir


@click.group()
def services():
    """Manage Phlo infrastructure services (Docker)."""
    pass


@services.command("init")
@click.option("--force", is_flag=True, help="Overwrite existing configuration")
@click.option("--name", "project_name", help="Project name (default: directory name)")
def init(force: bool, project_name: Optional[str]):
    """Initialize Phlo infrastructure in .phlo/ directory.

    Creates complete Docker Compose configuration for the full Phlo stack:
    - PostgreSQL, MinIO, Nessie (storage layer)
    - Trino (query engine)
    - Dagster (orchestration)
    - Superset (BI)
    - pgweb (database admin)
    - Optional: Prometheus, Loki, Grafana (observability)
    - Optional: PostgREST (API layer)

    Examples:
        phlo services init
        phlo services init --name my-lakehouse
        phlo services init --force
    """
    phlo_dir = get_phlo_dir()
    config_file = Path.cwd() / PHLO_CONFIG_FILE

    if phlo_dir.exists() and not force:
        click.echo(f"Directory {phlo_dir} already exists.", err=True)
        click.echo("Use --force to overwrite.", err=True)
        sys.exit(1)

    # Derive project name from directory if not specified
    if not project_name:
        project_name = Path.cwd().name.lower().replace(" ", "-").replace("_", "-")

    # Create phlo.yaml config file in project root
    config_content = PHLO_CONFIG_TEMPLATE.format(
        name=project_name,
        description=f"{project_name} data lakehouse",
    )
    config_file.write_text(config_content)
    click.echo(f"Created: {PHLO_CONFIG_FILE}")

    # Create .phlo directory
    phlo_dir.mkdir(parents=True, exist_ok=True)

    # Write docker-compose.yml
    compose_file = phlo_dir / "docker-compose.yml"
    compose_file.write_text(DOCKER_COMPOSE_CONTENT)
    click.echo(f"Created: {compose_file.relative_to(Path.cwd())}")

    # Write .env
    env_file = phlo_dir / ".env"
    env_file.write_text(ENV_CONTENT)
    click.echo(f"Created: {env_file.relative_to(Path.cwd())}")

    # Write .gitignore
    gitignore_file = phlo_dir / ".gitignore"
    gitignore_file.write_text(GITIGNORE_CONTENT)
    click.echo(f"Created: {gitignore_file.relative_to(Path.cwd())}")

    # Create volumes directory
    volumes_dir = phlo_dir / "volumes"
    volumes_dir.mkdir(exist_ok=True)

    # Create Trino catalog directory and config
    trino_dir = phlo_dir / "trino" / "catalog"
    trino_dir.mkdir(parents=True, exist_ok=True)
    iceberg_props = trino_dir / "iceberg.properties"
    iceberg_props.write_text(TRINO_ICEBERG_PROPERTIES)
    click.echo(f"Created: {iceberg_props.relative_to(Path.cwd())}")

    # Create Dagster directory and config
    dagster_dir = phlo_dir / "dagster"
    dagster_dir.mkdir(exist_ok=True)
    (dagster_dir / "Dockerfile").write_text(DAGSTER_DOCKERFILE)
    click.echo(f"Created: {(dagster_dir / 'Dockerfile').relative_to(Path.cwd())}")
    (dagster_dir / "workspace.yaml").write_text(DAGSTER_WORKSPACE_YAML)
    click.echo(f"Created: {(dagster_dir / 'workspace.yaml').relative_to(Path.cwd())}")
    (dagster_dir / "dagster.yaml").write_text(DAGSTER_YAML)
    click.echo(f"Created: {(dagster_dir / 'dagster.yaml').relative_to(Path.cwd())}")

    # Create Prometheus config (for optional observability profile)
    prometheus_dir = phlo_dir / "prometheus"
    prometheus_dir.mkdir(exist_ok=True)
    (prometheus_dir / "prometheus.yml").write_text(PROMETHEUS_CONFIG)
    click.echo(f"Created: {(prometheus_dir / 'prometheus.yml').relative_to(Path.cwd())}")

    # Create Loki config (for optional observability profile)
    loki_dir = phlo_dir / "loki"
    loki_dir.mkdir(exist_ok=True)
    (loki_dir / "loki-config.yml").write_text(LOKI_CONFIG)
    click.echo(f"Created: {(loki_dir / 'loki-config.yml').relative_to(Path.cwd())}")

    # Create Grafana provisioning (for optional observability profile)
    grafana_dir = phlo_dir / "grafana" / "provisioning" / "datasources"
    grafana_dir.mkdir(parents=True, exist_ok=True)
    (grafana_dir / "datasources.yml").write_text(GRAFANA_DATASOURCES)
    click.echo(f"Created: {(grafana_dir / 'datasources.yml').relative_to(Path.cwd())}")

    click.echo("")
    click.echo("Phlo infrastructure initialized.")
    click.echo("")
    click.echo("Services included:")
    click.echo("  Core:        PostgreSQL, MinIO, Nessie, Trino")
    click.echo("  Orchestration: Dagster (webserver + daemon)")
    click.echo("  BI:          Superset, pgweb")
    click.echo("  Optional:    Prometheus, Loki, Grafana (--profile observability)")
    click.echo("  Optional:    PostgREST (--profile api)")
    click.echo("")
    click.echo("Next steps:")
    click.echo("  1. Review .phlo/.env and adjust settings if needed")
    click.echo("  2. Run: phlo services start")
    click.echo("  3. Access services:")
    click.echo("     - Dagster:  http://localhost:3000")
    click.echo("     - Superset: http://localhost:8088")
    click.echo("     - Trino:    http://localhost:8080")
    click.echo("     - MinIO:    http://localhost:9001")
    click.echo("     - pgweb:    http://localhost:8081")


@services.command("start")
@click.option("-d", "--detach", is_flag=True, default=True, help="Run in background")
@click.option("--build", is_flag=True, help="Build images before starting")
@click.option(
    "--profile",
    multiple=True,
    type=click.Choice(["observability", "api", "all"]),
    help="Enable optional profiles",
)
@click.option(
    "--dev",
    is_flag=True,
    help="Development mode: mount local phlo source for instant iteration",
)
@click.option(
    "--phlo-source",
    type=click.Path(exists=True),
    help="Path to phlo source (default: auto-detect or use PHLO_DEV_SOURCE_PATH)",
)
def start(detach: bool, build: bool, profile: tuple[str, ...], dev: bool, phlo_source: str):
    """Start Phlo infrastructure services.

    Starts the complete Phlo data lakehouse stack.

    Examples:
        phlo services start
        phlo services start --build
        phlo services start --profile observability
        phlo services start --dev  # Mount local phlo source
        phlo services start --dev --phlo-source /path/to/phlo
    """
    require_docker()
    phlo_dir = ensure_phlo_dir()
    compose_file = phlo_dir / "docker-compose.yml"
    env_file = phlo_dir / ".env"
    project_name = get_project_name()

    if not compose_file.exists():
        click.echo("Error: docker-compose.yml not found.", err=True)
        click.echo("Run 'phlo services init' first.", err=True)
        sys.exit(1)

    # Handle dev mode
    dev_compose_file = None
    phlo_source_path = None

    if dev:
        click.echo(f"Starting {project_name} infrastructure in DEVELOPMENT mode...")

        # Determine phlo source path
        if phlo_source:
            phlo_source_path = Path(phlo_source).resolve()
        else:
            # Try to auto-detect: check PHLO_DEV_SOURCE_PATH env var
            import os

            phlo_source_path = os.environ.get("PHLO_DEV_SOURCE_PATH")
            if phlo_source_path:
                phlo_source_path = Path(phlo_source_path).resolve()
            else:
                # Try to find phlo source relative to current directory
                # Check if we're in examples/glucose-platform -> ../../src/phlo
                potential_paths = [
                    Path.cwd().parent.parent / "src" / "phlo",  # examples/project -> repo root
                    Path.cwd().parent / "src" / "phlo",  # direct child of repo
                    Path.cwd() / "src" / "phlo",  # in repo root
                ]
                for p in potential_paths:
                    if p.exists() and (p / "__init__.py").exists():
                        phlo_source_path = p.parent.parent.resolve()  # Get repo root
                        break

        if not phlo_source_path or not (Path(phlo_source_path) / "src" / "phlo").exists():
            click.echo("Error: Could not find phlo source.", err=True)
            click.echo("", err=True)
            click.echo(
                "Specify the path with --phlo-source or set PHLO_DEV_SOURCE_PATH:",
                err=True,
            )
            click.echo("  phlo services start --dev --phlo-source /path/to/phlo", err=True)
            click.echo("  export PHLO_DEV_SOURCE_PATH=/path/to/phlo", err=True)
            sys.exit(1)

        click.echo(f"Using phlo source: {phlo_source_path}")

        # Create dev Dockerfile if it doesn't exist
        dev_dockerfile = phlo_dir / "dagster" / "Dockerfile.dev"
        if not dev_dockerfile.exists():
            dev_dockerfile.write_text(DAGSTER_DOCKERFILE_DEV)
            click.echo(f"Created: {dev_dockerfile.relative_to(Path.cwd())}")

        # Create dev compose override
        dev_compose_file = phlo_dir / "docker-compose.dev.yml"
        dev_compose_file.write_text(DOCKER_COMPOSE_DEV_CONTENT)
        click.echo(f"Created: {dev_compose_file.relative_to(Path.cwd())}")

        # Set env var for compose interpolation
        import os

        os.environ["PHLO_DEV_SOURCE_PATH"] = str(phlo_source_path)

        # Force rebuild in dev mode to use dev Dockerfile
        build = True
    else:
        click.echo(f"Starting {project_name} infrastructure...")

    # Check Docker memory on macOS
    import platform
    import subprocess as sp

    if platform.system() == "Darwin":
        try:
            result = sp.run(
                ["docker", "info", "--format", "{{.MemTotal}}"],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode == 0:
                mem_bytes = int(result.stdout.strip())
                mem_gb = mem_bytes / (1024**3)
                if mem_gb < 16:
                    click.echo(
                        f"\nWarning: Docker has {mem_gb:.1f}GB memory allocated. "
                        f"Recommend 16GB+ for Dagster stability.",
                        err=True,
                    )
                    click.echo(
                        "Increase in Docker Desktop -> Settings -> Resources -> Memory\n",
                        err=True,
                    )
        except Exception:
            pass  # Silently ignore if docker info fails

    cmd = [
        "docker",
        "compose",
        "-p",
        project_name,
        "-f",
        str(compose_file),
    ]

    # Add dev compose override if in dev mode
    if dev and dev_compose_file:
        cmd.extend(["-f", str(dev_compose_file)])

    cmd.extend(["--env-file", str(env_file)])

    # Add profiles
    for p in profile:
        cmd.extend(["--profile", p])

    cmd.append("up")

    if detach:
        cmd.append("-d")

    if build:
        cmd.append("--build")

    try:
        result = subprocess.run(cmd, check=False)
        if result.returncode == 0:
            click.echo("")
            if dev:
                click.echo("Phlo infrastructure started in DEVELOPMENT mode.")
                click.echo("")
                click.echo(f"Local phlo source mounted from: {phlo_source_path}")
                click.echo("Changes to phlo code will be reflected after Dagster restart.")
                click.echo("")
            else:
                click.echo("Phlo infrastructure started.")
            click.echo("")
            click.echo("Core Services:")
            click.echo("  - PostgreSQL: localhost:5432")
            click.echo("  - MinIO API:  localhost:9000")
            click.echo("  - MinIO UI:   localhost:9001")
            click.echo("  - Nessie:     localhost:19120")
            click.echo("  - Trino:      localhost:8080")
            click.echo("  - Dagster:    localhost:3000")
            click.echo("  - Superset:   localhost:8088")
            click.echo("  - pgweb:      localhost:8081")
            if "observability" in profile or "all" in profile:
                click.echo("")
                click.echo("Observability:")
                click.echo("  - Prometheus: localhost:9090")
                click.echo("  - Loki:       localhost:3100")
                click.echo("  - Grafana:    localhost:3003")
            if "api" in profile or "all" in profile:
                click.echo("")
                click.echo("API Layer:")
                click.echo("  - PostgREST:  localhost:3002")
            if dev:
                click.echo("")
                click.echo("To apply phlo code changes, restart Dagster:")
                click.echo("  docker restart dagster-webserver dagster-daemon")

            # Initialize Nessie branches (main, dev)
            _init_nessie_branches(project_name)

            # Auto-run dbt deps + compile to generate manifest for Dagster
            _run_dbt_compile(project_name)
        else:
            click.echo(f"Error: docker compose failed with code {result.returncode}", err=True)
            sys.exit(result.returncode)
    except FileNotFoundError:
        click.echo("Error: docker command not found.", err=True)
        click.echo("Please install Docker: https://docs.docker.com/get-docker/", err=True)
        sys.exit(1)


@services.command("stop")
@click.option("-v", "--volumes", is_flag=True, help="Remove volumes (deletes data)")
@click.option(
    "--profile",
    multiple=True,
    type=click.Choice(["observability", "api", "all"]),
    help="Stop optional profile services",
)
def stop(volumes: bool, profile: tuple[str, ...]):
    """Stop Phlo infrastructure services.

    Examples:
        phlo services stop
        phlo services stop --volumes  # Also remove data
        phlo services stop --profile all  # Stop all including optional services
    """
    require_docker()
    phlo_dir = ensure_phlo_dir()
    compose_file = phlo_dir / "docker-compose.yml"
    env_file = phlo_dir / ".env"
    project_name = get_project_name()

    click.echo(f"Stopping {project_name} infrastructure...")

    cmd = [
        "docker",
        "compose",
        "-p",
        project_name,
        "-f",
        str(compose_file),
        "--env-file",
        str(env_file),
    ]

    # Add profiles to ensure profile services are also stopped
    for p in profile:
        cmd.extend(["--profile", p])

    cmd.append("down")

    if volumes:
        cmd.append("-v")
        click.echo("Warning: Removing volumes will delete all data.")

    try:
        result = subprocess.run(cmd, check=False)
        if result.returncode == 0:
            click.echo(f"{project_name} infrastructure stopped.")
        else:
            click.echo(f"Error: docker compose failed with code {result.returncode}", err=True)
            sys.exit(result.returncode)
    except FileNotFoundError:
        click.echo("Error: docker command not found.", err=True)
        sys.exit(1)


@services.command("status")
def status():
    """Show status of Phlo infrastructure services.

    Examples:
        phlo services status
    """
    require_docker()
    phlo_dir = ensure_phlo_dir()
    compose_file = phlo_dir / "docker-compose.yml"
    env_file = phlo_dir / ".env"
    project_name = get_project_name()

    cmd = [
        "docker",
        "compose",
        "-p",
        project_name,
        "-f",
        str(compose_file),
        "--env-file",
        str(env_file),
        "ps",
        "--format",
        "table {{.Name}}\t{{.Status}}\t{{.Ports}}",
    ]

    try:
        result = subprocess.run(cmd, check=False)
        if result.returncode != 0:
            click.echo("No services running or error checking status.", err=True)
    except FileNotFoundError:
        click.echo("Error: docker command not found.", err=True)
        sys.exit(1)


@services.command("logs")
@click.argument("service", required=False)
@click.option("-f", "--follow", is_flag=True, help="Follow log output")
@click.option("-n", "--tail", default=100, help="Number of lines to show")
def logs(service: str, follow: bool, tail: int):
    """View logs from Phlo infrastructure services.

    Examples:
        phlo services logs
        phlo services logs nessie
        phlo services logs -f
    """
    require_docker()
    phlo_dir = ensure_phlo_dir()
    compose_file = phlo_dir / "docker-compose.yml"
    env_file = phlo_dir / ".env"
    project_name = get_project_name()

    cmd = [
        "docker",
        "compose",
        "-p",
        project_name,
        "-f",
        str(compose_file),
        "--env-file",
        str(env_file),
        "logs",
        "--tail",
        str(tail),
    ]

    if follow:
        cmd.append("-f")

    if service:
        cmd.append(service)

    try:
        subprocess.run(cmd, check=False)
    except FileNotFoundError:
        click.echo("Error: docker command not found.", err=True)
        sys.exit(1)
    except KeyboardInterrupt:
        pass
