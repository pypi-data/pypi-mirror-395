# docker-compose.yml - Multi-service Docker Compose configuration for Phlo lakehouse platform
# Defines all services: databases, query engines, orchestration, BI, monitoring
# Provides complete development and production environment setup

services:
  # --- Database Services ---
  postgres:
    build:
      context: ./docker
      dockerfile: Dockerfile.postgres
    container_name: pg
    restart: unless-stopped
    environment:
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    ports: ["${POSTGRES_PORT}:5432"]
    volumes:
      - ./volumes/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 10

  # MinIO object storage service
  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    container_name: minio
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: ["server", "/data", "--console-address", ":${MINIO_CONSOLE_PORT}"]
    ports:
      - "${MINIO_API_PORT}:9000"
      - "${MINIO_CONSOLE_PORT}:${MINIO_CONSOLE_PORT}"
    volumes:
      - ./volumes/minio:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10

  # MinIO setup service to create required buckets
  minio-setup:
    image: alpine:latest
    container_name: minio-setup
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    command: >
      sh -c "
      apk add --no-cache curl &&
      curl -O https://dl.min.io/client/mc/release/linux-arm64/mc &&
      chmod +x mc &&
      sleep 10 &&
      ./mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD} &&
      ./mc mb --ignore-existing myminio/lake &&
      ./mc mb --ignore-existing myminio/lake/warehouse &&
      ./mc mb --ignore-existing myminio/lake/stage &&
      echo 'Buckets created successfully'
      "
    depends_on:
      minio:
        condition: service_healthy
    restart: "no"

  nessie:
    image: ghcr.io/projectnessie/nessie:${NESSIE_VERSION}
    container_name: nessie
    restart: unless-stopped
    environment:
      NESSIE_VERSION_STORE_TYPE: JDBC
      QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres:5432/${POSTGRES_DB}
      QUARKUS_DATASOURCE_USERNAME: ${POSTGRES_USER}
      QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD}
      # Iceberg REST catalog configuration
      nessie.catalog.default-warehouse: warehouse
      nessie.catalog.warehouses.warehouse.location: s3://lake/warehouse
      nessie.catalog.service.s3.default-options.endpoint: http://minio:9000/
      nessie.catalog.service.s3.default-options.path-style-access: "true"
      nessie.catalog.service.s3.default-options.region: us-east-1
      nessie.catalog.service.s3.default-options.access-key: urn:nessie-secret:quarkus:nessie.catalog.secrets.access-key
      nessie.catalog.secrets.access-key.name: ${MINIO_ROOT_USER}
      nessie.catalog.secrets.access-key.secret: ${MINIO_ROOT_PASSWORD}
    ports:
      - "${NESSIE_PORT}:19120"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v1/config"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  trino:
    image: trinodb/trino:${TRINO_VERSION}
    container_name: trino
    restart: unless-stopped
    ports:
      - "${TRINO_PORT}:8080"
    volumes:
      - ./docker/trino/catalog:/etc/trino/catalog
      - ./docker/trino/config.properties:/etc/trino/config.properties
    environment:
      # S3/MinIO credentials for Iceberg
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      nessie:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 45s

  dagster-webserver:
    build:
      context: .
      dockerfile: docker/Dockerfile.dagster
    container_name: dagster-web
    restart: unless-stopped
    environment:
      DAGSTER_HOME: /opt/dagster
      DBT_PROFILES_DIR: /dbt/profiles
      PYTHONPATH: /opt/dagster
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: us-east-1
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_S3_USE_SSL: "false"
      AWS_S3_URL_STYLE: "path"
      # Nessie configuration
      NESSIE_VERSION: ${NESSIE_VERSION}
      NESSIE_PORT: 19120
      NESSIE_HOST: nessie
      # Trino configuration
      TRINO_VERSION: ${TRINO_VERSION}
      TRINO_PORT: 8080
      TRINO_HOST: trino
      TRINO_CATALOG: iceberg
      # Iceberg configuration
      ICEBERG_WAREHOUSE_PATH: ${ICEBERG_WAREHOUSE_PATH}
      ICEBERG_STAGING_PATH: ${ICEBERG_STAGING_PATH}
      ICEBERG_DEFAULT_NAMESPACE: raw
      ICEBERG_NESSIE_REF: ${ICEBERG_NESSIE_REF:-dev}
      # MinIO/Postgres for dbt
      MINIO_HOST: ${MINIO_HOST:-minio}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_API_PORT: 9000
      POSTGRES_HOST: ${POSTGRES_HOST:-postgres}
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      # Host platform detection for executor selection
      CASCADE_HOST_PLATFORM: ${CASCADE_HOST_PLATFORM:-$$(uname -s)}
    command:
      [
        "dagster-webserver",
        "-h",
        "0.0.0.0",
        "-p",
        "3000",
        "-w",
        "/opt/dagster/workspace.yaml",
      ]
    ports: ["${DAGSTER_PORT}:3000"]
    volumes:
      - ./services/dagster:/opt/dagster
      - ./src/phlo:/opt/dagster/phlo:ro
      - ./transforms/dbt:/dbt # Needs write for target/
      - ./data:/data # Legacy mount retained for local artifacts
    depends_on:
      minio:
        condition: service_healthy
      minio-setup:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      nessie:
        condition: service_healthy
      trino:
        condition: service_healthy

  dagster-daemon:
    build:
      context: .
      dockerfile: docker/Dockerfile.dagster
    container_name: dagster-daemon
    restart: unless-stopped
    environment:
      DAGSTER_HOME: /opt/dagster
      DBT_PROFILES_DIR: /dbt/profiles
      PYTHONPATH: /opt/dagster
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD}
      AWS_REGION: us-east-1
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_S3_USE_SSL: "false"
      AWS_S3_URL_STYLE: "path"
      # Nessie configuration
      NESSIE_VERSION: ${NESSIE_VERSION}
      NESSIE_PORT: 19120
      NESSIE_HOST: nessie
      # Trino configuration
      TRINO_VERSION: ${TRINO_VERSION}
      TRINO_PORT: 8080
      TRINO_HOST: trino
      TRINO_CATALOG: iceberg
      # Iceberg configuration
      ICEBERG_WAREHOUSE_PATH: ${ICEBERG_WAREHOUSE_PATH}
      ICEBERG_STAGING_PATH: ${ICEBERG_STAGING_PATH}
      ICEBERG_DEFAULT_NAMESPACE: raw
      ICEBERG_NESSIE_REF: ${ICEBERG_NESSIE_REF:-dev}
      # MinIO/Postgres for dbt
      MINIO_HOST: ${MINIO_HOST:-minio}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      MINIO_API_PORT: 9000
      POSTGRES_HOST: ${POSTGRES_HOST:-postgres}
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      # Host platform detection for executor selection
      CASCADE_HOST_PLATFORM: ${CASCADE_HOST_PLATFORM:-$$(uname -s)}
    command: ["dagster-daemon", "run", "-w", "/opt/dagster/workspace.yaml"]
    volumes:
      - ./services/dagster:/opt/dagster
      - ./src/phlo:/opt/dagster/phlo:ro
      - ./transforms/dbt:/dbt # Needs write for target/
      - ./data:/data # Legacy mount retained for local artifacts
    depends_on:
      dagster-webserver:
        condition: service_started

  superset:
    build:
      context: ./docker
      dockerfile: Dockerfile.superset
    container_name: superset
    restart: unless-stopped
    command:
      ["/bin/sh", "-c", "/app/docker/docker-init.sh && /usr/bin/run-server.sh"]
    environment:
      SUPERSET_SECRET_KEY: superset_secret_key_change_me
      SUPERSET_CONFIG_PATH: /app/pythonpath/superset_config.py
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      SUPERSET_ADMIN_USER: ${SUPERSET_ADMIN_USER:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL:-admin@example.com}
    volumes:
      - ./volumes/superset:/app/superset_home
      - ./docker/superset_config.py:/app/pythonpath/superset_config.py
    ports: ["${SUPERSET_PORT}:8088"]
    depends_on:
      postgres:
        condition: service_healthy

  pgweb:
    image: sosedoff/pgweb
    container_name: pgweb
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable
    ports: ["${PGWEB_PORT}:8081"]
    depends_on:
      postgres:
        condition: service_healthy

  hub:
    build:
      context: .
      dockerfile: docker/Dockerfile.app
    container_name: lakehouse-hub
    restart: unless-stopped
    environment:
      APP_PORT: ${APP_PORT:-10009}
      FLASK_DEBUG: ${FLASK_DEBUG:-false}
      DAGSTER_PORT: ${DAGSTER_PORT:-10006}
      TRINO_PORT: ${TRINO_PORT:-10005}
      NESSIE_PORT: ${NESSIE_PORT:-10003}
      SUPERSET_PORT: ${SUPERSET_PORT:-10007}
      SUPERSET_ADMIN_USER: ${SUPERSET_ADMIN_USER:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD}
      PGWEB_PORT: ${PGWEB_PORT:-10008}
      MINIO_CONSOLE_PORT: ${MINIO_CONSOLE_PORT:-10002}
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
      POSTGRES_PORT: ${POSTGRES_PORT:-10000}
      POSTGRES_USER: ${POSTGRES_USER:-lake}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-lakepass}
      POSTGRES_DB: ${POSTGRES_DB:-lakehouse}
      API_PORT: ${API_PORT:-10010}
      HASURA_PORT: ${HASURA_PORT:-10011}
      GRAFANA_PORT: ${GRAFANA_PORT:-10016}
      PROMETHEUS_PORT: ${PROMETHEUS_PORT:-10013}
    ports:
      - "${APP_PORT:-10009}:${APP_PORT:-10009}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${APP_PORT:-10009}/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Observability Stack
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-v3.1.0}
    container_name: prometheus
    restart: unless-stopped
    profiles: ["observability", "all"]
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
      - "--web.enable-lifecycle"
      - "--storage.tsdb.retention.time=30d"
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./docker/prometheus:/etc/prometheus
      - ./volumes/prometheus:/prometheus
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9090/-/healthy",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  loki:
    image: grafana/loki:${LOKI_VERSION:-3.2.1}
    container_name: loki
    restart: unless-stopped
    profiles: ["observability", "all"]
    command: -config.file=/etc/loki/loki-config.yml
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - ./docker/loki:/etc/loki
      - ./volumes/loki:/loki
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3100/ready",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  alloy:
    image: grafana/alloy:${ALLOY_VERSION:-v1.5.1}
    container_name: alloy
    restart: unless-stopped
    profiles: ["observability", "all"]
    command:
      - run
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    ports:
      - "${ALLOY_PORT:-12345}:12345"
    volumes:
      - ./docker/alloy:/etc/alloy
      - ./volumes/alloy:/var/lib/alloy
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      HOSTNAME: alloy
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:12345/-/healthy",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-11.3.1}
    container_name: grafana
    restart: unless-stopped
    profiles: ["observability", "all"]
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin123}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_INSTALL_PLUGINS: ""
    ports:
      - "${GRAFANA_PORT:-3001}:3000"
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
      - ./volumes/grafana:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:3000/api/health",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:${POSTGRES_EXPORTER_VERSION:-v0.16.0}
    container_name: postgres-exporter
    restart: unless-stopped
    profiles: ["observability", "all"]
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
    ports:
      - "${POSTGRES_EXPORTER_PORT:-9187}:9187"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:9187/",
        ]
      interval: 10s
      timeout: 5s
      retries: 5

  # API Layer
  api:
    build:
      context: ./services/api
      dockerfile: Dockerfile
    container_name: phlo-api
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      # API Configuration
      JWT_SECRET: ${JWT_SECRET:-phlo-jwt-secret-change-in-production}
      JWT_ALGORITHM: ${JWT_ALGORITHM:-HS256}
      JWT_ACCESS_TOKEN_EXPIRE_MINUTES: ${JWT_ACCESS_TOKEN_EXPIRE_MINUTES:-60}
      # Hasura shared JWT secret
      HASURA_GRAPHQL_JWT_SECRET: ${JWT_SECRET:-phlo-jwt-secret-change-in-production}
      # Trino connection
      TRINO_HOST: ${TRINO_HOST:-trino}
      TRINO_PORT: ${TRINO_PORT:-8080}
      TRINO_CATALOG: ${TRINO_CATALOG:-iceberg}
      TRINO_USER: ${TRINO_USER:-phlo}
      # Postgres connection
      POSTGRES_HOST: ${POSTGRES_HOST:-postgres}
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${POSTGRES_DB:-lakehouse}
      POSTGRES_USER: ${POSTGRES_USER:-lake}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-lakepass}
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      trino:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  hasura:
    image: hasura/graphql-engine:${HASURA_VERSION:-v2.45.0}
    container_name: hasura
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      HASURA_GRAPHQL_DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      # JWT Authentication (shared with FastAPI)
      HASURA_GRAPHQL_JWT_SECRET: '{"type":"HS256","key":"${JWT_SECRET:-phlo-jwt-secret-change-in-production}"}'
      HASURA_GRAPHQL_ADMIN_SECRET: ${HASURA_ADMIN_SECRET:-phlo-admin-secret-change-me}
      # CORS
      HASURA_GRAPHQL_CORS_DOMAIN: "*"
    ports:
      - "${HASURA_PORT:-8081}:8080"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgrest:
    image: postgrest/postgrest:${POSTGREST_VERSION:-v12.2.3}
    container_name: postgrest
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      # Database connection (using authenticator role that can switch to other roles)
      PGRST_DB_URI: postgresql://authenticator:${POSTGREST_AUTHENTICATOR_PASSWORD:-authenticator_password_change_in_production}@postgres:5432/${POSTGRES_DB}
      PGRST_DB_SCHEMAS: api
      PGRST_DB_ANON_ROLE: anon
      PGRST_DB_POOL: 20
      PGRST_DB_POOL_TIMEOUT: 10
      PGRST_DB_MAX_ROWS: 10000
      PGRST_DB_EXTRA_SEARCH_PATH: public
      # JWT Configuration (shared with FastAPI and Hasura)
      PGRST_JWT_SECRET: ${JWT_SECRET:-phlo-jwt-secret-change-in-production}
      PGRST_JWT_SECRET_IS_BASE64: "false"
      PGRST_JWT_ROLE_CLAIM_KEY: .role
      # Server Configuration
      PGRST_SERVER_HOST: 0.0.0.0
      PGRST_SERVER_PORT: 3000
      PGRST_OPENAPI_SERVER_PROXY_URI: http://localhost:${POSTGREST_PORT:-10018}
      PGRST_OPENAPI_MODE: follow-privileges
      # Admin server for health checks
      PGRST_ADMIN_SERVER_PORT: 3001
      # Logging
      PGRST_LOG_LEVEL: info
      # Allow application settings (for JWT secret access in functions)
      PGRST_DB_CONFIG: "true"
    ports:
      - "${POSTGREST_PORT:-10018}:3000"
      - "${POSTGREST_ADMIN_PORT:-10019}:3001"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3001/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  mkdocs:
    build:
      context: .
      dockerfile: docker/Dockerfile.mkdocs
    container_name: mkdocs
    restart: unless-stopped
    profiles: ["docs", "all"]
    ports:
      - "${MKDOCS_PORT:-8001}:8000"
    volumes:
      - ./docs:/docs/docs:ro
      - ./mkdocs.yml:/docs/mkdocs.yml:ro
      - ./src:/docs/src:ro # For mkdocstrings to access source code
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Data Catalog - OpenMetadata
  openmetadata-mysql:
    image: mysql:${OPENMETADATA_MYSQL_VERSION:-8.3.0}
    container_name: openmetadata-mysql
    restart: unless-stopped
    profiles: ["catalog", "all"]
    environment:
      MYSQL_ROOT_PASSWORD: ${OPENMETADATA_MYSQL_ROOT_PASSWORD:-password}
      MYSQL_USER: ${OPENMETADATA_MYSQL_USER:-openmetadata_user}
      MYSQL_PASSWORD: ${OPENMETADATA_MYSQL_PASSWORD:-openmetadata_password}
      MYSQL_DATABASE: ${OPENMETADATA_MYSQL_DATABASE:-openmetadata_db}
    command:
      - "--character-set-server=utf8mb4"
      - "--collation-server=utf8mb4_unicode_ci"
      - "--default-authentication-plugin=mysql_native_password"
    volumes:
      - ./volumes/openmetadata-mysql:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${OPENMETADATA_MYSQL_ROOT_PASSWORD:-password}"]
      interval: 10s
      timeout: 5s
      retries: 10

  openmetadata-elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:${OPENMETADATA_ES_VERSION:-8.10.2}
    container_name: openmetadata-elasticsearch
    restart: unless-stopped
    profiles: ["catalog", "all"]
    environment:
      discovery.type: single-node
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
      xpack.security.transport.ssl.enabled: "false"
    volumes:
      - ./volumes/openmetadata-elasticsearch:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 10s
      timeout: 5s
      retries: 10

  openmetadata-migrate:
    image: openmetadata/server:${OPENMETADATA_VERSION:-1.6.1}
    container_name: openmetadata-migrate
    profiles: ["catalog", "all"]
    command: ["./bootstrap/openmetadata-ops.sh", "migrate"]
    environment:
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_SCHEME: mysql
      DB_USE_SSL: "false"
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
      DB_USER: ${OPENMETADATA_MYSQL_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${OPENMETADATA_MYSQL_PASSWORD:-openmetadata_password}
      OM_DATABASE: ${OPENMETADATA_MYSQL_DATABASE:-openmetadata_db}
      MIGRATION_LIMIT_PARAM: 1200
    depends_on:
      openmetadata-mysql:
        condition: service_healthy

  openmetadata-server:
    image: openmetadata/server:${OPENMETADATA_VERSION:-1.6.1}
    container_name: openmetadata-server
    restart: unless-stopped
    profiles: ["catalog", "all"]
    environment:
      # Database configuration
      DB_DRIVER_CLASS: com.mysql.cj.jdbc.Driver
      DB_SCHEME: mysql
      DB_USE_SSL: "false"
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
      DB_USER: ${OPENMETADATA_MYSQL_USER:-openmetadata_user}
      DB_USER_PASSWORD: ${OPENMETADATA_MYSQL_PASSWORD:-openmetadata_password}
      OM_DATABASE: ${OPENMETADATA_MYSQL_DATABASE:-openmetadata_db}
      # Elasticsearch configuration
      ELASTICSEARCH_HOST: openmetadata-elasticsearch
      ELASTICSEARCH_PORT: 9200
      ELASTICSEARCH_SCHEME: http
      ELASTICSEARCH_USER: ""
      ELASTICSEARCH_PASSWORD: ""
      # OpenMetadata configuration
      OPENMETADATA_ADMIN_USERNAME: ${OPENMETADATA_ADMIN_USERNAME:-admin}
      OPENMETADATA_ADMIN_PASSWORD: ${OPENMETADATA_ADMIN_PASSWORD:-admin}
      SERVER_PORT: 8585
      # JWT configuration
      AUTHORIZER_CLASS_NAME: org.openmetadata.service.security.DefaultAuthorizer
      AUTHORIZER_REQUEST_FILTER: org.openmetadata.service.security.JwtFilter
      AUTHORIZER_ADMIN_PRINCIPALS: '[admin]'
      AUTHORIZER_PRINCIPAL_DOMAIN: open-metadata.org
      AUTHENTICATION_PROVIDER: basic
      AUTHENTICATION_PUBLIC_KEYS: '[http://openmetadata-server:8585/api/v1/system/config/jwks]'
      AUTHENTICATION_AUTHORITY: https://accounts.google.com
      AUTHENTICATION_CLIENT_ID: ""
      AUTHENTICATION_CALLBACK_URL: ""
      # Airflow/Pipeline Service configuration
      PIPELINE_SERVICE_CLIENT_ENABLED: "true"
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient
      PIPELINE_SERVICE_CLIENT_ENDPOINT: http://openmetadata-ingestion:8080
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: no-ssl
      PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ""
      SERVER_HOST_API_URL: http://openmetadata-server:8585/api
      AIRFLOW_USERNAME: admin
      AIRFLOW_PASSWORD: admin
      AIRFLOW_TIMEOUT: 10
      # Java heap memory configuration
      OPENMETADATA_HEAP_OPTS: "-Xmx4G -Xms4G"
    ports:
      - "${OPENMETADATA_PORT:-10020}:8585"
    # volumes:
    #   - ./volumes/openmetadata-server:/opt/openmetadata
    depends_on:
      openmetadata-mysql:
        condition: service_healthy
      openmetadata-elasticsearch:
        condition: service_healthy
      openmetadata-migrate:
        condition: service_completed_successfully
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8585/api/v1/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s

  openmetadata-ingestion:
    image: openmetadata/ingestion:${OPENMETADATA_VERSION:-1.6.1}
    container_name: openmetadata-ingestion
    restart: unless-stopped
    profiles: ["catalog", "all"]
    command: ["airflow", "standalone"]
    environment:
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: /opt/airflow/dags
      AIRFLOW_DB: ${OPENMETADATA_MYSQL_DATABASE:-openmetadata_db}
      DB_HOST: openmetadata-mysql
      DB_PORT: 3306
      DB_USER: ${OPENMETADATA_MYSQL_USER:-openmetadata_user}
      DB_PASSWORD: ${OPENMETADATA_MYSQL_PASSWORD:-openmetadata_password}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mysql+pymysql://${OPENMETADATA_MYSQL_USER:-openmetadata_user}:${OPENMETADATA_MYSQL_PASSWORD:-openmetadata_password}@openmetadata-mysql:3306/${OPENMETADATA_MYSQL_DATABASE:-openmetadata_db}
      SERVER_HOST_API_URL: http://openmetadata-server:8585/api
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
    ports:
      - "8080:8080"
    volumes:
      - ./volumes/openmetadata-ingestion-logs:/opt/airflow/logs
      - ./volumes/openmetadata-ingestion-dags:/opt/airflow/dags
      - ./volumes/openmetadata-ingestion-config:/opt/airflow/config
      - ./transforms/dbt:/dbt:ro  # Mount dbt project for metadata ingestion
    depends_on:
      openmetadata-server:
        condition: service_started
      openmetadata-mysql:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
