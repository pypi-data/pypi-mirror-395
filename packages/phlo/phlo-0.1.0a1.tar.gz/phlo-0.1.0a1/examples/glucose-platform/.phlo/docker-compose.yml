# Phlo Infrastructure Stack
# Generated by: phlo services init
# Complete data lakehouse platform with orchestration and BI

services:
  # --- Storage Layer ---
  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - ./volumes/postgres:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-phlo}"]
      interval: 10s
      timeout: 5s
      retries: 10

  minio:
    image: minio/minio:RELEASE.2025-09-07T16-13-09Z
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
    command: ["server", "/data", "--console-address", ":9001"]
    ports:
      - "${MINIO_API_PORT:-9000}:9000"
      - "${MINIO_CONSOLE_PORT:-9001}:9001"
    volumes:
      - ./volumes/minio:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/ready"]
      interval: 10s
      timeout: 5s
      retries: 10

  minio-setup:
    image: minio/mc:latest
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minio}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
    entrypoint: >
      /bin/sh -c "
      sleep 5 &&
      mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD} &&
      mc mb --ignore-existing myminio/lake &&
      mc mb --ignore-existing myminio/lake/warehouse &&
      mc mb --ignore-existing myminio/lake/stage &&
      echo 'Buckets created successfully'
      "
    depends_on:
      minio:
        condition: service_healthy
    restart: "no"

  # --- Iceberg Catalog ---
  nessie:
    image: ghcr.io/projectnessie/nessie:${NESSIE_VERSION:-0.99.0}
    restart: unless-stopped
    environment:
      NESSIE_VERSION_STORE_TYPE: JDBC
      QUARKUS_DATASOURCE_JDBC_URL: jdbc:postgresql://postgres:5432/${POSTGRES_DB:-phlo}
      QUARKUS_DATASOURCE_USERNAME: ${POSTGRES_USER:-phlo}
      QUARKUS_DATASOURCE_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      nessie.catalog.default-warehouse: warehouse
      nessie.catalog.warehouses.warehouse.location: s3://lake/warehouse
      nessie.catalog.service.s3.default-options.endpoint: http://minio:9000/
      nessie.catalog.service.s3.default-options.path-style-access: "true"
      nessie.catalog.service.s3.default-options.region: us-east-1
      nessie.catalog.service.s3.default-options.access-key: urn:nessie-secret:quarkus:nessie.catalog.secrets.access-key
      nessie.catalog.secrets.access-key.name: ${MINIO_ROOT_USER:-minio}
      nessie.catalog.secrets.access-key.secret: ${MINIO_ROOT_PASSWORD:-minio123}
    ports:
      - "${NESSIE_PORT:-19120}:19120"
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:19120/api/v1/config"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  # --- Query Engine ---
  trino:
    image: trinodb/trino:${TRINO_VERSION:-467}
    restart: unless-stopped
    ports:
      - "${TRINO_PORT:-8080}:8080"
    volumes:
      - ./trino/catalog:/etc/trino/catalog:ro
    environment:
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
      AWS_REGION: us-east-1
      S3_ENDPOINT: http://minio:9000
      S3_PATH_STYLE_ACCESS: "true"
    depends_on:
      nessie:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 45s

  # --- Orchestration ---
  dagster-webserver:
    image: phlo/dagster:${DAGSTER_VERSION:-latest}
    build:
      context: .
      dockerfile: dagster/Dockerfile
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    restart: unless-stopped
    environment:
      DAGSTER_HOME: /opt/dagster
      PYTHONPATH: /opt/dagster:/app
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
      AWS_REGION: us-east-1
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_S3_USE_SSL: "false"
      AWS_S3_URL_STYLE: "path"
      NESSIE_HOST: nessie
      NESSIE_PORT: 19120
      TRINO_HOST: trino
      TRINO_PORT: 8080
      TRINO_CATALOG: iceberg
      ICEBERG_WAREHOUSE_PATH: s3://lake/warehouse
      ICEBERG_STAGING_PATH: s3://lake/stage
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      WORKFLOWS_PATH: /app/workflows
      CASCADE_HOST_PLATFORM: ${CASCADE_HOST_PLATFORM:-$$(uname -s)}
      DBT_PROJECT_DIR: /app/transforms/dbt
    command: ["dagster-webserver", "-h", "0.0.0.0", "-p", "3000", "-w", "/opt/dagster/workspace.yaml"]
    ports:
      - "${DAGSTER_PORT:-3000}:3000"
    volumes:
      - ./dagster:/opt/dagster
      - ../workflows:/app/workflows:ro
      - ../transforms:/app/transforms:ro
      - ../tests:/app/tests:ro
    depends_on:
      minio:
        condition: service_healthy
      minio-setup:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      nessie:
        condition: service_healthy
      trino:
        condition: service_healthy

  dagster-daemon:
    image: phlo/dagster:${DAGSTER_VERSION:-latest}
    build:
      context: .
      dockerfile: dagster/Dockerfile
      args:
        GITHUB_TOKEN: ${GITHUB_TOKEN:-}
    restart: unless-stopped
    environment:
      DAGSTER_HOME: /opt/dagster
      PYTHONPATH: /opt/dagster:/app
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-minio}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-minio123}
      AWS_REGION: us-east-1
      AWS_S3_ENDPOINT: http://minio:9000
      AWS_S3_USE_SSL: "false"
      AWS_S3_URL_STYLE: "path"
      NESSIE_HOST: nessie
      NESSIE_PORT: 19120
      TRINO_HOST: trino
      TRINO_PORT: 8080
      TRINO_CATALOG: iceberg
      ICEBERG_WAREHOUSE_PATH: s3://lake/warehouse
      ICEBERG_STAGING_PATH: s3://lake/stage
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minio123}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      WORKFLOWS_PATH: /app/workflows
      CASCADE_HOST_PLATFORM: ${CASCADE_HOST_PLATFORM:-$$(uname -s)}
      DBT_PROJECT_DIR: /app/transforms/dbt
    command: ["dagster-daemon", "run", "-w", "/opt/dagster/workspace.yaml"]
    volumes:
      - ./dagster:/opt/dagster
      - ../workflows:/app/workflows:ro
      - ../transforms:/app/transforms:ro
      - ../tests:/app/tests:ro
    depends_on:
      dagster-webserver:
        condition: service_started

  # --- Business Intelligence ---
  superset:
    image: apache/superset:${SUPERSET_VERSION:-4.0.0}
    restart: unless-stopped
    environment:
      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-phlo-superset-secret-change-me}
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      POSTGRES_USER: ${POSTGRES_USER:-phlo}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-phlo}
      POSTGRES_DB: ${POSTGRES_DB:-phlo}
      SUPERSET_ADMIN_USER: ${SUPERSET_ADMIN_USER:-admin}
      SUPERSET_ADMIN_PASSWORD: ${SUPERSET_ADMIN_PASSWORD:-admin}
      SUPERSET_ADMIN_EMAIL: ${SUPERSET_ADMIN_EMAIL:-admin@example.com}
    command: >
      /bin/sh -c "
      superset db upgrade &&
      superset fab create-admin --username $${SUPERSET_ADMIN_USER} --firstname Admin --lastname User --email $${SUPERSET_ADMIN_EMAIL} --password $${SUPERSET_ADMIN_PASSWORD} || true &&
      superset init &&
      superset run -h 0.0.0.0 -p 8088 --with-threads
      "
    ports:
      - "${SUPERSET_PORT:-8088}:8088"
    volumes:
      - ./volumes/superset:/app/superset_home
    depends_on:
      postgres:
        condition: service_healthy
      trino:
        condition: service_healthy

  # --- Database Admin ---
  pgweb:
    image: sosedoff/pgweb
    restart: unless-stopped
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-phlo}:${POSTGRES_PASSWORD:-phlo}@postgres:5432/${POSTGRES_DB:-phlo}?sslmode=disable
    ports:
      - "${PGWEB_PORT:-8081}:8081"
    depends_on:
      postgres:
        condition: service_healthy

  # --- Observability (optional, use --profile observability) ---
  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-v3.1.0}
    restart: unless-stopped
    profiles: ["observability", "all"]
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.enable-lifecycle"
      - "--storage.tsdb.retention.time=30d"
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./prometheus:/etc/prometheus:ro
      - ./volumes/prometheus:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5

  loki:
    image: grafana/loki:${LOKI_VERSION:-3.2.1}
    restart: unless-stopped
    profiles: ["observability", "all"]
    command: -config.file=/etc/loki/loki-config.yml
    ports:
      - "${LOKI_PORT:-3100}:3100"
    volumes:
      - ./loki:/etc/loki:ro
      - ./volumes/loki:/loki
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-11.3.1}
    restart: unless-stopped
    profiles: ["observability", "all"]
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
    ports:
      - "${GRAFANA_PORT:-3003}:3000"
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./volumes/grafana:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  # --- API Layer (optional, use --profile api) ---
  postgrest:
    image: postgrest/postgrest:${POSTGREST_VERSION:-v12.2.3}
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      PGRST_DB_URI: postgresql://${POSTGRES_USER:-phlo}:${POSTGRES_PASSWORD:-phlo}@postgres:5432/${POSTGRES_DB:-phlo}
      PGRST_DB_SCHEMAS: api,public
      PGRST_DB_ANON_ROLE: ${POSTGRES_USER:-phlo}
      PGRST_SERVER_HOST: 0.0.0.0
      PGRST_SERVER_PORT: 3000
      PGRST_OPENAPI_SERVER_PROXY_URI: http://localhost:${POSTGREST_PORT:-3002}
    ports:
      - "${POSTGREST_PORT:-3002}:3000"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/"]
      interval: 10s
      timeout: 5s
      retries: 5

  hasura:
    image: hasura/graphql-engine:${HASURA_VERSION:-v2.46.0}
    restart: unless-stopped
    profiles: ["api", "all"]
    environment:
      HASURA_GRAPHQL_DATABASE_URL: postgresql://${POSTGRES_USER:-phlo}:${POSTGRES_PASSWORD:-phlo}@postgres:5432/${POSTGRES_DB:-phlo}
      HASURA_GRAPHQL_ENABLE_CONSOLE: "true"
      HASURA_GRAPHQL_DEV_MODE: "true"
      HASURA_GRAPHQL_ENABLED_LOG_TYPES: startup, http-log, webhook-log, websocket-log, query-log
      HASURA_GRAPHQL_ADMIN_SECRET: ${HASURA_ADMIN_SECRET:-phlo-hasura-admin-secret}
      HASURA_GRAPHQL_UNAUTHORIZED_ROLE: anonymous
    ports:
      - "${HASURA_PORT:-8080}:8080"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
