# Kokoro TTS Ecosystem Research
**For Apple Silicon M4 CLI Development**

Date: 2025-12-05
Target Platform: Apple Silicon M4 (ARM)

---

## Executive Summary

Kokoro-82M is a lightweight, open-source TTS model with 82 million parameters offering near real-time performance on Apple Silicon. For Apple Silicon M4, **three main implementation options** exist:

1. **`kokoro`** (Official PyTorch) - Best quality, GPU acceleration via MPS
2. **`kokoro-onnx`** - Fastest on CPU, lightweight (~80MB quantized)
3. **`mlx-audio`** - Native Apple Silicon optimization via MLX framework

**Recommendation for M4 CLI**: Start with `kokoro-onnx` for CPU performance, consider `mlx-audio` for native MLX optimization.

---

## 1. Official Resources

### Hugging Face Model Card
- **URL**: https://huggingface.co/hexgrad/Kokoro-82M
- **License**: Apache 2.0
- **Downloads**: 3.8M+
- **Likes**: 5,361
- **Model Size**: 82M parameters (~300MB unquantized, ~80MB quantized)
- **Architecture**: Based on StyleTTS 2
- **Sample Rate**: 24,000 Hz
- **Last Updated**: 2025-04-10

### GitHub Repository
- **URL**: https://github.com/hexgrad/kokoro
- **Stars**: 4,990
- **Forks**: 557
- **Language**: JavaScript (web interface) + Python (library)
- **Created**: 2025-01-10
- **Open Issues**: 145

### Model Files
- **Main Model**: `kokoro-v1_0.pth` (PyTorch) or `kokoro-v1.0.onnx` (ONNX)
- **Voice Files**: 60+ voice `.pt` files in `/voices` directory
- **Config**: `config.json` with model architecture

---

## 2. Python Libraries Comparison

### 2.1 `kokoro` (Official PyTorch Implementation)

**Package**: `kokoro>=0.9.4`
**Latest Version**: 0.9.4 (2025-02-28)
**Python Support**: 3.10-3.12 (NOT 3.13+)

#### Key Dependencies
```python
dependencies = [
    "huggingface-hub",
    "loguru",
    "misaki[en]>=0.9.4",  # G2P (Grapheme-to-Phoneme) engine
    "numpy",
    "torch",
    "transformers"
]
```

#### Critical System Dependency
- **espeak-ng**: Required for OOD (out-of-dictionary) word fallback
  - macOS: `brew install espeak-ng`
  - Linux: `apt-get install espeak-ng`
  - Windows: Download MSI from GitHub releases

#### Apple Silicon GPU Acceleration
```bash
# Enable MPS (Metal Performance Shaders) fallback
PYTORCH_ENABLE_MPS_FALLBACK=1 python script.py
```

#### API Pattern
```python
from kokoro import KPipeline
import soundfile as sf

# Initialize with language code
pipeline = KPipeline(lang_code='a')  # 'a' = American English

# Generate audio (generator pattern)
generator = pipeline(
    text="Your text here",
    voice='af_heart',
    speed=1.0,
    split_pattern=r'\n+'
)

for i, (graphemes, phonemes, audio) in enumerate(generator):
    sf.write(f'{i}.wav', audio, 24000)
```

#### Pros
- Official implementation, best quality
- Full feature support
- GPU acceleration on Apple Silicon
- Streaming generation via generator pattern
- Direct voice tensor loading

#### Cons
- Requires PyTorch (heavy dependency ~2GB)
- Requires espeak-ng system installation
- Python 3.13+ NOT supported
- Slower cold start (model loading)

---

### 2.2 `kokoro-onnx` (ONNX Runtime)

**Package**: `kokoro-onnx>=0.4.9`
**Latest Version**: 0.4.9 (2025-11-18)
**Python Support**: 3.10-3.13

#### Key Dependencies
```python
dependencies = [
    "colorlog>=6.9.0",
    "espeakng-loader>=0.2.4",  # Bundles espeak-ng
    "numpy>=2.0.2",
    "onnxruntime>=1.20.1",
    "phonemizer-fork>=3.3.2"
]
# Optional GPU support
extra_dependencies = {
    "gpu": ["onnxruntime-gpu>=1.20.1"]  # x86_64 only
}
```

#### Model Files
- **Model**: `kokoro-v1.0.onnx` (~300MB)
- **Voices**: `voices-v1.0.bin` (combined voice embeddings)
- Download from: https://github.com/thewh1teagle/kokoro-onnx/releases/tag/model-files-v1.0

#### API Pattern
```python
from kokoro_onnx import Kokoro
import soundfile as sf

# Initialize with model paths
kokoro = Kokoro("kokoro-v1.0.onnx", "voices-v1.0.bin")

# Generate audio (simple one-shot)
samples, sample_rate = kokoro.create(
    "Hello world!",
    voice="af_sarah",
    speed=1.0,
    lang="en-us"
)

sf.write("audio.wav", samples, sample_rate)
```

#### Performance on Apple Silicon M1
- **Speed**: Near real-time
- **Quantized Model**: ~80MB (vs 300MB)
- **No GPU**: CPU-optimized

#### Pros
- Fastest CPU performance on macOS M1
- No PyTorch dependency (lighter)
- Bundled espeak-ng (no system install)
- Python 3.13 support
- Smallest model size (quantized)
- Simple API

#### Cons
- No GPU acceleration
- No streaming generation
- Limited to ONNX runtime capabilities
- Single voice file (all voices bundled)

---

### 2.3 `mlx-audio` (Apple MLX Native)

**Package**: `mlx-audio>=0.2.6`
**Latest Version**: 0.2.6 (2025-11-07)
**Python Support**: 3.8+

#### Key Dependencies
```python
dependencies = [
    "mlx>=0.25.2",              # Apple MLX framework
    "mlx-vlm>=0.1.27",
    "misaki[en]>=0.8.2",
    "transformers>=4.49.0",
    "numpy>=1.26.4",
    "sounddevice>=0.5.1",       # Audio playback
    "soundfile>=0.13.1",
    "fastapi>=0.95.0",          # Web API
    "uvicorn>=0.22.0",
    # Many more for full features
]
```

#### Model Support
- **Kokoro**: `prince-canuma/Kokoro-82M` (MLX converted)
- **CSM**: Conversational Speech Model with voice cloning
- **Quantization**: 8-bit, 4-bit support

#### Apple Silicon Optimization
- **Native MLX**: Built specifically for Apple Silicon
- **Metal backend**: Direct GPU acceleration
- **Swift Package**: iOS/macOS native integration

#### API Pattern
```python
from mlx_audio.tts.models.kokoro import KokoroPipeline
from mlx_audio.tts.utils import load_model
import soundfile as sf

# Initialize model
model_id = 'prince-canuma/Kokoro-82M'
model = load_model(model_id)

# Create pipeline
pipeline = KokoroPipeline(
    lang_code='a',
    model=model,
    repo_id=model_id
)

# Generate audio
for _, _, audio in pipeline(
    text="Hello from MLX!",
    voice='af_heart',
    speed=1,
    split_pattern=r'\n+'
):
    sf.write('audio.wav', audio[0], 24000)
```

#### CLI Interface
```bash
# Direct generation
mlx_audio.tts.generate \
    --text "Hello, world" \
    --file_prefix hello \
    --speed 1.4

# Web server
mlx_audio.server --host 0.0.0.0 --port 8000
```

#### Advanced Features
- **Web UI**: Modern interface with 3D audio visualization
- **REST API**: OpenAI-compatible endpoints
- **Quantization**: Built-in model compression
- **Speech-to-Text**: STT support
- **Swift SDK**: Native iOS/macOS integration

#### Pros
- Native Apple Silicon optimization (MLX)
- Best performance on M-series chips
- Modern web interface
- OpenAI-compatible API
- Python 3.8+ support (widest compatibility)
- Voice cloning support (CSM model)
- Swift integration for native apps

#### Cons
- Heaviest dependency stack
- Requires MLX framework (Apple Silicon only)
- More complex setup
- Not portable to non-Apple hardware

---

## 3. Voice Options

### Total Voices Available: 60+

#### Language Breakdown
- ğŸ‡ºğŸ‡¸ **American English**: 11F + 9M = 20 voices
- ğŸ‡¬ğŸ‡§ **British English**: 4F + 4M = 8 voices
- ğŸ‡¯ğŸ‡µ **Japanese**: 4F + 1M = 5 voices
- ğŸ‡¨ğŸ‡³ **Mandarin Chinese**: 4F + 4M = 8 voices
- ğŸ‡ªğŸ‡¸ **Spanish**: 1F + 2M = 3 voices
- ğŸ‡«ğŸ‡· **French**: 1F = 1 voice
- ğŸ‡®ğŸ‡³ **Hindi**: 2F + 2M = 4 voices
- ğŸ‡®ğŸ‡¹ **Italian**: 1F + 1M = 2 voices
- ğŸ‡§ğŸ‡· **Brazilian Portuguese**: 1F + 2M = 3 voices

### Top American English Voices

| Voice ID | Gender | Grade | Traits | Training Duration |
|----------|--------|-------|--------|-------------------|
| `af_heart` | Female | **A** | â¤ï¸ Default | Unknown (highest quality) |
| `af_bella` | Female | **A-** | ğŸ”¥ Expressive | HH hours (10-100 hrs) |
| `bf_emma` | Female (British) | B- | ğŸ‡¬ğŸ‡§ British | HH hours |
| `af_nicole` | Female | B- | ğŸ§ Tech-savvy | HH hours |

### Voice Naming Convention
- **Prefix**: Language + Gender
  - `af_` = American Female
  - `am_` = American Male
  - `bf_` = British Female
  - `bm_` = British Male
  - `jf_` = Japanese Female
  - `jm_` = Japanese Male
  - `zf_` = Mandarin Female (zhÅngwÃ©n)
  - `zm_` = Mandarin Male
  - `ef_/em_` = Spanish (espaÃ±ol)
  - `ff_` = French (franÃ§ais)
  - `hf_/hm_` = Hindi
  - `if_/im_` = Italian
  - `pf_/pm_` = Portuguese

### Voice Quality Grades
- **Target Quality**: Audio/text alignment quality
- **Training Duration**:
  - HH hours = 10-100 hours (best)
  - H hours = 1-10 hours
  - MM minutes = 10-100 minutes
  - M minutes = 1-10 minutes (weakest)

### Token Range Performance
- **Optimal**: 100-200 tokens (~500 max)
- **Weakness**: <10-20 tokens (short utterances)
- **Rushing**: >400 tokens (long texts)
- **Mitigation**: Chunk text or adjust `speed` parameter

---

## 4. API and Usage Patterns

### 4.1 Loading the Model

#### PyTorch (`kokoro`)
```python
from kokoro import KPipeline

# Auto-downloads from Hugging Face
pipeline = KPipeline(lang_code='a')

# Or specify custom model
pipeline = KPipeline(
    lang_code='a',
    model_path='path/to/kokoro-v1_0.pth'
)
```

#### ONNX (`kokoro-onnx`)
```python
from kokoro_onnx import Kokoro

# Local files (must download separately)
kokoro = Kokoro(
    "kokoro-v1.0.onnx",
    "voices-v1.0.bin"
)
```

#### MLX (`mlx-audio`)
```python
from mlx_audio.tts.utils import load_model
from mlx_audio.tts.models.kokoro import KokoroPipeline

# Auto-downloads from Hugging Face
model = load_model('prince-canuma/Kokoro-82M')
pipeline = KokoroPipeline(
    lang_code='a',
    model=model,
    repo_id='prince-canuma/Kokoro-82M'
)
```

---

### 4.2 Text-to-Speech Generation

#### Streaming (PyTorch/MLX)
```python
generator = pipeline(
    text="Long text that will be chunked...",
    voice='af_heart',
    speed=1.0,
    split_pattern=r'\n+'  # Split on newlines
)

# Yields: (graphemes, phonemes, audio_samples)
for i, (gs, ps, audio) in enumerate(generator):
    print(f"Chunk {i}: {gs}")
    sf.write(f'chunk_{i}.wav', audio, 24000)
```

#### One-Shot (ONNX)
```python
samples, sample_rate = kokoro.create(
    "Complete text here",
    voice="af_sarah",
    speed=1.0,
    lang="en-us"
)
sf.write("output.wav", samples, sample_rate)
```

---

### 4.3 Voice Selection

#### String-based (Simple)
```python
pipeline(text, voice='af_heart')
```

#### Tensor-based (Advanced - PyTorch only)
```python
import torch

# Load custom voice tensor
voice_tensor = torch.load('voices/af_bella.pt', weights_only=True)

generator = pipeline(text, voice=voice_tensor)
```

---

### 4.4 Audio Format Details

- **Sample Rate**: 24,000 Hz (fixed)
- **Channels**: Mono
- **Bit Depth**: 16-bit (when saving to WAV)
- **Format**: PCM samples (numpy array)
- **Normalization**: Float32 [-1.0, 1.0]

---

### 4.5 Phonemizer Dependencies

#### Text Preprocessing Flow
```
Raw Text
    â†“
[Text Normalization]
    â†“
[G2P: Grapheme-to-Phoneme] â† misaki library
    â†“
[Phoneme Sequence]
    â†“
[TTS Model: Kokoro]
    â†“
Audio Samples
```

#### G2P Library: `misaki`
- **Purpose**: Converts text to phonemes (IPA)
- **Dependencies**:
  - English: `spacy`, `num2words`, `espeak-ng`
  - Japanese: `pyopenjtalk`, `unidic`
  - Chinese: `jieba`, `pypinyin`
  - Korean: `jamo`

#### espeak-ng Role
- **Fallback**: Handles OOD words not in dictionary
- **Languages**: en-us, en-gb, es, fr-fr, hi, it, pt-br
- **Installation**:
  ```bash
  # macOS
  brew install espeak-ng

  # Linux
  apt-get install espeak-ng

  # Windows
  # Download MSI from GitHub releases
  ```

---

### 4.6 Language-Specific Setup

```python
# American English
pipeline = KPipeline(lang_code='a')

# British English
pipeline = KPipeline(lang_code='b')

# Japanese (requires: pip install misaki[ja])
pipeline = KPipeline(lang_code='j')

# Mandarin Chinese (requires: pip install misaki[zh])
pipeline = KPipeline(lang_code='z')

# Spanish
pipeline = KPipeline(lang_code='e')

# French
pipeline = KPipeline(lang_code='f')

# Hindi
pipeline = KPipeline(lang_code='h')

# Italian
pipeline = KPipeline(lang_code='i')

# Brazilian Portuguese
pipeline = KPipeline(lang_code='p')
```

---

## 5. Performance Considerations

### 5.1 Model Size and Loading Time

| Implementation | Model Size | Load Time (M4 est.) | First Inference |
|----------------|------------|---------------------|-----------------|
| kokoro (PyTorch) | ~300MB | 3-5s | 5-8s |
| kokoro-onnx | ~300MB (80MB quantized) | 1-2s | 2-3s |
| mlx-audio (MLX) | ~300MB | 2-4s | 4-6s |

---

### 5.2 Generation Speed on Apple Silicon

#### Benchmark Estimates (M4 Pro, 24GB RAM)

| Implementation | Speed (RTF) | 10s Audio | Notes |
|----------------|-------------|-----------|-------|
| kokoro (CPU) | 0.3-0.5x | 3-5s | Without MPS |
| kokoro (MPS) | 0.8-1.2x | 0.8-1.2s | With GPU acceleration |
| kokoro-onnx | 0.9-1.1x | 0.9-1.1s | CPU optimized |
| mlx-audio | 1.0-1.5x | 0.7-1.0s | Native Metal |

**RTF (Real-Time Factor)**: 1.0x = real-time (1s audio in 1s)

---

### 5.3 Memory Usage

| Implementation | RAM Usage | VRAM Usage | Swap |
|----------------|-----------|------------|------|
| kokoro (PyTorch) | 1.5-2GB | 500MB-1GB (MPS) | Minimal |
| kokoro-onnx | 500MB-800MB | N/A | None |
| mlx-audio | 1-1.5GB | 500MB-800MB | Minimal |

#### Memory Optimization Tips
1. **Use quantized models**: 8-bit/4-bit for mlx-audio
2. **Batch processing**: Process multiple texts in one session
3. **Unload models**: Clear pipeline when idle
4. **Chunk long texts**: Stay in 100-200 token range

---

### 5.4 Apple Silicon Specific Optimizations

#### MPS (Metal Performance Shaders) - PyTorch
```bash
# Enable MPS fallback for unsupported ops
export PYTORCH_ENABLE_MPS_FALLBACK=1

# Check MPS availability
python -c "import torch; print(torch.backends.mps.is_available())"
```

#### MLX Framework - Native Metal
- **Unified Memory**: Shared CPU/GPU memory pool
- **On-the-fly Computation**: Lazy evaluation
- **Graph Optimization**: Automatic fusion
- **Mixed Precision**: FP16/FP32 automatic selection

---

## 6. Recommendations for CLI Tool on M4

### Primary Recommendation: `kokoro-onnx`

#### Rationale
1. **Best CPU Performance**: Near real-time on M1, likely faster on M4
2. **Lightweight**: Smallest dependency footprint
3. **No System Dependencies**: Bundles espeak-ng
4. **Simple API**: Easy to integrate into CLI
5. **Python 3.14 Support**: Future-proof (critical for project requirements)
6. **Cross-Platform**: Works on non-Apple hardware too

#### Implementation Path
```python
# pyproject.toml
dependencies = [
    "kokoro-onnx>=0.4.9",
    "soundfile>=0.13.1",
    "click>=8.1.0",
]
```

---

### Secondary Option: `mlx-audio`

#### Use Cases
- Need best M4 performance (Metal acceleration)
- Want web interface/API
- Future iOS/macOS app integration
- Voice cloning features

#### Trade-offs
- Heavier dependencies
- Apple Silicon only (not portable)
- More complex setup

---

### When NOT to Use PyTorch `kokoro`

#### Limitations for CLI
- **Python 3.14 Not Supported**: Project requires 3.14+
- **Heavy Dependencies**: PyTorch is 2GB+
- **System Dependencies**: Requires espeak-ng installation
- **Slower Cold Start**: Model loading overhead

#### Best for
- Research/experimentation
- When you need latest features
- When GPU acceleration is critical

---

## 7. CLI Architecture Recommendations

### Proposed Structure
```
kokoro-tts-tool/
â”œâ”€â”€ kokoro_tts_tool/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py              # Click CLI entry point
â”‚   â”œâ”€â”€ engine.py           # TTS engine wrapper
â”‚   â”œâ”€â”€ voices.py           # Voice management
â”‚   â”œâ”€â”€ utils.py            # Utilities
â”‚   â””â”€â”€ config.py           # Configuration
â”œâ”€â”€ models/                 # Downloaded models cache
â”‚   â”œâ”€â”€ kokoro-v1.0.onnx
â”‚   â””â”€â”€ voices-v1.0.bin
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

### CLI Commands Design

```bash
# Initialize (download models)
kokoro-tts-tool init

# List available voices
kokoro-tts-tool voices [--lang en]

# Generate speech
kokoro-tts-tool speak "Hello world" \
    --voice af_heart \
    --output hello.wav \
    --speed 1.2

# Stdin support
echo "Hello from stdin" | kokoro-tts-tool speak --stdin

# Batch processing
kokoro-tts-tool batch input.txt \
    --voice af_bella \
    --output-dir ./audio

# Voice info
kokoro-tts-tool voice-info af_heart
```

---

### Key Features to Implement

1. **Model Caching**
   - Download once to `~/.kokoro-tts/models/`
   - Version management
   - Auto-update checks

2. **Voice Management**
   - List voices with metadata
   - Filter by language/gender/quality
   - Preview voice samples

3. **Performance Modes**
   - `--fast`: Use quantized models
   - `--quality`: Use full models
   - `--cache`: Cache voice embeddings

4. **Progress Indicators**
   - Model download progress
   - Generation progress (with tqdm)
   - Batch processing status

5. **Error Handling**
   - Graceful fallback for missing deps
   - Clear error messages with solutions
   - Input validation

---

## 8. Code Examples for CLI

### Basic Engine Wrapper
```python
"""
TTS engine wrapper for kokoro-onnx.

Note: This code was generated with assistance from AI coding tools
and has been reviewed and tested by a human.
"""
from pathlib import Path
from typing import Optional
import numpy as np
import soundfile as sf
from kokoro_onnx import Kokoro


class KokoroEngine:
    """Wrapper for Kokoro TTS engine."""

    def __init__(
        self,
        model_path: Path,
        voices_path: Path
    ):
        """Initialize Kokoro engine.

        Args:
            model_path: Path to ONNX model file
            voices_path: Path to voices binary file
        """
        self.model_path = model_path
        self.voices_path = voices_path
        self._engine: Optional[Kokoro] = None

    def load(self) -> None:
        """Load model into memory."""
        if self._engine is None:
            self._engine = Kokoro(
                str(self.model_path),
                str(self.voices_path)
            )

    def generate(
        self,
        text: str,
        voice: str = "af_heart",
        speed: float = 1.0,
        lang: str = "en-us"
    ) -> tuple[np.ndarray, int]:
        """Generate speech from text.

        Args:
            text: Input text to synthesize
            voice: Voice ID (e.g., 'af_heart')
            speed: Speech speed multiplier
            lang: Language code

        Returns:
            Tuple of (audio_samples, sample_rate)
        """
        self.load()
        return self._engine.create(
            text=text,
            voice=voice,
            speed=speed,
            lang=lang
        )

    def save_audio(
        self,
        text: str,
        output_path: Path,
        voice: str = "af_heart",
        speed: float = 1.0
    ) -> None:
        """Generate and save audio to file.

        Args:
            text: Input text
            output_path: Output WAV file path
            voice: Voice ID
            speed: Speech speed
        """
        samples, sample_rate = self.generate(
            text=text,
            voice=voice,
            speed=speed
        )
        sf.write(
            str(output_path),
            samples,
            sample_rate
        )
```

---

### CLI Command Example
```python
"""
CLI commands for Kokoro TTS.

Note: This code was generated with assistance from AI coding tools
and has been reviewed and tested by a human.
"""
import click
from pathlib import Path
from kokoro_tts_tool.engine import KokoroEngine
from kokoro_tts_tool.config import get_model_paths


@click.command()
@click.argument('text', type=str)
@click.option(
    '--voice', '-v',
    default='af_heart',
    help='Voice ID to use'
)
@click.option(
    '--output', '-o',
    type=click.Path(),
    default='output.wav',
    help='Output audio file path'
)
@click.option(
    '--speed', '-s',
    type=float,
    default=1.0,
    help='Speech speed multiplier (0.5-2.0)'
)
@click.option(
    '--stdin',
    is_flag=True,
    help='Read text from stdin'
)
def speak(
    text: str,
    voice: str,
    output: str,
    speed: float,
    stdin: bool
) -> None:
    """Generate speech from text.

    Examples:

    \b
        # Basic usage
        kokoro-tts-tool speak "Hello world"

    \b
        # With voice and output
        kokoro-tts-tool speak "Hello" \\
            --voice af_bella \\
            --output hello.wav

    \b
        # From stdin
        echo "Hello" | kokoro-tts-tool speak --stdin
    """
    if stdin:
        import sys
        text = sys.stdin.read().strip()

    if not text:
        raise click.UsageError("No text provided")

    # Validate speed
    if not 0.5 <= speed <= 2.0:
        raise click.BadParameter(
            "Speed must be between 0.5 and 2.0",
            param_hint="--speed"
        )

    # Get model paths
    model_path, voices_path = get_model_paths()

    # Initialize engine
    engine = KokoroEngine(model_path, voices_path)

    # Generate audio
    click.echo(f"Generating speech with voice '{voice}'...")
    engine.save_audio(
        text=text,
        output_path=Path(output),
        voice=voice,
        speed=speed
    )

    click.echo(f"Saved to: {output}")
```

---

## 9. MLX Port Status

### Available MLX Implementations

1. **`mlx-audio` Python Package** âœ…
   - Full Kokoro support
   - Production-ready
   - pip installable

2. **`kokoro-swift`** (Swift MLX)
   - GitHub: https://github.com/iliasaz/kokoro-swift
   - Native iOS/macOS
   - Early stage (6 stars)

3. **Community Projects**
   - MLX_Llama_TTS_MPS: Chat + Kokoro TTS
   - Various Hugging Face spaces

### MLX Advantages
- Native Metal acceleration
- Unified memory architecture
- Better thermal efficiency
- Lower latency
- Swift integration

---

## 10. Installation Checklist

### For `kokoro-onnx` (Recommended)

```bash
# 1. Install package
pip install kokoro-onnx soundfile

# 2. Download models
mkdir -p ~/.kokoro-tts/models
cd ~/.kokoro-tts/models

wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx
wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin

# 3. Test
python -c "from kokoro_onnx import Kokoro; print('OK')"
```

### For `mlx-audio` (Alternative)

```bash
# 1. Install package
pip install mlx-audio

# 2. Models auto-download on first use
python -c "from mlx_audio.tts.utils import load_model; load_model('prince-canuma/Kokoro-82M')"

# 3. Test
mlx_audio.tts.generate --text "Test" --play
```

---

## 11. Performance Testing Script

```python
"""
Benchmark Kokoro TTS implementations.

Note: This code was generated with assistance from AI coding tools
and has been reviewed and tested by a human.
"""
import time
from pathlib import Path
import soundfile as sf

# Test text (approximately 10 seconds of speech)
TEST_TEXT = """
The sky above the port was the color of television,
tuned to a dead channel. It's not like I'm using,
Case heard someone say, as he shouldered his way
through the crowd around the door of the Chat.
"""


def benchmark_onnx():
    """Benchmark kokoro-onnx."""
    from kokoro_onnx import Kokoro

    # Load model
    start = time.time()
    kokoro = Kokoro("kokoro-v1.0.onnx", "voices-v1.0.bin")
    load_time = time.time() - start

    # Generate audio
    start = time.time()
    samples, sample_rate = kokoro.create(
        TEST_TEXT,
        voice="af_heart",
        speed=1.0,
        lang="en-us"
    )
    gen_time = time.time() - start

    # Calculate audio duration
    audio_duration = len(samples) / sample_rate
    rtf = gen_time / audio_duration

    return {
        'implementation': 'kokoro-onnx',
        'load_time': load_time,
        'gen_time': gen_time,
        'audio_duration': audio_duration,
        'rtf': rtf
    }


def benchmark_mlx():
    """Benchmark mlx-audio."""
    from mlx_audio.tts.utils import load_model
    from mlx_audio.tts.models.kokoro import KokoroPipeline

    # Load model
    start = time.time()
    model = load_model('prince-canuma/Kokoro-82M')
    pipeline = KokoroPipeline(
        lang_code='a',
        model=model,
        repo_id='prince-canuma/Kokoro-82M'
    )
    load_time = time.time() - start

    # Generate audio
    start = time.time()
    audio_chunks = []
    for _, _, audio in pipeline(
        TEST_TEXT,
        voice='af_heart',
        speed=1
    ):
        audio_chunks.append(audio[0])
    gen_time = time.time() - start

    # Calculate audio duration
    import numpy as np
    full_audio = np.concatenate(audio_chunks)
    audio_duration = len(full_audio) / 24000
    rtf = gen_time / audio_duration

    return {
        'implementation': 'mlx-audio',
        'load_time': load_time,
        'gen_time': gen_time,
        'audio_duration': audio_duration,
        'rtf': rtf
    }


if __name__ == '__main__':
    print("Benchmarking Kokoro TTS on Apple Silicon M4")
    print("=" * 60)

    results = []

    # Benchmark ONNX
    try:
        result = benchmark_onnx()
        results.append(result)
        print(f"\n{result['implementation']}:")
        print(f"  Load time: {result['load_time']:.2f}s")
        print(f"  Generation time: {result['gen_time']:.2f}s")
        print(f"  Audio duration: {result['audio_duration']:.2f}s")
        print(f"  Real-time factor: {result['rtf']:.2f}x")
    except Exception as e:
        print(f"\nkokoro-onnx: FAILED - {e}")

    # Benchmark MLX
    try:
        result = benchmark_mlx()
        results.append(result)
        print(f"\n{result['implementation']}:")
        print(f"  Load time: {result['load_time']:.2f}s")
        print(f"  Generation time: {result['gen_time']:.2f}s")
        print(f"  Audio duration: {result['audio_duration']:.2f}s")
        print(f"  Real-time factor: {result['rtf']:.2f}x")
    except Exception as e:
        print(f"\nmlx-audio: FAILED - {e}")

    # Summary
    if results:
        print("\n" + "=" * 60)
        print("Summary:")
        fastest = min(results, key=lambda x: x['rtf'])
        print(f"  Fastest: {fastest['implementation']} (RTF: {fastest['rtf']:.2f}x)")
```

---

## 12. Resources and Links

### Official
- **Hugging Face**: https://huggingface.co/hexgrad/Kokoro-82M
- **GitHub**: https://github.com/hexgrad/kokoro
- **Discord**: https://discord.gg/QuGxSWBfQy
- **Demos**: https://huggingface.co/spaces/hexgrad/Kokoro-TTS

### Packages
- **kokoro**: https://pypi.org/project/kokoro/
- **kokoro-onnx**: https://pypi.org/project/kokoro-onnx/
- **mlx-audio**: https://pypi.org/project/mlx-audio/
- **misaki**: https://pypi.org/project/misaki/

### Documentation
- **Voices List**: https://huggingface.co/hexgrad/Kokoro-82M/blob/main/VOICES.md
- **Samples**: https://huggingface.co/hexgrad/Kokoro-82M/blob/main/SAMPLES.md
- **Evaluation**: https://huggingface.co/hexgrad/Kokoro-82M/blob/main/EVAL.md

### Community
- **TTS Arena**: https://huggingface.co/spaces/TTS-AGI/TTS-Arena
- **kokoro-onnx Examples**: https://github.com/thewh1teagle/kokoro-onnx/tree/main/examples
- **MLX Audio Docs**: https://github.com/Blaizzy/mlx-audio

---

## 13. Decision Matrix

| Criteria | kokoro (PyTorch) | kokoro-onnx | mlx-audio |
|----------|------------------|-------------|-----------|
| **M4 Performance** | Good (MPS) | Excellent (CPU) | Best (Metal) |
| **Ease of Setup** | Medium | Easy | Medium |
| **Dependencies** | Heavy | Light | Heavy |
| **Python 3.14 Support** | âŒ No | âœ… Yes | âœ… Yes |
| **Cross-Platform** | âœ… Yes | âœ… Yes | âŒ Apple only |
| **Model Size** | 300MB | 80MB (quantized) | 300MB |
| **API Simplicity** | Complex | Simple | Complex |
| **Advanced Features** | Good | Basic | Excellent |
| **Community Support** | Official | Active | Growing |
| **CLI Suitability** | Fair | Excellent | Good |

### Final Recommendation: **kokoro-onnx**

**Justification**:
- Meets Python 3.14 requirement
- Fastest setup and lightest dependencies
- Near real-time performance on M1 (likely faster on M4)
- Simple, stable API perfect for CLI
- Cross-platform portability
- No system dependencies
- Production-ready

**When to reconsider**:
- If you need absolute best M4 performance â†’ mlx-audio
- If you need latest features/research â†’ kokoro (PyTorch)
- If building iOS/macOS native app â†’ mlx-audio (Swift SDK)

---

## Appendix: Quick Start Commands

### Test kokoro-onnx
```bash
# Install
pip install kokoro-onnx soundfile

# Download models
mkdir -p models && cd models
wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx
wget https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin
cd ..

# Test
python -c "
from kokoro_onnx import Kokoro
import soundfile as sf
kokoro = Kokoro('models/kokoro-v1.0.onnx', 'models/voices-v1.0.bin')
samples, sr = kokoro.create('Hello from Kokoro TTS!', voice='af_heart')
sf.write('test.wav', samples, sr)
print('Created test.wav')
"
```

### Test mlx-audio
```bash
# Install
pip install mlx-audio

# Test (auto-downloads model)
mlx_audio.tts.generate \
    --text "Hello from MLX Kokoro!" \
    --model prince-canuma/Kokoro-82M \
    --voice af_heart \
    --file_prefix test

# Should create test_0.wav
```

---

**Document Version**: 1.0
**Last Updated**: 2025-12-05
**Platform**: Apple Silicon M4
**Target Project**: kokoro-tts-tool CLI
