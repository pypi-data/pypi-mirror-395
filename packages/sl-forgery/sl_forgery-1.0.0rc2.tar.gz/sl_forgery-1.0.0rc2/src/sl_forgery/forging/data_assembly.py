"""This module provides the assets used to aggregate the processed data from multiple sources into a unified Polars
dataframe that forms the basis of the Sun lab's analysis dataset hierarchy.
"""

from typing import Any
from pathlib import Path
from functools import partial
from concurrent.futures import ThreadPoolExecutor, as_completed

from tqdm import tqdm
from numba import njit
import numpy as np
import polars as pl
from numpy.typing import NDArray
from sl_shared_assets import MesoscopeHardwareState, MesoscopeExperimentConfiguration
from ataraxis_base_utilities import console, ensure_directory_exists

from .dataset import DatasetTypes
from ..shared_assets import interpolate_data


def _add_sls2p_fluorescence_column(
    df: pl.DataFrame, data_path: Path, filename: str, column_name: str, cell_mask: None | bool | NDArray[np.bool] = None
) -> pl.DataFrame:
    """Updates the input dataframe to include a column storing the fluorescence data loaded from the specified
    sl-suite2p .npy file.

    Args:
        df: The Polars dataframe to be updated with the loaded fluorescence data.
        data_path: The path to the directory that stores the fluorescence data to be loaded.
        filename: The name of the .npy file that stores the data to be loaded.
        column_name: The name of the column under which to store the extracted data in the modified dataframe.
        cell_mask: The optional boolean mask to filter out non-cell objects. Must match the shape of the fluorescence
            data vector.

    Returns:
        The dataframe that includes the loaded fluorescence data stored as an Array datatype under the specified column
        name.
    """
    # Extracts the data using memory-mapping.
    fluorescence = np.load(data_path.joinpath(filename), mmap_mode="r")

    # Applies the cell mask if provided
    if cell_mask is not None:
        fluorescence = fluorescence[cell_mask, :]

    # Gets ROI count for Array field width.
    roi_count = fluorescence.shape[0]

    # Transposes (to go from ROI, Frame to Frame, ROI) and converts to a list of lists
    # (required for conversion to the Polar Array type).
    fluorescence_lists = [row.tolist() for row in fluorescence.T.astype(np.float32)]

    # Updates the dataframe with the new column storing the extracted fluorescence data.
    return df.with_columns(
        [
            pl.Series(
                name=column_name,
                values=fluorescence_lists,
                dtype=pl.List(pl.Float32),  # Casts to float32 to match the original array's precision.
            ).list.to_array(width=roi_count)
        ]
    )


def _assemble_2p_fluorescence_dataset(session_data_path: Path, multiday_data_path: Path) -> pl.DataFrame:
    """Assembles the target session's 2-photon fluorescence dataset from the data generated by the sl-suite2p's
    single-day and multi-day processing pipelines.

    Args:
        session_data_path: The path to the session's processed data directory.
        multiday_data_path: The path to the session's multi-day data directory.

    Returns:
        The Polars DataFrame that contains the assembled single-day and multi-day cell fluorescence data.
    """
    # Resolves the path to the single-day mesoscope data directory
    single_day_data = session_data_path.joinpath("processed_data", "mesoscope_data", "suite2p", "combined")

    # Resolves the path to the behavior data directory
    behavior_data = session_data_path.joinpath("processed_data", "behavior_data")

    # Queries the number of frames processed by suite2p. This is used to handle rare cases where the log has
    # more frame stamps than recorded frames, which usually happens if the user manually triggers mesoscope scanning
    # (of any kind) outside the expected time slot. Specifically, if the log has more frames than the suite2p data, the
    # log is clipped at the front (since aberrant frames have to come from a period before the main experiment runtime)
    _, frames = np.load(single_day_data.joinpath("F.npy"), mmap_mode="r").shape

    # Loads the single-day 'ops' file and extracts the scanning frequency ('fs') from the dictionary. Uses it to
    # determine the minimum and maximum scan pulse duration in milliseconds. This is used to filter the logged scan
    # pulses to match expected scan durations.
    sd_ops = np.load(single_day_data.joinpath("ops.npy"), allow_pickle=True).item()
    scanning_frequency = sd_ops["fs"]
    min_duration = 1000 / scanning_frequency - 20
    max_duration = 1000 / scanning_frequency + 20

    # Loads the mesoscope frame acquisition timestamps collected by the microcontroller logging system during session's
    # data acquisition.
    mesoscope_frame_data = pl.read_ipc(
        behavior_data.joinpath("mesoscope_frame_data.feather"), use_pyarrow=True, memory_map=True, rechunk=True
    )

    # Sorts by time to ensure the correct order
    mesoscope_frame_data = mesoscope_frame_data.sort("time_us")

    # Creates pulse groups and find edges in one pass
    mesoscope_frame_data = mesoscope_frame_data.with_columns(
        [
            # Creates pulse ID by counting rising edges
            (pl.col("ttl_state").diff() == 1).fill_null(False).cum_sum().alias("pulse_id"),
            # Marks if this row is a rising edge
            (pl.col("ttl_state").diff() == 1).fill_null(False).alias("is_rising_edge"),
            # Marks if this row is a falling edge
            (pl.col("ttl_state").diff() == -1).fill_null(False).alias("is_falling_edge"),
        ]
    )

    # For each pulse, finds the rising and falling-edge times using window functions
    mesoscope_frame_data = mesoscope_frame_data.with_columns(
        [
            # Gets the time of the rising edge for this pulse
            pl.when(pl.col("is_rising_edge"))
            .then(pl.col("time_us"))
            .forward_fill()
            .over("pulse_id")
            .alias("pulse_start"),
            # Gets the time of the falling edge for this pulse
            pl.when(pl.col("is_falling_edge"))
            .then(pl.col("time_us"))
            .backward_fill()
            .over("pulse_id")
            .alias("pulse_end"),
        ]
    )

    # Calculates pulse duration for each recorded scan pulse
    mesoscope_frame_data = mesoscope_frame_data.with_columns(
        [((pl.col("pulse_end") - pl.col("pulse_start")) / 1000).alias("duration_ms")]
    )

    # Filters for valid duration pulses and gets unique frames
    frame_aligned_data: pl.DataFrame = (
        mesoscope_frame_data.filter(
            (pl.col("duration_ms") >= min_duration)
            & (pl.col("duration_ms") <= max_duration)
            & pl.col("duration_ms").is_not_null()
        )
        .group_by("pulse_id")
        .first()
        .select(
            [
                pl.col("pulse_id").alias("frame"),
                pl.col("pulse_start").alias("time_us"),
            ]
        )
        .sort("frame")
    )

    # If there are more frame pulses than expected based on the processed fluorescence data, keeps only the last
    # expected_frames number of datapoints
    original_frame_count = len(frame_aligned_data)
    if original_frame_count > frames:
        frame_aligned_data = frame_aligned_data.tail(frames)  # Keeps the last 'frames' number of pulses

    # Re-assigns frame IDs starting from 1 as unsigned 32-bit integers and adds a column that reflects the
    # within-session time as the number of minutes elapsed since session onset.
    frame_aligned_data = frame_aligned_data.with_columns(
        [
            pl.int_range(1, len(frame_aligned_data) + 1, dtype=pl.UInt32).alias("frame"),
            ((pl.col("time_us") - pl.col("time_us").min()) / (60 * 1_000_000)).round(2).alias("elapsed_minutes"),
        ]
    )

    # Uses the single-day cell classification data to create a filtering mask to exclude non-cells from the analysis
    # dataset.
    classification: NDArray[np.float64] = np.load(single_day_data.joinpath("iscell.npy"), mmap_mode="r")
    is_cell_mask = classification[:, 0] == 1

    # Extracts and adds single-day fluorescence data to each frame using cell classification to filter non-cell ROIs.
    single_day_files = [
        ("F.npy", "single_day_f"),
        ("Fneu.npy", "single_day_f_neuropil"),
        ("Fsub.npy", "single_day_dff"),
        ("spks.npy", "single_day_spikes"),
    ]
    for filename, column_name in single_day_files:
        frame_aligned_data = _add_sls2p_fluorescence_column(
            frame_aligned_data, single_day_data, filename, column_name, is_cell_mask
        )

    # Same as above, but adds the multi-day fluorescence data to each frame.
    multi_day_files = [
        ("F.npy", "multi_day_f"),
        ("Fneu.npy", "multi_day_f_neuropil"),
        ("Fsub.npy", "multi_day_dff"),
        ("spks.npy", "multi_day_spikes"),
    ]
    for filename, column_name in multi_day_files:
        frame_aligned_data = _add_sls2p_fluorescence_column(
            frame_aligned_data, multiday_data_path, filename, column_name, cell_mask=None
        )

    # Formats the elapsed minutes column to use fp32 precision and rounds to 2 decimal places.
    return frame_aligned_data.with_columns(pl.col("elapsed_minutes").cast(pl.Float32))


@njit(cache=True)
def _rectify_mesoscope_vr_experiment_state_assignment(
    experiment_states: NDArray[np.uint8], system_states: NDArray[np.uint8]
) -> NDArray[np.uint8]:
    """Fixes an issue with experiment state logging observed during interrupted experiment runtimes recorded with early
    sl-experiment library versions.

    This helper function fixes a coding error where interrupting and restarting an experiment runtime was logged as a
    sequence of 0-code values instead of the correct idle (0) and experiment (non-0) state code sequence.

    Notes:
        If the input experiment state sequence does not display the state logging error, the function returns it
        without any additional processing.

    Args:
        experiment_states: The incorrect experiment state sequence.
        system_states: The system state sequence used to determine when to apply the experiment state corrections.

    Returns:
        The rectified experiment state sequence.
    """
    # Due to memory mapping, this function needs to copy the data before modifying it.
    processed = experiment_states.copy()

    # Tracks the last non-zero experiment state
    last_nonzero = np.uint8(0)

    # Iterates over the sequence and fixes erroneous logging instances based on system states
    for i in range(len(experiment_states)):
        if experiment_states[i] != 0:
            # Updates the last non-zero when a non-zero experiment state is encountered
            last_nonzero = experiment_states[i]
        elif system_states[i] != 0 and last_nonzero != 0:
            # If the system state is non-zero and the experiment state is zero, sets the experiment state to the last
            # non-zero value
            processed[i] = last_nonzero

    return processed


@njit(cache=True)
def _check_reward_zones(
    traversed_distance: NDArray[np.float64],
    reward_zone_starts: NDArray[np.float64],
    reward_zone_ends: NDArray[np.float64],
):
    """Uses the provided reward zone boundary data to determine which portion of the processed runtime data corresponds
    to the animal traversing the reward zone.

    Args:
        traversed_distance: The NumPy array containing the cumulative distance traveled by the animal during the
            experiment at each sampling time-point.
        reward_zone_starts: The NumPy array containing the reward zone start boundaries for each sequential
            experiment trial.
        reward_zone_ends: The NumPy array containing the reward zone end boundaries for each sequential experiment
            trial.

    Returns:
        A NumPy array that stores whether each distance-point corresponds to a reward zone (1) or not (0).
    """
    # Preallocates the output boolean array.
    distance_value_count = len(traversed_distance)
    reward_zone_count = len(reward_zone_starts)
    in_zone = np.zeros(distance_value_count, dtype=np.uint8)

    # If no reward zones are defined, returns the binary array set to 0 everywhere.
    if reward_zone_count == 0:
        return in_zone

    # Tracks the current zone being checked
    zone_index = 0

    # Determines whether each distance-point falls into a reward zone. Note, this relies on the distance and reward zone
    # data being sorted and monotonically increasing.
    for i in range(distance_value_count):
        evaluated_distance = traversed_distance[i]

        # Moves the zone_index backward if needed (handles slight non-monotonicity in the distance data).
        while zone_index > 0 and reward_zone_ends[zone_index - 1] >= evaluated_distance:
            zone_index -= 1

        # Checks zone boundaries starting from the current position (evaluated_distance) onward.
        while zone_index < reward_zone_count:
            # If the checked distance less than the start of the next reward zone, the distance is not within a reward
            # zone.
            if evaluated_distance < reward_zone_starts[zone_index]:
                break

            # If the distance falls within the reward zone, marks the corresponding mask point as 1 (in reward zone).
            if evaluated_distance <= reward_zone_ends[zone_index]:
                in_zone[i] = 1
                break

            # If the distance is past the evaluated reward zone, moves to the next zone.
            zone_index += 1

    return in_zone


def _mask_non_run_experiment_data(experiment_data: pl.DataFrame) -> pl.DataFrame:
    """Masks cue, trial, and trial_type column values with 255 for non-run system states.

    When the system is in a non-run state (idle or rest), the cue, trial, and trial_type values are not meaningful.
    This function replaces them with 255 to indicate invalid/masked data. For trial_type, the value is set to the
    "undefined" Enum member.

    Args:
        experiment_data: The experiment dataset containing system_state, cue, trial, and trial_type columns.

    Returns:
        The experiment dataset with cue, trial, and trial_type values masked for non-run system states.
    """
    # Extracts the Enum dtypes to ensure type consistency.
    trial_type_dtype = experiment_data.schema["trial_type"]
    system_state_dtype = experiment_data.schema["system_state"]

    # Defines the non-run system states that should trigger masking, cast to the Enum type.
    non_run_states = pl.Series(["idle", "rest"]).cast(system_state_dtype)

    # Creates a boolean mask for rows where the system state is not "run".
    is_non_run = pl.col("system_state").is_in(non_run_states)

    return experiment_data.with_columns(
        pl.when(is_non_run).then(pl.lit(255, dtype=pl.UInt8)).otherwise(pl.col("cue")).alias("cue"),
        pl.when(is_non_run).then(pl.lit(255, dtype=pl.UInt16)).otherwise(pl.col("trial")).alias("trial"),
        pl.when(is_non_run)
        .then(pl.lit("undefined").cast(trial_type_dtype))
        .otherwise(pl.col("trial_type"))
        .alias("trial_type"),
    )


def _assemble_experiment_dataset(session_data_path: Path, reference_time: NDArray[np.uint64]) -> pl.DataFrame:
    """Assembles the target session's experiment dataset from the experiment metadata generated by the sl-behavior
    processing pipeline.

    Args:
        session_data_path: The path to the session's processed data directory.
        reference_time: The reference time vector to which to align the assembled dataset.

    Returns:
        The Polars DataFrame that contains the assembled experiment metadata.
    """
    # Resolves the paths to the root data directories.
    behavior_data_path = session_data_path.joinpath("processed_data", "behavior_data")
    source_data_path = session_data_path.joinpath("source_data")

    # Loads experiment configuration early to have mappings ready.
    experiment_config = MesoscopeExperimentConfiguration.from_yaml(
        source_data_path.joinpath("experiment_configuration.yaml")
    )

    # Uses the experiment configuration file to map the integer trial type codes and experiment state codes to
    # descriptive names. Adds "undefined" as a special value for masking non-run experiment states.
    trial_type_mapping = {i: name for i, name in enumerate(experiment_config.trial_structures.keys())}
    trial_type_categories = list(trial_type_mapping.values()) + ["undefined"]
    trial_enum_dtype = pl.Enum(trial_type_categories)
    experiment_state_mapping = {
        state_config.experiment_state_code: state_name
        for state_name, state_config in experiment_config.experiment_states.items()
    }
    experiment_state_mapping[0] = "idle"  # Adds the default system state
    experiment_state_enum_dtype = pl.Enum(list(experiment_state_mapping.values()))

    # Loads all experiment data sources.
    encoder_df = pl.read_ipc(behavior_data_path.joinpath("encoder_data.feather"), use_pyarrow=True, memory_map=True)
    reward_zones_df = pl.read_ipc(
        behavior_data_path.joinpath("vr_reward_zone_data.feather"), use_pyarrow=True, memory_map=True
    )
    cue_df = pl.read_ipc(behavior_data_path.joinpath("vr_cue_data.feather"), use_pyarrow=True, memory_map=True)
    trial_df = pl.read_ipc(behavior_data_path.joinpath("trial_data.feather"), use_pyarrow=True, memory_map=True)
    experiment_state_df = pl.read_ipc(
        behavior_data_path.joinpath("experiment_state_data.feather"), use_pyarrow=True, memory_map=True
    )
    guidance_state_df = pl.read_ipc(
        behavior_data_path.joinpath("guidance_state_data.feather"), use_pyarrow=True, memory_map=True
    )
    system_state_df = pl.read_ipc(
        behavior_data_path.joinpath("system_state_data.feather"), use_pyarrow=True, memory_map=True
    )

    # Adds a trial number column to the trials dataframe.
    trial_df = trial_df.with_columns(pl.int_range(start=1, end=len(trial_df) + 1, dtype=pl.UInt32).alias("trial"))
    trial_distance = trial_df["traveled_distance_cm"].to_numpy()

    # Interpolates the traveled distance first as it's used as a reference for other interpolations.
    # noinspection PyTypeChecker
    reference_distance: NDArray[np.float64] = interpolate_data(
        source_coordinates=encoder_df["time_us"].to_numpy(),
        source_values=encoder_df["traveled_distance_cm"].to_numpy(),
        target_coordinates=reference_time,
        is_discrete=False,
    )

    # Aligns all data sources to the reference time (or distance) and builds an aligned data dictionary.
    aligned_data: dict[str, NDArray[Any]] = {
        # Distance-based interpolations
        "trial": interpolate_data(
            source_coordinates=trial_distance,
            source_values=trial_df["trial"].to_numpy(),
            target_coordinates=reference_distance,
            is_discrete=True,
        ),
        "trial_type": interpolate_data(
            source_coordinates=trial_distance,
            source_values=trial_df["trial_type_index"].to_numpy(),
            target_coordinates=reference_distance,
            is_discrete=True,
        ),
        "cue": interpolate_data(
            source_coordinates=cue_df["traveled_distance_cm"].to_numpy(),
            source_values=cue_df["vr_cue"].to_numpy(),
            target_coordinates=reference_distance,
            is_discrete=True,
        ),
        "in_reward_zone": _check_reward_zones(
            traversed_distance=reference_distance,
            reward_zone_starts=reward_zones_df["reward_zone_start_cm"].to_numpy(),
            reward_zone_ends=reward_zones_df["reward_zone_end_cm"].to_numpy(),
        ),
        # Time-based interpolations
        "experiment_state": interpolate_data(
            source_coordinates=experiment_state_df["time_us"].to_numpy(),
            source_values=_rectify_mesoscope_vr_experiment_state_assignment(
                experiment_states=experiment_state_df["experiment_state"].to_numpy(),
                system_states=system_state_df["system_state"].to_numpy(),
            ),
            target_coordinates=reference_time,
            is_discrete=True,
        ),
        "guided": interpolate_data(
            source_coordinates=guidance_state_df["time_us"].to_numpy(),
            source_values=guidance_state_df["lick_guidance_state"].to_numpy().astype(np.uint8),
            target_coordinates=reference_time,
            is_discrete=True,
        ),
    }

    # Creates the aligned dataframe, replaced categorical data with Polars Enum types and optimizes how the data is
    # stored in memory by casting some columns to preferred types.
    return pl.DataFrame(aligned_data).with_columns(
        [
            # Converts trial_type and experiment_state to Enum types
            pl.col("trial_type").replace_strict(trial_type_mapping).cast(trial_enum_dtype),
            pl.col("experiment_state").replace_strict(experiment_state_mapping).cast(experiment_state_enum_dtype),
            # Optimizes the trial column's datatype
            pl.col("trial").cast(pl.UInt16),
        ]
    )


@njit(cache=True)
def _calculate_running_speed(
    time: NDArray[np.uint64], distance: NDArray[np.float64], window_size_us: int = 100000
) -> NDArray[np.float64]:
    """Calculates the animal's running speed using the input data and the requested sliding window.

    Args:
        time: The sampling time, in microseconds elapsed since UTC epoch onset, for each cumulative traveled distance
            value.
        distance: The cumulative distance, in centimeters, traveled by the animal at each time-point.
        window_size_us: The size of the sliding window, microseconds.
    """
    # Preallocates the output running speed array based on the requested number of time-points for which to compute
    # the running speed.
    value_count = len(time)
    running_speed = np.zeros(value_count, dtype=np.float32)

    # If there are no data points to process, returns early.
    if value_count == 0:
        return running_speed

    # Pre-computes the microsecond-to-second conversion constant.
    us_to_s = np.float64(1.0 / 1_000_000.0)

    # Maintains a sliding window start index that advances monotonically through the data.
    # This avoids redundant searching and reduces complexity from O(nÂ²) to O(n).
    window_start_idx = 0

    # Processes each time point to calculate its running speed.
    for i in range(value_count):
        # Defines the target start time for the current window.
        window_start_time = time[i] - window_size_us

        # Advances the window start index until it reaches the first point within the window.
        while window_start_idx < i and time[window_start_idx] < window_start_time:
            window_start_idx += 1

        # Calculates the speed only if there are distinct distance-points in the window.
        if window_start_idx < i:
            time_delta = time[i] - time[window_start_idx]

            # Calculates the running speed using the full available window.
            if time_delta > 0:
                distance_delta = distance[i] - distance[window_start_idx]
                speed = distance_delta / (time_delta * us_to_s)
                # noinspection PyTypeChecker
                running_speed[i] = max(0.0, speed)

    return running_speed


def _assemble_behavior_dataset(
    session_data_path: Path, reference_time: NDArray[np.uint64], *, drop_time_columns: bool = False
) -> pl.DataFrame:
    """Assembles the target session's behavior dataset from the data generated by the sl-behavior processing pipeline.

    Args:
        session_data_path: The path to the session's processed data directory.
        reference_time: The reference time vector to which to align the assembled dataset.
        drop_time_columns: Determines whether to drop the 'time_us' and 'elapsed_minutes' columns from the assembled
            dataset before returning it to the caller. This option should be enabled if the time columns are resolved
            as part of a different dataset that is later combined with the behavior dataset.

    Returns:
        The Polars DataFrame that contains the assembled behavior data.
    """
    # Resolves the paths to the root data directories.
    behavior_data_path = session_data_path.joinpath("processed_data", "behavior_data")
    source_data_path = session_data_path.joinpath("source_data")

    # Loads hardware configuration and precreates the assets to map system state codes to descriptive names.
    hardware_state_data = MesoscopeHardwareState.from_yaml(source_data_path.joinpath("hardware_state.yaml"))
    state_mapping = hardware_state_data.system_state_codes
    inverted_mapping = {v: k for k, v in state_mapping.items()}
    state_enum = pl.Enum(list(state_mapping.keys()))

    # Loads the core behavior data present for all session types.
    valve_df = pl.read_ipc(behavior_data_path.joinpath("valve_data.feather"), use_pyarrow=True, memory_map=True)
    system_state_df = pl.read_ipc(
        behavior_data_path.joinpath("system_state_data.feather"), use_pyarrow=True, memory_map=True
    )
    lick_df = pl.read_ipc(behavior_data_path.joinpath("lick_data.feather"), use_pyarrow=True, memory_map=True)
    valve_time = valve_df["time_us"].to_numpy()

    # Creates the aligned data dictionary using the reference time vector and interpolating all other dat sources to
    # the reference time vector.
    aligned_data: dict[str, NDArray[Any]] = {
        "time_us": reference_time,
        "system_state": interpolate_data(
            source_coordinates=system_state_df["time_us"].to_numpy(),
            source_values=system_state_df["system_state"].to_numpy(),
            target_coordinates=reference_time,
            is_discrete=True,
        ),
        "lick": interpolate_data(
            source_coordinates=lick_df["time_us"].to_numpy(),
            source_values=lick_df["lick_state"].to_numpy(),
            target_coordinates=reference_time,
            is_discrete=True,
        ),
        "water_uL": interpolate_data(
            source_coordinates=valve_time,
            source_values=valve_df["dispensed_water_volume_uL"].to_numpy(),
            target_coordinates=reference_time,
            # Technically continuous, but uses power law, rather than a linear function, so using linear interpolation
            # is not correct here either.
            is_discrete=True,
        ).astype(np.float32),
        "_tone_state": interpolate_data(  # Temporary column for reward processing
            source_coordinates=valve_time,
            source_values=valve_df["tone_state"].to_numpy(),
            target_coordinates=reference_time,
            is_discrete=True,
        ),
    }

    # Encoder data. Is not present for lick training.
    if behavior_data_path.joinpath("encoder_data.feather").exists():
        encoder_df = pl.read_ipc(behavior_data_path.joinpath("encoder_data.feather"), use_pyarrow=True, memory_map=True)
        encoder_time = encoder_df["time_us"].to_numpy()
        encoder_distance = encoder_df["traveled_distance_cm"].to_numpy()

        # Calculates the running speed using the original encoder sampling rate
        running_speed = _calculate_running_speed(
            time=encoder_df["time_us"].to_numpy(),
            distance=encoder_df["traveled_distance_cm"].to_numpy(),
            window_size_us=100000,
        )

        # Downsamples the running speed and the traveled distance.
        aligned_data["distance_cm"] = interpolate_data(
            source_coordinates=encoder_time,
            source_values=encoder_distance,
            target_coordinates=reference_time,
            is_discrete=False,
        )
        aligned_data["speed_cm_s"] = interpolate_data(
            source_coordinates=encoder_time,
            source_values=running_speed,
            target_coordinates=reference_time,
            is_discrete=False,
        ).astype(np.float32)

    # Screen data. Only present for mesoscope experiments.
    if behavior_data_path.joinpath("screen_data.feather").exists():
        screen_df = pl.read_ipc(behavior_data_path.joinpath("screen_data.feather"), use_pyarrow=True, memory_map=True)
        aligned_data["screens"] = interpolate_data(
            source_coordinates=screen_df["time_us"].to_numpy(),
            source_values=screen_df["screen_state"].to_numpy(),
            target_coordinates=reference_time,
            is_discrete=True,
        )

    # Brake data. Only present for mesoscope experiments.
    # Note: Checks for both 'brake_data.feather' (correct) and 'break_data.feather' (legacy typo) for backward
    # compatibility. The legacy filename and column name support will be removed in a future version.
    brake_file = behavior_data_path.joinpath("brake_data.feather")
    if not brake_file.exists():
        brake_file = behavior_data_path.joinpath("break_data.feather")  # Legacy fallback
    if brake_file.exists():
        brake_df = pl.read_ipc(brake_file, use_pyarrow=True, memory_map=True)
        # Supports both the correct column name and legacy typo
        brake_torque_col = "brake_torque_N_cm" if "brake_torque_N_cm" in brake_df.columns else "break_torque_N_cm"
        brake_torque = interpolate_data(
            source_coordinates=brake_df["time_us"].to_numpy(),
            source_values=brake_df[brake_torque_col].to_numpy(),
            target_coordinates=reference_time,
            is_discrete=True,
        )
        # Uses default brake strength values if the hardware state file does not define them.
        min_brake_strength = hardware_state_data.minimum_brake_strength
        if min_brake_strength is None:
            min_brake_strength = 0.42383811
        aligned_data["brake"] = (brake_torque > min_brake_strength).astype(np.uint8)

    # Torque data. Is not present for run training.
    if behavior_data_path.joinpath("torque_data.feather").exists():
        torque_df = pl.read_ipc(behavior_data_path.joinpath("torque_data.feather"), use_pyarrow=True, memory_map=True)
        aligned_data["torque_N_cm"] = interpolate_data(
            source_coordinates=torque_df["time_us"].to_numpy(),
            source_values=torque_df["torque_N_cm"].to_numpy(),
            target_coordinates=reference_time,
            is_discrete=False,
        ).astype(np.float32)

    # Creates the aligned dataframe from the assembled data.
    behavior_data = pl.DataFrame(aligned_data)

    # Cleans up minor inconsistencies and data formatting issues that result from the interpolation process. Also
    # reformats certain data columns to improve future data analysis.
    behavior_data = (
        behavior_data
        # Convert system_state to an Enum type using the hardware state mapping resolved earlier.
        .with_columns(pl.col("system_state").replace_strict(inverted_mapping, return_dtype=pl.Utf8).cast(state_enum))
        # Adds a column that tracks elapsed session time in minutes.
        .with_columns(
            ((pl.col("time_us") - pl.col("time_us").min()) / (60 * 1_000_000))
            .round(2)
            .cast(pl.Float32)
            .alias("elapsed_minutes")
        )
        # Uses valve information to create a 'reward' Enum column that distinguishes reward, tone, and no reward events.
        .with_columns(
            # Creates reward event boundaries
            (pl.col("_tone_state") > 0).alias("_tone_active"),
        )
        # Uses the tone column to discover reward events.
        .with_columns(
            (pl.col("_tone_active") != pl.col("_tone_active").shift(1))
            .fill_null(False)
            .cum_sum()
            .alias("_reward_event_id")
        )
        .with_columns(
            # Calculates dispensed water volume per event and creates the reward column
            pl.col("water_uL").sum().over("_reward_event_id").alias("_reward_event_water_uL"),
        )
        # Classifies each time-point as belonging to one of three categories: 'no' (no reward or tone), 'yes' (reward),
        # or 'tone' (tone played, but no water delivered).
        .with_columns(
            pl.when(~pl.col("_tone_active"))
            .then(pl.lit("no"))
            .when(pl.col("_reward_event_water_uL") > 0)
            .then(pl.lit("yes"))
            .otherwise(pl.lit("tone"))
            .cast(pl.Enum(["no", "tone", "yes"]))
            .alias("reward")
        )
        # Cleans up interpolation artifacts based on the system state
        .with_columns(
            # Torque sensor is disabled in the run state, so sets torque readout to 0 when the system state is 'run'
            pl.when(pl.col("system_state") == "run").then(0.0).otherwise(pl.col("torque_N_cm")).alias("torque_N_cm")
        )
    )

    # Handle distance_cm cleanup if the encoder data column exists:
    if "distance_cm" in behavior_data.columns:
        behavior_data = (
            behavior_data
            # The encoder is disabled unless the system is in the run state. Ensures that the traveled distance only
            # increases when the system is in the run state and that the initial distance is set to 0.
            .with_columns((pl.col("system_state") != "idle").cum_sum().alias("_past_idle"))
            .with_columns(
                pl.when((pl.col("system_state") == "idle") & (pl.col("_past_idle") == 0))
                .then(0.0)
                .when(pl.col("system_state") == "run")
                .then(pl.col("distance_cm"))
                .otherwise(None)
                .forward_fill()
                .fill_null(0.0)
                .alias("distance_cm"),
                # Fixes running speed at the same time
                pl.when(pl.col("system_state") == "run").then(pl.col("speed_cm_s")).otherwise(0.0).alias("speed_cm_s"),
            )
            .drop("_past_idle")
        )

    # Final cleanup: ensures a particular column order and optimize column datatypes:
    final_columns = [
        "time_us",
        "elapsed_minutes",
        "brake",
        "screens",
        "torque_N_cm",
        "distance_cm",
        "speed_cm_s",
        "lick",
        "water_uL",
        "reward",
        "system_state",
    ]

    # Only selects columns that exist.
    columns_to_select = [col for col in final_columns if col in behavior_data.columns]

    # If requested, drops the time columns before returning the dataset to the caller.
    if drop_time_columns:
        columns_to_select = columns_to_select[2:]

    return behavior_data.select(columns_to_select)


def assemble_session_dataset(
    session_data_path: Path,
    session_multiday_path: Path,
    output_path: Path,
    dataset_type: DatasetTypes | int,
    progress: bool = False,
) -> None:
    """Assembles the requested analysis dataset for the target session.

    This function acts as the entry-point for all dataset assembly (forging) runtimes. It extracts, post-processes, and
    combines all relevant data for the processed session into a Polars DataFrame object and saves it to an uncompressed
    .feather file under the output_path directory.

    Args:
        session_data_path: The path to the session's processed data directory.
        session_multiday_path: The path to the session's multi-day data directory.
        output_path: The path to the directory where to save the assembled dataset as a .feather file.
        dataset_type: The type of the processed session. Must be one of the valid DatasetTypes enumeration members.
        progress: Determines whether to display the session's data assembly progress via the terminal progress bar.
    """
    # Ensures that the output directory exists.
    ensure_directory_exists(output_path)

    # Experiment dataset.
    if dataset_type == DatasetTypes.MESOSCOPE_VR_EXPERIMENT:
        # First assembles the fluorescence data, which is needed to generate the reference time vector for other
        # datasets
        with tqdm(total=3, desc=f"Assembling session {session_data_path.stem} datasets", disable=not progress) as pbar:
            fluorescence_data = _assemble_2p_fluorescence_dataset(
                session_data_path=session_data_path, multiday_data_path=session_multiday_path
            )
            pbar.update(1)

            # Extracts reference time to assembled other datasets in parallel
            reference_time = fluorescence_data["time_us"].to_numpy()

            # Defines tasks for parallel execution
            tasks = {
                "behavior": partial(
                    _assemble_behavior_dataset,
                    session_data_path=session_data_path,
                    reference_time=reference_time,
                    drop_time_columns=True,
                ),
                "experiment": partial(
                    _assemble_experiment_dataset, session_data_path=session_data_path, reference_time=reference_time
                ),
            }

            # Executes the processing in parallel
            results = {}
            with ThreadPoolExecutor(max_workers=2) as executor:
                future_to_name = {executor.submit(task): name for name, task in tasks.items()}

                for future in as_completed(future_to_name):
                    name = future_to_name[future]
                    results[name] = future.result()
                    pbar.update(1)

            # Extracts processing results
            behavior_data = results["behavior"]
            experiment_data = results["experiment"]

        # Concatenates all dataframes into the unified dataset
        result = pl.concat([fluorescence_data, behavior_data, experiment_data], how="horizontal")

        # Post-processing: masks cue, trial, and trial_type with 255 (or "undefined") for non-run experiment states.
        result = _mask_non_run_experiment_data(result)

        # Saves the unified dataset to disk as an uncompressed .feather file (to support memory-mapping).
        result.write_ipc(file=output_path)

    # Behavior-only training dataset.
    elif DatasetTypes.MESOSCOPE_VR_LICK_TRAINING | DatasetTypes.MESOSCOPE_VR_RUN_TRAINING:
        # Training session data is always aligned to the face camera frame acquisition time. Extracts the reference
        # timepoints from the face camera timestamp data.
        face_camera_path = session_data_path.joinpath("processed_data", "camera_data", "face_camera_timestamps.feather")
        face_camera_df = pl.read_ipc(face_camera_path, memory_map=True)
        reference_time = face_camera_df["frame_time_us"].to_numpy()

        # Assembles and saves the behavior dataset to disk as an uncompressed.feather file (to support memory-mapping).
        with tqdm(total=1, desc=f"Assembling session {session_data_path.stem} datasets", disable=not progress) as pbar:
            behavior_data = _assemble_behavior_dataset(
                session_data_path=session_data_path, reference_time=reference_time
            )
            behavior_data.write_ipc(file=output_path)
            pbar.update(1)

    # If the input dataset type is not supported, raises a ValueError.
    else:
        message = (
            f"Unsupported dataset type '{dataset_type}' encountered when assembling the dataset for the session "
            f"{session_data_path.stem}. Use one of the valid DatasetTypes enumeration members."
        )
        console.error(message=message, error=ValueError)
