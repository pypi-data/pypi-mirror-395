"""This module provides the assets that jointly support the runtime of all data management, processing, and analysis
pipelines available from this library.
"""

from enum import StrEnum
from typing import TYPE_CHECKING

from tqdm import tqdm
from sl_shared_assets import (
    SessionTypes,
    ProcessingStatus,
    AcquisitionSystems,
)

from .utilities import delay_timer

if TYPE_CHECKING:
    from pathlib import Path

    from ..server import Server, ProcessingPipeline
    from .metadata import ProjectManifest


class ProcessingPipelines(StrEnum):
    """Defines the set of data processing pipelines currently supported by the Sun lab data workflow."""

    MANIFEST = "manifest"
    """The project manifest generation pipeline. This pipeline generates a .feather file that stores the snapshot of the
    target project's data processing state. The created manifest file is then used as the entry-point for all other 
    data processing pipelines other than the ADOPTION pipeline."""
    ADOPTION = "adoption"
    """The session data adoption pipeline. This pipeline copies session's data stored in the shared Sun lab 
    data directory on the remote compute server to the user's working directory. This is the entry-point for all 
    further interactions with the data, as it gives the calling user the permission to modify the copied data."""
    CHECKSUM = "checksum"
    """The session's raw data integrity checksum verification pipeline. This pipeline verifies or regenerates
    the session's data integrity checksum."""
    BEHAVIOR = "behavior"
    """The behavior data processing pipeline. This pipeline parses the non-video behavior data from the .npz log 
    archives generated by the sl-experiment library during the session's data acquisition."""
    VIDEO = "video"
    """The video data processing pipeline. This pipeline identifies and extracts the animal's pose and movement data 
    from the frames recorded by behavior video cameras during the session's data acquisition."""
    SUITE2P = "suite2p"
    """The suite2p calcium imaging data processing pipeline. This pipeline motion-corrects, identifies, and extracts 
    the calcium fluorescence traces from cells recorded with 2-Photon Random Access Mesoscope (2P-RAM) 
    during the session's data acquisition."""
    MULTIDAY = "multiday"
    """The suite2p multi-day cell tracking pipeline. This pipeline uses the output of the 'suite2p' pipeline for each 
    of the processed sessions to identify and track the activity of stably present cells across days."""
    FORGING = "forging"
    """The dataset assembly (forging) pipeline. This pipeline integrates the single-day and multi-day data from all 
    available sources for each processed session into a unified analysis Dataset structure. The assembled 
    dataset then serves as an entry-point for all further data analysis tasks."""


def check_session_eligibility(
    manifest: ProjectManifest,
    session: str,
    pipeline: str | ProcessingPipelines,
    server: Server,
    supported_systems: set[str | AcquisitionSystems],
    supported_sessions: set[str | SessionTypes],
    *,
    allow_reprocessing: bool = False,
    configuration_path: Path | None = None,
) -> str | None:
    """Checks whether the target session meets the eligibility criteria for being processed with the specified pipeline.

    This function aggregates common eligibility checks to streamline the process for all supported processing pipelines.
    It handles all pipelines defined in ProcessingPipelines except for ADOPTION, which is handled differently.

    Args:
        manifest: The ProjectManifest instance that stores the metadata for the project under which the session was
            conducted.
        session: The unique identifier of the session to be processed.
        pipeline: The processing pipeline with which to process the session's data. Must be one of the
            ProcessingPipelines values except ADOPTION.
        server: The Server instance that communicates with the remote compute server used to execute the pipeline.
        supported_systems: The data acquisition systems that support this type of processing.
        supported_sessions: The session types that support this type of processing.
        allow_reprocessing: Determines whether to reprocess the session if it has already been processed with this
            pipeline.
        configuration_path: The path to the pipeline's configuration file on the remote server. Required for the
            SUITE2P, VIDEO, and MULTIDAY pipelines. If provided, the function verifies the file exists on the remote
            server.

    Returns:
        None if the session is eligible for processing. Otherwise, returns a string describing why the session
        was excluded from processing.
    """
    # Parses the target session data from the manifest file.
    session_data = manifest.get_session_data(session=session)
    session_type = session_data["type"][0]
    session_system = session_data["system"][0]
    complete = session_data["complete"][0]
    integrity = bool(session_data["integrity"][0])

    # Ensures that the pipeline's name is stored as a ProcessingPipelines instance.
    pipeline = ProcessingPipelines(pipeline)

    # Determines whether the session has already been processed using the specified pipeline and whether the pipeline
    # requires a configuration file or prior processing steps.
    requires_configuration = False
    requires_integrity = True
    requires_suite2p = False
    if pipeline == ProcessingPipelines.CHECKSUM:
        processed = integrity
        requires_integrity = False  # Checksum pipeline does not require prior integrity verification
    elif pipeline == ProcessingPipelines.BEHAVIOR:
        processed = bool(session_data["behavior"][0])
    elif pipeline == ProcessingPipelines.VIDEO:
        processed = bool(session_data["video"][0])
        requires_configuration = True
    elif pipeline == ProcessingPipelines.SUITE2P:
        processed = bool(session_data["suite2p"][0])
        requires_configuration = True
    elif pipeline == ProcessingPipelines.MULTIDAY:
        # Multiday processing requires suite2p to be completed first; uses tracker-based reprocessing check
        processed = False  # Determined by the tracker check below
        requires_configuration = True
        requires_suite2p = True
    elif pipeline == ProcessingPipelines.FORGING:
        # Forging pipeline performs internal checks for available data and adjusts its runtime accordingly
        processed = False  # Determined by the tracker check below
    else:
        return (
            f"The pipeline '{pipeline}' is not supported. "
            f"Use one of the supported pipelines other than ADOPTION: {list(ProcessingPipelines)}."
        )

    # If the session was acquired using a data acquisition system that does not support this type of processing,
    # skips processing the session.
    if session_system not in supported_systems:
        return (
            f"The session was acquired using the acquisition system '{session_system},' "
            f"which does not support {pipeline} processing."
        )

    # If the session type is not one of the supported types, skips processing the session.
    if session_type not in supported_sessions:
        return f"The session is of type '{session_type},' which does not support {pipeline} processing."

    # Prevents processing incomplete sessions.
    if not complete:
        return "The session is marked as 'incomplete,' which excludes it from unsupervised data processing."

    # For all pipelines except CHECKSUM, the session must have passed the integrity verification pipeline.
    if requires_integrity and not integrity:
        return (
            "The session has not been processed with the integrity verification pipeline. "
            "Run the CHECKSUM pipeline first to verify the session's data integrity."
        )

    # For the MULTIDAY pipeline, the session must have been processed with the SUITE2P pipeline first.
    if requires_suite2p and not bool(session_data["suite2p"][0]):
        return (
            "The session has not been processed with the single-day suite2p pipeline. "
            "Run the SUITE2P pipeline first to extract calcium fluorescence data."
        )

    # If the session has already been processed and reprocessing is not allowed, skips processing the session.
    if processed and not allow_reprocessing:
        return "The session has already been processed with this pipeline and reprocessing is disabled."

    # If the target processing pipeline requires a specific server-side configuration file, ensures that the file is
    # present at the expected remote server location.
    if requires_configuration:
        if configuration_path is None:
            return "The pipeline requires a configuration file, but no configuration path was provided."
        if not server.exists(remote_path=configuration_path):
            return f"The pipeline's configuration file does not exist on the remote server at: {configuration_path}."

    # The session is eligible for processing with this pipeline.
    return None


class ManagingTrackers(StrEnum):
    """Defines the filenames for tracker files used by data managing pipelines."""

    CHECKSUM = "checksum.yaml"
    """The tracker file used by the checksum resolution pipeline."""
    MANIFEST = "manifest.yaml"
    """The tracker file used by the project manifest generation pipeline."""


class ProcessingTrackers(StrEnum):
    """Defines the filenames for tracker files used by data processing pipelines."""

    SUITE2P = "suite2p.yaml"
    """The tracker file used by the suite2p processing pipeline."""
    BEHAVIOR = "behavior.yaml"
    """The tracker file used by the behavior extraction pipeline."""
    VIDEO = "video.yaml"
    """The tracker file used by the video (DeepLabCut) processing pipeline."""


class DatasetTrackers(StrEnum):
    """Defines the filenames for tracker files used by dataset forging and multi-day analysis pipelines."""

    FORGING = "forging.yaml"
    """The tracker file used by the dataset forging pipeline."""
    MULTIDAY = "multiday.yaml"
    """The tracker file used by the multi-day suite2p registration pipeline."""


def execute_pipelines(
    pipelines: tuple[ProcessingPipeline, ...],
    stage_name: str,
    batch_size: int | None = 1,
    poll_delay: int = 10,
) -> tuple[int, int]:
    """Executes the input processing pipelines as sequential batches.

    This function provides a standardized interface for executing ProcessingPipeline instances in configurable batch
    sizes. Batch execution allows controlling the degree of parallelism to balance the throughput against the I/O load
    on the remote compute server.

    Args:
        pipelines: The ProcessingPipeline instances that define the pipelines execute.
        stage_name: The name of the current processing stage, used for the progress bar labeling.
        batch_size: The maximum number of pipelines to execute concurrently within each batch. A batch size of 1
            executes pipelines sequentially. A batch size of None submits and executes all pipelines at once.
        poll_delay: The delay (in seconds) between polling the server for job status updates.

    Returns:
        A tuple of two integers: (successful_count, failed_count).
    """
    successful_count = 0
    failed_count = 0

    # Tracks which pipelines have been counted using their index
    counted_indices: set[int] = set()

    # Splits the overall sequence of pipelines into batches. If batch_size is None, all pipelines are executed at once.
    indexed_pipelines = list(enumerate(pipelines))
    effective_batch_size = len(pipelines) if batch_size is None else batch_size
    batches = [
        indexed_pipelines[i : i + effective_batch_size] for i in range(0, len(indexed_pipelines), effective_batch_size)
    ]

    with tqdm(total=len(pipelines), desc=f"Executing {stage_name} pipelines", unit="pipeline") as pbar:
        for batch in batches:
            batch_complete = False

            # Processes the current batch until all batch pipelines complete
            while not batch_complete:
                batch_complete = True

                for idx, pipeline in batch:
                    # Checks if the pipeline is still running
                    if pipeline.is_running:
                        pipeline.runtime_cycle()
                        batch_complete = False

                    # If the pipeline completed and is not yet counted, updates the counters
                    if idx not in counted_indices:
                        if pipeline.pipeline_status == ProcessingStatus.FAILED:
                            failed_count += 1
                            counted_indices.add(idx)
                            pbar.update()
                        elif pipeline.pipeline_status == ProcessingStatus.SUCCEEDED:
                            successful_count += 1
                            counted_indices.add(idx)
                            pbar.update()

                # Delays between pipeline resolution cycles to avoid overwhelming the communication line
                if not batch_complete:
                    delay_timer.delay(delay=poll_delay, allow_sleep=True, block=False)

    return successful_count, failed_count
