# sl-forgery
STUB

## Usage

This section has been transferred from sl-shared-assets and requires verification before 1.0.0 release!

### Generating Access Credentials

To access any remote server, the user is required to first generate the access credentials. The credentials are stored 
inside the 'server_credentials.yaml' file, which is generated by using the `sl-create-server-credentials` command.
**Note!** Users are advised to generate this file in a secure (non-shared) location on their local machine.

### Running Headless Jobs

A headless job is a job that does not require any user interaction during runtime. Currently, all headless jobs in the 
lab rely on pip-installable packages that expose a callable Command-Line Interface to carry out some type of
data processing. In this regard, **running a headless job is equivalent to calling a CLI command on your local 
machine**, except that the command is executed on a remote compute server. Therefore, the primary purpose of the API 
exposed by this library is to transfer the target command request to the remote server, execute it, and monitor the 
runtime status until it is complete.

For example, the [sl-suite2p package](https://github.com/Sun-Lab-NBB/suite2p) maintained in the lab exposes a CLI to 
process 2-Photon data from experiment sessions. During data processing by the 
[sl-forgery](https://github.com/Sun-Lab-NBB/sl-forgery) library, a remote job is sent to the server that uses the CLI 
exposed by the sl-suite2p package to process target session(s).

### Creating Jobs
All remote jobs are sent to the server in the form of an executable *shell* (.sh) script. The script is composed on the 
local machine that uses this library and transferred to a temporary server directory using Secure Shell File 
Transfer Protocol (SFTP). The server is then instructed to evaluate (run) the script using SLURM job manager, via a 
Secure Shell (SSH) session.

Broadly, each job consists of three major steps, which correspond to three major sections of the job shell script:
1. **Setting up the job environment**. Each job script starts with a SLURM job parameter block, which tells SLURM 
   what resources (CPUs, GPUs, RAM, etc.) the job requires. When resources become available, SLURM generates a virtual
   environment and runs the rest of the job script in that environment. This forms the basis for using the shared
   compute resources fairly, as SLURM balances resource allocation and the order of job execution for all users.
2. **Activating the target conda environment**. Currently, all jobs are assumed to use Python libraries to execute the 
   intended data processing. Similar to processing data locally, each job expects the remote server to provide a 
   Conda environment preconfigured with necessary assets (packages) to run the job. Therefore, each job contains a 
   section that activates the user-defined conda environment before running the rest of the job.
3. **Executing processing**. The final section is typically unique to each job and calls specific CLI commands or runs 
   specific Python modules. Since each job is submitted as a shell script, it can do anything a server shell can
   do. Therefore, despite python-centric approach to data processing in the lab, a remote job composed via this library 
   can execute ***any*** arbitrary command available to the user on the remove server.

Use the *Job* class exposed by this library to compose remote jobs. **Steps 1 and 2** of each job are configured when 
initializing the Job instance, while **step 3** is added via the `add_command()` method of the Job class:
```
# First, import the job class
from pathlib import Path
from sl_shared_assets import Job

# Next, instantiate a new Job object. For example, this job is used to verify the integrity of raw experiment data as
# it is transferred to the long-term storage destination (server) by the sl-experiment library.
job = Job(
    job_name="data_integrity_verification",
    output_log=Path("/temp/output.txt"),
    error_log=Path("/temp/errors.txt"),
    working_directory=Path("/temp/test_job"),
    conda_environment="test_environment",
    cpus_to_use=20,
    ram_gb=50,
    time_limit=20,
)

# Finally, add a CLI command call (the actual work to be done by the job). Here, the job calls the
# 'sl-verify-session' command exposed by the sl-shared-assets library installed in the target environment on the server.
# Use this method to add commands as you would type them in your local terminal / shell / command line.
job.add_command(f"sl-verify-session -sp /temp/test_session")
```

### Submitting and Monitoring Jobs:
To submit the job to the remote server, use a **Server** class instance. This class encapsulates access to the target 
remote compute server and uses the server_credentials.yaml file to determine server access credentials (see above):
```
# Initialize the Server class using precreated server credentials file
server = Server(credentials_path=Path("/temp/server_credentials.yaml"))

# Submit the job (generated in the previous code snippet) to the server
job = server.submit_job(job)

# Wait for the server to complete the job
from sl_forgery.server import JobStatus
delay_timer = PrecisionTimer("s")
while server.get_job_status(int(job.job_id)) in (JobStatus.PENDING, JobStatus.RUNNING):
    delay_timer.delay_noblock(delay=5, allow_sleep=True)

# Check the final job status
final_status = server.get_job_status(int(job.job_id))
if final_status == JobStatus.COMPLETED:
    print("Job completed successfully!")
elif final_status == JobStatus.FAILED:
    print("Job failed!")
```

**Note!** The Server class provides detailed job status information via the `get_job_status()` method, which returns
a `JobStatus` enumeration value (PENDING, RUNNING, COMPLETED, FAILED, CANCELLED, TIMEOUT, etc.). All developers are
highly advised to study the API documentation for the Job and Server classes to use them most effectively.

**Critical!** Since running remote jobs is largely equivalent to executing them locally, all users are highly encouraged
to test their job scripts locally before deploying them server-side. If a script works on a local machine, it is likely 
that the script would behave similarly and work on the server.

### Interactive Jobs

Interactive jobs are a special extension of the headless job type discussed above. Specifically, an interactive job is 
a headless job, whose only purpose is to **create and maintain a Jupyter lab server** under the SLURM control. 
Specifically, it requests SLURM to set up an isolated environment, starts a Jupyter server in that environment, and 
sends the credentials for the started server back to the user.

In essence, this allocates a set of resources the user can use interactively by running various Jupyter notebooks. 
While convenient for certain data analysis cases, this type of jobs has the potential to inefficiently hog server 
resources for prolonged periods of time. Therefore, users are encouraged to only resort to this type of jobs when 
strictly necessary and to minimize the resources and time allocated to running these jobs.

To run an interactive job, call the `sl-start-jupyter` CLI command exposed by this library and follow the instructions 
printed to the terminal by the command during runtime.

**Critical!** While this command tries to minimize collisions with other users, it is possible that an access port 
collision occurs when multiple users try to instantiate a jupyter server at the same time. If you cannot authenticate
with the Jupyter server, this likely indicates that the target port was in use and Jupyter automatically incremented the
port number by 1. In this case, add 1 to your port number and try connecting to that port using the Jupyter credentials 
provided by the command. For example, if your target port was '8888,' try port '8889.'

---