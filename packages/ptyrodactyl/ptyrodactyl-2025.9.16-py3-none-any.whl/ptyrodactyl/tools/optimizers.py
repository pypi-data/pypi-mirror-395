"""Complex-valued optimizers with Wirtinger derivatives for ptychography.

Extended Summary
----------------
This module implements complex-valued optimization algorithms including Adam,
Adagrad, and RMSprop using Wirtinger calculus. It also provides learning rate
schedulers for training optimization. All functions are JAX-compatible and
support automatic differentiation.

Classes
-------
LRSchedulerState : NamedTuple
    State maintained by learning rate schedulers.
OptimizerState : NamedTuple
    State maintained by optimizers (moments, step count).
Optimizer : NamedTuple
    Optimizer configuration with init and update functions.

Routine Listings
----------------
wirtinger_grad : function
    Compute the Wirtinger gradient of a complex-valued function.
complex_adam : function
    Complex-valued Adam optimizer based on Wirtinger derivatives.
complex_adagrad : function
    Complex-valued Adagrad optimizer based on Wirtinger derivatives.
complex_rmsprop : function
    Complex-valued RMSprop optimizer based on Wirtinger derivatives.
init_adam : function
    Initialize Adam optimizer state.
init_adagrad : function
    Initialize Adagrad optimizer state.
init_rmsprop : function
    Initialize RMSprop optimizer state.
adam_update : function
    Update parameters using Adam optimizer with Wirtinger derivatives.
adagrad_update : function
    Update parameters using Adagrad optimizer with Wirtinger derivatives.
rmsprop_update : function
    Update parameters using RMSprop optimizer with Wirtinger derivatives.
create_cosine_scheduler : function
    Creates a cosine learning rate scheduler with smooth decay.
create_step_scheduler : function
    Creates a step decay scheduler with periodic learning rate drops.
create_warmup_cosine_scheduler : function
    Creates a scheduler with linear warmup followed by cosine decay.
init_scheduler_state : function
    Initialize scheduler state with given learning rate.

Notes
-----
All optimizers use Wirtinger calculus for proper handling of complex-valued
parameters. The Wirtinger derivative is defined as ∂f/∂z = ½(∂f/∂x - i∂f/∂y).
All functions are designed to work with JAX transformations including jit,
grad, and vmap.
"""

import jax
import jax.numpy as jnp
from beartype.typing import (
    Any,
    Callable,
    NamedTuple,
    Optional,
    Sequence,
    Tuple,
    Union,
)
from jaxtyping import Array, Complex, Float


class LRSchedulerState(NamedTuple):
    """State maintained by learning rate schedulers.

    Attributes
    ----------
    step : int
        Current optimization step
    learning_rate : float
        Current learning rate
    initial_lr : float
        Initial learning rate value
    """

    step: int
    learning_rate: float
    initial_lr: float


SchedulerFn = Callable[[LRSchedulerState], tuple[float, LRSchedulerState]]


def create_cosine_scheduler(
    total_steps: int,
    final_lr_factor: float = 0.01,
) -> SchedulerFn:
    """
    Description
    -----------
    Creates a cosine learning rate scheduler.

    This scheduler implements a cosine annealing schedule that smoothly
    decreases the learning rate from the initial value to a final value
    over the specified number of steps.

    Parameters
    ----------
    - `total_steps` (int):
        Total number of optimization steps
    - `final_lr_factor` (float):
        Final learning rate as a fraction of initial learning rate.
        Default is 0.01.

    Returns
    -------
    - `scheduler_fn` (SchedulerFn):
        A function that takes the current scheduler state and returns
        the new learning rate and updated state.

    Flow
    ----
    - Calculate progress as min(step / total_steps, 1.0)
    - Compute cosine decay factor using 0.5 * (1 + cos(π * progress))
    - Calculate new learning rate using linear interpolation
    - Update scheduler state with new step and learning rate
    - Return new learning rate and updated state
    """

    @jax.jit
    def scheduler_fn(
        state: LRSchedulerState,
    ) -> tuple[float, LRSchedulerState]:
        progress = jnp.minimum(state.step / total_steps, 1.0)
        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * progress))
        lr = state.initial_lr * (
            final_lr_factor + (1 - final_lr_factor) * cosine_decay
        )
        new_state = LRSchedulerState(
            step=state.step + 1, learning_rate=lr, initial_lr=state.initial_lr
        )
        return lr, new_state

    return scheduler_fn


def create_step_scheduler(step_size: int, gamma: float = 0.1) -> SchedulerFn:
    """
    Description
    -----------
    Creates a step decay scheduler that reduces learning rate by gamma every step_size steps.

    This scheduler implements a step-wise learning rate decay where the learning rate
    is multiplied by gamma every step_size steps.

    Parameters
    ----------
    - `step_size` (int):
        Number of steps between learning rate drops
    - `gamma` (float):
        Multiplicative factor for learning rate decay.
        Default is 0.1.

    Returns
    -------
    - `scheduler_fn` (SchedulerFn):
        A function that takes the current scheduler state and returns
        the new learning rate and updated state.

    Flow
    ----
    - Calculate number of learning rate drops as step // step_size
    - Compute new learning rate as initial_lr * (gamma ^ num_drops)
    - Update scheduler state with new step and learning rate
    - Return new learning rate and updated state
    """

    @jax.jit
    def scheduler_fn(
        state: LRSchedulerState,
    ) -> tuple[float, LRSchedulerState]:
        num_drops = state.step // step_size
        lr = state.initial_lr * (gamma**num_drops)
        new_state = LRSchedulerState(
            step=state.step + 1, learning_rate=lr, initial_lr=state.initial_lr
        )
        return lr, new_state

    return scheduler_fn


def create_warmup_cosine_scheduler(
    total_steps: int,
    warmup_steps: int,
    final_lr_factor: float = 0.01,
) -> SchedulerFn:
    """
    Description
    -----------
    Creates a scheduler with linear warmup followed by cosine decay.

    This scheduler combines a linear warmup phase with a cosine annealing decay.
    During warmup, the learning rate increases linearly from 0 to the initial value.
    After warmup, it follows a cosine decay schedule.

    Parameters
    ----------
    - `total_steps` (int):
        Total number of optimization steps
    - `warmup_steps` (int):
        Number of warmup steps
    - `final_lr_factor` (float):
        Final learning rate as a fraction of initial learning rate.
        Default is 0.01.

    Returns
    -------
    - `scheduler_fn` (SchedulerFn):
        A function that takes the current scheduler state and returns
        the new learning rate and updated state.

    Flow
    ----
    - During warmup phase (step < warmup_steps):
        - Calculate linear warmup learning rate
    - During decay phase (step >= warmup_steps):
        - Calculate cosine decay learning rate
    - Choose appropriate learning rate based on current step
    - Update scheduler state with new step and learning rate
    - Return new learning rate and updated state
    """

    @jax.jit
    def scheduler_fn(
        state: LRSchedulerState,
    ) -> tuple[float, LRSchedulerState]:
        # Linear warmup
        warmup_progress = jnp.minimum(state.step / warmup_steps, 1.0)
        warmup_lr = state.initial_lr * warmup_progress

        # Cosine decay after warmup
        remaining_steps = total_steps - warmup_steps
        decay_progress = (
            jnp.maximum(0.0, state.step - warmup_steps) / remaining_steps
        )
        decay_progress = jnp.minimum(decay_progress, 1.0)
        cosine_decay = 0.5 * (1 + jnp.cos(jnp.pi * decay_progress))
        decay_lr = state.initial_lr * (
            final_lr_factor + (1 - final_lr_factor) * cosine_decay
        )

        # Choose between warmup and decay
        lr = jnp.where(state.step < warmup_steps, warmup_lr, decay_lr)

        new_state = LRSchedulerState(
            step=state.step + 1, learning_rate=lr, initial_lr=state.initial_lr
        )
        return lr, new_state

    return scheduler_fn


def init_scheduler_state(initial_lr: float) -> LRSchedulerState:
    """
    Description
    -----------
    Initialize scheduler state with given learning rate.

    Parameters
    ----------
    - `initial_lr` (float):
        Initial learning rate value

    Returns
    -------
    - `state` (LRSchedulerState):
        Initialized scheduler state with step=0 and learning_rate=initial_lr
    """
    return LRSchedulerState(
        step=0, learning_rate=initial_lr, initial_lr=initial_lr
    )


class OptimizerState(NamedTuple):
    """
    Description
    -----------
    State maintained by optimizers.

    Attributes
    ----------
    - `m` (Array):
        First moment estimate (for Adam-like optimizers)
    - `v` (Array):
        Second moment estimate (for Adam-like optimizers)
    - `step` (Array):
        Step count
    """

    m: Array  # First moment estimate
    v: Array  # Second moment estimate
    step: Array  # Step count


class Optimizer(NamedTuple):
    """
    Description
    -----------
    Optimizer configuration.

    Attributes
    ----------
    - `init` (Callable):
        Function to initialize optimizer state
    - `update` (Callable):
        Function to update parameters using optimizer
    """

    init: Callable
    update: Callable


def wirtinger_grad(
    func2diff: Callable[..., Float[Array, " ..."]],
    argnums: Optional[Union[int, Sequence[int]]] = 0,
) -> Callable[
    ..., Union[Complex[Array, " ..."], Tuple[Complex[Array, " ..."], ...]]
]:
    """
    Description
    -----------
    Compute the Wirtinger gradient of a complex-valued function.
    This function returns a new function that computes the Wirtinger gradient
    of the input function f with respect to the specified argument(s).
    This is based on the formula for Wirtinger derivative:

    ∂f/∂z = ½(∂f/∂x - i∂f/∂y)

    Parameters
    ----------
    - `func2diff` (Callable[..., Float[Array, " ..."]]):
        A complex-valued function to differentiate.
    - `argnums` (Union[int, Sequence[int]]):
        Specifies which argument(s) to compute the gradient with respect to.
        Can be an int or a sequence of ints. Default is 0.

    Returns
    -------
    - grad_f (Callable[..., Union[Complex[Array, " ..."],
              Tuple[Complex[Array, " ..."], ...]]]):
        A function that computes the Wirtinger gradient of f with respect to
        the specified argument(s).
    """

    def grad_f(
        *args: Any,
    ) -> Union[Complex[Array, " ..."], Tuple[Complex[Array, " ..."], ...]]:
        def split_complex(args):
            return tuple(
                jnp.real(arg) if jnp.iscomplexobj(arg) else arg for arg in args
            ) + tuple(
                jnp.imag(arg) if jnp.iscomplexobj(arg) else jnp.zeros_like(arg)
                for arg in args
            )

        def combine_complex(r, i):
            return tuple(
                rr + 1j * ii if jnp.iscomplexobj(arg) else rr
                for rr, ii, arg in zip(r, i, args, strict=False)
            )

        split_args = split_complex(args)
        n = len(args)

        def f_real(*split_args):
            return jnp.real(
                func2diff(*combine_complex(split_args[:n], split_args[n:]))
            )

        def f_imag(*split_args):
            return jnp.imag(
                func2diff(*combine_complex(split_args[:n], split_args[n:]))
            )

        gr = jax.grad(f_real, argnums=argnums)(*split_args)
        gi = jax.grad(f_imag, argnums=argnums)(*split_args)

        if isinstance(argnums, int):
            return 0.5 * (gr - 1j * gi)
        return tuple(
            0.5 * (grr - 1j * gii) for grr, gii in zip(gr, gi, strict=False)
        )

    return grad_f


def complex_adam(
    params: Complex[Array, " ..."],
    grads: Complex[Array, " ..."],
    state: Tuple[Complex[Array, " ..."], Complex[Array, " ..."], int],
    learning_rate: float = 0.001,
    beta1: float = 0.9,
    beta2: float = 0.999,
    eps: float = 1e-8,
) -> Tuple[
    Complex[Array, " ..."],
    Tuple[Complex[Array, " ..."], Complex[Array, " ..."], int],
]:
    """
    Description
    -----------
    Complex-valued Adam optimizer based on Wirtinger derivatives.

    This function performs one step of the Adam optimization algorithm
    for complex-valued parameters using Wirtinger calculus.

    Parameters
    ----------
    - `params` (Complex[Array, " ..."]):
        Current complex-valued parameters
    - `grads` (Complex[Array, " ..."]):
        Complex-valued gradients computed using Wirtinger derivatives
    - `state` (Tuple[Complex[Array, " ..."], Complex[Array, " ..."], int]):
        Optimizer state containing (first moment, second moment, timestep)
    - `learning_rate` (float):
        Learning rate for parameter updates.
        Default is 0.001.
    - `beta1` (float):
        Exponential decay rate for first moment estimates.
        Default is 0.9.
    - `beta2` (float):
        Exponential decay rate for second moment estimates.
        Default is 0.999.
    - `eps` (float):
        Small value to avoid division by zero.
        Default is 1e-8.

    Returns
    -------
    - `new_params` (Complex[Array, " ..."]):
        Updated complex-valued parameters
    - `new_state` (Tuple[Complex[Array, " ..."], Complex[Array, " ..."], int]):
        Updated optimizer state

    Flow
    ----
    - Increment timestep counter
    - Update first moment estimate: m = β₁ * m + (1 - β₁) * grads
    - Update second moment estimate: v = β₂ * v + (1 - β₂) * |grads|²
    - Compute bias-corrected moments: m̂ = m / (1 - β₁^t), v̂ = v / (1 - β₂^t)
    - Calculate parameter update: update = lr * m̂ / (√v̂ + ε)
    - Apply update: new_params = params - update
    - Return updated parameters and state
    """
    m, v, t = state
    t += 1
    m = beta1 * m + (1 - beta1) * grads
    v = beta2 * v + (1 - beta2) * jnp.abs(grads) ** 2
    m_hat = m / (1 - beta1**t)
    v_hat = v / (1 - beta2**t)
    update = learning_rate * m_hat / (jnp.sqrt(v_hat) + eps)
    new_params = params - update
    return new_params, (m, v, t)


def complex_adagrad(
    params: Complex[Array, " ..."],
    grads: Complex[Array, " ..."],
    state: Complex[Array, " ..."],
    learning_rate: float = 0.01,
    eps: float = 1e-8,
) -> Tuple[Complex[Array, " ..."], Complex[Array, " ..."]]:
    """
    Description
    -----------
    Complex-valued Adagrad optimizer based on Wirtinger derivatives.

    This function performs one step of the Adagrad optimization algorithm
    for complex-valued parameters using Wirtinger calculus.

    Parameters
    ----------
    - `params` (Complex[Array, " ..."]):
        Current complex-valued parameters
    - `grads` (Complex[Array, " ..."]):
        Complex-valued gradients computed using Wirtinger derivatives
    - `state` (Complex[Array, " ..."]):
        Optimizer state containing accumulated squared gradients
    - `learning_rate` (float):
        Learning rate for parameter updates.
        Default is 0.01.
    - `eps` (float):
        Small value to avoid division by zero.
        Default is 1e-8.

    Returns
    -------
    - `new_params` (Complex[Array, " ..."]):
        Updated complex-valued parameters
    - `new_state` (Complex[Array, " ..."]):
        Updated optimizer state with accumulated gradients

    Flow
    ----
    - Update accumulated squared gradients: G = G + |grads|²
    - Calculate adaptive learning rate: lr_adaptive = lr / (√G + ε)
    - Apply update: new_params = params - lr_adaptive * grads
    - Return updated parameters and accumulated gradients
    """
    accumulated_grads = state

    # Update accumulated squared gradients
    new_accumulated_grads = accumulated_grads + jnp.abs(grads) ** 2

    # Compute adaptive learning rate
    adaptive_lr = learning_rate / (jnp.sqrt(new_accumulated_grads) + eps)

    # Update parameters
    new_params = params - adaptive_lr * grads

    return new_params, new_accumulated_grads


def complex_rmsprop(
    params: Complex[Array, " ..."],
    grads: Complex[Array, " ..."],
    state: Complex[Array, " ..."],
    learning_rate: float = 0.001,
    decay_rate: float = 0.9,
    eps: float = 1e-8,
) -> Tuple[Complex[Array, " ..."], Complex[Array, " ..."]]:
    """
    Description
    -----------
    Complex-valued RMSprop optimizer based on Wirtinger derivatives.

    This function performs one step of the RMSprop optimization algorithm
    for complex-valued parameters using Wirtinger calculus.

    Parameters
    ----------
    - `params` (Complex[Array, " ..."]):
        Current complex-valued parameters
    - `grads` (Complex[Array, " ..."]):
        Complex-valued gradients computed using Wirtinger derivatives
    - `state` (Complex[Array, " ..."]):
        Optimizer state containing moving average of squared gradients
    - `learning_rate` (float):
        Learning rate for parameter updates.
        Default is 0.001.
    - `decay_rate` (float):
        Decay rate for moving average of squared gradients.
        Default is 0.9.
    - `eps` (float):
        Small value to avoid division by zero.
        Default is 1e-8.

    Returns
    -------
    - `new_params` (Complex[Array, " ..."]):
        Updated complex-valued parameters
    - `new_state` (Complex[Array, " ..."]):
        Updated optimizer state with moving average

    Flow
    ----
    - Update moving average of squared gradients: v = ρ * v + (1 - ρ) * |grads|²
    - Calculate adaptive learning rate: lr_adaptive = lr / (√v + ε)
    - Apply update: new_params = params - lr_adaptive * grads
    - Return updated parameters and moving average
    """
    moving_avg = state

    # Update moving average of squared gradients
    new_moving_avg = (
        decay_rate * moving_avg + (1 - decay_rate) * jnp.abs(grads) ** 2
    )

    # Compute adaptive learning rate
    adaptive_lr = learning_rate / (jnp.sqrt(new_moving_avg) + eps)

    # Update parameters
    new_params = params - adaptive_lr * grads

    return new_params, new_moving_avg


def init_adam(shape: tuple) -> OptimizerState:
    """
    Description
    -----------
    Initialize Adam optimizer state.

    Parameters
    ----------
    - `shape` (tuple):
        Shape of the parameters to be optimized

    Returns
    -------
    - `state` (OptimizerState):
        Initialized Adam optimizer state with zero moments and step=0
    """
    return OptimizerState(
        m=jnp.zeros(shape), v=jnp.zeros(shape), step=jnp.array(0)
    )


def init_adagrad(shape: tuple) -> OptimizerState:
    """
    Description
    -----------
    Initialize Adagrad optimizer state.

    Parameters
    ----------
    - `shape` (tuple):
        Shape of the parameters to be optimized

    Returns
    -------
    - `state` (OptimizerState):
        Initialized Adagrad optimizer state with zero accumulated gradients
    """
    return OptimizerState(
        m=jnp.zeros(shape), v=jnp.zeros(shape), step=jnp.array(0)
    )


def init_rmsprop(shape: tuple) -> OptimizerState:
    """
    Description
    -----------
    Initialize RMSprop optimizer state.

    Parameters
    ----------
    - `shape` (tuple):
        Shape of the parameters to be optimized

    Returns
    -------
    - `state` (OptimizerState):
        Initialized RMSprop optimizer state with zero moving average
    """
    return OptimizerState(
        m=jnp.zeros(shape), v=jnp.zeros(shape), step=jnp.array(0)
    )


def adam_update(
    params: Complex[Array, " ..."],
    grads: Complex[Array, " ..."],
    state: OptimizerState,
    learning_rate: float = 0.001,
    beta1: float = 0.9,
    beta2: float = 0.999,
    eps: float = 1e-8,
) -> tuple[Complex[Array, " ..."], OptimizerState]:
    """
    Description
    -----------
    Update parameters using Adam optimizer with Wirtinger derivatives.

    Parameters
    ----------
    - `params` (Complex[Array, " ..."]):
        Current complex-valued parameters
    - `grads` (Complex[Array, " ..."]):
        Complex-valued gradients computed using Wirtinger derivatives
    - `state` (OptimizerState):
        Current optimizer state
    - `learning_rate` (float):
        Learning rate for parameter updates.
        Default is 0.001.
    - `beta1` (float):
        Exponential decay rate for first moment estimates.
        Default is 0.9.
    - `beta2` (float):
        Exponential decay rate for second moment estimates.
        Default is 0.999.
    - `eps` (float):
        Small value to avoid division by zero.
        Default is 1e-8.

    Returns
    -------
    - `new_params` (Complex[Array, " ..."]):
        Updated complex-valued parameters
    - `new_state` (OptimizerState):
        Updated optimizer state

    Flow
    ----
    - Extract current state components (m, v, step)
    - Call complex_adam to perform the update
    - Return updated parameters and state
    """
    m, v, step = state
    new_params, (new_m, new_v, new_step) = complex_adam(
        params, grads, (m, v, step), learning_rate, beta1, beta2, eps
    )
    return new_params, OptimizerState(m=new_m, v=new_v, step=new_step)


def adagrad_update(
    params: Complex[Array, " ..."],
    grads: Complex[Array, " ..."],
    state: OptimizerState,
    learning_rate: float = 0.01,
    eps: float = 1e-8,
) -> tuple[Complex[Array, " ..."], OptimizerState]:
    """
    Description
    -----------
    Update parameters using Adagrad optimizer with Wirtinger derivatives.

    Parameters
    ----------
    - `params` (Complex[Array, " ..."]):
        Current complex-valued parameters
    - `grads` (Complex[Array, " ..."]):
        Complex-valued gradients computed using Wirtinger derivatives
    - `state` (OptimizerState):
        Current optimizer state
    - `learning_rate` (float):
        Learning rate for parameter updates.
        Default is 0.01.
    - `eps` (float):
        Small value to avoid division by zero.
        Default is 1e-8.

    Returns
    -------
    - `new_params` (Complex[Array, " ..."]):
        Updated complex-valued parameters
    - `new_state` (OptimizerState):
        Updated optimizer state

    Flow
    ----
    - Extract current state components (m, v, step)
    - Call complex_adagrad to perform the update
    - Return updated parameters and state
    """
    m, v, step = state
    new_params, new_v = complex_adagrad(params, grads, v, learning_rate, eps)
    return new_params, OptimizerState(m=m, v=new_v, step=step + 1)


def rmsprop_update(
    params: Complex[Array, " ..."],
    grads: Complex[Array, " ..."],
    state: OptimizerState,
    learning_rate: float = 0.001,
    decay_rate: float = 0.9,
    eps: float = 1e-8,
) -> tuple[Complex[Array, " ..."], OptimizerState]:
    """
    Description
    -----------
    Update parameters using RMSprop optimizer with Wirtinger derivatives.

    Parameters
    ----------
    - `params` (Complex[Array, " ..."]):
        Current complex-valued parameters
    - `grads` (Complex[Array, " ..."]):
        Complex-valued gradients computed using Wirtinger derivatives
    - `state` (OptimizerState):
        Current optimizer state
    - `learning_rate` (float):
        Learning rate for parameter updates.
        Default is 0.001.
    - `decay_rate` (float):
        Decay rate for moving average of squared gradients.
        Default is 0.9.
    - `eps` (float):
        Small value to avoid division by zero.
        Default is 1e-8.

    Returns
    -------
    - `new_params` (Complex[Array, " ..."]):
        Updated complex-valued parameters
    - `new_state` (OptimizerState):
        Updated optimizer state

    Flow
    ----
    - Extract current state components (m, v, step)
    - Call complex_rmsprop to perform the update
    - Return updated parameters and state
    """
    m, v, step = state
    new_params, new_v = complex_rmsprop(
        params, grads, v, learning_rate, decay_rate, eps
    )
    return new_params, OptimizerState(m=m, v=new_v, step=step + 1)
