"""
èåˆç‰ˆæœ¬ï¼š
- ä½¿ç”¨ test_13.py çš„é€»è¾‘ï¼ˆå›¾ç‰‡å¢å¼ºã€è§£æé€»è¾‘ï¼‰
- ä½¿ç”¨ run_dpsk_ocr_image.py çš„ç¦»çº¿ vLLM å¯åŠ¨æ–¹å¼
"""

import os, re, io, base64, json
from PIL import Image
import torch

# è®¾ç½®ç¯å¢ƒå˜é‡
if torch.version.cuda == '11.8':
    os.environ["TRITON_PTXAS_PATH"] = "/usr/local/cuda-11.8/bin/ptxas"
os.environ['VLLM_USE_V1'] = '0'
os.environ["CUDA_VISIBLE_DEVICES"] = '0'

import sys
sys.path.insert(0, '/home/ad/tianhaoyang/deepseek_ocr/DeepSeek-OCR/DeepSeek-OCR-master/DeepSeek-OCR-vllm')

from vllm import LLM, SamplingParams
from vllm.model_executor.models.registry import ModelRegistry
from deepseek_ocr import DeepseekOCRForCausalLM
from process.ngram_norepeat import NoRepeatNGramLogitsProcessor
from process.image_process import DeepseekOCRProcessor

# é…ç½®
MODEL_PATH = '/home/ad/tianhaoyang/vllm_model/deepseek-ai/DeepSeek-OCR'
CROP_MODE = True

# é»˜è®¤æç¤ºè¯ï¼ˆä¸ test_13.py ä¸€è‡´ï¼‰
DEFAULT_PROMPT = (
    "You are an OCR & document understanding assistant.\n"
    "Analyze this image region and produce:\n"
    "1) ALT: a very short alt text (<=12 words).\n"
    "2) CAPTION: a 1-2 sentence concise caption.\n"
    "3) CONTENT_MD: if the image contains a table, output a clean Markdown table;"
    "   if it contains a formula, output LaTeX ($...$ or $$...$$);"
    "   otherwise provide 3-6 bullet points summarizing key content, in Markdown.\n"
    "Return strictly in the following format:\n"
    "ALT: <short alt>\n"
    "CAPTION: <one or two sentences>\n"
    "CONTENT_MD:\n"
    "<markdown content here>\n"
)

IMG_PATTERN = re.compile(r'!\[[^\]]*\]\(([^)]+)\)')

# å…¨å±€å˜é‡
_llm = None
_sampling_params = None

def init_vllm():
    """åˆå§‹åŒ– vLLMï¼ˆå‚è€ƒ run_dpsk_ocr_image.pyï¼‰"""
    global _llm, _sampling_params
    
    if _llm is not None:
        return
    
    print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    # æ³¨å†Œæ¨¡å‹
    ModelRegistry.register_model("DeepseekOCRForCausalLM", DeepseekOCRForCausalLM)
    
    # åˆå§‹åŒ– LLM
    _llm = LLM(
        model=MODEL_PATH,
        hf_overrides={"architectures": ["DeepseekOCRForCausalLM"]},
        block_size=256,
        max_model_len=8192,
        enforce_eager=False,
        trust_remote_code=True,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        disable_mm_preprocessor_cache=True
    )
    
    # é…ç½®é‡‡æ ·å‚æ•°
    logits_processors = [
        NoRepeatNGramLogitsProcessor(
            ngram_size=20,
            window_size=50,
            whitelist_token_ids={128821, 128822}
        )
    ]
    
    _sampling_params = SamplingParams(
        temperature=0.2,
        max_tokens=2048,
        logits_processors=logits_processors,
        skip_special_tokens=False,
        include_stop_str_in_output=True,
    )
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")


def call_deepseek_ocr_image(img_path, temperature=0.2, max_tokens=2048, prompt=DEFAULT_PROMPT):
    """è°ƒç”¨ DeepSeek-OCR è¿›è¡Œå›¾ç‰‡è§£æï¼ˆä¸ test_13.py é€»è¾‘ä¸€è‡´ï¼‰"""
    global _llm, _sampling_params
    
    if _llm is None:
        init_vllm()
    
    # è¯»å–å›¾ç‰‡
    with Image.open(img_path) as im:
        image = im.convert('RGB')
    
    # å‡†å¤‡è¾“å…¥ï¼ˆå‚è€ƒ run_dpsk_ocr_image.pyï¼‰
    full_prompt = f"<image>\n{prompt}"
    image_features = DeepseekOCRProcessor().tokenize_with_images(
        images=[image],
        bos=True,
        eos=True,
        cropping=CROP_MODE
    )
    
    request = {
        "prompt": full_prompt,
        "multi_modal_data": {"image": image_features}
    }
    
    # ç”Ÿæˆ
    outputs = _llm.generate([request], sampling_params=_sampling_params)
    text = outputs[0].outputs[0].text.strip()
    
    # æ¸…ç†ç»“æŸæ ‡è®°
    if '<ï½œendâ–ofâ–sentenceï½œ>' in text:
        text = text.replace('<ï½œendâ–ofâ–sentenceï½œ>', '')
    
    # è°ƒè¯•ï¼šæ‰“å°æ¨¡å‹åŸå§‹è¾“å‡º
    print(f"\n{'='*60}")
    print(f"å›¾ç‰‡: {os.path.basename(img_path)}")
    print(f"æ¨¡å‹è¾“å‡º:\n{text[:500]}")  # æ‰“å°å‰500ä¸ªå­—ç¬¦
    print(f"{'='*60}\n")
    
    # è§£æ DeepSeek-OCR åŸç”Ÿæ ¼å¼
    import re
    
    # æ–¹æ³•1ï¼šé€è¡Œå¤„ç†ï¼Œç§»é™¤åŒ…å«æ ‡è®°çš„è¡Œï¼Œä¿ç•™å®é™…å†…å®¹
    lines = []
    for line in text.splitlines():
        line = line.strip()
        # è·³è¿‡åªåŒ…å«æ ‡è®°çš„è¡Œ
        if line.startswith('<|ref|>') or line.startswith('<|det|>'):
            continue
        # ç§»é™¤è¡Œå†…çš„æ ‡è®°
        line = re.sub(r'<\|ref\|>.*?</\|ref\|>', '', line)
        line = re.sub(r'<\|det\|>.*?</\|det\|>', '', line)
        line = line.strip()
        if line:  # åªä¿ç•™éç©ºè¡Œ
            lines.append(line)
    
    content_md = "\n\n".join(lines)  # ç”¨åŒæ¢è¡Œåˆ†éš”ï¼Œä½¿ Markdown æ ¼å¼æ›´æ¸…æ™°
    
    # ç”Ÿæˆç®€å•çš„ captionï¼ˆå–ç¬¬ä¸€è¡Œæˆ–å‰50ä¸ªå­—ç¬¦ï¼‰
    caption = lines[0][:50] if lines else ""
    
    result = {
        "alt": "Figure",
        "caption": caption,
        "content_md": content_md
    }
    
    # è°ƒè¯•ï¼šæ‰“å°è§£æç»“æœ
    print(f"è§£æç»“æœ: ALT='{result['alt']}', CAPTION='{result['caption'][:50] if result['caption'] else '(ç©º)'}', CONTENT_MDé•¿åº¦={len(result['content_md'])}")
    
    return result


def augment_markdown(md_path, out_path,
                     temperature=0.2, max_tokens=2048,
                     image_root=".",
                     cache_json=None):
    """å¢å¼º Markdownï¼ˆä¸ test_13.py å®Œå…¨ä¸€è‡´ï¼‰"""
    with open(md_path, "r", encoding="utf-8") as f:
        md_lines = f.read().splitlines()

    cache = {}
    if cache_json and os.path.exists(cache_json):
        try:
            cache = json.load(open(cache_json, "r", encoding="utf-8"))
        except Exception:
            cache = {}

    # åˆå§‹åŒ–æ¨¡å‹
    init_vllm()

    out_lines = []
    for line in md_lines:
        out_lines.append(line)
        m = IMG_PATTERN.search(line)
        if not m:
            continue

        img_rel = m.group(1).strip().split("?")[0]
        img_path = img_rel if os.path.isabs(img_rel) else os.path.join(image_root, img_rel)

        if not os.path.exists(img_path):
            out_lines.append(f"<!-- WARN: image not found: {img_rel} -->")
            continue

        if cache_json and img_path in cache:
            result = cache[img_path]
        else:
            result = call_deepseek_ocr_image(img_path, temperature, max_tokens)
            if cache_json:
                cache[img_path] = result

        alt, cap, body = result["alt"], result["caption"], result["content_md"]

        if cap:
            out_lines.append(f"*{cap}*")
        if body:
            out_lines.append("<details><summary>è§£æ</summary>\n")
            out_lines.append(body)
            out_lines.append("\n</details>")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(out_lines))

    if cache_json:
        with open(cache_json, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)

    print(f"âœ… å·²å†™å…¥å¢å¼ºåçš„ Markdownï¼š{out_path}")


# è¿è¡Œ
augment_markdown(
    md_path="/home/ad/tianhaoyang/deepseek_ocr/image_output_2/pdf_to_markdown/0.LangChainæŠ€æœ¯ç”Ÿæ€ä»‹ç».md",
    out_path="/home/ad/tianhaoyang/deepseek_ocr/image_output_2/pdf_to_markdown_augmented/0.LangChainæŠ€æœ¯ç”Ÿæ€ä»‹ç»_augmented.md",
    image_root="/home/ad/tianhaoyang/deepseek_ocr/image_output_2/pdf_to_markdown",
    cache_json="/home/ad/tianhaoyang/deepseek_ocr/image_output_2/pdf_to_markdown_augmented/image_cache.json"
)

