# TPU Inference Server Requirements
# Note: torch, torch_xla, and transformers are installed separately
# via setup.sh using Google's TPU-specific PyTorch index

flask>=2.0.0
pyyaml>=6.0
requests>=2.28.0

# Note: Install PyTorch/XLA for TPU using:
# pip install torch torch_xla transformers \
#   -f https://storage.googleapis.com/libtpu-releases/index.html \
#   -f https://storage.googleapis.com/libtpu-wheels/index.html
