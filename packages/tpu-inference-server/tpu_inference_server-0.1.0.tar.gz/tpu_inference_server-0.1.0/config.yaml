# TPU Inference Server Configuration

# Server settings
server:
  host: "0.0.0.0"
  port: 8080

# Models to load on startup
# Comment out models you don't want to load automatically
models:
  # Mistral 7B - Good general-purpose model
  - model_id: "mistralai/Mistral-7B-Instruct-v0.2"
    name: "mistral-7b"
    dtype: "bfloat16"

  # Gemma 2B - Smaller, faster model
  # - model_id: "google/gemma-2b-it"
  #   name: "gemma-2b"
  #   dtype: "bfloat16"

  # GPT-2 - Very small, good for testing
  # - model_id: "gpt2"
  #   name: "gpt2"
  #   dtype: "float32"

  # Llama 2 7B Chat (requires HuggingFace access token)
  # - model_id: "meta-llama/Llama-2-7b-chat-hf"
  #   name: "llama-7b"
  #   dtype: "bfloat16"

# Example configurations for different use cases:

# --- Minimal Setup (for testing) ---
# models:
#   - model_id: "gpt2"
#     name: "gpt2"
#     dtype: "float32"

# --- Production Setup (multiple models) ---
# models:
#   - model_id: "mistralai/Mistral-7B-Instruct-v0.2"
#     name: "mistral-7b"
#     dtype: "bfloat16"
#   - model_id: "google/gemma-2b-it"
#     name: "gemma-2b"
#     dtype: "bfloat16"

# --- Memory-Constrained Setup ---
# models:
#   - model_id: "google/gemma-2b-it"
#     name: "gemma-2b"
#     dtype: "bfloat16"
