"""
æ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬ - æ¨¡å—åŒ–é‡æ„
ç›¸æ¯”åŸºç¡€ç‰ˆæœ¬(model.py)çš„ä¼˜åŒ–ç‚¹ï¼š
1. âœ… å¤šé‡å…±çº¿æ€§å¤„ç† - ç§»é™¤é«˜åº¦ç›¸å…³ç‰¹å¾
2. âœ… ç‰¹å¾é‡è¦æ€§ç­›é€‰ - åŸºäºXGBoosté‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©
3. âœ… å¤šæ¨¡å‹é›†æˆ - XGBoost + LightGBM + CatBoost
4. âœ… å·®å¼‚åŒ–ç‰¹å¾ç­–ç•¥ - ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒç‰¹å¾é›†
5. âœ… è¶…å‚æ•°ä¼˜åŒ– - é’ˆå¯¹æ€§çš„å‚æ•°è°ƒä¼˜
6. âœ… æ—©åœæœºåˆ¶ - XGBoostçš„æ—©åœè®­ç»ƒ
7. âœ… å¤šèåˆç­–ç•¥ - ç®€å•å¹³å‡/åŠ æƒå¹³å‡/æ’åå¹³å‡/Stacking
8. âœ… æ¨¡å—åŒ–æ¶æ„ - æ¸…æ™°çš„ä»£ç ç»„ç»‡ç»“æ„
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½®ç®¡ç†æ¨¡å— ====================

class ModelConfig:
    """æ¨¡å‹é…ç½®ç±» - é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°"""
    
    # æ•°æ®é…ç½®
    DATA_FILE = "lasso_reduce.csv"
    RANDOM_STATE = 42
    N_SPLITS = 5
    
    # ç‰¹å¾å·¥ç¨‹é…ç½®
    MULTICOLLINEARITY_THRESHOLD = 0.95
    FEATURE_SELECTION_PERCENTILE = 25  # ä¿ç•™å‰75%ç‰¹å¾
    
    # XGBoosté…ç½®ï¼ˆæ¿€è¿›ä¼˜åŒ–ï¼‰
    XGBOOST_PARAMS = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'learning_rate': 0.03,
        'max_depth': 6,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 10,
        'reg_alpha': 15,
        'reg_lambda': 15,
        'gamma': 0.2,
        'random_state': RANDOM_STATE,
        'tree_method': 'hist'
    }
    XGBOOST_ROUNDS = 2000
    XGBOOST_EARLY_STOPPING = 100
    
    # LightGBMé…ç½®ï¼ˆä¿å®ˆä¼˜åŒ–ï¼‰
    LIGHTGBM_PARAMS = {
        'random_state': RANDOM_STATE,
        'learning_rate': 0.04,
        'n_estimators': 300,
        'max_depth': 6,
        'num_leaves': 40,
        'min_child_samples': 30,
        'subsample': 0.85,
        'subsample_freq': 1,
        'colsample_bytree': 0.85,
        'reg_alpha': 5,
        'reg_lambda': 5,
        'min_split_gain': 0.1,
        'verbose': -1
    }
    
    # CatBoosté…ç½®ï¼ˆåŸç‰ˆç¨³å®šï¼‰
    CATBOOST_PARAMS = {
        'random_state': RANDOM_STATE,
        'learning_rate': 0.05,
        'iterations': 200,
        'depth': 6,
        'verbose': 0
    }

# ==================== æ•°æ®åŠ è½½æ¨¡å— ====================

def load_data(file_path=ModelConfig.DATA_FILE):
    """åŠ è½½æ•°æ®"""
    data = pd.read_csv(file_path)
    return data

# ==================== å·¥å…·å‡½æ•°æ¨¡å— ====================

class PrintUtils:
    """æ‰“å°å·¥å…·ç±» - ç»Ÿä¸€çš„è¾“å‡ºæ ¼å¼"""
    
    @staticmethod
    def print_section(title, width=50):
        """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
        print(f"\n{'='*width}")
        print(title)
        print(f"{'='*width}")
    
    @staticmethod
    def print_feature_stats(original_count, current_count, removed_count):
        """æ‰“å°ç‰¹å¾ç»Ÿè®¡"""
        reduction_pct = (removed_count / original_count * 100) if original_count > 0 else 0
        print(f"\nç§»é™¤çš„ç‰¹å¾æ•°é‡: {removed_count}")
        print(f"å¤„ç†åç‰¹å¾æ•°é‡: {current_count}")
        print(f"ç‰¹å¾å‡å°‘æ¯”ä¾‹: {reduction_pct:.2f}%")
    
    @staticmethod
    def print_model_scores(scores_dict, title="æ¨¡å‹æ€§èƒ½"):
        """æ‰“å°æ¨¡å‹åˆ†æ•°"""
        sorted_scores = sorted(scores_dict.items(), key=lambda x: x[1], reverse=True)
        print(f"\n{title}:")
        for rank, (name, score) in enumerate(sorted_scores, 1):
            print(f"  {rank}. {name:20s}: {score:.4f}")
        return sorted_scores

# ==================== ç‰¹å¾å·¥ç¨‹æ¨¡å— ====================

class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹ç±» - å°è£…æ‰€æœ‰ç‰¹å¾å¤„ç†æ–¹æ³•"""
    
    @staticmethod
    def remove_multicollinearity(X, threshold=ModelConfig.MULTICOLLINEARITY_THRESHOLD):
        """
        å¤„ç†å¤šé‡å…±çº¿æ€§é—®é¢˜
        ä¼˜åŒ–ç‚¹1: ç§»é™¤é«˜åº¦ç›¸å…³çš„ç‰¹å¾ï¼Œå‡å°‘å†—ä½™ä¿¡æ¯
        """
        PrintUtils.print_section("å¤„ç†å¤šé‡å…±çº¿æ€§")
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
        
        # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
        corr_matrix = X.corr().abs()
        upper_triangle = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        # æ‰¾å‡ºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
        high_corr_pairs = []
        for column in upper_triangle.columns:
            correlated_features = upper_triangle[column][upper_triangle[column] > threshold]
            if len(correlated_features) > 0:
                for corr_feature in correlated_features.index:
                    high_corr_pairs.append({
                        'feature1': column,
                        'feature2': corr_feature,
                        'correlation': upper_triangle.loc[corr_feature, column]
                    })
        
        # æ‰“å°é«˜ç›¸å…³ç‰¹å¾å¯¹
        if high_corr_pairs:
            print(f"\nå‘ç° {len(high_corr_pairs)} å¯¹é«˜åº¦ç›¸å…³çš„ç‰¹å¾ (ç›¸å…³ç³»æ•° > {threshold}):")
            for i, pair in enumerate(high_corr_pairs[:10], 1):
                print(f"  {i}. {pair['feature1']} <-> {pair['feature2']}: {pair['correlation']:.4f}")
            if len(high_corr_pairs) > 10:
                print(f"  ... è¿˜æœ‰ {len(high_corr_pairs) - 10} å¯¹")
        else:
            print(f"\næœªå‘ç°ç›¸å…³ç³»æ•°è¶…è¿‡ {threshold} çš„ç‰¹å¾å¯¹")
        
        # è¯†åˆ«éœ€è¦ç§»é™¤çš„ç‰¹å¾
        to_drop = set()
        for column in upper_triangle.columns:
            correlated = upper_triangle[column][upper_triangle[column] > threshold]
            if len(correlated) > 0 and column not in to_drop:
                to_drop.update(correlated.index.tolist())
        
        X_reduced = X.drop(columns=list(to_drop), errors='ignore')
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if len(to_drop) > 0:
            print(f"\nè¢«ç§»é™¤çš„ç‰¹å¾: {sorted(list(to_drop))[:20]}")
            if len(to_drop) > 20:
                print(f"  ... è¿˜æœ‰ {len(to_drop) - 20} ä¸ªç‰¹å¾")
        
        PrintUtils.print_feature_stats(X.shape[1], X_reduced.shape[1], len(to_drop))
        
        return X_reduced, list(to_drop)

    @staticmethod
    def select_by_importance(X, y, threshold_percentile=ModelConfig.FEATURE_SELECTION_PERCENTILE, method='xgboost'):
        """
        åŸºäºç‰¹å¾é‡è¦æ€§è¿›è¡Œç‰¹å¾ç­›é€‰
        ä¼˜åŒ–ç‚¹2: ä½¿ç”¨æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ç§»é™¤ä½ä»·å€¼ç‰¹å¾
        
        å‚æ•°ï¼š
        - X: DataFrameï¼Œç‰¹å¾çŸ©é˜µ
        - y: Seriesï¼Œç›®æ ‡å˜é‡
        - threshold_percentile: intï¼Œä¿ç•™ç‰¹å¾çš„ç™¾åˆ†ä½ï¼ˆé»˜è®¤25ï¼Œå³ä¿ç•™å‰75%é‡è¦çš„ç‰¹å¾ï¼‰
        - method: strï¼Œä½¿ç”¨çš„æ¨¡å‹ï¼ˆ'xgboost', 'lightgbm', 'catboost'ï¼‰
        
        è¿”å›ï¼š
        - X_selected: DataFrameï¼Œç­›é€‰åçš„ç‰¹å¾çŸ©é˜µ
        - selected_features: listï¼Œè¢«é€‰ä¸­çš„ç‰¹å¾åˆ—è¡¨
        - feature_importance_df: DataFrameï¼Œç‰¹å¾é‡è¦æ€§è¯¦æƒ…
        """
        PrintUtils.print_section("ç‰¹å¾é‡è¦æ€§ç­›é€‰")
        print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
        print(f"ä½¿ç”¨æ¨¡å‹: {method}")
    
        # è®­ç»ƒæ¨¡å‹è·å–ç‰¹å¾é‡è¦æ€§
        if method == 'xgboost':
            model = XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=ModelConfig.RANDOM_STATE,
                eval_metric='auc'
            )
        elif method == 'lightgbm':
            try:
                from lightgbm import LGBMClassifier
                model = LGBMClassifier(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=ModelConfig.RANDOM_STATE,
                    verbose=-1
                )
            except ImportError:
                print("LightGBMæœªå®‰è£…ï¼Œä½¿ç”¨XGBoost")
                model = XGBClassifier(n_estimators=100, random_state=ModelConfig.RANDOM_STATE)
        elif method == 'catboost':
            try:
                from catboost import CatBoostClassifier
                model = CatBoostClassifier(
                    iterations=100,
                    depth=6,
                    learning_rate=0.1,
                    random_state=ModelConfig.RANDOM_STATE,
                    verbose=0
                )
            except ImportError:
                print("CatBoostæœªå®‰è£…ï¼Œä½¿ç”¨XGBoost")
                model = XGBClassifier(n_estimators=100, random_state=ModelConfig.RANDOM_STATE)
        else:
            model = XGBClassifier(n_estimators=100, random_state=ModelConfig.RANDOM_STATE)
        
        # è®­ç»ƒæ¨¡å‹
        print("è®­ç»ƒæ¨¡å‹ä»¥è®¡ç®—ç‰¹å¾é‡è¦æ€§...")
        model.fit(X, y)
        
        # è·å–ç‰¹å¾é‡è¦æ€§
        importances = model.feature_importances_
        
        # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
        feature_importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': importances
        }).sort_values('importance', ascending=False)
        
        # è®¡ç®—é˜ˆå€¼
        threshold = np.percentile(importances, threshold_percentile)
        
        # ç­›é€‰é‡è¦ç‰¹å¾
        important_features = feature_importance_df[
            feature_importance_df['importance'] > threshold
        ]['feature'].tolist()
        
        X_selected = X[important_features]
        
        print(f"\nç­›é€‰é˜ˆå€¼ (ç¬¬{threshold_percentile}ç™¾åˆ†ä½): {threshold:.6f}")
        print(f"ç­›é€‰åç‰¹å¾æ•°é‡: {len(important_features)}")
        print(f"ç§»é™¤çš„ç‰¹å¾æ•°é‡: {X.shape[1] - len(important_features)}")
        print(f"ç‰¹å¾ä¿ç•™æ¯”ä¾‹: {len(important_features)/X.shape[1]*100:.1f}%")
        
        # æ˜¾ç¤ºTop 20é‡è¦ç‰¹å¾
        print(f"\nTop 20 é‡è¦ç‰¹å¾:")
        for i, row in feature_importance_df.head(20).iterrows():
            print(f"  {i+1:2d}. {row['feature']:40s}: {row['importance']:.6f}")
        
        # æ˜¾ç¤ºè¢«ç§»é™¤çš„ç‰¹å¾ï¼ˆå¦‚æœä¸å¤šï¼‰
        removed_features = [f for f in X.columns if f not in important_features]
        if len(removed_features) <= 20:
            print(f"\nè¢«ç§»é™¤çš„ç‰¹å¾:")
            for f in removed_features:
                imp = feature_importance_df[feature_importance_df['feature'] == f]['importance'].values[0]
                print(f"  - {f}: {imp:.6f}")
        elif len(removed_features) > 0:
            print(f"\nç§»é™¤äº† {len(removed_features)} ä¸ªä½é‡è¦æ€§ç‰¹å¾")
        
        return X_selected, important_features, feature_importance_df
    
    @staticmethod
    def standardize_features(X, feature_names=None):
        """æ ‡å‡†åŒ–ç‰¹å¾"""
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        if feature_names is not None:
            return pd.DataFrame(X_scaled, columns=feature_names, index=X.index if isinstance(X, pd.DataFrame) else None)
        return X_scaled

def preprocess_data(data, remove_collinearity=True, corr_threshold=ModelConfig.MULTICOLLINEARITY_THRESHOLD, 
                    use_feature_selection=True, selection_threshold=ModelConfig.FEATURE_SELECTION_PERCENTILE):
    """
    ç‰¹å¾å¤„ç†ï¼ˆæ•´åˆå¤šé‡å…±çº¿æ€§å¤„ç†å’Œç‰¹å¾é‡è¦æ€§ç­›é€‰ï¼‰
    ä¼˜åŒ–ç‚¹4: å·®å¼‚åŒ–ç‰¹å¾ç­–ç•¥ - ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒç‰¹å¾é›†
    
    è¿”å›ä¸¤ä¸ªç‰¹å¾é›†ï¼š
    - X_original: åªç»è¿‡å¤šé‡å…±çº¿æ€§å¤„ç†ï¼ˆç»™LightGBMç”¨ï¼‰
    - X_selected: ç»è¿‡å¤šé‡å…±çº¿æ€§+ç‰¹å¾ç­›é€‰ï¼ˆç»™XGBoostå’ŒCatBoostç”¨ï¼‰
    
    å‚æ•°ï¼š
    - data: DataFrameï¼ŒåŸå§‹æ•°æ®
    - remove_collinearity: boolï¼Œæ˜¯å¦ç§»é™¤å¤šé‡å…±çº¿æ€§ç‰¹å¾
    - corr_threshold: floatï¼Œç›¸å…³ç³»æ•°é˜ˆå€¼
    - use_feature_selection: boolï¼Œæ˜¯å¦ä½¿ç”¨ç‰¹å¾é‡è¦æ€§ç­›é€‰
    - selection_threshold: intï¼Œç‰¹å¾ç­›é€‰ç™¾åˆ†ä½é˜ˆå€¼
    """
    X = data.iloc[:, :-1]
    y = data.iloc[:, -1]
    
    print(f"\nåŸå§‹æ•°æ®ç»´åº¦: {X.shape}")
    
    # 1. å¤„ç†å¤šé‡å…±çº¿æ€§
    removed_features = []
    if remove_collinearity:
        X, removed_features = FeatureEngineer.remove_multicollinearity(X, threshold=corr_threshold)
    
    # ä¿å­˜å¤šé‡å…±çº¿æ€§å¤„ç†åçš„ç‰¹å¾ï¼ˆç»™LightGBMç”¨ï¼‰
    X_after_collinearity = X.copy()
    
    # 2. ç‰¹å¾é‡è¦æ€§ç­›é€‰ï¼ˆåªå¯¹XGBoostå’ŒCatBoostï¼‰
    selected_features = []
    feature_importance_df = None
    X_selected = X.copy()
    
    if use_feature_selection:
        X_selected, selected_features, feature_importance_df = FeatureEngineer.select_by_importance(
            X, y, 
            threshold_percentile=selection_threshold,
            method='xgboost'
        )
    
    # 3. ç‰¹å¾æ ‡å‡†åŒ–
    PrintUtils.print_section("ç‰¹å¾æ ‡å‡†åŒ–")
    
    # æ ‡å‡†åŒ–åŸå§‹ç‰¹å¾é›†ï¼ˆç»™LightGBMï¼‰
    X_original_df = FeatureEngineer.standardize_features(X_after_collinearity, X_after_collinearity.columns)
    
    # æ ‡å‡†åŒ–ç­›é€‰åç‰¹å¾é›†ï¼ˆç»™XGBoostå’ŒCatBoostï¼‰
    X_selected_df = FeatureEngineer.standardize_features(X_selected, X_selected.columns)
    
    print(f"LightGBMç‰¹å¾æ•°: {X_original_df.shape[1]}")
    print(f"XGBoost/CatBoostç‰¹å¾æ•°: {X_selected_df.shape[1]}")
    
    return X_original_df, X_selected_df, y, removed_features, selected_features, feature_importance_df

# ==================== æ¨¡å‹ç®¡ç†æ¨¡å— ====================

class ModelManager:
    """æ¨¡å‹ç®¡ç†ç±» - ç®¡ç†æ‰€æœ‰æ¨¡å‹çš„é…ç½®å’Œè®­ç»ƒ"""
    
    @staticmethod
    def get_base_models():
        """
        è·å–ä¼˜åŒ–åçš„åŸºç¡€æ¨¡å‹
        ä¼˜åŒ–ç‚¹3: å¤šæ¨¡å‹é›†æˆ - XGBoost + LightGBM + CatBoost
        ä¼˜åŒ–ç‚¹5: è¶…å‚æ•°ä¼˜åŒ– - é’ˆå¯¹æ€§çš„å‚æ•°è°ƒä¼˜
        """
        models = {}
        
        # XGBoost - ä½¿ç”¨åŸç”Ÿæ¥å£æ”¯æŒæ—©åœï¼ˆæ¿€è¿›ä¼˜åŒ–ï¼‰
        models['XGBoost'] = (ModelConfig.XGBOOST_PARAMS.copy(), True)
        
        # LightGBM - ä¿å®ˆä¼˜åŒ–ç­–ç•¥
        try:
            from lightgbm import LGBMClassifier
            models['LightGBM'] = (LGBMClassifier(**ModelConfig.LIGHTGBM_PARAMS), True)
        except ImportError:
            models['LightGBM'] = (None, False)
        
        # CatBoost - åŸç‰ˆç¨³å®šå‚æ•°
        try:
            from catboost import CatBoostClassifier
            models['CatBoost'] = (CatBoostClassifier(**ModelConfig.CATBOOST_PARAMS), True)
        except ImportError:
            models['CatBoost'] = (None, False)
        
        return models
    
    @staticmethod
    def train_with_cv(model_name, model_params, X, y, n_splits=ModelConfig.N_SPLITS):
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼ˆæ”¯æŒæ—©åœï¼‰
        ä¼˜åŒ–ç‚¹6: æ—©åœæœºåˆ¶ - XGBoostçš„æ—©åœè®­ç»ƒ
        """
        PrintUtils.print_section(f"è®­ç»ƒæ¨¡å‹: {model_name}")
        
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=ModelConfig.RANDOM_STATE)
        oof_predictions = np.zeros(len(X))
        cv_scores = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):
            print(f"  Fold {fold}/{n_splits}...", end=' ')
            
            if isinstance(X, pd.DataFrame):
                X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
                y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            else:
                X_train, X_val = X[train_idx], X[val_idx]
                y_train, y_val = y[train_idx], y[val_idx]
            
            # XGBoostä½¿ç”¨æ—©åœ
            if model_name == 'XGBoost':
                dtrain = xgb.DMatrix(X_train, label=y_train)
                dval = xgb.DMatrix(X_val, label=y_val)
                
                model = xgb.train(
                    model_params,
                    dtrain,
                    num_boost_round=ModelConfig.XGBOOST_ROUNDS,
                    evals=[(dval, 'val')],
                    early_stopping_rounds=ModelConfig.XGBOOST_EARLY_STOPPING,
                    verbose_eval=False
                )
                
                y_pred = model.predict(dval)
            else:
                # LightGBMå’ŒCatBoost - ä½¿ç”¨åŸç‰ˆè®­ç»ƒæ–¹å¼
                model_params.fit(X_train, y_train)
                y_pred = model_params.predict_proba(X_val)[:, 1]
            
            oof_predictions[val_idx] = y_pred
            
            auc = roc_auc_score(y_val, y_pred)
            cv_scores.append(auc)
            print(f"AUC: {auc:.4f}")
        
        avg_auc = np.mean(cv_scores)
        std_auc = np.std(cv_scores)
        print(f"\n  {model_name} å¹³å‡ AUC: {avg_auc:.4f} Â± {std_auc:.4f}")
        
        return oof_predictions, cv_scores, avg_auc

# ==================== èåˆç­–ç•¥æ¨¡å— ====================

class EnsembleStrategy:
    """èåˆç­–ç•¥ç±» - å®ç°å¤šç§æ¨¡å‹èåˆæ–¹æ³•"""
    
    @staticmethod
    def simple_average(predictions_dict, y_true):
        """ç®€å•å¹³å‡èåˆ"""
        PrintUtils.print_section("ç­–ç•¥1: ç®€å•å¹³å‡èåˆ")
        
        predictions = list(predictions_dict.values())
        ensemble_pred = np.mean(predictions, axis=0)
        auc = roc_auc_score(y_true, ensemble_pred)
        
        print(f"èåˆæ¨¡å‹æ•°é‡: {len(predictions)}")
        print(f"ç®€å•å¹³å‡ AUC: {auc:.4f}")
        
        return ensemble_pred, auc
    
    @staticmethod
    def weighted_average(predictions_dict, weights, y_true):
        """åŠ æƒå¹³å‡èåˆ"""
        PrintUtils.print_section("ç­–ç•¥2: åŠ æƒå¹³å‡èåˆ")
        
        predictions = list(predictions_dict.values())
        weights = np.array(weights) / np.sum(weights)
        
        ensemble_pred = np.zeros(len(predictions[0]))
        for pred, weight in zip(predictions, weights):
            ensemble_pred += pred * weight
        
        auc = roc_auc_score(y_true, ensemble_pred)
        
        print(f"æ¨¡å‹æƒé‡:")
        for name, weight in zip(predictions_dict.keys(), weights):
            print(f"  {name}: {weight:.4f}")
        print(f"åŠ æƒå¹³å‡ AUC: {auc:.4f}")
        
        return ensemble_pred, auc
    
    @staticmethod
    def rank_average(predictions_dict, y_true):
        """æ’åå¹³å‡èåˆ"""
        PrintUtils.print_section("ç­–ç•¥3: æ’åå¹³å‡èåˆ")
        
        rank_predictions = []
        for name, pred in predictions_dict.items():
            ranks = pd.Series(pred).rank(pct=True)
            rank_predictions.append(ranks.values)
        
        ensemble_pred = np.mean(rank_predictions, axis=0)
        auc = roc_auc_score(y_true, ensemble_pred)
        
        print(f"èåˆæ¨¡å‹æ•°é‡: {len(rank_predictions)}")
        print(f"æ’åå¹³å‡ AUC: {auc:.4f}")
        
        return ensemble_pred, auc
    
    @staticmethod
    def stacking(predictions_dict, y_true, n_splits=ModelConfig.N_SPLITS):
        """Stackingèåˆ"""
        PrintUtils.print_section("ç­–ç•¥4: Stackingèåˆ")
        
        X_meta = np.column_stack(list(predictions_dict.values()))
        
        print(f"å…ƒç‰¹å¾ç»´åº¦: {X_meta.shape}")
        print(f"åŸºæ¨¡å‹æ•°é‡: {len(predictions_dict)}")
        
        y_true_array = y_true if isinstance(y_true, np.ndarray) else y_true.values
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=ModelConfig.RANDOM_STATE)
        meta_predictions = np.zeros(len(y_true_array))
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(X_meta, y_true_array), 1):
            X_train, X_val = X_meta[train_idx], X_meta[val_idx]
            y_train, y_val = y_true_array[train_idx], y_true_array[val_idx]
            
            meta_model = LogisticRegression(random_state=ModelConfig.RANDOM_STATE, max_iter=1000)
            meta_model.fit(X_train, y_train)
            meta_predictions[val_idx] = meta_model.predict_proba(X_val)[:, 1]
        
        auc = roc_auc_score(y_true_array, meta_predictions)
        print(f"Stacking AUC: {auc:.4f}")
        
        final_meta = LogisticRegression(random_state=ModelConfig.RANDOM_STATE, max_iter=1000)
        final_meta.fit(X_meta, y_true_array)
        print(f"\nå…ƒå­¦ä¹ å™¨æƒé‡:")
        for name, coef in zip(predictions_dict.keys(), final_meta.coef_[0]):
            print(f"  {name}: {coef:.4f}")
        
        return meta_predictions, auc

# ==================== ä¸»è®­ç»ƒæµç¨‹ ====================

def train_model_with_ensemble(X_original, X_selected, y, n_splits=ModelConfig.N_SPLITS, ensemble_methods='all'):
    """
    ä½¿ç”¨å¤šä¸ªæ¨¡å‹å’Œèåˆç­–ç•¥è¿›è¡Œè®­ç»ƒ
    ä¼˜åŒ–ç‚¹7: å¤šèåˆç­–ç•¥ - ç®€å•å¹³å‡/åŠ æƒå¹³å‡/æ’åå¹³å‡/Stacking
    
    å‚æ•°ï¼š
    - X_original: åŸå§‹ç‰¹å¾ï¼ˆç»™LightGBMç”¨ï¼‰
    - X_selected: ç­›é€‰åç‰¹å¾ï¼ˆç»™XGBoostå’ŒCatBoostç”¨ï¼‰
    - y: ç›®æ ‡å˜é‡
    """
    PrintUtils.print_section("å¼€å§‹æ¨¡å‹èåˆ", 70)
    
    all_models = ModelManager.get_base_models()
    available_models = {name: params for name, (params, available) in all_models.items() if available}
    
    print(f"\nå¯ç”¨çš„ä¼˜åŒ–æ¨¡å‹:")
    for name in available_models.keys():
        feature_info = "åŸå§‹ç‰¹å¾" if name == 'LightGBM' else "ç­›é€‰ç‰¹å¾"
        if name == 'XGBoost':
            param_info = "ä¼˜åŒ–å‚æ•°+æ—©åœ"
        elif name == 'LightGBM':
            param_info = "ä¼˜åŒ–å‚æ•°"
        else:  # CatBoost
            param_info = "åŸç‰ˆå‚æ•°"
        print(f"  âœ“ {name} ({param_info} + {feature_info})")
    
    unavailable_models = [name for name, (_, available) in all_models.items() if not available]
    if unavailable_models:
        print(f"\nä¸å¯ç”¨çš„æ¨¡å‹ (éœ€è¦å®‰è£…):")
        for name in unavailable_models:
            print(f"  âœ— {name}")
    
    PrintUtils.print_section("å¼€å§‹è®­ç»ƒåŸºç¡€æ¨¡å‹", 70)
    print(f"ä½¿ç”¨ {n_splits} æŠ˜äº¤å‰éªŒè¯ + æ—©åœæœºåˆ¶")
    print(f"LightGBM: ä½¿ç”¨åŸå§‹ç‰¹å¾ ({X_original.shape[1]}ä¸ª)")
    print(f"XGBoost/CatBoost: ä½¿ç”¨ç­›é€‰ç‰¹å¾ ({X_selected.shape[1]}ä¸ª)")
    
    predictions_dict = {}
    model_scores = {}
    
    for name, params in available_models.items():
        # LightGBMä½¿ç”¨åŸå§‹ç‰¹å¾ï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨ç­›é€‰åçš„ç‰¹å¾
        X_to_use = X_original if name == 'LightGBM' else X_selected
        
        oof_pred, cv_scores, avg_auc = ModelManager.train_with_cv(
            name, params, X_to_use, y, n_splits
        )
        predictions_dict[name] = oof_pred
        model_scores[name] = avg_auc
    
    # æ˜¾ç¤ºåŸºç¡€æ¨¡å‹æ€§èƒ½å¯¹æ¯”
    sorted_scores = PrintUtils.print_model_scores(model_scores, "åŸºç¡€æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    
    # åº”ç”¨æ¨¡å‹èåˆç­–ç•¥
    PrintUtils.print_section("åº”ç”¨æ¨¡å‹èåˆç­–ç•¥", 70)
    
    ensemble_results = {}
    y_array = y if isinstance(y, np.ndarray) else y.values
    
    if ensemble_methods == 'all':
        methods = ['simple', 'weighted', 'rank', 'stacking']
    elif isinstance(ensemble_methods, str):
        methods = [ensemble_methods]
    else:
        methods = ensemble_methods
    
    if 'simple' in methods:
        pred, auc = EnsembleStrategy.simple_average(predictions_dict, y_array)
        ensemble_results['Simple Average'] = auc
    
    if 'weighted' in methods:
        weights = [model_scores[name] for name in predictions_dict.keys()]
        pred, auc = EnsembleStrategy.weighted_average(predictions_dict, weights, y_array)
        ensemble_results['Weighted Average'] = auc
    
    if 'rank' in methods:
        pred, auc = EnsembleStrategy.rank_average(predictions_dict, y_array)
        ensemble_results['Rank Average'] = auc
    
    if 'stacking' in methods:
        pred, auc = EnsembleStrategy.stacking(predictions_dict, y_array, n_splits)
        ensemble_results['Stacking'] = auc
    
    # æœ€ç»ˆç»“æœæ±‡æ€»
    PrintUtils.print_section("æœ€ç»ˆç»“æœæ±‡æ€»", 70)
    
    print(f"\nåŸºç¡€æ¨¡å‹:")
    for rank, (name, score) in enumerate(sorted_scores, 1):
        print(f"  {rank}. {name:20s}: {score:.4f}")
    
    if ensemble_results:
        sorted_ensemble = PrintUtils.print_model_scores(ensemble_results, "èåˆæ¨¡å‹")
    
    all_results = {**model_scores, **ensemble_results}
    best_model = max(all_results.items(), key=lambda x: x[1])
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model[0]}")
    print(f"   AUCåˆ†æ•°: {best_model[1]:.4f}")
    
    best_base_auc = max(model_scores.values())
    if ensemble_results:
        best_ensemble_auc = max(ensemble_results.values())
        improvement = best_ensemble_auc - best_base_auc
        improvement_pct = (improvement / best_base_auc) * 100
        print(f"\nğŸ“ˆ èåˆæå‡:")
        print(f"   æœ€ä½³åŸºç¡€æ¨¡å‹ AUC: {best_base_auc:.4f}")
        print(f"   æœ€ä½³èåˆæ¨¡å‹ AUC: {best_ensemble_auc:.4f}")
        print(f"   ç»å¯¹æå‡: {improvement:.4f}")
        print(f"   ç›¸å¯¹æå‡: {improvement_pct:.2f}%")
    
    return {
        'model_scores': model_scores,
        'ensemble_results': ensemble_results,
        'predictions': predictions_dict,
        'best_model': best_model
    }

# ==================== ä¸»å‡½æ•° ====================

def main(remove_collinearity=True, 
         corr_threshold=ModelConfig.MULTICOLLINEARITY_THRESHOLD, 
         use_feature_selection=True, 
         selection_threshold=ModelConfig.FEATURE_SELECTION_PERCENTILE,
         ensemble_methods='all', 
         n_splits=ModelConfig.N_SPLITS):
    """
    ä¸»å‡½æ•°ï¼ˆæ¨¡å—åŒ–é‡æ„ç‰ˆï¼‰
    ä¼˜åŒ–ç‚¹8: æ¨¡å—åŒ–æ¶æ„ - æ¸…æ™°çš„ä»£ç ç»„ç»‡ç»“æ„
    
    ç›¸æ¯”åŸºç¡€ç‰ˆæœ¬(model.py)çš„8å¤§ä¼˜åŒ–ç‚¹ï¼š
    1. âœ… å¤šé‡å…±çº¿æ€§å¤„ç† - ç§»é™¤é«˜åº¦ç›¸å…³ç‰¹å¾
    2. âœ… ç‰¹å¾é‡è¦æ€§ç­›é€‰ - åŸºäºXGBoosté‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©
    3. âœ… å¤šæ¨¡å‹é›†æˆ - XGBoost + LightGBM + CatBoost
    4. âœ… å·®å¼‚åŒ–ç‰¹å¾ç­–ç•¥ - ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒç‰¹å¾é›†
    5. âœ… è¶…å‚æ•°ä¼˜åŒ– - é’ˆå¯¹æ€§çš„å‚æ•°è°ƒä¼˜
    6. âœ… æ—©åœæœºåˆ¶ - XGBoostçš„æ—©åœè®­ç»ƒ
    7. âœ… å¤šèåˆç­–ç•¥ - ç®€å•å¹³å‡/åŠ æƒå¹³å‡/æ’åå¹³å‡/Stacking
    8. âœ… æ¨¡å—åŒ–æ¶æ„ - æ¸…æ™°çš„ä»£ç ç»„ç»‡ç»“æ„
    
    å·®å¼‚åŒ–ç­–ç•¥è¯´æ˜ï¼š
    - XGBoost:  ç­›é€‰ç‰¹å¾ + ä¼˜åŒ–å‚æ•° + æ—©åœæœºåˆ¶ï¼ˆæ¿€è¿›ä¼˜åŒ–ï¼‰
    - LightGBM: åŸå§‹ç‰¹å¾ + ä¼˜åŒ–å‚æ•°ï¼ˆä¿å®ˆä¼˜åŒ–ç­–ç•¥ï¼‰
    - CatBoost: ç­›é€‰ç‰¹å¾ + åŸç‰ˆå‚æ•°ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰
    
    å‚æ•°ï¼š
    - remove_collinearity: boolï¼Œæ˜¯å¦ç§»é™¤å¤šé‡å…±çº¿æ€§ç‰¹å¾
    - corr_threshold: floatï¼Œç›¸å…³ç³»æ•°é˜ˆå€¼
    - use_feature_selection: boolï¼Œæ˜¯å¦ä½¿ç”¨ç‰¹å¾é‡è¦æ€§ç­›é€‰
    - selection_threshold: intï¼Œä¿ç•™ç‰¹å¾ç™¾åˆ†ä½ï¼ˆ25=ä¿ç•™å‰75%ï¼‰
    - ensemble_methods: str or list, èåˆæ–¹æ³•
    - n_splits: int, äº¤å‰éªŒè¯æŠ˜æ•°
    """
    PrintUtils.print_section("Model V1+V3 Update - æ¨¡å—åŒ–é‡æ„ç‰ˆ", 70)
    print("å·®å¼‚åŒ–ä¼˜åŒ–ç­–ç•¥:")
    print("  XGBoost:  ç­›é€‰ç‰¹å¾ + ä¼˜åŒ–å‚æ•° + æ—©åœæœºåˆ¶")
    print("  LightGBM: åŸå§‹ç‰¹å¾ + ä¼˜åŒ–å‚æ•°ï¼ˆä¿å®ˆä¼˜åŒ–ï¼‰")
    print("  CatBoost: ç­›é€‰ç‰¹å¾ + åŸç‰ˆå‚æ•°ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰")
    print("="*70)
    
    print("\nåŠ è½½æ•°æ®...")
    data = load_data(ModelConfig.DATA_FILE)
    print(f"æ•°æ®ç»´åº¦: {data.shape}")
    
    print("\nç‰¹å¾å¤„ç†...")
    X_original, X_selected, y, removed_features, selected_features, feature_importance_df = preprocess_data(
        data, 
        remove_collinearity=remove_collinearity,
        corr_threshold=corr_threshold,
        use_feature_selection=use_feature_selection,
        selection_threshold=selection_threshold
    )
    
    results = train_model_with_ensemble(
        X_original, X_selected, y, 
        n_splits=n_splits, 
        ensemble_methods=ensemble_methods
    )
    
    print("\n" + "="*70)
    print("å®Œæˆï¼")
    print("="*70)
    
    results['removed_features'] = removed_features
    results['selected_features'] = selected_features
    results['feature_importance'] = feature_importance_df
    
    return results

if __name__ == "__main__":
    """
    æ¨¡å—åŒ–é‡æ„ç‰ˆæœ¬æ‰§è¡Œå…¥å£
    
    ä»£ç ç»„ç»‡ç»“æ„ï¼š
    1. é…ç½®ç®¡ç†æ¨¡å— (ModelConfig) - é›†ä¸­ç®¡ç†æ‰€æœ‰é…ç½®å‚æ•°
    2. å·¥å…·å‡½æ•°æ¨¡å— (PrintUtils) - ç»Ÿä¸€çš„è¾“å‡ºæ ¼å¼
    3. ç‰¹å¾å·¥ç¨‹æ¨¡å— (FeatureEngineer) - å°è£…æ‰€æœ‰ç‰¹å¾å¤„ç†æ–¹æ³•
    4. æ¨¡å‹ç®¡ç†æ¨¡å— (ModelManager) - ç®¡ç†æ¨¡å‹é…ç½®å’Œè®­ç»ƒ
    5. èåˆç­–ç•¥æ¨¡å— (EnsembleStrategy) - å®ç°å¤šç§æ¨¡å‹èåˆæ–¹æ³•
    6. ä¸»è®­ç»ƒæµç¨‹ (train_model_with_ensemble, main) - åè°ƒå„æ¨¡å—
    
    ç›¸æ¯”åŸºç¡€ç‰ˆæœ¬çš„ä¼˜åŠ¿ï¼š
    - ä»£ç æ›´æ˜“ç»´æŠ¤ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼ŒèŒè´£æ¸…æ™°
    - é…ç½®æ›´çµæ´»ï¼šé›†ä¸­é…ç½®ç®¡ç†ï¼Œæ˜“äºè°ƒæ•´
    - æ‰©å±•æ€§æ›´å¼ºï¼šæ–°å¢æ¨¡å‹æˆ–ç­–ç•¥åªéœ€ä¿®æ”¹ç›¸åº”æ¨¡å—
    - å¤ç”¨æ€§æ›´é«˜ï¼šå„æ¨¡å—æ–¹æ³•å¯ç‹¬ç«‹ä½¿ç”¨
    
    æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥ï¼š
    1. ç‰¹å¾å·®å¼‚åŒ–ï¼š
       - LightGBM: åŸå§‹ç‰¹å¾ï¼ˆ105ä¸ªï¼‰+ ä¿å®ˆå‚æ•°ä¼˜åŒ–
       - XGBoost:  ç­›é€‰ç‰¹å¾ï¼ˆ80ä¸ªï¼‰+ æ¿€è¿›å‚æ•°ä¼˜åŒ– + æ—©åœ
       - CatBoost: ç­›é€‰ç‰¹å¾ï¼ˆ80ä¸ªï¼‰+ åŸç‰ˆç¨³å®šå‚æ•°
    
    2. é…ç½®å‚æ•°ï¼ˆå¯åœ¨ ModelConfig ä¸­ä¿®æ”¹ï¼‰ï¼š
       - selection_threshold=25: ä¿ç•™å‰75%é‡è¦ç‰¹å¾
       - corr_threshold=0.95: å¤šé‡å…±çº¿æ€§é˜ˆå€¼
       - n_splits=5: äº¤å‰éªŒè¯æŠ˜æ•°
       - å„æ¨¡å‹çš„è¶…å‚æ•°é…ç½®
    """
    results = main(
        remove_collinearity=True,                              # å¤šé‡å…±çº¿æ€§å¤„ç†
        corr_threshold=ModelConfig.MULTICOLLINEARITY_THRESHOLD,  # ç›¸å…³ç³»æ•°é˜ˆå€¼
        use_feature_selection=True,                             # ç‰¹å¾é‡è¦æ€§ç­›é€‰
        selection_threshold=ModelConfig.FEATURE_SELECTION_PERCENTILE,  # ä¿ç•™å‰75%ç‰¹å¾
        ensemble_methods='all',                                 # æ‰€æœ‰èåˆç­–ç•¥
        n_splits=ModelConfig.N_SPLITS                          # 5æŠ˜äº¤å‰éªŒè¯
    )
