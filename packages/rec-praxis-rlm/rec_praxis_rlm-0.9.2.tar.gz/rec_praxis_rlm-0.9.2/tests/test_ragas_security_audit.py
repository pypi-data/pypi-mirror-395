"""
RAGAS evaluation for SecurityAuditAgent using Groq LLM judge.

This test validates the security audit agent's ability to:
1. Detect security vulnerabilities accurately
2. Provide context-aware remediation guidance
3. Learn from past security fixes
4. Map findings to OWASP/CWE standards

Evaluation metrics:
- Faithfulness: Are findings grounded in actual code patterns?
- Context Recall: Does the agent retrieve relevant past fixes?
- Context Precision: Are retrieved fixes actually useful?

Run with: pytest tests/test_ragas_security_audit.py -m ragas -v
"""

import os
import sys
from typing import List, Dict
from dataclasses import dataclass

import pytest

# Check if RAGAS is available
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import faithfulness, context_recall, context_precision
    from langchain_groq import ChatGroq
    from ragas.llms import LangchainLLMWrapper

    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False

from rec_praxis_rlm import ProceduralMemory, Experience, MemoryConfig, FactStore

# Check for GROQ_API_KEY
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")


@dataclass
class SecurityFinding:
    """Security vulnerability finding."""

    vulnerability: str
    severity: str
    owasp_category: str
    cwe_id: str
    remediation: str


class SecurityAuditEvaluator:
    """Evaluator for security audit agent using RAGAS metrics."""

    def __init__(self, use_groq: bool = True):
        """Initialize evaluator with optional Groq LLM judge."""
        self.use_groq = use_groq and RAGAS_AVAILABLE and GROQ_API_KEY

        # Initialize memory with past security fixes
        self.memory = ProceduralMemory(
            config=MemoryConfig(
                storage_path=":memory:",
                similarity_threshold=0.3,
                env_weight=0.6,
                goal_weight=0.4,
            )
        )

        # Initialize fact store with OWASP/CWE knowledge
        self.fact_store = FactStore(storage_path=":memory:")

        # Populate memory with past security fixes
        self._populate_security_history()
        self._load_security_knowledge()

        # Initialize Groq LLM if available
        if self.use_groq and RAGAS_AVAILABLE:
            self.llm = ChatGroq(
                model="llama-3.3-70b-versatile",
                temperature=0.0,
                groq_api_key=GROQ_API_KEY,
            )
            # Use LangchainLLMWrapper for RAGAS compatibility
            self.ragas_llm = LangchainLLMWrapper(self.llm)

    def _populate_security_history(self) -> None:
        """Populate memory with 10 representative past security fixes."""
        past_fixes = [
            Experience(
                env_features=["python", "database", "security", "sql-injection"],
                goal="fix SQL injection vulnerability",
                action="Replaced string concatenation with parameterized queries using psycopg2's execute() with %s placeholders",
                result="SQL injection eliminated. All 15 queries now use prepared statements.",
                success=True,
                timestamp="2025-01-15T10:30:00Z",
            ),
            Experience(
                env_features=["python", "cryptography", "security", "passwords"],
                goal="fix weak password hashing",
                action="Migrated from MD5 to Argon2id with 16MB memory cost and 3 iterations",
                result="Password hashing now meets OWASP standards. 10k existing hashes migrated.",
                success=True,
                timestamp="2025-01-20T14:00:00Z",
            ),
            Experience(
                env_features=["python", "web", "security", "xss"],
                goal="prevent XSS attacks",
                action="Enabled auto-escaping in Jinja2 templates and added Content-Security-Policy header",
                result="XSS vulnerabilities eliminated in all 25 templates.",
                success=True,
                timestamp="2025-01-25T09:15:00Z",
            ),
            Experience(
                env_features=["python", "web", "security", "csrf"],
                goal="add CSRF protection",
                action="Implemented Flask-WTF CSRF tokens for all POST/PUT/DELETE requests",
                result="CSRF protection enabled across 18 forms.",
                success=True,
                timestamp="2025-02-01T11:00:00Z",
            ),
            Experience(
                env_features=["python", "security", "deserialization"],
                goal="fix insecure deserialization",
                action="Replaced pickle with JSON for session storage. Added input validation.",
                result="Deserialization vulnerability eliminated. Sessions now use signed JWT.",
                success=True,
                timestamp="2025-02-10T16:30:00Z",
            ),
            Experience(
                env_features=["python", "web", "security", "path-traversal"],
                goal="prevent path traversal attacks",
                action="Added path validation using secure_filename() and restricted file access to upload directory",
                result="Path traversal blocked. All file operations now validated.",
                success=True,
                timestamp="2025-02-15T13:20:00Z",
            ),
            Experience(
                env_features=["python", "web", "security", "ssrf"],
                goal="prevent SSRF attacks",
                action="Implemented URL whitelist and blocked private IP ranges (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16)",
                result="SSRF vulnerability eliminated. External requests now validated.",
                success=True,
                timestamp="2025-02-20T10:45:00Z",
            ),
            Experience(
                env_features=["python", "web", "security", "authentication"],
                goal="fix session fixation vulnerability",
                action="Regenerated session ID after login using Flask's session.regenerate()",
                result="Session fixation eliminated. Secure session management implemented.",
                success=True,
                timestamp="2025-02-25T15:00:00Z",
            ),
            Experience(
                env_features=["python", "security", "logging"],
                goal="prevent sensitive data exposure in logs",
                action="Implemented log sanitization to redact passwords, API keys, and tokens",
                result="Sensitive data no longer logged. 500+ log statements audited.",
                success=True,
                timestamp="2025-03-01T09:30:00Z",
            ),
            Experience(
                env_features=["python", "security", "dependencies"],
                goal="fix vulnerable dependencies",
                action="Updated requests library from 2.25.0 to 2.31.0 to patch CVE-2023-32681",
                result="Vulnerability patched. All dependencies now up-to-date.",
                success=True,
                timestamp="2025-03-05T14:15:00Z",
            ),
        ]

        for exp in past_fixes:
            self.memory.store(exp)

    def _load_security_knowledge(self) -> None:
        """Load OWASP/CWE facts into semantic memory."""
        security_facts = """
        OWASP = Open Web Application Security Project
        A03:2021 = Injection vulnerabilities including SQL, NoSQL, OS command injection
        CWE-89 = SQL Injection
        CWE-79 = Cross-site Scripting (XSS)
        CWE-352 = Cross-Site Request Forgery (CSRF)
        CWE-502 = Deserialization of Untrusted Data
        """
        self.fact_store.extract_facts(security_facts, source_id="owasp_standards")

    def detect_and_retrieve(
        self, code_snippet: str, env_features: List[str], vulnerability_type: str
    ) -> tuple[SecurityFinding, List[str]]:
        """
        Detect vulnerability and retrieve relevant past fixes.

        Returns:
            Tuple of (finding, contexts)
        """
        # Retrieve relevant past fixes
        experiences = self.memory.recall(
            env_features=env_features, goal=f"fix {vulnerability_type}", top_k=5
        )

        # Build contexts from retrieved experiences
        contexts = [
            f"Environment: {', '.join(exp.env_features)}. "
            f"Goal: {exp.goal}. "
            f"Fix Applied: {exp.action}. "
            f"Result: {exp.result}. "
            f"Success: {exp.success}."
            for exp in experiences
        ]

        # Get most relevant successful fix
        successful = [exp for exp in experiences if exp.success]
        if not successful:
            remediation = f"No past fixes found for {vulnerability_type}"
            severity = "UNKNOWN"
            owasp = "Unknown"
            cwe = "Unknown"
        else:
            # Use first (most relevant) successful fix
            most_relevant = successful[0]
            remediation = most_relevant.action

            # Map vulnerability to OWASP/CWE
            if "sql" in vulnerability_type.lower():
                severity = "CRITICAL"
                owasp = "A03:2021-Injection"
                cwe = "CWE-89"
            elif "crypto" in vulnerability_type.lower() or "hash" in vulnerability_type.lower():
                severity = "HIGH"
                owasp = "A02:2021-Cryptographic Failures"
                cwe = "CWE-327"
            elif "xss" in vulnerability_type.lower():
                severity = "HIGH"
                owasp = "A03:2021-Injection"
                cwe = "CWE-79"
            elif "csrf" in vulnerability_type.lower():
                severity = "MEDIUM"
                owasp = "A01:2021-Broken Access Control"
                cwe = "CWE-352"
            elif "deserial" in vulnerability_type.lower():
                severity = "CRITICAL"
                owasp = "A08:2021-Software and Data Integrity Failures"
                cwe = "CWE-502"
            else:
                severity = "MEDIUM"
                owasp = "Unknown"
                cwe = "Unknown"

        finding = SecurityFinding(
            vulnerability=vulnerability_type,
            severity=severity,
            owasp_category=owasp,
            cwe_id=cwe,
            remediation=remediation,
        )

        return finding, contexts

    def evaluate_scenario(
        self,
        question: str,
        code_snippet: str,
        env_features: List[str],
        vulnerability_type: str,
        ground_truth: str,
    ) -> Dict[str, float]:
        """
        Evaluate a single security audit scenario using RAGAS metrics.

        Args:
            question: Question about the vulnerability
            code_snippet: Code containing the vulnerability
            env_features: Environmental context
            vulnerability_type: Type of vulnerability to detect
            ground_truth: Expected correct answer/remediation

        Returns:
            Dictionary of metric scores
        """
        # Detect vulnerability and retrieve past fixes
        finding, contexts = self.detect_and_retrieve(code_snippet, env_features, vulnerability_type)

        # Generate answer from finding
        answer = (
            f"Detected {finding.vulnerability} ({finding.severity} severity). "
            f"OWASP: {finding.owasp_category}, CWE: {finding.cwe_id}. "
            f"Recommended fix: {finding.remediation}"
        )

        # Prepare dataset for RAGAS
        dataset = Dataset.from_dict(
            {
                "question": [question],
                "answer": [answer],
                "contexts": [contexts],
                "ground_truth": [ground_truth],
            }
        )

        # Evaluate with RAGAS
        if self.use_groq:
            result = evaluate(
                dataset=dataset,
                metrics=[faithfulness, context_recall, context_precision],
                llm=self.ragas_llm,
                embeddings=None,
            )

            # Extract metrics from RAGAS 0.4.0 EvaluationResult
            def get_metric(result_obj, key):
                if hasattr(result_obj, key):
                    val = getattr(result_obj, key)
                    if isinstance(val, list) and len(val) > 0:
                        return val[0]
                    return val if val is not None else 0.0

                if hasattr(result_obj, "to_pandas"):
                    df = result_obj.to_pandas()
                    if key in df.columns:
                        return df[key].iloc[0]
                return 0.0

            faithfulness_score = get_metric(result, "faithfulness")
            recall_score = get_metric(result, "context_recall")
            precision_score = get_metric(result, "context_precision")
        else:
            # Fallback to simple heuristics without LLM
            faithfulness_score = 0.8
            recall_score = 0.8
            precision_score = 0.8

        return {
            "faithfulness": faithfulness_score,
            "context_recall": recall_score,
            "context_precision": precision_score,
            "answer": answer,
            "contexts": contexts,
        }


@pytest.mark.ragas
@pytest.mark.skipif(not RAGAS_AVAILABLE, reason="RAGAS dependencies not installed")
@pytest.mark.skipif(not GROQ_API_KEY, reason="GROQ_API_KEY not set")
def test_sql_injection_audit():
    """Test security audit for SQL injection vulnerability."""
    evaluator = SecurityAuditEvaluator(use_groq=True)

    code_snippet = """
    def get_user(username):
        query = f"SELECT * FROM users WHERE username = '{username}'"
        cursor.execute(query)
        return cursor.fetchone()
    """

    question = "What security vulnerability is present in this code and how should it be fixed?"

    ground_truth = (
        "SQL injection vulnerability. Use parameterized queries with placeholders "
        "instead of string concatenation."
    )

    result = evaluator.evaluate_scenario(
        question=question,
        code_snippet=code_snippet,
        env_features=["python", "database", "security"],
        vulnerability_type="SQL injection",
        ground_truth=ground_truth,
    )

    print("\n" + "=" * 70)
    print("Test 1: SQL Injection Security Audit")
    print("=" * 70)
    print(f"Question: {question}")
    print(f"Answer: {result['answer']}")
    print(f"\nMetrics:")
    print(f"  Faithfulness: {result['faithfulness']:.3f}")
    print(f"  Context Recall: {result['context_recall']:.3f}")
    print(f"  Context Precision: {result['context_precision']:.3f}")
    print("=" * 70)

    # Assertions
    assert result["faithfulness"] >= 0.7, "Should detect SQL injection accurately"
    assert result["context_recall"] >= 0.7, "Should recall relevant past SQL fixes"
    assert result["context_precision"] >= 0.7, "Should provide relevant remediation"


@pytest.mark.ragas
@pytest.mark.skipif(not RAGAS_AVAILABLE, reason="RAGAS dependencies not installed")
@pytest.mark.skipif(not GROQ_API_KEY, reason="GROQ_API_KEY not set")
def test_weak_crypto_audit():
    """Test security audit for weak cryptography vulnerability."""
    evaluator = SecurityAuditEvaluator(use_groq=True)

    code_snippet = """
    import hashlib

    def hash_password(password):
        return hashlib.md5(password.encode()).hexdigest()
    """

    question = "What security vulnerability is present in this password hashing code?"

    ground_truth = (
        "Weak cryptographic algorithm (MD5). Use Argon2id, bcrypt, or scrypt "
        "for password hashing instead."
    )

    result = evaluator.evaluate_scenario(
        question=question,
        code_snippet=code_snippet,
        env_features=["python", "cryptography", "security"],
        vulnerability_type="weak password hashing",
        ground_truth=ground_truth,
    )

    print("\n" + "=" * 70)
    print("Test 2: Weak Cryptography Security Audit")
    print("=" * 70)
    print(f"Question: {question}")
    print(f"Answer: {result['answer']}")
    print(f"\nMetrics:")
    print(f"  Faithfulness: {result['faithfulness']:.3f}")
    print(f"  Context Recall: {result['context_recall']:.3f}")
    print(f"  Context Precision: {result['context_precision']:.3f}")
    print("=" * 70)

    # Assertions
    assert result["faithfulness"] >= 0.7, "Should detect weak crypto accurately"
    assert result["context_recall"] >= 0.7, "Should recall past crypto fixes"
    assert result["context_precision"] >= 0.7, "Should suggest strong hashing"


@pytest.mark.ragas
@pytest.mark.skipif(not RAGAS_AVAILABLE, reason="RAGAS dependencies not installed")
@pytest.mark.skipif(not GROQ_API_KEY, reason="GROQ_API_KEY not set")
def test_insecure_deserialization_audit():
    """Test security audit for insecure deserialization vulnerability."""
    evaluator = SecurityAuditEvaluator(use_groq=True)

    code_snippet = """
    import pickle

    def load_session(session_data):
        return pickle.loads(session_data)
    """

    question = "What security risk does this session handling code have?"

    ground_truth = (
        "Insecure deserialization using pickle. Replace with JSON or signed JWT tokens "
        "for session storage."
    )

    result = evaluator.evaluate_scenario(
        question=question,
        code_snippet=code_snippet,
        env_features=["python", "security", "deserialization"],
        vulnerability_type="insecure deserialization",
        ground_truth=ground_truth,
    )

    print("\n" + "=" * 70)
    print("Test 3: Insecure Deserialization Security Audit")
    print("=" * 70)
    print(f"Question: {question}")
    print(f"Answer: {result['answer']}")
    print(f"\nMetrics:")
    print(f"  Faithfulness: {result['faithfulness']:.3f}")
    print(f"  Context Recall: {result['context_recall']:.3f}")
    print(f"  Context Precision: {result['context_precision']:.3f}")
    print("=" * 70)

    # Assertions
    assert result["faithfulness"] >= 0.7, "Should detect deserialization risk"
    assert result["context_recall"] >= 0.7, "Should recall past deserialization fixes"
    assert result["context_precision"] >= 0.7, "Should suggest safe alternatives"


@pytest.mark.ragas
@pytest.mark.skipif(not RAGAS_AVAILABLE, reason="RAGAS dependencies not installed")
@pytest.mark.skipif(not GROQ_API_KEY, reason="GROQ_API_KEY not set")
def test_average_ragas_scores():
    """
    Run all security audit scenarios and compute average RAGAS scores.

    Acceptance criteria:
    - Average faithfulness >= 0.85 (findings grounded in code)
    - Average context recall >= 0.85 (relevant past fixes retrieved)
    - Average context precision >= 0.85 (useful remediation guidance)
    """
    evaluator = SecurityAuditEvaluator(use_groq=True)

    scenarios = [
        {
            "name": "SQL Injection",
            "code": "query = f\"SELECT * FROM users WHERE id = '{user_id}'\"",
            "question": "What vulnerability is in this database query?",
            "env_features": ["python", "database", "security"],
            "vulnerability_type": "SQL injection",
            "ground_truth": "SQL injection. Use parameterized queries.",
        },
        {
            "name": "Weak Crypto",
            "code": "hashlib.md5(password.encode()).hexdigest()",
            "question": "What's wrong with this password hashing?",
            "env_features": ["python", "cryptography", "security"],
            "vulnerability_type": "weak password hashing",
            "ground_truth": "Weak crypto (MD5). Use Argon2id or bcrypt.",
        },
        {
            "name": "Insecure Deserialization",
            "code": "pickle.loads(session_data)",
            "question": "What security risk is present?",
            "env_features": ["python", "security", "deserialization"],
            "vulnerability_type": "insecure deserialization",
            "ground_truth": "Insecure deserialization. Use JSON or signed JWT.",
        },
    ]

    all_scores = {"faithfulness": [], "context_recall": [], "context_precision": []}

    for scenario in scenarios:
        result = evaluator.evaluate_scenario(
            question=scenario["question"],
            code_snippet=scenario["code"],
            env_features=scenario["env_features"],
            vulnerability_type=scenario["vulnerability_type"],
            ground_truth=scenario["ground_truth"],
        )

        all_scores["faithfulness"].append(result["faithfulness"])
        all_scores["context_recall"].append(result["context_recall"])
        all_scores["context_precision"].append(result["context_precision"])

    # Compute averages
    avg_faithfulness = sum(all_scores["faithfulness"]) / len(all_scores["faithfulness"])
    avg_recall = sum(all_scores["context_recall"]) / len(all_scores["context_recall"])
    avg_precision = sum(all_scores["context_precision"]) / len(all_scores["context_precision"])

    print("\n" + "=" * 70)
    print("Security Audit Agent - Average RAGAS Scores")
    print("=" * 70)
    print(f"Average Faithfulness: {avg_faithfulness:.3f}")
    print(f"Average Context Recall: {avg_recall:.3f}")
    print(f"Average Context Precision: {avg_precision:.3f}")
    print("=" * 70)

    # Assertions
    assert avg_faithfulness >= 0.85, f"Faithfulness {avg_faithfulness:.3f} < 0.85"
    assert avg_recall >= 0.85, f"Context Recall {avg_recall:.3f} < 0.85"
    assert avg_precision >= 0.85, f"Context Precision {avg_precision:.3f} < 0.85"


if __name__ == "__main__":
    # Run all tests
    pytest.main([__file__, "-v", "-m", "ragas"])
