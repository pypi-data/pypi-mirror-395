[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "llm-benchmark-toolkit"
version = "2.3.0"
description = "Benchmark LLMs with 9 benchmarks & 100K+ questions. 8 providers: OpenAI, Anthropic, Groq, Together, Fireworks, DeepSeek, Ollama, HuggingFace. Web dashboard included."
readme = "README.md"
authors = [
    {name = "Nahuel Giudizi", email = "nahuelgiudizi@hotmail.com"}
]
license = "MIT"
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Intended Audience :: Education",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Software Development :: Testing",
    "Framework :: FastAPI",
    "Environment :: Web Environment",
    "Natural Language :: English",
    "Natural Language :: Spanish",
]
keywords = [
    "llm", "evaluation", "benchmark", "mmlu", "truthfulqa", "hellaswag",
    "arc", "winogrande", "commonsenseqa", "boolq", "safetybench",
    "gpt", "gpt-4", "claude", "ollama", "openai", "anthropic",
    "ai", "machine-learning", "deep-learning", "nlp", "language-model",
    "dashboard", "academic", "research", "testing", "metrics"
]
requires-python = ">=3.11"
dependencies = [
    # Core
    "ollama>=0.1.0",
    "datasets>=2.14.0",
    "scikit-learn>=1.3.0",
    "numpy>=1.24.0",
    "pandas>=2.0.0",
    "matplotlib>=3.7.0",
    "plotly>=5.17.0",
    "seaborn>=0.12.0",
    "scipy>=1.11.0",
    "click>=8.1.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "psutil>=5.9.0",
    "tqdm>=4.66.0",
    # Dashboard (included by default)
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "sse-starlette>=1.8.0",
    # Cloud providers (included by default)
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "huggingface-hub>=0.20.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    "black>=23.0.0",
    "flake8>=6.0.0",
    "ruff>=0.1.0",
    "mypy>=1.8.0",
    "types-requests>=2.31.0",
]
notebooks = [
    "jupyter>=1.0.0",
    "ipykernel>=6.25.0",
    "ipywidgets>=8.1.0",
]
openai = [
    "openai>=1.0.0",
]
anthropic = [
    "anthropic>=0.18.0",
]
huggingface = [
    "huggingface-hub>=0.20.0",
]
all-providers = [
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "huggingface-hub>=0.20.0",
]
dashboard = [
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "sse-starlette>=1.8.0",
]
full = [
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "huggingface-hub>=0.20.0",
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "sse-starlette>=1.8.0",
]
# Alias for maximum compatibility - installs everything
all = [
    "openai>=1.0.0",
    "anthropic>=0.18.0",
    "huggingface-hub>=0.20.0",
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "sse-starlette>=1.8.0",
    "jupyter>=1.0.0",
    "ipykernel>=6.25.0",
]

[project.scripts]
llm-eval = "llm_evaluator.cli:cli"
llm-dashboard = "llm_evaluator.dashboard:run_dashboard"

[project.urls]
Homepage = "https://github.com/NahuelGiudizi/llm-evaluation"
Repository = "https://github.com/NahuelGiudizi/llm-evaluation"
Issues = "https://github.com/NahuelGiudizi/llm-evaluation/issues"
Blog = "https://dev.to/nahuelgiudizi/building-an-honest-llm-evaluation-framework-from-fake-metrics-to-real-benchmarks-2b90"
PyPI = "https://pypi.org/project/llm-benchmark-toolkit/"

[tool.setuptools]
package-dir = {"" = "src"}
include-package-data = true

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
"llm_evaluator.dashboard" = ["static/**/*"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "-v --cov=src --cov-report=html --cov-report=term"

[tool.coverage.run]
source = ["src"]
omit = [
    "*/dashboard/*",
    "*openai_provider*",
    "*anthropic_provider*",
    "*deepseek_provider*",
    "*huggingface_provider*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "except ImportError:",
    "def _run_.*_full",
    "logger.info.*Loading.*dataset",
]
fail_under = 71
omit = [
    "*/dashboard/*",
    "*openai_provider*",
    "*anthropic_provider*",
    "*deepseek_provider*",
    "*huggingface_provider*",
]

[tool.black]
line-length = 100
target-version = ['py311']
include = '\.pyi?$'

[tool.mypy]
python_version = "3.11"
warn_return_any = false
warn_unused_configs = true
disallow_untyped_defs = false
disallow_any_generics = false
disallow_subclassing_any = false
disallow_untyped_calls = false
disallow_incomplete_defs = false
check_untyped_defs = false
disallow_untyped_decorators = false
no_implicit_optional = false
warn_redundant_casts = true
warn_unused_ignores = false
warn_no_return = true
warn_unreachable = false
strict_equality = true
show_error_codes = true
show_column_numbers = true
pretty = true
ignore_missing_imports = true

# Ignore missing imports for third-party libraries without stubs
[[tool.mypy.overrides]]
module = [
    "datasets.*",
    "plotly.*",
    "seaborn.*",
    "scipy.*",
    "ollama.*",
    "openai.*",
    "anthropic.*",
    "huggingface_hub.*",
    "psutil.*",
    "tqdm.*",
    "pandas.*",
    "pydantic.*",
    "pydantic_settings.*",
]
ignore_missing_imports = true
