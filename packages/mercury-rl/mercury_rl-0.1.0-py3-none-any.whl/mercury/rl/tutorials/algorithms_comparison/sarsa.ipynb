{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28fe9fe2",
   "metadata": {},
   "source": [
    "# SARSA\n",
    "SARSA (State-Action-Reward-State-Action) is an on-policy reinforcement learning algorithm used to learn the optimal policy for a Markov Decision Process. At each step, SARSA updates the action-value function (Q-value) based on the current state, the action taken, the reward received, the next state, and the next action chosen by the current policy. The update rule is:\n",
    "\n",
    "Q(s, a) ← Q(s, a) + α [r + γ Q(s', a') − Q(s, a)]\n",
    "\n",
    "where:\n",
    "- s: current state\n",
    "- a: current action\n",
    "- r: reward received after taking action a in state s\n",
    "- s': next state\n",
    "- a': next action chosen in state s'\n",
    "- α: learning rate\n",
    "- γ: discount factor\n",
    "\n",
    "SARSA is called \"on-policy\" because it updates its Q-values using the actions actually taken by the current policy, rather than the best possible actions.\n",
    "\n",
    "## Key Point of SARSA\n",
    "\n",
    "The main point of the SARSA algorithm is to learn the action-value function (Q-function) for the current policy, enabling the agent to improve its policy over time by balancing exploration and exploitation. SARSA updates its estimates based on the actions actually taken, making it sensitive to the exploration strategy used during learning.\n",
    "\n",
    "## Relationship to TD(λ)\n",
    "\n",
    "SARSA applies TD(lambda) prediction methods to state-action pairs, rather than to states\n",
    "We then need a trace for each action-state pair: E(s,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fed192bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compress_state, generate_extreme_value_state_image\n",
    "import numpy as np\n",
    "\n",
    "import minari\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, HTML\n",
    "import uuid\n",
    "\n",
    "def sarsa_algorithm(dataset_id, gamma = 0.99, lambda_ = 0.0, alpha = 1.0):\n",
    "    \"\"\"\n",
    "    Implements the SARSA algorithm for reinforcement learning: updates the Q-values based on the agent's actions and rewards.\n",
    "    Args:\n",
    "        dataset_id (str): The ID of the dataset to use.\n",
    "        gamma (float): Discount factor for future rewards.\n",
    "        lambda_ (float): Eligibility trace decay factor.\n",
    "        alpha (float): Learning rate for Q-value updates.\n",
    "    Returns:\n",
    "        Q (dict): A dictionary mapping state-action pairs to Q-values.\n",
    "\n",
    "        state_locations (dict): maps state keys to (episode_index, timestep) of first occurrence.\n",
    "\n",
    "    \"\"\"\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    Q = defaultdict(float)\n",
    "    state_locations = {}\n",
    "\n",
    "    for episode_index, episode in enumerate(dataset.iterate_episodes()):\n",
    "        observations = episode.observations\n",
    "        rewards = episode.rewards\n",
    "        actions = episode.actions\n",
    "\n",
    "        for t in range(len(actions)):\n",
    "            obs_t = {k: v[t] for k, v in observations.items()}\n",
    "            obs_tp1 = {k: v[t + 1] for k, v in observations.items()} if t + 1 < len(actions) else None\n",
    "            reward = rewards[t]\n",
    "            action = actions[t]\n",
    "\n",
    "            s_t = compress_state(obs_t)\n",
    "            s_tp1 = compress_state(obs_tp1) if obs_tp1 is not None else None\n",
    "\n",
    "            # SARSA update\n",
    "            if s_tp1 is not None:\n",
    "                next_action = np.argmax([Q[(s_tp1, a)] for a in range(dataset.action_space.n)])\n",
    "                Q[(s_t, action)] += alpha * (reward + gamma * Q[(s_tp1, next_action)] - Q[(s_t, action)])\n",
    "            else:\n",
    "                # Fix: dataset.action_space is a gym.spaces.Discrete, use range(n) to iterate actions\n",
    "                next_action = np.argmax([Q[(s_tp1, a)] for a in range(dataset.action_space.n)])\n",
    "                Q[(s_t, action)] += alpha * (reward + gamma * Q[(s_tp1, next_action)] - Q[(s_t, action)])\n",
    "        \n",
    "        if s_t not in state_locations:\n",
    "                state_locations[s_t] = (episode_index, t)\n",
    "\n",
    "        # Print Q-value stats\n",
    "        q_values = np.array(list(Q.values()))\n",
    "        print(\"Q-value function statistics:\")\n",
    "        print(f\"  Count:       {len(q_values)}\")\n",
    "        print(f\"  Min value:   {np.min(q_values):.4f}\")\n",
    "        print(f\"  Max value:   {np.max(q_values):.4f}\")\n",
    "        print(f\"  Mean value:  {np.mean(q_values):.4f}\")\n",
    "        print(f\"  Std dev:     {np.std(q_values):.4f}\")\n",
    "        return Q, state_locations\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da715e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value function statistics:\n",
      "  Count:       175\n",
      "  Min value:   0.0000\n",
      "  Max value:   0.9594\n",
      "  Mean value:  0.0055\n",
      "  Std dev:     0.0723\n",
      "Selected state: ((2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 5, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 3, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 5, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 5, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 5, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 2, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 0, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 2, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 5, 3, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 1, 0, 1, 0, 0, 4, 2, 1, 1, 0, 0, 1, 0, 0, 7, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 6, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 5, 1, 1, 0, 0, 1, 0, 0, 7, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 5, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 5, 3, 0, 1, 0, 0, 4, 3, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 10, 0, 2, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0), 'pick up a purple key', 2), Best action: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/BBVA/mercury-rl/rl-venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rejected: unreachable object at (15, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/q_learning/highest_value_function.png?v=cbcf3c635a44494c8b78b94f5f3a6404\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import generate_extreme_value_state_image_from_q_table\n",
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/q_learning/highest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image_from_q_table(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    q_fn_generator=sarsa_algorithm,\n",
    "    highest=True\n",
    ") # Generate image of highest Q-values\n",
    "\n",
    "# === Display in notebook ===\n",
    "cache_buster = uuid.uuid4().hex\n",
    "HTML(f'<img src=\"{output_path}?v={cache_buster}\" width=\"400\">')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
