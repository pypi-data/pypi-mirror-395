{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09334846",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07377c",
   "metadata": {},
   "source": [
    "Monte Carlo in RL refers to a technique used to estimate the V(s) of a given policy (on-policy).\n",
    "\n",
    "It can be summed up as: \"How good is to be in a given state based on its future cumulative reward for a given policy? I will calculate the mean returns for every episode that goes through s.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109ab1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compress_state, generate_extreme_value_state_image\n",
    "import numpy as np\n",
    "\n",
    "import minari\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def monte_carlo_evaluation(dataset_id, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Estimates the state-value function V(s) for a given policy using Monte Carlo first-visit evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        The ID of a Minari dataset, assumed to be generated by following a fixed policy in a known environment.\n",
    "        Each episode in the dataset is treated as an independent rollout under this policy.\n",
    "    \n",
    "    gamma : float, optional (default=0.99)\n",
    "        Discount factor used to calculate the return G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V : dict\n",
    "        A dictionary mapping each encountered state (compressed as a hashable key from image, mission, and direction)\n",
    "        to its estimated value V(s), computed as the average of returns following the first visit to that state\n",
    "        across all episodes in the dataset.\n",
    "\n",
    "    Methodology\n",
    "    -----------\n",
    "    This function uses **first-visit Monte Carlo evaluation**, meaning that for each episode, only the **first**\n",
    "    occurrence of each state is used to compute its return. If a state appears multiple times within an episode,\n",
    "    only the first encounter contributes to the value estimate.\n",
    "\n",
    "    The rationale behind first-visit MC:\n",
    "    - Prevents overcounting correlated returns from repeated visits within the same trajectory.\n",
    "    - More robust in environments or policies where states may be revisited often due to loops or deterministic behavior.\n",
    "\n",
    "    Assumptions & Implications\n",
    "    --------------------------\n",
    "    - The policy used to generate the dataset is fixed (though not necessarily deterministic).\n",
    "    - The environment may be stochastic or deterministic — this method does not assume determinism.\n",
    "    - This method provides an unbiased estimate of V(s) in the limit of many episodes, under standard MC assumptions.\n",
    "\n",
    "    If you want to use **every-visit Monte Carlo** instead (i.e., update V(s) for every occurrence of s in an episode),\n",
    "    simply remove the `visited` set and update the returns for every time step where state s appears.\n",
    "\n",
    "    Comparison: First-Visit vs Every-Visit Monte Carlo\n",
    "    --------------------------------------------------\n",
    "    Every-Visit Monte Carlo — Pros:\n",
    "    - ✅ More sample-efficient: Updates V(s) at every occurrence of a state, not just the first per episode.\n",
    "    - ✅ Faster convergence in small datasets by making better use of available data.\n",
    "    - ✅ Better suited to stochastic policies or environments where state re-visits yield diverse outcomes.\n",
    "    - ✅ Lower variance early in learning due to averaging over more samples.\n",
    "\n",
    "    Every-Visit Monte Carlo — Cons:\n",
    "    - ❌ Can lead to overcounting in deterministic or loop-heavy environments.\n",
    "    - ❌ May introduce bias when repeated visits in a trajectory provide highly correlated returns.\n",
    "    - ❌ Less robust in structured or low-entropy environments where state visitation is redundant.\n",
    "\n",
    "    Use first-visit MC when you want stable, conservative estimates that are less sensitive to trajectory-specific artifacts.\n",
    "    Use every-visit MC when you need more aggressive learning from limited data, especially in noisy or high-entropy settings.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The state is defined as a combination of symbolic image, mission text, and agent direction, as extracted from\n",
    "      the observation dictionary.\n",
    "    - The function assumes that the symbolic image is sufficient to index the state when combined with mission and direction.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Sutton & Barto, Reinforcement Learning: An Introduction — Chapter 5 (Monte Carlo Methods)\n",
    "    \"\"\"\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    state_locations = {}\n",
    "\n",
    "    for episode_idx, episode in enumerate(dataset.iterate_episodes()):\n",
    "        episode_rewards = episode.rewards\n",
    "        visited = set() # set of states already visited in this episode\n",
    "        G = 0\n",
    "\n",
    "        for t in reversed(range(len(episode_rewards))):\n",
    "            observation = {k: v[t] for k, v in episode.observations.items()}\n",
    "            state_key = compress_state(observation)\n",
    "            reward = episode_rewards[t]\n",
    "            G = gamma * G + reward # total return calculated as discounted sum of rewards\n",
    "\n",
    "            if state_key not in visited:\n",
    "                visited.add(state_key)\n",
    "                returns_sum[state_key] += G\n",
    "                returns_count[state_key] += 1\n",
    "                V[state_key] = returns_sum[state_key] / returns_count[state_key]\n",
    "\n",
    "                # Track where the state was first seen in reverse\n",
    "                if state_key not in state_locations:\n",
    "                    state_locations[state_key] = (episode_idx, t)\n",
    "\n",
    "    values = np.array(list(V.values()))\n",
    "    print(\"Value function statistics:\")\n",
    "    print(f\"  Count:       {len(values)}\")\n",
    "    print(f\"  Min value:   {np.min(values):.4f}\")\n",
    "    print(f\"  Max value:   {np.max(values):.4f}\")\n",
    "    print(f\"  Mean value:  {np.mean(values):.4f}\")\n",
    "    print(f\"  Std dev:     {np.std(values):.4f}\")\n",
    "\n",
    "    return V, state_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2704cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monte_carlo_evaluation_every_visit(dataset_id, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Estimates the state-value function V(s) for a given policy using Monte Carlo every-visit evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        The ID of a Minari dataset, assumed to be generated by following a fixed policy in a known environment.\n",
    "        Each episode in the dataset is treated as an independent rollout under this policy.\n",
    "    \n",
    "    gamma : float, optional (default=0.99)\n",
    "        Discount factor used to calculate the return G_t = r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + ...\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V : dict\n",
    "        A dictionary mapping each encountered state (compressed as a hashable key from image, mission, and direction)\n",
    "        to its estimated value V(s), computed as the average of returns following the first visit to that state\n",
    "        across all episodes in the dataset.\n",
    "\n",
    "    Methodology\n",
    "    -----------\n",
    "    This function uses **every-visit Monte Carlo evaluation**, meaning that for each episode, every occurrence\n",
    "    of each state is used to compute its return. If a state appears multiple times within an episode,\n",
    "    all occurrences contribute to the value estimate.\n",
    "    The rationale behind every-visit MC:\n",
    "    - Allows for more sample-efficient updates by leveraging all available data.\n",
    "    - Better suited for environments where states may be revisited frequently, especially in stochastic settings.\n",
    "    - Reduces variance in early learning by averaging over multiple samples of the same state.\n",
    "    Assumptions & Implications\n",
    "    --------------------------\n",
    "    - The policy used to generate the dataset is fixed (though not necessarily deterministic).\n",
    "    - The environment may be stochastic or deterministic — this method does not assume determinism.\n",
    "    - This method provides an unbiased estimate of V(s) in the limit of many episodes, under standard MC assumptions.\n",
    "\n",
    "\n",
    "\n",
    "    Comparison: First-Visit vs Every-Visit Monte Carlo\n",
    "    --------------------------------------------------\n",
    "    Every-Visit Monte Carlo — Pros:\n",
    "    - ✅ More sample-efficient: Updates V(s) at every occurrence of a state, not just the first per episode.\n",
    "    - ✅ Faster convergence in small datasets by making better use of available data.\n",
    "    - ✅ Better suited to stochastic policies or environments where state re-visits yield diverse outcomes.\n",
    "    - ✅ Lower variance early in learning due to averaging over more samples.\n",
    "\n",
    "    Every-Visit Monte Carlo — Cons:\n",
    "    - ❌ Can lead to overcounting in deterministic or loop-heavy environments.\n",
    "    - ❌ May introduce bias when repeated visits in a trajectory provide highly correlated returns.\n",
    "    - ❌ Less robust in structured or low-entropy environments where state visitation is redundant.\n",
    "\n",
    "    Use first-visit MC when you want stable, conservative estimates that are less sensitive to trajectory-specific artifacts.\n",
    "    Use every-visit MC when you need more aggressive learning from limited data, especially in noisy or high-entropy settings.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The state is defined as a combination of symbolic image, mission text, and agent direction, as extracted from\n",
    "        the observation dictionary.\n",
    "    - The function assumes that the symbolic image is sufficient to index the state when combined with mission and direction.\n",
    "    - This function is similar to `monte_carlo_evaluation`, but it updates the value for every occurrence of a state\n",
    "        in an episode, rather than just the first visit.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    Sutton & Barto, Reinforcement Learning: An Introduction — Chapter 5 (Monte Carlo Methods)\n",
    "    \"\"\"\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    V = defaultdict(float)\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(int)\n",
    "    state_locations = {}\n",
    "\n",
    "    for episode_idx, episode in enumerate(dataset.iterate_episodes()):\n",
    "        episode_rewards = episode.rewards\n",
    "        visited = set() # set of states already visited in this episode\n",
    "        G = 0\n",
    "\n",
    "        for t in reversed(range(len(episode_rewards))):\n",
    "            observation = {k: v[t] for k, v in episode.observations.items()}\n",
    "            state_key = compress_state(observation)\n",
    "            reward = episode_rewards[t]\n",
    "            G = gamma * G + reward # total return calculated as discounted sum of rewards\n",
    "\n",
    "            if state_key not in visited:\n",
    "                visited.add(state_key)\n",
    "                returns_sum[state_key] += G\n",
    "                returns_count[state_key] += 1\n",
    "                V[state_key] = returns_sum[state_key] / returns_count[state_key]\n",
    "\n",
    "                # Track where the state was first seen in reverse\n",
    "                if state_key not in state_locations:\n",
    "                    state_locations[state_key] = (episode_idx, t)\n",
    "\n",
    "    values = np.array(list(V.values()))\n",
    "    print(\"Value function statistics:\")\n",
    "    print(f\"  Count:       {len(values)}\")\n",
    "    print(f\"  Min value:   {np.min(values):.4f}\")\n",
    "    print(f\"  Max value:   {np.max(values):.4f}\")\n",
    "    print(f\"  Mean value:  {np.mean(values):.4f}\")\n",
    "    print(f\"  Std dev:     {np.std(values):.4f}\")\n",
    "\n",
    "    return V, state_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3530956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_incremental_evaluation(dataset_id, gamma=0.99, alpha=None):\n",
    "    \"\"\"\n",
    "    Estimates the state-value function V(s) using First-Visit Incremental Monte Carlo Evaluation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        ID of a Minari dataset generated by following a fixed policy.\n",
    "\n",
    "    gamma : float, optional (default=0.99)\n",
    "        Discount factor for computing return.\n",
    "    alpha : float, optional (default=None)\n",
    "        Step-size parameter for incremental averaging. If None, it is computed as 1 / N(s) where N(s) is the number of visits to state s.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V : dict\n",
    "        A dictionary mapping compressed state keys to their estimated values.\n",
    "\n",
    "    state_locations : dict\n",
    "        A dictionary mapping state keys to the (episode index, time step) where they were first encountered.\n",
    "\n",
    "\n",
    "    Methodology\n",
    "    ----------\n",
    "    This function implements **First-Visit Incremental Monte Carlo Evaluation**. It computes the value function V(s)\n",
    "    by averaging the returns G_t for each state s, updating the value incrementally as new episodes are processed.\n",
    "\n",
    "    The key steps are:\n",
    "    1. For each episode, traverse the rewards in reverse order to compute the return G_t at each time step.\n",
    "    2. Maintain a set of visited states to ensure only the first visit in each episode contributes to the value estimate.\n",
    "    3. Use incremental averaging to update the value function V(s) for each state encountered.\n",
    "    4. Store the first occurrence of each state in the dataset for reference.\n",
    "    5. Return the final value function and the locations of each state.\n",
    "    This method is particularly useful for environments where states may be revisited multiple times within an episode,\n",
    "    as it avoids overcounting returns from repeated visits.\n",
    "\n",
    "\n",
    "    Assumptions & Implications\n",
    "    --------------------------\n",
    "    - The dataset is generated by a fixed policy, ensuring that the state visitation distribution is stable.\n",
    "    - The environment may be stochastic or deterministic; this method does not assume determinism.\n",
    "    - The function assumes that the symbolic image, mission text, and agent direction together uniquely identify a state.\n",
    "    - The value function V(s) converges to the true expected return for each state as the number of episodes increases.\n",
    "    - This method provides an unbiased estimate of V(s) in the limit of many episodes, under standard MC assumptions.\n",
    "        Assumptions & Implications\n",
    "\n",
    "        \n",
    "    If you want to use **every-visit Monte Carlo** instead (i.e., update V(s) for every occurrence of s in an episode),\n",
    "    simply remove the `visited` set and update the returns for every time step where state s appears.\n",
    "\n",
    "    \n",
    "    Comparison with First-Visit Monte Carlo\n",
    "    --------------------------------------------------\n",
    "    First-Visit Incremental Monte Carlo — Pros:\n",
    "    - ✅ More sample-efficient: Updates V(s) incrementally, allowing for faster convergence with fewer episodes.\n",
    "    - ✅ Lower memory footprint: Does not require storing all returns for each state, only the current estimate.\n",
    "    - ✅ Better suited for online learning scenarios where episodes are processed sequentially.\n",
    "    - ✅ Provides a running estimate of the value function, which can be useful for real-time applications.\n",
    "    \n",
    "    First-Visit Incremental Monte Carlo — Cons:\n",
    "    - ❌ May introduce bias if the state visitation distribution changes over time (e.g., in non-stationary environments).\n",
    "    - ❌ Requires careful management of step-size parameters to ensure convergence. \n",
    "    - ❌ Less robust to high variance in returns compared to batch methods that average over multiple episodes.\n",
    "\n",
    "    Use First-Visit Incremental MC when you need a fast, online estimate of the value function that can adapt to new data\n",
    "    without requiring a full pass over all previous episodes. It is particularly useful in environments where states are revisited frequently,\n",
    "    or when computational resources are limited and you need to maintain a running estimate of the value function.\n",
    "    Notes\n",
    "    -----\n",
    "    - The state is defined as a combination of symbolic image, mission text, and agent direction, as extracted from\n",
    "      the observation dictionary.\n",
    "    - The function assumes that the symbolic image is sufficient to index the state when combined with mission and direction.\n",
    "    - The function uses incremental averaging to update the value function, which is efficient for large datasets.\n",
    "    - The first occurrence of each state is tracked to provide context for where the value was estimated in the dataset.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    - Katerina Fragkiadaki, Monte Carlo Learning — Lecture 4, CMU 10-403\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    V = defaultdict(float)\n",
    "    N = defaultdict(int)  # visitation count for incremental averaging\n",
    "    state_locations = {}\n",
    "\n",
    "    for episode_idx, episode in enumerate(dataset.iterate_episodes()):\n",
    "        episode_rewards = episode.rewards\n",
    "        visited = set() # set of states already visited in this episode\n",
    "        # visited set ensures First-Visit Monte Carlo: only the first occurrence of each state in an episode is used\n",
    "        G = 0\n",
    "\n",
    "        for t in reversed(range(len(episode_rewards))):\n",
    "            observation = {k: v[t] for k, v in episode.observations.items()}\n",
    "            state_key = compress_state(observation) # convert observation to a hashable state key: set of unique observations -> state\n",
    "            reward = episode_rewards[t]\n",
    "            G = gamma * G + reward\n",
    "\n",
    "            if state_key not in visited:\n",
    "                visited.add(state_key)\n",
    "                N[state_key] += 1\n",
    "\n",
    "                # Apply incremental averaging for the value function\n",
    "                if alpha is None:\n",
    "                    alpha = 1 / N[state_key]  # step-size for incremental average\n",
    "                    \n",
    "                V[state_key] += alpha * (G - V[state_key])\n",
    "\n",
    "                if state_key not in state_locations:\n",
    "                    state_locations[state_key] = (episode_idx, t)\n",
    "\n",
    "    values = np.array(list(V.values()))\n",
    "    print(\"Value function statistics:\")\n",
    "    print(f\"  Count:       {len(values)}\")\n",
    "    print(f\"  Min value:   {np.min(values):.4f}\")\n",
    "    print(f\"  Max value:   {np.max(values):.4f}\")\n",
    "    print(f\"  Mean value:  {np.mean(values):.4f}\")\n",
    "    print(f\"  Std dev:     {np.std(values):.4f}\")\n",
    "\n",
    "    return V, state_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ed0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function statistics:\n",
      "  Count:       51916\n",
      "  Min value:   0.0977\n",
      "  Max value:   0.9969\n",
      "  Mean value:  0.5987\n",
      "  Std dev:     0.2118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/montecarlo/highest_value_function.png\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/montecarlo/highest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    value_fn_generator=\n",
    "        monte_carlo_evaluation,  # or monte_carlo_incremental_evaluation\n",
    "    highest=True\n",
    ")\n",
    "\n",
    "# === Display in notebook ===\n",
    "HTML(f'<img src=\"{output_path}\" width=\"400\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81f2634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function statistics:\n",
      "  Count:       51916\n",
      "  Min value:   0.0977\n",
      "  Max value:   0.9969\n",
      "  Mean value:  0.5987\n",
      "  Std dev:     0.2118\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/montecarlo/lowest_value_function.png\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/montecarlo/lowest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    value_fn_generator=monte_carlo_evaluation,\n",
    "    highest=False\n",
    ")\n",
    "\n",
    "# === Display in notebook ===\n",
    "HTML(f'<img src=\"{output_path}\" width=\"400\">')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
