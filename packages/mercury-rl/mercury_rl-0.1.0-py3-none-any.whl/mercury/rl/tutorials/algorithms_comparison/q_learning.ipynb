{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98659f53",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "This is very similar to TD(0), but also takes into account the actions available at the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27517ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compress_state, generate_extreme_value_state_image_from_q_table\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import minari\n",
    "from IPython.display import HTML\n",
    "import uuid\n",
    "\n",
    "\n",
    "def q_learning_offline(dataset_id, gamma=0.99, alpha=0.1, num_actions=7):\n",
    "    \"\"\"\n",
    "    Estimates Q(s, a) using Q-learning from an offline dataset (tabular, no function approximation),\n",
    "    and tracks the episode and timestep where each state was first seen.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        Minari dataset ID containing offline trajectories.\n",
    "\n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "\n",
    "    num_actions : int\n",
    "        Number of discrete actions in the environment.\n",
    "    \n",
    "    Methodology\n",
    "    ----------\n",
    "    1. Load the dataset using Minari.\n",
    "    2. Initialize Q-values for each (state, action) pair to zero.\n",
    "    3. For each episode in the dataset:\n",
    "        - Iterate through each timestep.\n",
    "        - Compress the state observation from the observation dictionary into a unique key.\n",
    "        - Update Q-values using the Q-learning update rule.\n",
    "        - Track the first occurrence of each state.\n",
    "    4. Print statistics about the Q-value function estimates.\n",
    "    5. Return the Q-value function and the state locations.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : dict\n",
    "        A dictionary mapping (state, action) to Q-values.\n",
    "\n",
    "    state_locations : dict\n",
    "        A dictionary mapping state keys to (episode_index, timestep) of first occurrence.\n",
    "    \"\"\"\n",
    "    dataset: minari.Dataset = minari.load_dataset(dataset_id)\n",
    "    Q = defaultdict(float)\n",
    "    state_locations: dict = {}  # maps state keys to (episode_index, timestep) of first occurrence\n",
    "\n",
    "    for episode_idx, episode in enumerate(dataset.iterate_episodes()):\n",
    "        observations = episode.observations\n",
    "        rewards = episode.rewards\n",
    "        actions = episode.actions\n",
    "\n",
    "        for t in range(len(actions)):  # actions and rewards both have length T\n",
    "            obs_t = {k: v[t] for k, v in observations.items()} # observations of current timestep t\n",
    "            obs_tp1 = {k: v[t + 1] for k, v in observations.items()} # observations of next timestep t+1\n",
    "            reward = rewards[t]\n",
    "            action = actions[t]\n",
    "\n",
    "            s_t = compress_state(obs_t) # State at time t, s_t\n",
    "            s_tp1 = compress_state(obs_tp1) # State at time t+1, s_t+1\n",
    "\n",
    "            if s_t not in state_locations:\n",
    "                state_locations[s_t] = (episode_idx, t)\n",
    "\n",
    "            # Q-learning TD target: max over next state's actions\n",
    "            max_q_next = max(Q[(s_tp1, a)] for a in range(num_actions))\n",
    "\n",
    "            # Q-learning update (off-policy)\n",
    "            Q[(s_t, action)] += alpha * (reward + gamma * max_q_next - Q[(s_t, action)])\n",
    "\n",
    "        # Track missing state at the end of the episode\n",
    "        if s_tp1 not in state_locations:\n",
    "            state_locations[s_tp1] = (episode_idx, len(actions))\n",
    "\n",
    "    # Print Q-value stats\n",
    "    q_values = np.array(list(Q.values()))\n",
    "    print(\"Q-value function statistics:\")\n",
    "    print(f\"  Count:       {len(q_values)}\")\n",
    "    print(f\"  Min value:   {np.min(q_values):.4f}\")\n",
    "    print(f\"  Max value:   {np.max(q_values):.4f}\")\n",
    "    print(f\"  Mean value:  {np.mean(q_values):.4f}\")\n",
    "    print(f\"  Std dev:     {np.std(q_values):.4f}\")\n",
    "\n",
    "    return Q, state_locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "348e5bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value function statistics:\n",
      "  Count:       365258\n",
      "  Min value:   0.0000\n",
      "  Max value:   0.0997\n",
      "  Mean value:  0.0003\n",
      "  Std dev:     0.0048\n",
      "Selected state: ((2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 5, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 5, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 10, 0, 2, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 5, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 7, 3, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 5, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 5, 4, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 1, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 2, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 3, 1, 5, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 7, 1, 0, 2, 5, 0, 6, 4, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 5, 2, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 5, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 6, 3, 0, 5, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 4, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 2, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 5, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 7, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 4, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0), 'pick up a grey ball', 2), Best action: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/BBVA/mercury-rl/rl-venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/q_learning/highest_value_function.png?v=beae283b73ce4b4bba8cdac472a5e723\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/q_learning/highest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image_from_q_table(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    q_fn_generator=q_learning_offline,\n",
    "    highest=True\n",
    ") # Generate image of highest Q-values\n",
    "\n",
    "# === Display in notebook ===\n",
    "cache_buster = uuid.uuid4().hex\n",
    "HTML(f'<img src=\"{output_path}?v={cache_buster}\" width=\"400\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1d1891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-value function statistics:\n",
      "  Count:       365258\n",
      "  Min value:   0.0000\n",
      "  Max value:   0.0997\n",
      "  Mean value:  0.0003\n",
      "  Std dev:     0.0048\n",
      "Selected state: ((2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 5, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 3, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 5, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 5, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 5, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 2, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 0, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 2, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 7, 4, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 5, 3, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 1, 0, 1, 0, 0, 4, 2, 1, 1, 0, 0, 1, 0, 0, 7, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 6, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 5, 1, 1, 0, 0, 1, 0, 0, 7, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 4, 5, 1, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 10, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 5, 3, 0, 1, 0, 0, 4, 3, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 6, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 6, 3, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 4, 5, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0, 2, 5, 0), 'pick up a purple key', 1), Best action: 0\n",
      "Sampling rejected: unreachable object at (15, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/q_learning/lowest_value_function.png?v=62c6f1bf3d3943519238cb0a9107bdaf\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/q_learning/lowest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image_from_q_table(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    q_fn_generator=q_learning_offline,\n",
    "    highest=False\n",
    ") # Generate image of lowest Q-values\n",
    "\n",
    "# === Display in notebook ===\n",
    "cache_buster = uuid.uuid4().hex\n",
    "HTML(f'<img src=\"{output_path}?v={cache_buster}\" width=\"400\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5c7c3",
   "metadata": {},
   "source": [
    "- **Highest Q-value image**: state s where action a has the highest Q-value- agent can pick up the object\n",
    "\n",
    "\n",
    "- **Lowest Q-value image**: state s where action a has the lowest Q-value- agent cannot pick up the object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb3752",
   "metadata": {},
   "source": [
    "## How do we create an optimal policy\n",
    "\n",
    "Q-learning is an off-policy algorithm. That means that you don't need to follow a given policy while collecting the data. You can derive a policy from the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1178acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_greedy_policy(Q: dict, num_actions: int, epsilon: float) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts an epsilon-greedy policy π(s) from a tabular Q-function.\n",
    "    \n",
    "    For each state s, the greedy action is selected as:\n",
    "        π(s) = argmax_a Q(s, a)\n",
    "\n",
    "    The returned policy assigns probability (1 - epsilon) + epsilon/num_actions to the best action,\n",
    "    and epsilon/num_actions to all other actions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : dict\n",
    "        A dictionary mapping (state, action) pairs to Q-values.\n",
    "    num_actions : int\n",
    "        The total number of discrete actions in the environment.\n",
    "    epsilon : float\n",
    "        Probability of taking a random action (exploration rate).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    policy : dict\n",
    "        A dictionary mapping state keys to action probability vectors (np.ndarray).\n",
    "    \"\"\"\n",
    "    state_action_values = defaultdict(lambda: [0.0] * num_actions)\n",
    "\n",
    "    for (state, action), q_value in Q.items():\n",
    "        state_action_values[state][action] = q_value\n",
    "\n",
    "    policy = {}\n",
    "    for state, q_values in state_action_values.items():\n",
    "        best_action = int(np.argmax(q_values))  # greedy action\n",
    "        prob_vector = np.full(num_actions, epsilon / num_actions)\n",
    "        prob_vector[best_action] += (1.0 - epsilon)\n",
    "        policy[state] = prob_vector\n",
    "\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96c73a",
   "metadata": {},
   "source": [
    "But the approach above is only interesting if we are dealing with offline training. In online training, we would sometimes prefer to explore the state space over exploiting the results we already know are profitable.\n",
    "\n",
    "This is what is called exploration-explation approach, and we apply an epsilon-greedy approach, where with a probability of epsilon we would explore one of the other suboptimal approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9770820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epsilon_greedy_policy(Q: dict, num_actions: int, epsilon=0.1) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts an epsilon-greedy policy π(s) from a tabular Q-function.\n",
    "\n",
    "    For each state s, the greedy action is selected as:\n",
    "        π(s) = argmax_a Q(s, a)\n",
    "\n",
    "    The resulting policy assigns a probability distribution over actions as follows:\n",
    "    For each state s (with constant epsilon):\n",
    "\n",
    "        π(a|s) = {\n",
    "            1 - ε + ε/n,  if a = argmax_a Q(s, a)\n",
    "            ε/n,          if a ≠ argmax_a Q(s, a)\n",
    "        }\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : dict(state, action) -> float\n",
    "        Dictionary mapping (state, action) to Q-values.\n",
    "    num_actions : int\n",
    "        Total number of discrete actions in the environment.\n",
    "    epsilon : float\n",
    "        Probability of taking a random action (exploration rate).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : dict(state) -> np.ndarray\n",
    "        Dictionary mapping states to action probability vectors as a probability distribution.\n",
    "    \"\"\"\n",
    "    # Initialize policy with epsilon-greedy strategy\n",
    "    # For each state, find the action with the highest Q-value and assign it a higher probability\n",
    "    policy = {}\n",
    "\n",
    "    for (state, action), q_value in Q.items():\n",
    "        if state not in policy:\n",
    "            policy[state] = (action, q_value)\n",
    "        else:\n",
    "            # Greedy update: for each state, if this action has a higher Q-value, update the policy\n",
    "            if q_value > policy[state][1]:  # update if this action has a higher Q-value\n",
    "                policy[state] = (action, q_value)\n",
    "\n",
    "    # Convert to probabilities\n",
    "    pi = {}\n",
    "    for state, (best_action, _) in policy.items():\n",
    "        pi[state] = np.full(num_actions, epsilon / num_actions)\n",
    "        pi[state][best_action] += 1.0 - epsilon\n",
    "\n",
    "    return pi  # state -> action probability vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f413b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_epsilon_greedy_policy_episode(Q: dict, num_actions: int, episode: int,\n",
    "                                   epsilon_start=1.0, epsilon_min=0.01, epsilon_decay=0.995) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts an epsilon-greedy policy π(s) from a tabular Q-function,\n",
    "    with epsilon decreasing over episodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : dict(state, action) -> float\n",
    "        Dictionary mapping (state, action) to Q-values.\n",
    "    num_actions : int\n",
    "        Total number of discrete actions in the environment.\n",
    "    episode : int\n",
    "        Current episode number, used to decay epsilon.\n",
    "    epsilon_start : float\n",
    "        Initial epsilon value.\n",
    "    epsilon_min : float\n",
    "        Minimum value for epsilon.\n",
    "    epsilon_decay : float\n",
    "        Multiplicative decay factor per episode.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pi : dict(state) -> np.ndarray\n",
    "        Dictionary mapping states to action probability vectors as a probability distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decay epsilon based on episode number\n",
    "    epsilon = max(epsilon_min, epsilon_start * (epsilon_decay ** episode))\n",
    "\n",
    "    # Extract greedy actions\n",
    "    policy = {}\n",
    "    for (state, action), q_value in Q.items():\n",
    "        if state not in policy or q_value > policy[state][1]:\n",
    "            policy[state] = (action, q_value)\n",
    "\n",
    "    # Convert to epsilon-greedy probability distributions\n",
    "    pi = {}\n",
    "    for state, (best_action, _) in policy.items():\n",
    "        pi[state] = np.full(num_actions, epsilon / num_actions)\n",
    "        pi[state][best_action] += 1.0 - epsilon\n",
    "\n",
    "    return pi\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
