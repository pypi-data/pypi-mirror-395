{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "719b5638-e388-474c-be5f-8dfa87ee4a0d",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with the MountainCar Environment\n",
    "\n",
    "In this tutorial, we will guide you through training a reinforcement learning agent using the MountainCar environment from the Gym library. This classic problem involves an underpowered car driving up a steep hill. We will leverage the `mercury2` library for training the agent.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Import Libraries**: Import necessary standard and third-party libraries and custom modules from `mercury2`.\n",
    "\n",
    "2. **Set Up Paths**: Determine the root path of the project and append it to the system path.\n",
    "\n",
    "3. **Load Dataset**: Read the MountainCar dataset into a pandas DataFrame.\n",
    "\n",
    "4. **Initialize Agent**: Set batch size and initialize the imitation agent with a learning rate of 0.001.\n",
    "\n",
    "5. **Train Agent**: Train the agent.\n",
    "\n",
    "6. **Save Model**: Save the trained policy model to a specified path.\n",
    "\n",
    "7. **Initialize Environment**: Set up the MountainCar environment in Gym with rendering.\n",
    "\n",
    "8. **Load Model**: Initialize a new imitation agent and load the saved model.\n",
    "\n",
    "9. **Run Agent**: Execute the agent in the environment and calculate the total reward.\n",
    "\n",
    "10. **Close Environment**: Close the environment to release resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23d797-f1b2-48f0-858f-f8fffebef7f6",
   "metadata": {},
   "source": [
    "## Setup and Libraries\n",
    "\n",
    "In this section, we will import and set up the necessary libraries. We will use `mercury2` for reinforcement learning agents and environment configuration, along with other essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56eeffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary standard libraries\n",
    "import sys  #Provides access to system-specific parameters and functions\n",
    "import os # Provides a way of using operating system-dependent functionality\n",
    "from pathlib import Path # Provides an object-oriented interface for filesystem paths\n",
    "\n",
    "# Determine the root path of the project, assuming the current working directory is within the project structure\n",
    "# `Path(os.getcwd())` creates a Path object for the current working directory\n",
    "# `.parents[3]` navigates three levels up from the current directory, adjusting as needed based on your project structure\n",
    "root_path = str(Path(os.getcwd()).parents[3])\n",
    "\n",
    "# Append the root path to the system path, allowing the interpreter to locate project modules regardless of the current working directory\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b6d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:34:05.028370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 11:34:05.046109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 11:34:05.051707: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 11:34:05.064713: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 11:34:05.823239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import third-party libraries\n",
    "import gymnasium as gym # Import the Gymnasium library for creating and managing reinforcement learning environments\n",
    "import pandas as pd # Import Pandas for data manipulation and analysis\n",
    "\n",
    "# Import TensorFlow for building and training ML models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import custom modules from the mercury2 library\n",
    "from mercury2.rl.agents import ImitationAgent # Import the ImitationAgent class, which will be used to create an agent for reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edc4fe",
   "metadata": {},
   "source": [
    "## Download the Dataset\n",
    "\n",
    "The **MountainCar** dataset, available on [Kaggle](https://www.kaggle.com/datasets/gibrano/offline-mountaincar?select=MountainCar.csv), is a benchmark dataset used for reinforcement learning tasks. This dataset is specifically designed for the MountainCar environment, a classic problem in reinforcement learning where an underpowered car must drive up a steep hill.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset contains data collected from an agent interacting with the MountainCar environment. Each row in the dataset represents a single step taken by the agent, including the state of the environment before and after the action, the action taken, and the reward received.\n",
    "\n",
    "### Columns\n",
    "\n",
    "- `state_0`: The position of the car at the start of the step.\n",
    "- `state_1`: The velocity of the car at the start of the step.\n",
    "- `action`: The action taken by the agent (0 = push left, 1 = no push, 2 = push right).\n",
    "- `reward`: The reward received after taking the action.\n",
    "- `next_state_0`: The position of the car after the action.\n",
    "- `next_state_1`: The velocity of the car after the action.\n",
    "- `done`: A boolean indicating whether the episode has ended (True or False).\n",
    "\n",
    "### Usage\n",
    "\n",
    "This dataset can be used to train and evaluate reinforcement learning algorithms, particularly those that rely on offline data. It provides a fixed set of experiences from which an agent can learn without requiring interaction with a live environment. This can be useful for debugging, testing new algorithms, and comparing performance against established benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc1887",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1f20b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = root_path+\"/data/MountainCarExpert.csv\"\n",
    "\n",
    "# Define the column names in the dataset for states, actions, rewards, episode IDs, and sequence/order\n",
    "states_cols = ['x', 'vel'] # Columns representing the state of the environment (position 'x' and velocity 'vel')\n",
    "action_col = 'action' # Column representing the action taken by the agent\n",
    "reward_col = 'reward' # Column representing the reward received after the action\n",
    "episode_col_id = 'episode_id' # Column representing the unique identifier for each episode\n",
    "order_col = 'seq' # Column representing the sequence/order of the steps within an episode\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86af3649-9ff6-4683-a877-4d5c07592983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.episode_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25569235",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this section, we will train our reinforcement learning agent using the offline environment created earlier. The agent will learn from pre-recorded trajectories by iterating over the dataset for a specified number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "909ea1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729877670.932428  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877670.971330  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877670.971577  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877670.973083  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877670.973310  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877670.973489  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877671.049200  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877671.049432  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729877671.049624  136770 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-25 11:34:31.049771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1002 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti SUPER, pci bus id: 0000:07:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6830408\n",
      "Epoch: 1 Loss: 0.659641\n",
      "Epoch: 2 Loss: 0.64032567\n",
      "Epoch: 3 Loss: 0.62700754\n",
      "Epoch: 4 Loss: 0.61185896\n",
      "Epoch: 5 Loss: 0.6022146\n",
      "Epoch: 6 Loss: 0.59757\n",
      "Epoch: 7 Loss: 0.5945625\n",
      "Epoch: 8 Loss: 0.58951503\n",
      "Epoch: 9 Loss: 0.5828993\n",
      "Epoch: 10 Loss: 0.57632357\n",
      "Epoch: 11 Loss: 0.571684\n",
      "Epoch: 12 Loss: 0.56850845\n",
      "Epoch: 13 Loss: 0.56353176\n",
      "Epoch: 14 Loss: 0.5603784\n",
      "Epoch: 15 Loss: 0.5587101\n",
      "Epoch: 16 Loss: 0.5558426\n",
      "Epoch: 17 Loss: 0.55460846\n",
      "Epoch: 18 Loss: 0.55363977\n",
      "Epoch: 19 Loss: 0.55416286\n",
      "Epoch: 20 Loss: 0.55410564\n",
      "Epoch: 21 Loss: 0.555156\n",
      "Epoch: 22 Loss: 0.55579066\n",
      "Epoch: 23 Loss: 0.55592906\n",
      "Epoch: 24 Loss: 0.55600363\n",
      "Epoch: 25 Loss: 0.55653787\n",
      "Epoch: 26 Loss: 0.55680287\n",
      "Epoch: 27 Loss: 0.5563757\n",
      "Epoch: 28 Loss: 0.554058\n",
      "Epoch: 29 Loss: 0.554322\n"
     ]
    }
   ],
   "source": [
    "# Set the batch size for processing episodes\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize the imitation agent with a specified learning rate\n",
    "agent = ImitationAgent(learning_rate=0.001)\n",
    "\n",
    "# Define the number of epochs for training\n",
    "epochs = 30\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Get the unique episode IDs from the dataframe\n",
    "    episodes = df.episode_id.unique()\n",
    "    # Convert the episode IDs to a list\n",
    "    episodes = list(episodes)\n",
    "    \n",
    "    # Loop until there are no more episodes to process\n",
    "    while len(episodes) > 0:\n",
    "        # Pop the first episode ID from the list\n",
    "        episode_id = episodes.pop(0)\n",
    "\n",
    "        # Filter the dataframe for the current episode and sort by episode ID and sequence\n",
    "        df_batch = df[df.episode_id == episode_id].sort_values(by=[\"episode_id\",\"seq\"], ascending=True)\n",
    "\n",
    "        # Extract the current states, actions, and rewards from the dataframe\n",
    "        curr_states = df_batch[states_cols].values\n",
    "        actions = df_batch[action_col].values\n",
    "        rewards = df_batch[reward_col].values\n",
    "\n",
    "        # Store each transition in the agent's memory\n",
    "        for j in range(df_batch.shape[0]):\n",
    "            agent.store_transition(curr_states[j], actions[j], rewards[j])\n",
    "\n",
    "        # Train the agent with the stored transitions\n",
    "        agent.learn()  \n",
    "\n",
    "    # Print the current epoch and the loss value after training\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", agent.loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d94d6a-fe5b-4866-a8d2-1d8d2302bfdb",
   "metadata": {},
   "source": [
    "## Saving the Trained Model\n",
    "\n",
    "After training the reinforcement learning agent, it is important to save the trained model for future use. This allows us to load the model later for further training, evaluation, or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad98fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained policy model to a specified file path\n",
    "agent.policy.save(root_path+'/models/offline_mountain_car_imitation_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db95b9d-73a0-43e0-ac96-16e3bd2ff702",
   "metadata": {},
   "source": [
    "## Initializing the Gym Environment\n",
    "\n",
    "To test the performance of our trained reinforcement learning agent, we need to initialize the Gym environment. The Gym library provides a standard API for interacting with a wide variety of reinforcement learning environments, including the MountainCar environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7012b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MountainCar environment from the Gym library with human-readable rendering\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a24a9-0201-43e7-9248-01d9fb9bea24",
   "metadata": {},
   "source": [
    "## Loading the Trained Model into a New Agent\n",
    "\n",
    "To utilize the trained policy in a new instance of the agent, we need to load the saved model and assign it to the new agent's policy. This allows the new agent to leverage the learned policy without retraining from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5c62591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new imitation agent\n",
    "agent2 = ImitationAgent()\n",
    "\n",
    "# Load the trained model from the specified file path\n",
    "agent2.policy = tf.keras.models.load_model(root_path+'/models/offline_mountain_car_imitation_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb2f516-2727-4119-953c-fc56f0c35b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: -203.0\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment to get the initial state and information\n",
    "curr_state, info = env.reset()\n",
    "\n",
    "# Initialize the total reward counter\n",
    "total_R = 0\n",
    "\n",
    "# Loop to interact with the environment until termination\n",
    "while True:\n",
    "    \n",
    "    # Agent chooses an action based on the current state\n",
    "    action = agent2.choose_action(curr_state)\n",
    "\n",
    "    # Take the action in the environment and receive the next state, reward, and termination info\n",
    "    next_state, reward, terminated, _, _ = env.step(action)\n",
    "    \n",
    "    # Update the current state to the next state\n",
    "    curr_state = next_state.copy()\n",
    "    # Accumulate the reward\n",
    "    total_R += reward\n",
    "\n",
    "    # Break the loop if the episode is terminated\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "# Print the total reward obtained in this episode\n",
    "print(\"Total reward:\", total_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a85f3b-a512-4685-8eed-b01321ada75f",
   "metadata": {},
   "source": [
    "### Closing the Environment\n",
    "\n",
    "After running the agent in the environment and evaluating its performance, it is important to properly close the environment. This ensures that all resources are released and the environment is cleanly shut down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44483bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment to release resources\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16df373d-2d6e-4622-8038-bb944660908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09031ed-093b-403c-b7d3-1d5b62ceaed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
