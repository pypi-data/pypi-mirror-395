{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab12314-f0b8-45e7-ad46-3e52450f0541",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with the MountainCar Environment\n",
    "\n",
    "In this tutorial, we will guide you through training a reinforcement learning agent using the MountainCar environment from the Gym library. This classic problem involves an underpowered car driving up a steep hill. We will leverage the `mercury-rl` library for configuring the environment and training the agent.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. **Download the Dataset**: Obtain and load the MountainCar dataset from Kaggle.\n",
    "2. **Creating the Offline Environment**: Configure the offline environment using the dataset and `mercury-rl`.\n",
    "3. **Training the Agent**: Train the imitation agent from `mercury-rl` using pre-recorded trajectories.\n",
    "4. **Saving the Model**: Save the trained policy model to disk.\n",
    "5. **Initializing the Environment**: Set up the MountainCar environment in Gym.\n",
    "6. **Loading the Model**: Load the saved model into a new agent.\n",
    "7. **Running the Agent**: Execute the agent in the environment and calculate the total reward.\n",
    "8. **Closing the Environment**: Ensure the environment is properly closed to release resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09bed0-e084-4aa2-9268-bf2cabfb65a5",
   "metadata": {},
   "source": [
    "## Setup and Libraries\n",
    "\n",
    "In this section, we will import and set up the necessary libraries. We will use `mercury-rl` for reinforcement learning agents and environment configuration, along with other essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56eeffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary standard libraries\n",
    "import sys  #Provides access to system-specific parameters and functions\n",
    "import os # Provides a way of using operating system-dependent functionality\n",
    "from pathlib import Path # Provides an object-oriented interface for filesystem paths\n",
    "\n",
    "# Determine the root path of the project, assuming the current working directory is within the project structure\n",
    "# `Path(os.getcwd())` creates a Path object for the current working directory\n",
    "# `.parents[3]` navigates three levels up from the current directory, adjusting as needed based on your project structure\n",
    "root_path = str(Path(os.getcwd()).parents[3])\n",
    "\n",
    "# Append the root path to the system path, allowing the interpreter to locate project modules regardless of the current working directory\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da39ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import third-party libraries\n",
    "import gymnasium as gym # Import the Gymnasium library for creating and managing reinforcement learning environments\n",
    "import numpy as np # Import NumPy for numerical operations and array handling\n",
    "import pandas as pd # Import Pandas for data manipulation and analysis\n",
    "\n",
    "# Import TensorFlow for building and training ML models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import custom modules from the mercury-rl library\n",
    "from mercury.rl.agents import ImitationAgent # Import the ImitationAgent class, which will be used to create an agent for reinforcement learning\n",
    "from mercury.rl.environment import ENV # Import the ENV environment configuration from the mercury-rl library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd6a93-6984-47f1-819f-9fa7eadab4c6",
   "metadata": {},
   "source": [
    "## Download the Dataset\n",
    "\n",
    "The **MountainCar** dataset, available on [Kaggle](https://www.kaggle.com/datasets/gibrano/offline-mountaincar?select=MountainCar.csv), is a benchmark dataset used for reinforcement learning tasks. This dataset is specifically designed for the MountainCar environment, a classic problem in reinforcement learning where an underpowered car must drive up a steep hill.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "The dataset contains data collected from an agent interacting with the MountainCar environment. Each row in the dataset represents a single step taken by the agent, including the state of the environment before and after the action, the action taken, and the reward received.\n",
    "\n",
    "### Columns\n",
    "\n",
    "- `state_0`: The position of the car at the start of the step.\n",
    "- `state_1`: The velocity of the car at the start of the step.\n",
    "- `action`: The action taken by the agent (0 = push left, 1 = no push, 2 = push right).\n",
    "- `reward`: The reward received after taking the action.\n",
    "- `next_state_0`: The position of the car after the action.\n",
    "- `next_state_1`: The velocity of the car after the action.\n",
    "- `done`: A boolean indicating whether the episode has ended (True or False).\n",
    "\n",
    "### Usage\n",
    "\n",
    "This dataset can be used to train and evaluate reinforcement learning algorithms, particularly those that rely on offline data. It provides a fixed set of experiences from which an agent can learn without requiring interaction with a live environment. This can be useful for debugging, testing new algorithms, and comparing performance against established benchmarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b62a36",
   "metadata": {},
   "source": [
    "## Creating the offline environment.\n",
    "\n",
    "To create an offline environment for training our reinforcement learning agent, we need to specify the path to our dataset and define the relevant columns that our environment will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1f20b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the dataset\n",
    "data_path = root_path+\"/data/MountainCarExpert.csv\"\n",
    "\n",
    "# Define the column names in the dataset for states, actions, rewards, episode IDs, and sequence/order\n",
    "states_cols = ['x', 'vel'] # Columns representing the state of the environment (position 'x' and velocity 'vel')\n",
    "action_col = 'action' # Column representing the action taken by the agent\n",
    "reward_col = 'reward' # Column representing the reward received after the action\n",
    "episode_col_id = 'episode_id' # Column representing the unique identifier for each episode\n",
    "order_col = 'seq' # Column representing the sequence/order of the steps within an episode\n",
    "\n",
    "offline_env = ENV(data_path, states_cols, action_col, reward_col, episode_col_id, order_col, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a5888",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In this section, we will train our reinforcement learning agent using the offline environment created earlier. The agent will learn from pre-recorded trajectories by iterating over the dataset for a specified number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ea1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the imitation agent with a specified learning rate\n",
    "agent = ImitationAgent(learning_rate=0.001)\n",
    "\n",
    "# Define the number of epochs for training\n",
    "epochs = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):  # Loop over each epoch\n",
    "\n",
    "    offline_env.reset()\n",
    "    \n",
    "    # Loop over each batch in the offline environment\n",
    "    for batch_id in range(offline_env.env.episodes):\n",
    "        \n",
    "        # Retrieve trajectories (episodes) for the current batch\n",
    "        episode_ids, sequence, states, actions, rewards = offline_env.get_replay(batch_id)\n",
    "\n",
    "        # Loop over each trajectory in the batch\n",
    "        for j in range(len(episode_ids)):\n",
    "            # Store the transition (state, action, reward) in the agent's memory\n",
    "            agent.store_transition(states[j], actions[j], rewards[j])\n",
    "\n",
    "        # Train the agent using the stored transitions\n",
    "        agent.learn()\n",
    "\n",
    "    # Print the epoch number and the current loss of the agent\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", agent.loss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a9ac5c-f5be-4129-9d7d-58de12d658a8",
   "metadata": {},
   "source": [
    "## Saving the Trained Model\n",
    "\n",
    "After training the reinforcement learning agent, it is important to save the trained model for future use. This allows us to load the model later for further training, evaluation, or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained policy model to a specified file path\n",
    "agent.policy.save('models/mountain_car_imitation_model_env.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9535e782-6f22-44df-a9e4-c95878fb3cfb",
   "metadata": {},
   "source": [
    "## Initializing the Gym Environment\n",
    "\n",
    "To test the performance of our trained reinforcement learning agent, we need to initialize the Gym environment. The Gym library provides a standard API for interacting with a wide variety of reinforcement learning environments, including the MountainCar environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7012b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MountainCar environment from the Gym library with human-readable rendering\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b36af-e428-46ef-bf1c-c48050ea5508",
   "metadata": {},
   "source": [
    "## Loading the Trained Model into a New Agent\n",
    "\n",
    "To utilize the trained policy in a new instance of the agent, we need to load the saved model and assign it to the new agent's policy. This allows the new agent to leverage the learned policy without retraining from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c62591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new imitation agent\n",
    "agent2 = ImitationAgent()\n",
    "\n",
    "# Load the trained model from the specified file path\n",
    "agent2.policy = tf.keras.models.load_model('models/mountain_car_imitation_model_env.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68afc6-dd0b-44ea-bfba-16dcfacd82e3",
   "metadata": {},
   "source": [
    "## Running the Agent in the Environment\n",
    "\n",
    "After loading the trained policy into a new agent, we can run the agent in the MountainCar environment to observe its performance and calculate the total reward obtained during an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to start a new episode and retrieve the initial state\n",
    "curr_state, info = env.reset()\n",
    "\n",
    "# Initialize the total reward accumulator\n",
    "total_R = 0\n",
    "\n",
    "# Run the agent in the environment\n",
    "while True:\n",
    "    # Choose an action based on the current state using the trained agent\n",
    "    action = agent2.choose_action(curr_state)\n",
    "    \n",
    "    # Take the action in the environment and receive the next state and reward\n",
    "    next_state, reward, terminated, _, _ = env.step(action)\n",
    "    \n",
    "    # Update the current state\n",
    "    curr_state = next_state.copy()\n",
    "    \n",
    "    # Accumulate the total reward\n",
    "    total_R += reward\n",
    "\n",
    "    # Check if the episode has terminated\n",
    "    if terminated:\n",
    "        break\n",
    "\n",
    "# Print the total reward obtained in the episode\n",
    "print(\"Total reward:\", total_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8699ee1-f7a7-496a-a7a1-93eacbf03ac1",
   "metadata": {},
   "source": [
    "By running the agent in the MountainCar environment, we can evaluate its performance based on the total reward obtained. In this case, the total reward was **-84.0**. This process involves resetting the environment, choosing actions based on the agent's policy, updating the state, and accumulating rewards until the episode terminates.\n",
    "\n",
    "Running this loop allows us to observe how well the agent performs and make adjustments if necessary to improve its behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954abf81-9bc2-4ed8-9f5b-e0a1d692774e",
   "metadata": {},
   "source": [
    "### Closing the Environment\n",
    "\n",
    "After running the agent in the environment and evaluating its performance, it is important to properly close the environment. This ensures that all resources are released and the environment is cleanly shut down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44483bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the environment to release resources\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
