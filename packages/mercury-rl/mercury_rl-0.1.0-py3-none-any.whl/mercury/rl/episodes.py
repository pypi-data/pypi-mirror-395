import re

import numpy as np
import pandas as pd
from minari.data_collector import EpisodeBuffer
from gymnasium.spaces import Box


class SingleFrameIterator:
	"""
	This is the class of the iterator object returned by Episodes.items() when episodes are stored in a single file and identified
	via a column with ID.

	Args:
		col_episodes (str): The name of the column containing episode IDs.
		df (pd.DataFrame): The pandas DataFrame containing all episodes.

	Yields:
		(pd.DataFrame): Individual dataframes for each episode ID and without the episode ID column.
	"""

	def __init__(self, col_episodes, df):
		self.idx = 0

		self.episode_ids = df[col_episodes].drop_duplicates().tolist()
		self.episode_ids.sort()

		self.col_episodes = col_episodes
		self.df			  = df


	def __iter__(self):
		"""
		Implements the iterator protocol.
		"""

		return self


	def __len__(self):
		"""
		Returns the number of datasets.
		"""

		return len(self.episode_ids)


	def __next__(self):
		"""
		Get the next dataset as a pandas DataFrame.
		"""

		if self.idx >= len(self.episode_ids):
			raise StopIteration

		ii = self.episode_ids[self.idx]

		self.idx += 1

		df = self.df[self.df[self.col_episodes] == ii].copy()
		df.drop(columns = [self.col_episodes], inplace = True)

		return df


class DataframeIterator:
	"""
	This is the class of the iterator object returned by Episodes.items() which returns pandas DataFrames loading the objects one by one.

	It is intended as a class to be returned by a Episodes.items() method.

	Args:
		dataframes (list of dict): A list of dictionaries, each containing:
			- 'type': one of 'pandas', 'numpy', 'spark', 'csv'
			- 'data': the actual data (pandas DataFrame, numpy array, Spark DataFrame, or path to CSV file)
			- Optional keys for CSV type:
				- 'sep': separator used in the CSV file (default is ',')
				- 'names': list of column names to use when reading the CSV file. When this is given, the CSV file is read
				  without a header row.
	"""

	def __init__(self, dataframes):
		self.idx  = 0
		self.data = dataframes


	def __iter__(self):
		"""
		Implements the iterator protocol.
		"""

		return self


	def __len__(self):
		"""
		Returns the number of datasets.
		"""

		return len(self.data)


	def __next__(self):
		"""
		Get the next dataset as a pandas DataFrame.
		"""

		if self.idx >= len(self.data):
			raise StopIteration

		df = self.load()
		self.idx += 1

		return df


	def load(self):
		"""
		Load the current dataset and return it as a pandas DataFrame.
		"""
		meta = self.data[self.idx]

		if meta['type'] == 'pandas':
			return meta['data']

		if meta['type'] == 'spark':
			return meta['data'].toPandas()

		assert meta['type'] == 'csv'

		sep = meta.get('sep', ',')

		if 'names' in meta:
			return pd.read_csv(meta['data'], sep = sep, header = None, names = meta['names'])

		return pd.read_csv(meta['data'], sep = sep)


class Episodes:
	"""
	Episode aware container for multiple datasets of different types.

	Each dataset can be a pandas DataFrame, a numpy array, a Spark DataFrame, or a path to a CSV file.

	When iterated over, it yields pandas DataFrames, loading each dataset as needed.

	Args:
		data (): A single dataset or a list of datasets. Each dataset can be:
			- A pandas DataFrame
			- A numpy array. All episodes must have the same column structure and the names must be given in args['numpy_column_names'].
			- A Spark DataFrame
			- A string path to a CSV file
			- A dictionary with:
				- 'path' (mandatory): path to the CSV file
				- 'sep' (optional): separator used in the CSV file (default is ',')
				- 'names' (optional): list of column names to use when reading the CSV file. When this is given, the CSV file is read
				  without a header row.
	args: A dictionary with additional arguments required for interpreting episode structure and adding metadata. The keys are:
			- 'episodes': Either the name of a column that contains episode IDs or None. In the latter case, the dataset must be a list
			  of datasets where each dataset corresponds to a single episode. (Default: None)
			- 'completion': (optional) A regex string that returns 0 or 1 when applied to the dataset name. If the episode is complete,
			  a final termination will be autogenerated, else a final truncation will be autogenerated. (Default: Not defined)
			- 'observations': Either a regex string to match column names or a list of column names that correspond to the observation.
			(Default: '^obs.*$')
			- 'actions': Either a regex string to match column names or a list of column names that correspond to the action.
			(Default: '^action.*$')
			- 'reward': The column name that corresponds to the reward. (Default: 'reward')
			- 'termination': The column name that corresponds to the termination. It should be None when 'completion' is given.
			(Default: 'finished')
			- 'truncation': The column name that corresponds to the truncation. It should be None when 'completion' is given. It can
			also be None when it is just complementary to 'termination'. In that case, the last timestep of each episode will be either
			a termination or a truncation depending on the value of 'termination'. (Default: None)
			- 'obs_space': (optional) A gymnasium.spaces.box.Box defining the observation space. If not given, it will be inferred
			from the data. (Default: Not defined)
			- 'act_space': (optional) A gymnasium.spaces.box.Box defining the action space. If not given, it will be inferred from
			the data. (Default: Not defined)
			- 'numpy_column_names': (only for numpy arrays) A list of column names corresponding to the numpy array columns.

	Methods:
		items(): Returns an iterator that yields episodes as pandas DataFrames. (It applies the logic in args.)
		episodes: A property that returns all episodes as a list of dictionaries suitable for a minari create_dataset_from_buffers() call.
		obs_space: A property with the observation space as a gymnasium.spaces.Box suitable for a create_dataset_from_buffers() call.
		act_space: A property with the action space as a gymnasium.spaces.Box suitable for a create_dataset_from_buffers() call.
		len(): Returns the number of episodes.
	"""

	def __init__(self, data, args = None):
		self.dataframes = []

		if not isinstance(data, list):
			data = [data]

		for item in data:
			if isinstance(item, pd.DataFrame):
				self.dataframes.append({'type': 'pandas', 'data': item})

			elif isinstance(item, np.ndarray):
				df = pd.DataFrame(item, columns = args['numpy_column_names'])
				self.dataframes.append({'type': 'pandas', 'data': df})

			elif 'pyspark.sql.dataframe.DataFrame' in str(type(item)):
				self.dataframes.append({'type': 'spark', 'data': item})

			elif isinstance(item, str):
				self.dataframes.append({'type': 'csv', 'data': item})

			elif isinstance(item, dict):
				item['type'] = 'csv'
				item['data'] = item['path']
				self.dataframes.append(item)
			else:
				raise ValueError('Unsupported data type: %s' % str(type(item)))     # pylint: disable=C0209

		self.args = args if args is not None else {}
		self.data = data

		self._obs_space = self.args.get('obs_space', None)
		self._act_space = self.args.get('act_space', None)

		self._col_episodes = self.args.get('episodes', None)		# Episodes defined via a column with ID or separate files (if None)
		self._single_file = len(self.dataframes) == 1

		if self._col_episodes is not None:
			assert self._single_file, 'If episodes are defined via a column, there must be only one file.'


	def __len__(self):
		"""
		Returns the number of episodes.
		"""

		if self._col_episodes is None:
			return len(self.dataframes)

		sfi = self.items()

		return len(sfi.episode_ids)


	def items(self):
		"""
		Returns an iterator of pandas DataFrames.
		"""

		di = DataframeIterator(self.dataframes)

		if self._col_episodes is None:
			return di

		df = next(di)

		return SingleFrameIterator(self._col_episodes, df)


	@property
	def episodes(self):
		"""
		Returns all the episode as numpy arrays built in a way suitable for a minari create_dataset_from_buffers() call.
		"""

		calc_obs_space = self._obs_space is None
		calc_act_space = self._act_space is None

		rex_completion = self.args.get('completion', None)			# If used, self.data must contain a list of names

		if rex_completion is not None:
			rex_completion = re.compile(rex_completion)

		observations = self.args.get('observations', None)			# Either a regex or a list.
		if observations is None or type(observations) is str:
			rex_observations = re.compile(self.args.get('observations', '^obs.*$'))
			observations = None

		actions = self.args.get('actions', None)					# Either a regex or a list.
		if actions is None or type(actions) is str:
			rex_actions = re.compile(self.args.get('actions', '^action.*$'))
			actions = None

		col_reward = self.args.get('reward', 'reward')			# Column name for reward
		assert type(col_reward) is str

		col_termination = self.args.get('termination', 'finished')	# Column name for completion
		col_truncation  = self.args.get('truncation', None)		# Column name for truncation (if any)

		episodes = []

		dim_obs = None
		dim_act = None

		for id, df in enumerate(self.items()):
			obs = observations
			if obs is None:
				obs = [c for c in df.columns if rex_observations.match(c)]

			assert type(obs) is list and len(obs) > 0, 'No observation columns found.'
			np_obs = df[obs].to_numpy()

			if dim_obs is None:
				dim_obs = list(np_obs.shape)[1:]

			assert list(np_obs.shape)[1:] == dim_obs, 'Observation shape mismatch between episodes.'

			if calc_obs_space:
				lo = np_obs.min(axis = 0)
				hi = np_obs.max(axis = 0)
				if self._obs_space is None:
					self._obs_space = Box(low = lo, high = hi)
				else:
					self._obs_space.low  = np.minimum(self._obs_space.low, lo)
					self._obs_space.high = np.maximum(self._obs_space.high, hi)

			act = actions
			if act is None:
				act = [c for c in df.columns if rex_actions.match(c)]

			assert type(act) is list and len(act) > 0, 'No action columns found.'
			np_act = df[act].to_numpy()[:-1]		# The last action is always None, so we drop it.

			if dim_act is None:
				dim_act = list(np_act.shape)[1:]

			assert list(np_act.shape)[1:] == dim_act, 'Action shape mismatch between episodes.'

			if calc_act_space:
				lo = np_act.min(axis = 0)
				hi = np_act.max(axis = 0)
				if self._act_space is None:
					self._act_space = Box(low = lo, high = hi)
				else:
					self._act_space.low  = np.minimum(self._act_space.low, lo)
					self._act_space.high = np.maximum(self._act_space.high, hi)

			np_r = df[col_reward].to_numpy()[:-1]	# The last reward is always None, so we drop it.

			if np_r.dtype != np.float32 and np_r.dtype != np.float64:
				np_r = np_r.astype(np.float32)

			if rex_completion is not None:
				assert not self._single_file, 'If a completion regex is given, there must be multiple file names.'

				fn = self.data[id]
				assert type(fn) is str, 'If a completion regex is given, data must be a list of file names.'

				term = np.zeros(shape = np_r.shape, dtype = bool)
				if rex_completion.match(fn):
					term[-1] = True
			else:
				assert col_termination is not None, 'Either a completion regex or a termination column name must be given.'
				term = df[col_termination].to_numpy()[:-1]	# The last reward is always None, so we drop it.
				if term.dtype != bool:
					term = term.astype(bool)

			if col_truncation is not None:
				trunc = df[col_truncation].to_numpy()[:-1]	# The last reward is always None, so we drop it.
				if trunc.dtype != bool:
					trunc = trunc.astype(bool)
			else:
				trunc = np.zeros(shape = np_r.shape, dtype = bool)
				if not term[-1]:
					trunc[-1] = True

			epi = EpisodeBuffer(id = id, observations = np_obs, actions = np_act, rewards = np_r, terminations = term, truncations = trunc)

			episodes.append(epi)

		return episodes


	@property
	def obs_space(self):
		"""
		Returns the observation space (as a gymnasium.spaces.Box) suitable for a minari create_dataset_from_buffers() call.
		"""

		if self._obs_space is None:
			_ = self.episodes

		return self._obs_space


	@property
	def act_space(self):
		"""
		Returns the action space (as a gymnasium.spaces.Box) suitable for a minari create_dataset_from_buffers() call.
		"""

		if self._act_space is None:
			_ = self.episodes

		return self._act_space
