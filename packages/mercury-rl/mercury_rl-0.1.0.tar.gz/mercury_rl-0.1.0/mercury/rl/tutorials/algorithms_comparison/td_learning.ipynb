{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb2f756",
   "metadata": {},
   "source": [
    "# TD($λ$) learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af296f",
   "metadata": {},
   "source": [
    "TD(0)'s objective is the same as Monte Carlo - V(s) on-policy estimation.\n",
    "\n",
    "The TD(0) strategy can be loosely described as: \"I don't want to wait for the calculation of cumulative rewards. I will estimate the V(s) by the proxy of immediate rewards instead. I will also look at the next state in the training episode - if it is a good state, that means my current state must also be good.\"\n",
    "\n",
    "TD($λ$), where $λ∈[0,1]$, goes one step further. When performing the look-ahead bootstrap, if a big reward is going to be achieved in the next state, TD(0) only gives credit to the state before that big reward, whereas TD(1) would be equivalent to Monte Carlo, giving credit up to the first state in the episode. *In-between* values would give proportionally less merit to those states further from the big reward state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f9ecf",
   "metadata": {},
   "source": [
    "TD(λ) (Temporal-Difference learning with λ-returns and eligibility traces) is a method in reinforcement learning for estimating the value function of a given policy.\n",
    "\n",
    "It combines ideas from:\n",
    "\n",
    "- TD(0) — bootstrapping with one-step lookahead\n",
    "\n",
    "- Monte Carlo — using full-episode returns\n",
    "\n",
    "The parameter $λ∈[0,1]$ controls the trade-off between these two.\n",
    "\n",
    "Use cases\n",
    "\n",
    "- TD(0): When you want stable, incremental learning\n",
    "\n",
    "- TD(λ): When you want faster convergence and more credit assignment flexibility\n",
    "\n",
    "- TD(1) or Monte Carlo: When full returns are available and sample efficiency is less of a concern\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9264265",
   "metadata": {},
   "source": [
    "Monte Carlo vs TD(0): Summary and Trade-offs\n",
    "\n",
    "Monte Carlo:\n",
    "\n",
    "- Updates occur only after the episode ends.\n",
    "\n",
    "- Uses the full return from the current state to the end of the episode as the target.\n",
    "\n",
    "- Does not use bootstrapping — it relies purely on actual returns.\n",
    "\n",
    "- Provides unbiased estimates of the value function.\n",
    "\n",
    "- Has high variance, especially in long or stochastic episodes.\n",
    "\n",
    "- Slower learning because it updates each state only once per episode.\n",
    "\n",
    "- Not suitable for online learning (can't learn during an episode).\n",
    "\n",
    "- Less sample-efficient.\n",
    "\n",
    "- Can be unstable when combined with function approximation due to noisy targets.\n",
    "\n",
    "TD(0):\n",
    "\n",
    "- Updates occur after every step.\n",
    "\n",
    "- Uses the immediate reward plus the current estimate of the next state's value as the target.\n",
    "\n",
    "- Uses bootstrapping — updates are based partly on existing value estimates.\n",
    "\n",
    "- Introduces bias due to using its own estimates in the update.\n",
    "\n",
    "- Lower variance, resulting in more stable learning.\n",
    "\n",
    "- Learns faster because it updates continuously throughout the episode.\n",
    "\n",
    "- Suitable for online learning and real-time applications.\n",
    "\n",
    "- More sample-efficient.\n",
    "\n",
    "- More stable with function approximation because targets are smoother.\n",
    "\n",
    "Trade-offs:\n",
    "Monte Carlo is simple and unbiased but has high variance and slower learning. It's suitable for environments with short episodes or dense rewards. TD(0) is biased but more efficient, stable, and responsive, making it a better fit for long or continuing tasks, online learning, and function approximation scenarios.\n",
    "\n",
    "Key takeaway:\n",
    "Use TD(0) when you need faster, online, or stable learning, especially in sparse or long-horizon tasks. Use Monte Carlo when you can afford to wait for full returns and want unbiased estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac73f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compress_state, generate_extreme_value_state_image\n",
    "import numpy as np\n",
    "\n",
    "import minari\n",
    "from collections import defaultdict\n",
    "from IPython.display import HTML\n",
    "import uuid\n",
    "\n",
    "\n",
    "def td_lambda_evaluation(dataset_id, gamma=0.90, lambda_=0.0, alpha=1.0):\n",
    "    \"\"\"\n",
    "    Evaluates the state-value function V(s) for the implicit policy in a Minari dataset\n",
    "    using TD(lambda) learning with eligibility traces.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_id : str\n",
    "        The Minari dataset ID (assumed to contain episodes generated by a fixed policy).\n",
    "    \n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "    \n",
    "    lambda_ : float\n",
    "        Trace decay parameter (controls bias-variance trade-off).\n",
    "    \n",
    "    alpha : float\n",
    "        Learning rate for TD updates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    V : dict\n",
    "        A dictionary mapping each state (as a hashable key) to its estimated value.\n",
    "\n",
    "    state_locations : dict\n",
    "        A dictionary mapping state keys to (episode_index, timestep) of first occurrence.\n",
    "    \"\"\"\n",
    "    dataset = minari.load_dataset(dataset_id)\n",
    "    V = defaultdict(float)\n",
    "    state_locations = {}\n",
    "\n",
    "    for episode_idx, episode in enumerate(dataset.iterate_episodes()):\n",
    "        observations = episode.observations\n",
    "        rewards = episode.rewards\n",
    "        actions = episode.actions\n",
    "\n",
    "        E = defaultdict(float)  # eligibility traces: propagation of TD error\n",
    "\n",
    "        for t in range(len(rewards)):\n",
    "            obs_t = {k: v[t] for k, v in observations.items()}\n",
    "            obs_tp1 = {k: v[t + 1] for k, v in observations.items()}\n",
    "            reward = rewards[t]\n",
    "\n",
    "            s_t = compress_state(obs_t)\n",
    "            s_tp1 = compress_state(obs_tp1)\n",
    "\n",
    "            # Record first occurrence of the state\n",
    "            if s_t not in state_locations:\n",
    "                state_locations[s_t] = (episode_idx, t)\n",
    "\n",
    "            # TD error\n",
    "            delta = reward + gamma * V[s_tp1] - V[s_t] # difference between expected and actual value\n",
    "\n",
    "            # Update eligibility trace\n",
    "            E[s_t] += 1\n",
    "\n",
    "            # Propagate the TD error through traces\n",
    "            for s in E:\n",
    "                V[s] += alpha * delta * E[s] # apply TD update\n",
    "                E[s] *= gamma * lambda_ # every state trace decays: so that earlier states are less influential\n",
    "        \n",
    "        # Track missing state at the end of the episode\n",
    "        if s_tp1 not in state_locations:\n",
    "            state_locations[s_tp1] = (episode_idx, len(actions))\n",
    "\n",
    "    values = np.array(list(V.values()))\n",
    "    print(\"Value function statistics:\")\n",
    "    print(f\"  Count:       {len(values)}\")\n",
    "    print(f\"  Min value:   {np.min(values):.4f}\")\n",
    "    print(f\"  Max value:   {np.max(values):.4f}\")\n",
    "    print(f\"  Mean value:  {np.mean(values):.4f}\")\n",
    "    print(f\"  Std dev:     {np.std(values):.4f}\")\n",
    "\n",
    "    return V, state_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5fa27",
   "metadata": {},
   "source": [
    "#### Highest value image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a7141a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function statistics:\n",
      "  Count:       52916\n",
      "  Min value:   0.0000\n",
      "  Max value:   0.9969\n",
      "  Mean value:  0.0173\n",
      "  Std dev:     0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorge/BBVA/mercury-rl/rl-venv/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/td_lambda/highest_value_function.png?v=28779c47c6e54667bfd9ddd8973563d7\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/td_lambda/highest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    value_fn_generator=td_lambda_evaluation,\n",
    "    highest=True\n",
    ")\n",
    "\n",
    "# === Display in notebook ===\n",
    "cache_buster = uuid.uuid4().hex\n",
    "HTML(f'<img src=\"{output_path}?v={cache_buster}\" width=\"400\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28830a7a",
   "metadata": {},
   "source": [
    "#### Lowest value image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa9c921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function statistics:\n",
      "  Count:       52916\n",
      "  Min value:   0.0000\n",
      "  Max value:   0.9969\n",
      "  Mean value:  0.0173\n",
      "  Std dev:     0.1253\n",
      "Sampling rejected: unreachable object at (15, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/td_lambda/lowest_value_function.png?v=72aacc926fa3420492dadff3decc9e81\" width=\"400\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_id = \"minigrid/BabyAI-Pickup/optimal-fullobs-v0\"\n",
    "output_path = \"./minigrid/BabyAI-Pickup/optimal-fullobs-v0/td_lambda/lowest_value_function.png\"\n",
    "\n",
    "generate_extreme_value_state_image(\n",
    "    dataset_id=dataset_id,\n",
    "    output_path=output_path,\n",
    "    value_fn_generator=td_lambda_evaluation,\n",
    "    highest=False\n",
    ")\n",
    "\n",
    "# === Display in notebook ===\n",
    "cache_buster = uuid.uuid4().hex\n",
    "HTML(f'<img src=\"{output_path}?v={cache_buster}\" width=\"400\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c21543",
   "metadata": {},
   "source": [
    "As seen before, highest value states are those that are close to the goal, while lowest value states are those that are far from the goal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
