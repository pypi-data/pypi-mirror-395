# ORCA Model Test Configuration with Performance Monitoring Demo
# ================================================================
# This configuration demonstrates the new performance monitoring features
# and provides a comprehensive example of all available configuration options.
# This is primarily used for testing and development purposes.

# ================================================================
# MODEL EXECUTION SEQUENCE
# ================================================================
# Define the sequence of model steps to be executed in order.
# Available steps: popsim, quetzal_starter, activitysim, cvm, quetzal
# For testing, we use simplified demo steps to validate the orchestrator functionality.
model_steps:
  - demo_echo # Simple demo command to test basic orchestrator functionality
  - demo_activitysim # Simplified ActivitySim test run

# ================================================================
# SHARED INPUT DATA (COMMENTED OUT FOR TEST CONFIG)
# ================================================================
# Define shared input data across multiple sub-components
# This section allows you to specify common data sources that multiple components use
# input_data:
#   landuse:
#     sources:
#       - ABM/BaseNetworks/Inputs/landuse_synpop_2017.zip
#       - ABM/BaseNetworks/Inputs/landuse_synpop_2023.zip
#       - ABM/BaseNetworks/Inputs/landuse_synpop_2033.zip
#   networks:
#     sources:
#       - ABM/BaseNetworks/base_network_*.txt
#       - ABM/BaseNetworks/bridge_crossing_gy.csv
#       - ABM/BaseNetworks/dist_factors_gy.csv.gz
#   externals:
#     sources:
#       - ABM/BaseNetworks/externals_bikescore_*.csv.gz

# ================================================================
# ITERATION CONTROL AND CONVERGENCE
# ================================================================
# Control the number of iterations and convergence criteria
iterations:
  total: 1 # Total number of iterations to run (must be >= 1)
  start_at: 1 # Starting iteration number (set to 1 for human-readable numbering)
  # convergence_criteria:  # COMMENTED OUT - not needed for test config
  #   enabled: false        # Set to true to enable early stopping based on convergence
  #   metric: link_flow_rmse # Metric to use for convergence checking
  #   threshold: 0.01       # Threshold value for convergence

# ================================================================
# SUB-COMPONENT DEFINITIONS
# ================================================================
# Define each sub-component and its execution parameters
# Each component can have: environment, commands, cleanup_patterns, output_archives
sub_components:
  # ================================================================
  # DEMO ECHO COMPONENT - Simple test component
  # ================================================================
  demo_echo:
    # Environment configuration - specifies Python interpreter to use
    environment:
      env_var: "ORCA_BASE_ENV" # Environment variable name to check first
      default: "%USERPROFILE%/AppData/Local/miniforge3/python.exe" # Fallback if env_var not set
    # Commands to execute in sequence
    commands:
      - command: '{python} -c "import time; import sys; print(''Demo command started''); time.sleep(2); print(''Demo command completed'')"'
        description: "Demo command with 2-second runtime" # Human-readable description for logging
        iterations: all # When to run: 'all', 'first', 'last', or list like [1, 3, 4]
    # Template directories/files to copy during databank initialization (optional)
    # source_template:  # COMMENTED OUT - not needed for this demo
    #   - demo_data
    # Cleanup patterns - files to remove after execution (optional)
    # cleanup_patterns:  # COMMENTED OUT - not needed for this demo
    #   - "*.tmp"
    # Output archives - files to archive after execution
    output_archives:
      - archive_name: logs # Name of the archive
        patterns: # Glob patterns for files to include
          - "*.txt"
          - "*.log"

  # ================================================================
  # DEMO ACTIVITYSIM COMPONENT - Simplified ActivitySim test
  # ================================================================
  demo_activitysim:
    # Environment configuration for ActivitySim
    environment:
      env_var: "ORCA_ACTIVITYSIM_ENV" # Name of environment variable to check first
      default: "C:/ProgramData/activitysim/.venv/Scripts/python.exe" # Default if env_var is not set
      # For ActivitySim, you might use a specific environment
      # Example on Windows: "C:/Users/username/AppData/Local/miniforge3/envs/asim1_4"
      # Or using expanduser: "%USERPROFILE%/AppData/Local/miniforge3/envs/asim1_4"
    # Commands to execute for this component
    commands:
      # Example of commented out command - copying template files
      # - command: "{python} -m orchestrator --action copy_template_file"
      #   description: "Copying skim data to ActivitySim folder"
      #   iterations: all
      # Main ActivitySim test command using test data
      - command: "{python} -m activitysim run -c test/configs -c configs_modified_for_estimation -c configs -d test/data -o output"
        description: "Run ActivitySim model" # Description for logging and monitoring
        iterations: all # Run for all iterations
    # Files to clean up after execution to save space
    cleanup_patterns:
      - "*.tmp" # Temporary files
      - "output/*.h5" # Large HDF5 files that don't need to be kept
      - "output/mp_households_*-pipeline.parquetpipeline/**" # Large intermediate data files
      - "output/pipeline.parquetpipeline/**" # Large intermediate data files
      # NOTE: Files specified in cleanup will not be available in archive unless an error occurred during runs
    # Output archives to create after successful execution
    output_archives:
      - archive_name: outputs # Name of the archive
        patterns: # Patterns for files to include in archive
          - "output/**" # Include all output files

# ================================================================
# PERFORMANCE MONITORING CONFIGURATION - NEW FEATURE
# ================================================================
# Configure system resource monitoring during command execution
# This feature tracks CPU, memory usage, and execution time for performance analysis
performance_monitoring:
  enabled: true # Whether to enable performance monitoring for commands
  poll_interval: 0.5 # Interval (in seconds) for polling system metrics during command execution
  track_memory: true # Whether to track memory usage (requires psutil library)
  track_cpu: true # Whether to track CPU usage (requires psutil library)
  export_detailed_logs: true # Export detailed profiling data to CSV files for analysis

# ================================================================
# OPERATIONAL MODE CONFIGURATION
# ================================================================
# Define the operational mode - determines how the orchestrator behaves
operational_mode:
  type: local_testing # Options: 'local_testing' or 'cloud_production'
  # cloud:  # COMMENTED OUT - not needed for local testing
  #   # Azure Data Lake Storage (ADLS) settings for cloud_production mode
  #   adls_url: "https://your_adls_resource_name.dfs.core.windows.net"  # ADLS endpoint URL
  #   adls_container: "raw"                    # Container/top-level folder in ADLS
  #   adls_folder: "proj_unnamed"     # Project folder within the ADLS container

# ================================================================
# ERROR HANDLING CONFIGURATION
# ================================================================
# Configure how errors are handled and what debugging information is collected
error_handling:
  create_error_dump: true # Whether to create error dumps when models fail
  include_full_databank_in_dump: false # Whether to include the entire databank in error dumps (can be large)
  upload_error_dumps_to_cloud: false # Whether to automatically upload error dumps to cloud (only relevant in cloud_production mode)
