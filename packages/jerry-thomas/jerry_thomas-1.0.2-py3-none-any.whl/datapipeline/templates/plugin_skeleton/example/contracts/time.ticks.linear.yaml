kind: ingest
source: synthetic.ticks # raw source alias (see example/sources)
id: time.ticks.linear # canonical stream id (format: domain.dataset.(variant))

# Fine-grained cadence for this stream. Defaults to the dataset group_by via project.globals.
cadence: ${group_by}

mapper: # normalize/reshape DTO -> TemporalRecord
  entrypoint: encode_time
  args: { mode: linear }
# partition_by: station_id      # optional: add partition suffixes to feature ids

record: # record-level transforms
  - filter: { operator: ge, field: time, comparand: "${start_time}" }
  - filter: { operator: le, field: time, comparand: "${end_time}" }
  - floor_time: { cadence: "${cadence}" }          # snap timestamps to cadence boundaries
  # - lag: { lag: "${cadence}" }                      # optional: shift timestamps backwards

stream:                       # per-feature stream transforms (input sorted by id,time)
  - dedupe: {}                                # drop exact-duplicate records per tick
  - granularity: { mode: first }              # aggregate duplicates within a tick
  - ensure_cadence: { cadence: "${cadence}" }           # insert missing ticks (value=None)
  # Consider adding a fill transform to impute None values before sequence/windowing:
  # - fill: { statistic: median, window: 6, min_samples: 1 }

debug:                        # optional validation-only transforms
  - lint: { mode: error, tick: "${cadence}" }  # strict cadence/order; value issues handled by downstream transforms

# sort_batch_size: 100000       # in-memory chunk size used by internal sorting

