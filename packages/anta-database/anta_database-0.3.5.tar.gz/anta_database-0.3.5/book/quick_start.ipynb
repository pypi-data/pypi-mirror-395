{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb9d44e",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "## Installation\n",
    "\n",
    "The Python module can be directly installed from [PyPI](https://pypi.org/project/anta-database/) with:\n",
    "\n",
    "    pip install anta_database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bbc91",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "This Python module is designed to query and visualized data in a AntADatabase folder. The latter is not published yet, but you can contact me for access.\n",
    "You can already have a look at this guide to have an idea of the features of this tool. This Jupyter Notebook can be directly downloaded (top bar) and ran locally (assuming you had downloaded or compiled the AntADatabase folder).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19165338",
   "metadata": {},
   "source": [
    "## Browsing the database\n",
    "\n",
    "First, initialize the Database class by providing the full path to the AntADatabase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bff0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anta_database import Database\n",
    "\n",
    "db = Database('/home/anthe/documents/data/isochrones/AntADatabase/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867679c7",
   "metadata": {},
   "source": [
    "Use the query() function to browse the database. 'query()' without argument will return all the metadata from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24813ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d8b67",
   "metadata": {},
   "source": [
    "The query() function takes as argument:\n",
    "- dataset: dataset(s) of interest\n",
    "- institute: institutes(s) that produced the data\n",
    "- project: project(s) under which the data were collected\n",
    "- acquisition_year: year(s) in which the radar data were acquired\n",
    "- age: age(s) in yrs before present of the layer(s) of interest\n",
    "- region: region(s) of interest. E.g.: 'EAIS', 'WAIS'\n",
    "- IMBIE_basin: IMBIE basin(s) of interest. E.g.: 'G-H', 'Ap-B'.\n",
    "- var: variables(s) of interest. Possible variables currently are: 'IRH_DEPTH', 'IRH_NUM', 'ICE_THK', 'BED_ELEV', 'SURF_ELEV'\n",
    "- flight_id: ID of a particular flight line. This is useful for explicit flight IDs such as 'DC_LDC_DIVIDE'\n",
    "\n",
    "One can also combine field queries as well as providing lists. Use '%' for searching by regular expressions. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe48269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples of queries:\n",
    "db.query(dataset='Cavitte_2020') # all data from Cavitte et al. 2020\n",
    "db.query(institute='BAS') # all data that was acquired by BAS\n",
    "db.query(project='OIB') # all data that was acquired during OIB campaigns\n",
    "db.query(age='38100') # all datasets with the 38.1ka isochrone\n",
    "db.query(var='ICE_THK') # all datasets with ICE_THK variable\n",
    "db.query(IMBIE_basin='G-H') # all flight lines that cross the G-H basin\n",
    "db.query(flight_id='DC_LDC_DIVIDE') # all layers with the flight ID DC_LDC_DIVIDE\n",
    "db.query(flight_id='%WSB%') # all flight lines with WSB in the flight ID\n",
    "db.query(dataset=['Franke_2025', 'Winter_2018'], age='38100') # example of multiple criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d80bdcd",
   "metadata": {},
   "source": [
    "The filter_out() function allows the pre-filter out some data so they would never be included in the next queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "db.filter_out(acquisition_year='<1990')  # filter out all data acquired before 1990\n",
    "db.query() # now all queries will exclude data acquired before 1990\n",
    "\n",
    "db.filter_out()  # reset all filters to include all data again\n",
    "db.query()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925cf0c3",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Use the results of the query in the plotting functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e071c",
   "metadata": {},
   "source": [
    "Current implemented plotting functions are:\n",
    "- plot.dataset(): plots locations of the data, with different colors for the different datasets\n",
    "- plot.institute(): plots locations of the data, with different colors for the different institutes\n",
    "- plot.var(): color-coded scatter plot of the variable of interest.\n",
    "- plot.flight_id(): color-coded trace IDs. Useful for identifying specific traces of interest.\n",
    "- plot.transect_1D(): plots depths of the IRH and Bed along a single flight line \n",
    "\n",
    "In Jupyter Notebook, use '%matplotlib qt' or '%matplotlib widget' depending on your IDE, to switch to the matplotlib widget that allows you to zoom in etc.\n",
    "Use '%matplotlib inline' (default) to plot the figure in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fe92e",
   "metadata": {},
   "source": [
    "### Plot datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c68119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "results = db.query(IMBIE_basin='G-H')\n",
    "db.plot.dataset(results,\n",
    "                title='IRH data crossing the G-H IMBIE basin',\n",
    "                xlim=(-2000, -500), # set the plot extent in km\n",
    "                ylim=(-1000, 250),\n",
    "                marker_size=1, # adjust the size of the markers\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0217254",
   "metadata": {},
   "source": [
    "Note: all flight lines in the database are associated with one or several IMBIE basin(s). This depends if the flight line crosses one or multiple basins. The plot above shows all the flight lines which have traced IRH data and that crosses at some point the G-H basin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735309e",
   "metadata": {},
   "source": [
    "### Plot variables\n",
    "Example of plotting the IRH depth of a specific layer found across multiple datasets. Here we still select multiple ages close from each other, which could be attributed to the same layer (but different dating method used maybe). Note the warning in the case, but we can ignore it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89a2cbc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "results = db.query(age=['37600', '38000', '38100', '38200', '38500'], var='IRH_DEPTH')\n",
    "db.plot.var(results, title='AntArchitecture 38ka isochrone depth',\n",
    "                downsampling_factor=10, # downscale the datasets n times, which makes little visual difference but lighter to plot\n",
    "                xlim=(-500, 2400),\n",
    "                ylim=(-2200, 2200),\n",
    "                scale_factor=0.7, # adjust the size of the plot \n",
    "                marker_size=1.2,\n",
    "                # save='AntA_38ka_depth.png'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ada192",
   "metadata": {},
   "source": [
    "The above plot shows the absolute IRH depth relative to the ice surface as it is traced. It is often more informative to look at the IRH fraction depth: the depth of a layer relative to the ice thickness (IRH_DEPTH/ICE_THK*100). The fraction depth variable is not directly stored in the database to reduce disk usage. It is probably more efficient to compute it when needed. For this, use the option fraction_depth=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29343c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "results = db.query(age=['37600', '38000', '38100', '38200', '38500'], var='IRH_DEPTH')\n",
    "db.plot.var(results, \n",
    "            fraction_depth=True,\n",
    "            title='AntArchitecture 38ka isochrone fractional depth',\n",
    "                downsampling_factor=10, # downscale the datasets n times, which makes little visual difference but lighter to plot\n",
    "                xlim=(-500, 2400),\n",
    "                ylim=(-2200, 2200),\n",
    "                scale_factor=0.7, # adjust the size of the plot \n",
    "                marker_size=1.2,\n",
    "                # save='AntA_38ka_depth.png'\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b53478",
   "metadata": {},
   "source": [
    "Note that if the variable ICE_THK is not present in a dataset, the fractional depth cannot be computed. In the case, NaN values will be generated instead without warning. So if you get a blank plot, please check if the dataset you queried contains the ICE_THK with db.query(dataset='Author_YYYY')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee330fa6",
   "metadata": {},
   "source": [
    "### Plot datasets\n",
    "\n",
    "The IRH_NUM variable shows the number of traced isochrones (layers) per data point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(var='IRH_NUM')\n",
    "db.plot.var(results, title='IRH Density over the Antarctic Ice Sheet',\n",
    "                downsampling_factor=100,\n",
    "                scale_factor=1,\n",
    "                marker_size=1.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9185b390",
   "metadata": {},
   "source": [
    "### Plot flight IDs\n",
    "\n",
    "This plot is useful when we want to identify a specific flight line. One can then identify a flight line of interest on the 2D map, then plot the traced IRHs along the transect (see below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93287978",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(dataset='Winter%', flight_id=['EPICA%'])\n",
    "db.plot.flight_id(results, title='Winter et al. 2018 - EPICA',\n",
    "                xlim=(-500, 1000),\n",
    "                ylim=(1000, 2200),\n",
    "                marker_size=1.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b02a87",
   "metadata": {},
   "source": [
    "### Plot layer depths along transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a358b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(dataset='Cavitte_2020', flight_id='DC_LDC_DIVIDE')\n",
    "db.plot.transect_1D(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1d77ba",
   "metadata": {},
   "source": [
    "Use the elevation=True option to plot the transect in absolute elevation above sea level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c3457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.plot.transect_1D(results, elevation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cee8359",
   "metadata": {},
   "source": [
    "Other possible arguments for the plot methods:\n",
    "- cmap: provide your colormap of choice (as LinearSegmentedColormap). Tip: 'import colormaps as cmaps' for a large choice of colormaps (see [Colormaps docs](https://pratiman-91.github.io/colormaps/)\n",
    "- vmin and vmax: sets the minimum and maximum values for the colorbar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcf714",
   "metadata": {},
   "source": [
    "## BEDMAP\n",
    "\n",
    "The AntADatabase now contains all the BEDMAP (1, 2 and 3) data. This is useful to see the whole extent of the existing radar data, or to reconnect the IRH datasets to BEDMAP in order to get the Bed Elevation or Ice Thickness when those are not included.\n",
    "\n",
    "By default, BEDMAP is not shown in the query, since it is not an IRH dataset. To include it, initialize the Database with the option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de501f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database('/home/anthe/documents/data/isochrones/AntADatabase/', include_BEDMAP=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dcee6d",
   "metadata": {},
   "source": [
    "## Get files from the database\n",
    "\n",
    "You may want to make your own plots or further process the data after querying the database. One option is to get the list of the files from your query and open them individually with either xarray or h5py. For this, use the get_files() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "results = db.query(dataset='Cavitte_2020', flight_id='DC_LDC_DIVIDE')\n",
    "file_list = db.get_files(results)\n",
    "f = file_list[0]\n",
    "ds = xr.open_dataset(f, engine='h5netcdf')\n",
    "print(ds)\n",
    "plt.plot(ds.Distance, ds.IRH_DEPTH)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0f26c",
   "metadata": {},
   "source": [
    "xarray provides a nice interface for interacting with the data. This works great when dealing with one file, and for example quickly plot all the layers (see above). But xarray adds some overload, which feels slow when reading many files. h5py on the other hand reads the underlying data right away, which is much more efficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "results = db.query(dataset='Cavitte_2020')\n",
    "file_list = db.get_files(results)\n",
    "for f in file_list:\n",
    "    with h5py.File(f, 'r') as ds:\n",
    "        plt.scatter(ds['PSX'][:], ds['PSY'][:], c=ds['ICE_THK'][:], s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e3be1",
   "metadata": {},
   "source": [
    "## Generate data from the database\n",
    "\n",
    "Note: This part could be developed further in the future if there is the need. But for now, I am designing a separate Python module for constraining my ice sheet model of use, which is tailored to this database and other parallel processing libraries. However, the [Model-comparison](https://antoinehermant.github.io/anta_database/pism_example.html) section already give some bits of code about it.\n",
    "\n",
    "The data_generator() function reads the query and 'yield' the dataframes for later use. It uses h5py to read all the data efficiently, and creates pandas dataframes, including all the variables and ages from the query. Columns for IRH DEPTH are named after the age. Basically, it reads the data with h5py as shown above and restructure it as bit, which can be easier to manage layers than with h5py dimensions.\n",
    "Here is a quick example of how this can be used for computing the mean layer depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c22b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = db.query(age=['37600', '38000', '38100', '38200', '38500'], var='IRH_DEPTH')\n",
    "lazy_dfs = db.data_generator(results)\n",
    "\n",
    "import numpy as np\n",
    "mean_depth_trs = []\n",
    "min_depth = float('inf')\n",
    "max_depth = float('-inf')\n",
    "for df, md in lazy_dfs:\n",
    "    depth_values = df[md['age']].values\n",
    "    mean_depth_trs.append(np.nanmean(depth_values))\n",
    "    min_depth = min(min_depth, np.nanmin(depth_values))\n",
    "    max_depth = max(max_depth, np.nanmax(depth_values))\n",
    "\n",
    "\n",
    "mean_depth = np.nanmean(mean_depth_trs)\n",
    "std_dev = np.nanstd(mean_depth_trs, ddof=1)\n",
    "print(f\"The mean depth of the 38ka isochrone across East Antarctica is {round(mean_depth, 2)} m ranging from {round(min_depth, 2)} m to {round(max_depth, 2)} m.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0f078",
   "metadata": {},
   "source": [
    "Note that the data_generator returns a simple pandas DataFrame for each flight line, containing all queried variables and ages (if exist). The IRH depth of each layer is stored by columns named by age (so the IRH_DEPTH of the layer 38000 is df['38000']). As shown above, one can use the metadata (md) of the current dataframe (df) to get its age (md['age']).\n",
    "Furthermore, if one needs the fraction depth, one can compute it using the ICE_THK (has to be included in the query), or the data_generator has this option: db.data_generator(results, fraction_depth=True). With this option, the layer columns will now be IRH fraction depth instead of absolute IRH_DEPTH."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d2825",
   "metadata": {},
   "source": [
    "# Pro tips\n",
    "\n",
    "The Database methods always keep the last query in memory. This means that one does not have to actually pass argument.\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4396357",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.query(var='IRH_NUM')\n",
    "# db.plot.var() # This is equivalent to the overview plot above. Note that without downscaling it is very heavy to plot.\n",
    "\n",
    "import numpy as np\n",
    "mean_density_trs = []\n",
    "min_density = float('inf')\n",
    "max_density = float('-inf')\n",
    "for df, md in db.data_generator():    # without explicitly passing results, it uses the last query.\n",
    "    density_values = df['IRH_NUM']\n",
    "    mean_density_trs.append(np.mean(density_values))\n",
    "    min_density = min(min_density, min(density_values))\n",
    "    max_density = max(max_density, max(density_values))\n",
    "\n",
    "mean_density = np.mean(mean_density_trs)\n",
    "print(f\"The average number of picked layers per data point in the AntADatabase is {int(mean_density)}, ranging from {int(min_density)} to {int(max_density)}. \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
