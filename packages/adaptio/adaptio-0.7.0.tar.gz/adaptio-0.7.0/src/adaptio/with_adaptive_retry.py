import asyncio
import inspect
import logging
from collections.abc import Callable
from functools import wraps
from typing import Any, ParamSpec, TypeVar

from adaptio.adaptive_async_concurrency_limiter import (
    AdaptiveAsyncConcurrencyLimiter,
    ServiceOverloadError,
)

P = ParamSpec("P")
R = TypeVar("R")
T = TypeVar("T")


def with_adaptive_retry(
    scheduler: AdaptiveAsyncConcurrencyLimiter | None = None,
    max_retries: int = 1024,
    retry_interval_seconds: float = 1,
    max_concurrency: int = 256,
    min_concurrency: int = 1,
    initial_concurrency: int = 1,
    adjust_overload_rate: float = 0.1,
    overload_exception: type[BaseException] = ServiceOverloadError,
    log_level: str = "INFO",
    log_prefix: str = "",
    ignore_loop_bound_exception: bool = True,
):
    """è£…é¥°å™¨ï¼šä¸ºå¼‚æ­¥å‡½æ•°æˆ–å¼‚æ­¥ç”Ÿæˆå™¨æ·»åŠ è‡ªé€‚åº”é‡è¯•æœºåˆ¶ã€‚

    è‡ªåŠ¨æ£€æµ‹è¢«è£…é¥°çš„å‡½æ•°ç±»å‹ï¼š
    - æ™®é€šå¼‚æ­¥å‡½æ•° (async def func() -> T): å¯¹å‡½æ•°è°ƒç”¨è¿›è¡Œå¹¶å‘æ§åˆ¶
    - å¼‚æ­¥ç”Ÿæˆå™¨ (async def func() -> AsyncGenerator[T, None]): å¯¹ç”Ÿæˆå™¨è¿­ä»£è¿›è¡Œå¹¶å‘æ§åˆ¶

    å½“å‡½æ•°è§¦å‘è¿‡è½½å¼‚å¸¸æ—¶ï¼Œä¼šè‡ªåŠ¨é‡è¯•å¹¶é€šè¿‡ AdaptiveConcurrencyLimiter åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°ã€‚

    Args:
        scheduler: AdaptiveConcurrencyLimiter å®ä¾‹ã€‚å¦‚æœä¸º Noneï¼Œåˆ™ä¸ºæ¯ä¸ªè£…é¥°çš„å‡½æ•°åˆ›å»ºç‹¬ç«‹çš„é™åˆ¶å™¨
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_interval_seconds: é‡è¯•é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        max_concurrency: å½“ scheduler ä¸º None æ—¶ä½¿ç”¨çš„æœ€å¤§å¹¶å‘æ•°
        min_concurrency: å½“ scheduler ä¸º None æ—¶ä½¿ç”¨çš„æœ€å°å¹¶å‘æ•°
        initial_concurrency: å½“ scheduler ä¸º None æ—¶ä½¿ç”¨çš„åˆå§‹å¹¶å‘æ•°
        adjust_overload_rate: å½“ scheduler ä¸º None æ—¶ä½¿ç”¨çš„è¿‡è½½è°ƒæ•´ç‡
            æ„æ€æ˜¯åœ¨æœ€è¿‘ä¸€è½®å¹¶å‘è°ƒç”¨ä¸­ï¼Œè‹¥è§¦å‘è¿‡è½½é”™è¯¯çš„è°ƒç”¨æ•°é‡è¶…è¿‡è¿™ä¸ªæ¯”ä¾‹ï¼Œæ‰ä¼šè¿›è¡Œé™ä½å¹¶å‘æ•°æ“ä½œ
        overload_exception: å½“ scheduler ä¸º None æ—¶æ£€æµ‹çš„è¿‡è½½å¼‚å¸¸ç±»å‹
        log_level: å½“ scheduler ä¸º None æ—¶ä½¿ç”¨çš„æ—¥å¿—çº§åˆ«
        log_prefix: å½“ scheduler ä¸º None æ—¶ä½¿ç”¨çš„æ—¥å¿—å‰ç¼€
        ignore_loop_bound_exception: æ˜¯å¦å¿½ç•¥äº‹ä»¶å¾ªç¯ç»‘å®šå¼‚å¸¸
            å¦‚æœåœ¨å¦ä¸€ä¸ª asyncio å¾ªç¯ä¸­ä½¿ç”¨ä¿¡å·é‡ï¼Œæ ‡å‡†ä¿¡å·é‡ä¼šå¼•å‘ RuntimeErrorã€‚
            è®¾ç½®ä¸º Trueï¼ˆé»˜è®¤ï¼‰æ—¶ï¼Œä¼šä¸ºæ¯ä¸ª event loop åˆ›å»ºç‹¬ç«‹çš„ä¿¡å·é‡å®ä¾‹ï¼ˆLoopLocalAdjustableSemaphoreï¼‰ï¼Œ
            è¿™æ ·æ¯ä¸ª loop éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„å¹¶å‘é™åˆ¶ï¼Œé¿å…è·¨ loop ä½¿ç”¨å¼‚å¸¸ã€‚
            è®¾ç½®ä¸º False æ—¶ï¼Œä½¿ç”¨æ ‡å‡†ä¿¡å·é‡ï¼Œè·¨ loop ä½¿ç”¨æ—¶ä¼šæŠ›å‡º RuntimeErrorã€‚
            é€šå¸¸æƒ…å†µä¸‹å¾ˆéš¾åœ¨å®é™…åº”ç”¨ä¸­è§¦å‘è¿™ä¸ªé”™è¯¯ï¼Œé™¤éåˆ»æ„å†™å‡ºåœ¨åŒæ­¥å‡½æ•°ä¸­ä½¿ç”¨å¤šçº¿ç¨‹è°ƒç”¨å¼‚æ­¥å‡½æ•°çš„ä»£ç ã€‚
            https://github.com/python/cpython/blob/v3.13.3/Lib/asyncio/mixins.py#L20

    Returns:
        è£…é¥°åçš„å‡½æ•°ï¼Œå…·æœ‰è‡ªé€‚åº”é‡è¯•èƒ½åŠ›

    Example:
        ```python
        # è£…é¥°æ™®é€šå¼‚æ­¥å‡½æ•°
        @with_adaptive_retry()
        async def fetch_data(url: str) -> dict:
            async with session.get(url) as resp:
                if resp.status == 429:
                    raise ServiceOverloadError("Rate limited")
                return await resp.json()

        # è£…é¥°å¼‚æ­¥ç”Ÿæˆå™¨ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
        @with_adaptive_retry()
        async def fetch_pages(base_url: str):
            for page in range(1, 100):
                data = await fetch_page(f"{base_url}?page={page}")
                for item in data:
                    yield item
        ```
    """
    # å¦‚æœæ²¡æœ‰ä¼ å…¥ schedulerï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„é™åˆ¶å™¨å®ä¾‹
    _scheduler = scheduler or AdaptiveAsyncConcurrencyLimiter(
        max_concurrency=max_concurrency,
        min_concurrency=min_concurrency,
        initial_concurrency=initial_concurrency,
        adjust_overload_rate=adjust_overload_rate,
        overload_exception=overload_exception,
        log_level=log_level,
        log_prefix=log_prefix,
        ignore_loop_bound_exception=ignore_loop_bound_exception,
    )

    def decorator(func: Callable[P, Any]) -> Callable[P, Any]:
        if not _scheduler.log_prefix:
            _scheduler.log_prefix = getattr(func, "__name__", "unnamed_function")

        retry_logger = logging.getLogger(f"retry_{id(func)}")

        # ğŸ” å…³é”®ï¼šæ£€æµ‹å‡½æ•°ç±»å‹
        is_async_gen = inspect.isasyncgenfunction(func)

        if is_async_gen:
            # ========== å¼‚æ­¥ç”Ÿæˆå™¨å¤„ç†é€»è¾‘ ==========
            @wraps(func)
            async def generator_wrapper(*args, **kwargs):
                retries = 0

                while True:
                    try:
                        async with _scheduler.workers_lock:
                            _scheduler.current_running_count += 1

                            try:
                                # åˆ›å»ºå¹¶è¿­ä»£ç”Ÿæˆå™¨
                                generator = func(*args, **kwargs)
                                item_count = 0

                                async for item in generator:
                                    yield item
                                    item_count += 1

                                # æˆåŠŸå®Œæˆ
                                _scheduler.current_succeed_count += 1
                                retry_logger.debug(
                                    f"{_scheduler.log_prefix} -- "
                                    f"ç”Ÿæˆå™¨æˆåŠŸå®Œæˆï¼Œäº§å‡º {item_count} ä¸ªé¡¹ç›®"
                                )
                                return  # æˆåŠŸé€€å‡º

                            except _scheduler.overload_exception as e:
                                _scheduler.current_overload_count += 1
                                retries += 1

                                if retries > max_retries:
                                    retry_logger.error(
                                        f"{_scheduler.log_prefix} -- "
                                        f"é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™({retries}æ¬¡)ï¼Œç”Ÿæˆå™¨ä»å¤„äºè¿‡è½½çŠ¶æ€"
                                    )
                                    raise

                                retry_logger.warning(
                                    f"{_scheduler.log_prefix} -- "
                                    f"ç”Ÿæˆå™¨è§¦å‘è¿‡è½½ (å°è¯• {retries}/{max_retries}): {e}"
                                )

                                # ç­‰å¾…åé‡è¯•æ•´ä¸ªç”Ÿæˆå™¨
                                await asyncio.sleep(retry_interval_seconds)
                                continue  # é‡æ–°å¼€å§‹

                            except Exception:
                                _scheduler.current_failed_count += 1
                                raise

                            finally:
                                _scheduler.current_finished_count += 1
                                _scheduler.current_running_count -= 1

                                # è°ƒæ•´å¹¶å‘åº¦
                                if _scheduler.workers_lock.get_value() < 0:
                                    _scheduler.reset_counters()

                                if (
                                    _scheduler.current_finished_count
                                    > _scheduler.workers_lock.initial_value
                                ):
                                    await _scheduler.adjust_concurrency()
                                    _scheduler.reset_counters()

                        # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œbreak é€€å‡ºé‡è¯•å¾ªç¯
                        break

                    except _scheduler.overload_exception:
                        if retries > max_retries:
                            raise
                        continue

            return generator_wrapper

        else:
            # ========== æ™®é€šå¼‚æ­¥å‡½æ•°å¤„ç†é€»è¾‘ï¼ˆä¿æŒåŸæœ‰å®ç°ï¼‰==========
            @wraps(func)
            async def function_wrapper(*args: Any, **kwargs: Any) -> Any:
                retries = 0

                while True:
                    try:
                        coro = func(*args, **kwargs)
                        task = _scheduler.submit(coro)  # type: ignore[arg-type]
                        return await task  # type: ignore
                    except _scheduler.overload_exception:
                        retries += 1
                        if retries > max_retries:
                            retry_logger.error(
                                f"{_scheduler.log_prefix} -- "
                                f"é‡è¯•æ¬¡æ•°å·²è¾¾ä¸Šé™({retries}æ¬¡)ï¼ŒæœåŠ¡ä»å¤„äºè¿‡è½½çŠ¶æ€"
                            )
                            raise
                        await asyncio.sleep(retry_interval_seconds)
                        continue

            return function_wrapper

    return decorator


if __name__ == "__main__":
    import random

    # è®¾è®¡ä¸€ä¸ªè¾¾åˆ° 32 å¹¶å‘å°±ä¼šè§¦å‘ ServiceOverloadError çš„æµ‹è¯•ä»»åŠ¡
    sample_task_overload_threshold = 32
    sample_task_running_count = 0

    async def sample_task(task_id):
        """A sample task that simulates workload and triggers overload at a certain concurrency."""
        global sample_task_running_count
        sample_task_running_count += 1
        # æ¨¡æ‹Ÿéšæœºä»»åŠ¡è€—æ—¶
        await asyncio.sleep(random.uniform(1, 3))
        # æ¨¡æ‹Ÿè¿‡è½½é”™è¯¯

        if sample_task_running_count > sample_task_overload_threshold:
            sample_task_running_count -= 1
            logging.error(
                f"===sample_task {sample_task_running_count} tasks > {sample_task_overload_threshold}==="
            )
            raise ServiceOverloadError(
                f"Service overloaded with {sample_task_running_count} tasks > {sample_task_overload_threshold}"
            )
        else:
            sample_task_running_count -= 1
        return f"Task {task_id} done"

    @with_adaptive_retry(
        initial_concurrency=4, log_level="DEBUG", ignore_loop_bound_exception=False
    )
    async def sample_task_with_retry(task_id: int) -> str:
        return await sample_task(task_id)  # type: ignore[arg-type]

    async def get_result():
        tasks = [sample_task_with_retry(i) for i in range(200)]
        for res in asyncio.as_completed(tasks):
            try:
                logging.info(f"SUCCESS: {await res}")
            except Exception as e:
                logging.error(f"error: {e}")
                pass

    asyncio.run(get_result())
