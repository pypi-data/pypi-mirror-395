

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chapter 5: Approximation Theory &mdash; pydelt 0.7.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5a057da9"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 6: Differential Equations from Data" href="06_differential_equations.html" />
    <link rel="prev" title="Chapter 4: Multivariate Derivatives" href="04_multivariate_derivatives.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            pydelt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Start Here:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematical Foundations:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../theory_index.html">Theory: Numerical Calculus for Real Systems</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#the-central-thesis">The Central Thesis</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../theory_index.html#chapters">Chapters</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="00_introduction.html">Introduction: The Approximation Paradigm</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_numerical_differentiation.html">Chapter 1: Numerical Differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="02_noise_and_smoothing.html">Chapter 2: Noise and Smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="03_interpolation_methods.html">Chapter 3: Interpolation Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="04_multivariate_derivatives.html">Chapter 4: Multivariate Derivatives</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Chapter 5: Approximation Theory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#connection-to-the-central-thesis">Connection to the Central Thesis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-approximation-problem">The Approximation Problem</a></li>
<li class="toctree-l4"><a class="reference internal" href="#function-spaces">Function Spaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="#polynomial-approximation">Polynomial Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#spline-approximation">Spline Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#local-polynomial-regression">Local Polynomial Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural-network-approximation">Neural Network Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#error-analysis-framework">Error Analysis Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convergence-diagnostics">Convergence Diagnostics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#practical-guidelines">Practical Guidelines</a></li>
<li class="toctree-l4"><a class="reference internal" href="#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="06_differential_equations.html">Chapter 6: Differential Equations from Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="07_stochastic_calculus.html">Chapter 7: Stochastic Calculus</a></li>
<li class="toctree-l3"><a class="reference internal" href="08_applications.html">Chapter 8: Applications Under Error</a></li>
<li class="toctree-l3"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#who-this-is-for">Who This Is For</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../theory.html">Mathematical Theory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Master the Methods:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic_interpolation.html">Basic Interpolation &amp; Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neural_networks.html">Neural Networks &amp; Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate_calculus.html">Multivariate Calculus &amp; Vector Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stochastic_computing.html">Stochastic Computing &amp; Probabilistic Derivatives</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference &amp; Help:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_examples.html">Visual Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature_comparison.html">Feature Comparison Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pydelt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../theory_index.html">Theory: Numerical Calculus for Real Systems</a></li>
      <li class="breadcrumb-item active">Chapter 5: Approximation Theory</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/theory/05_approximation_theory.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-5-approximation-theory">
<h1>Chapter 5: Approximation Theory<a class="headerlink" href="#chapter-5-approximation-theory" title="Link to this heading"></a></h1>
<blockquote>
<div><p><em>“Every approximation has an error. The question is: can we bound it, minimize it, and understand when it fails?”</em></p>
</div></blockquote>
<section id="connection-to-the-central-thesis">
<h2>Connection to the Central Thesis<a class="headerlink" href="#connection-to-the-central-thesis" title="Link to this heading"></a></h2>
<p>We’ve established that exact derivatives from data are impossible. We approximate instead. But how good is the approximation? When does it work? When does it fail?</p>
<p><strong>Approximation theory</strong> provides the mathematical framework to answer these questions. It tells us:</p>
<ul class="simple">
<li><p><strong>Convergence rates</strong>: How fast does error decrease with more data?</p></li>
<li><p><strong>Error bounds</strong>: What’s the worst-case error for a given method?</p></li>
<li><p><strong>Optimality</strong>: Is there a better method, or are we at the limit?</p></li>
</ul>
<p>This chapter gives you the tools to reason rigorously about approximation quality—essential for trusting your derivative estimates.</p>
</section>
<section id="the-approximation-problem">
<h2>The Approximation Problem<a class="headerlink" href="#the-approximation-problem" title="Link to this heading"></a></h2>
<section id="setup">
<h3>Setup<a class="headerlink" href="#setup" title="Link to this heading"></a></h3>
<p>We want to approximate an unknown function f from a class F (e.g., smooth functions) using a function f̂ from a class G (e.g., polynomials, splines).</p>
<p><strong>Key quantities</strong>:</p>
<ul class="simple">
<li><p><strong>Approximation error</strong>: ‖f - f̂‖ in some norm</p></li>
<li><p><strong>Best approximation</strong>: inf_{g ∈ G} ‖f - g‖</p></li>
<li><p><strong>Achievable rate</strong>: How does error scale with complexity of G?</p></li>
</ul>
</section>
<section id="for-derivatives">
<h3>For Derivatives<a class="headerlink" href="#for-derivatives" title="Link to this heading"></a></h3>
<p>If we approximate f with f̂, how well does f̂’ approximate f’?</p>
<p><strong>Key insight</strong>: Derivative approximation is harder than function approximation. If ‖f - f̂‖ = O(hᵏ), then typically ‖f’ - f̂’‖ = O(hᵏ⁻¹).</p>
<p>Differentiation “costs” one order of accuracy.</p>
</section>
</section>
<section id="function-spaces">
<h2>Function Spaces<a class="headerlink" href="#function-spaces" title="Link to this heading"></a></h2>
<section id="smoothness-classes">
<h3>Smoothness Classes<a class="headerlink" href="#smoothness-classes" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Class</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>C⁰</p></td>
<td><p>Continuous</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>C¹</p></td>
<td><p>Continuous first derivative</p></td>
<td><p>x</p></td>
</tr>
<tr class="row-even"><td><p>Cᵏ</p></td>
<td><p>k continuous derivatives</p></td>
<td><p>Polynomials</p></td>
</tr>
<tr class="row-odd"><td><p>C^∞</p></td>
<td><p>Infinitely differentiable</p></td>
<td><p>sin(x), exp(x)</p></td>
</tr>
<tr class="row-even"><td><p>Analytic</p></td>
<td><p>Equals its Taylor series</p></td>
<td><p>sin(x), exp(x)</p></td>
</tr>
</tbody>
</table>
<p>Smoother functions are easier to approximate.</p>
</section>
<section id="sobolev-spaces">
<h3>Sobolev Spaces<a class="headerlink" href="#sobolev-spaces" title="Link to this heading"></a></h3>
<p>Functions with derivatives in Lᵖ:</p>
<div class="math notranslate nohighlight">
\[W^{k,p} = \{f : D^\alpha f \in L^p \text{ for } |\alpha| \leq k\}\]</div>
<p>These spaces are natural for PDE theory and provide sharp approximation results.</p>
</section>
<section id="why-smoothness-matters">
<h3>Why Smoothness Matters<a class="headerlink" href="#why-smoothness-matters" title="Link to this heading"></a></h3>
<p><strong>Theorem (Jackson)</strong>: If f ∈ Cᵏ[a,b], then the best polynomial approximation of degree n satisfies:</p>
<div class="math notranslate nohighlight">
\[\inf_{\deg p \leq n} \|f - p\|_\infty \leq C \cdot \frac{\|f^{(k)}\|_\infty}{n^k}\]</div>
<p>Smoother functions (larger k) → faster convergence (higher power of n).</p>
</section>
</section>
<section id="polynomial-approximation">
<h2>Polynomial Approximation<a class="headerlink" href="#polynomial-approximation" title="Link to this heading"></a></h2>
<section id="weierstrass-theorem">
<h3>Weierstrass Theorem<a class="headerlink" href="#weierstrass-theorem" title="Link to this heading"></a></h3>
<p>Any continuous function on [a,b] can be uniformly approximated by polynomials.</p>
<p><strong>Implication</strong>: Polynomials are “universal approximators” for continuous functions.</p>
</section>
<section id="rate-of-convergence">
<h3>Rate of Convergence<a class="headerlink" href="#rate-of-convergence" title="Link to this heading"></a></h3>
<p>For f ∈ Cᵏ:</p>
<ul class="simple">
<li><p>Best polynomial of degree n: error O(n⁻ᵏ)</p></li>
<li><p>Interpolating polynomial at n+1 points: error O(n⁻ᵏ) if points chosen well</p></li>
</ul>
</section>
<section id="runge-s-phenomenon">
<h3>Runge’s Phenomenon<a class="headerlink" href="#runge-s-phenomenon" title="Link to this heading"></a></h3>
<p>Equally-spaced interpolation points can cause divergence:</p>
<div class="math notranslate nohighlight">
\[\max_{x \in [-1,1]} |f(x) - p_n(x)| \to \infty \text{ as } n \to \infty\]</div>
<p>for some smooth functions (e.g., 1/(1+25x²)).</p>
<p><strong>Solution</strong>: Use Chebyshev points (clustered near endpoints) or piecewise methods.</p>
</section>
</section>
<section id="spline-approximation">
<h2>Spline Approximation<a class="headerlink" href="#spline-approximation" title="Link to this heading"></a></h2>
<section id="why-splines-work">
<h3>Why Splines Work<a class="headerlink" href="#why-splines-work" title="Link to this heading"></a></h3>
<p>Instead of one high-degree polynomial, use many low-degree pieces.</p>
<p><strong>Theorem</strong>: For f ∈ Cᵏ and cubic splines with n knots:</p>
<div class="math notranslate nohighlight">
\[\|f - S_n\|_\infty = O(h^4), \quad \|f' - S_n'\|_\infty = O(h^3)\]</div>
<p>where h = 1/n is the knot spacing.</p>
</section>
<section id="optimal-rates">
<h3>Optimal Rates<a class="headerlink" href="#optimal-rates" title="Link to this heading"></a></h3>
<p>For smoothing splines minimizing ∫(f’’)² + λ⁻¹∑(yᵢ - f(xᵢ))²:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[\|f - \hat{f}\|^2] = O(n^{-4/5})\]</div>
<p>when f ∈ C² and noise is Gaussian. This is <strong>minimax optimal</strong>—no method can do better without additional assumptions.</p>
</section>
<section id="derivative-approximation">
<h3>Derivative Approximation<a class="headerlink" href="#derivative-approximation" title="Link to this heading"></a></h3>
<p>For cubic splines:</p>
<ul class="simple">
<li><p>Function: O(h⁴)</p></li>
<li><p>First derivative: O(h³)</p></li>
<li><p>Second derivative: O(h²)</p></li>
</ul>
<p>Each derivative costs one order.</p>
</section>
</section>
<section id="local-polynomial-regression">
<h2>Local Polynomial Regression<a class="headerlink" href="#local-polynomial-regression" title="Link to this heading"></a></h2>
<section id="bias-variance-for-local-methods">
<h3>Bias-Variance for Local Methods<a class="headerlink" href="#bias-variance-for-local-methods" title="Link to this heading"></a></h3>
<p>For kernel regression with bandwidth h:</p>
<div class="math notranslate nohighlight">
\[\text{Bias}^2 \approx h^{2(p+1)} \cdot |f^{(p+1)}|^2\]</div>
<div class="math notranslate nohighlight">
\[\text{Variance} \approx \frac{\sigma^2}{nh^d}\]</div>
<p>where p is polynomial order, d is dimension, σ² is noise variance.</p>
</section>
<section id="optimal-bandwidth">
<h3>Optimal Bandwidth<a class="headerlink" href="#optimal-bandwidth" title="Link to this heading"></a></h3>
<p>Minimize MSE = Bias² + Variance:</p>
<div class="math notranslate nohighlight">
\[h_{\text{opt}} \propto n^{-1/(2p+2+d)}\]</div>
<p>For local linear (p=1) in 1D (d=1):</p>
<div class="math notranslate nohighlight">
\[h_{\text{opt}} \propto n^{-1/5}, \quad \text{MSE} = O(n^{-4/5})\]</div>
<p>Same rate as smoothing splines—this is the optimal rate for C² functions.</p>
</section>
<section id="id1">
<h3>For Derivatives<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<p>Estimating f’ with local polynomials:</p>
<div class="math notranslate nohighlight">
\[\text{MSE}(f') = O(n^{-2(p-1)/(2p+3)})\]</div>
<p>For local quadratic (p=2): MSE(f’) = O(n^{-2/7}) ≈ O(n^{-0.29})</p>
<p>Slower than function estimation—derivatives are harder.</p>
</section>
</section>
<section id="neural-network-approximation">
<h2>Neural Network Approximation<a class="headerlink" href="#neural-network-approximation" title="Link to this heading"></a></h2>
<section id="universal-approximation">
<h3>Universal Approximation<a class="headerlink" href="#universal-approximation" title="Link to this heading"></a></h3>
<p><strong>Theorem (Cybenko, Hornik)</strong>: A feedforward network with one hidden layer can approximate any continuous function on a compact set to arbitrary accuracy.</p>
<p><strong>Caveat</strong>: Says nothing about:</p>
<ul class="simple">
<li><p>How many neurons needed</p></li>
<li><p>How to find the weights</p></li>
<li><p>Generalization to unseen data</p></li>
</ul>
</section>
<section id="approximation-rates">
<h3>Approximation Rates<a class="headerlink" href="#approximation-rates" title="Link to this heading"></a></h3>
<p>For ReLU networks with L layers and W total weights:</p>
<div class="math notranslate nohighlight">
\[\inf_{\text{networks}} \|f - f_{\text{NN}}\| = O(W^{-2/d})\]</div>
<p>for f ∈ C² in d dimensions. Deep networks can achieve better rates for certain function classes.</p>
</section>
<section id="derivatives-via-autodiff">
<h3>Derivatives via Autodiff<a class="headerlink" href="#derivatives-via-autodiff" title="Link to this heading"></a></h3>
<p>Neural network derivatives are exact (via automatic differentiation), but the network itself is an approximation. The derivative of an approximate function is an approximate derivative.</p>
</section>
</section>
<section id="error-analysis-framework">
<h2>Error Analysis Framework<a class="headerlink" href="#error-analysis-framework" title="Link to this heading"></a></h2>
<section id="total-error-decomposition">
<h3>Total Error Decomposition<a class="headerlink" href="#total-error-decomposition" title="Link to this heading"></a></h3>
<div class="math notranslate nohighlight">
\[\text{Total Error} = \text{Approximation Error} + \text{Estimation Error} + \text{Computational Error}\]</div>
<ul class="simple">
<li><p><strong>Approximation</strong>: Best possible in function class (bias)</p></li>
<li><p><strong>Estimation</strong>: From finite, noisy data (variance)</p></li>
<li><p><strong>Computational</strong>: From numerical precision (usually negligible)</p></li>
</ul>
</section>
<section id="for-derivative-estimation">
<h3>For Derivative Estimation<a class="headerlink" href="#for-derivative-estimation" title="Link to this heading"></a></h3>
<div class="math notranslate nohighlight">
\[\|\hat{f}' - f'\| \leq \|\hat{f}' - \hat{f}_{\text{true}}'\| + \|\hat{f}_{\text{true}}' - f'\|\]</div>
<p>where f̂_true is the best approximation to f in the function class.</p>
<p>First term: estimation error (from noise)
Second term: approximation error (from function class)</p>
</section>
</section>
<section id="convergence-diagnostics">
<h2>Convergence Diagnostics<a class="headerlink" href="#convergence-diagnostics" title="Link to this heading"></a></h2>
<section id="how-to-know-if-approximation-is-good">
<h3>How to Know If Approximation Is Good<a class="headerlink" href="#how-to-know-if-approximation-is-good" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Residual analysis</strong>: Do residuals look like noise?</p></li>
<li><p><strong>Cross-validation</strong>: Does error decrease with more data?</p></li>
<li><p><strong>Sensitivity analysis</strong>: Do results change with smoothing parameter?</p></li>
<li><p><strong>Physical plausibility</strong>: Do derivatives make sense?</p></li>
</ol>
</section>
<section id="warning-signs">
<h3>Warning Signs<a class="headerlink" href="#warning-signs" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Derivatives change sign rapidly (noise dominating)</p></li>
<li><p>Derivatives are very different at nearby points (instability)</p></li>
<li><p>Cross-validation error increases with model complexity (overfitting)</p></li>
<li><p>Results sensitive to smoothing parameter (ill-conditioned)</p></li>
</ul>
</section>
</section>
<section id="practical-guidelines">
<h2>Practical Guidelines<a class="headerlink" href="#practical-guidelines" title="Link to this heading"></a></h2>
<section id="choosing-approximation-method">
<h3>Choosing Approximation Method<a class="headerlink" href="#choosing-approximation-method" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Data Characteristics</p></th>
<th class="head"><p>Recommended Method</p></th>
<th class="head"><p>Expected Rate</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Smooth, low noise</p></td>
<td><p>Splines</p></td>
<td><p>O(n^{-4/5})</p></td>
</tr>
<tr class="row-odd"><td><p>Smooth, high noise</p></td>
<td><p>Smoothing splines</p></td>
<td><p>O(n^{-4/5})</p></td>
</tr>
<tr class="row-even"><td><p>Non-smooth, low noise</p></td>
<td><p>Local linear</p></td>
<td><p>O(n^{-2/3})</p></td>
</tr>
<tr class="row-odd"><td><p>Complex patterns</p></td>
<td><p>Neural networks</p></td>
<td><p>Depends on architecture</p></td>
</tr>
<tr class="row-even"><td><p>High dimensional</p></td>
<td><p>Neural networks</p></td>
<td><p>Avoids curse of dimensionality</p></td>
</tr>
</tbody>
</table>
</section>
<section id="rules-of-thumb">
<h3>Rules of Thumb<a class="headerlink" href="#rules-of-thumb" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>More data always helps</strong> (until computational limits)</p></li>
<li><p><strong>Smoother functions are easier</strong> (higher convergence rates)</p></li>
<li><p><strong>Derivatives are harder than values</strong> (lose one order)</p></li>
<li><p><strong>Higher dimensions are harder</strong> (curse of dimensionality)</p></li>
<li><p><strong>Noise hurts more for derivatives</strong> (amplification)</p></li>
</ol>
</section>
</section>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Approximation error is unavoidable</strong> but can be bounded</p></li>
<li><p><strong>Smoothness determines convergence rate</strong> (smoother = faster)</p></li>
<li><p><strong>Derivatives cost one order of accuracy</strong> compared to function values</p></li>
<li><p><strong>Optimal rates exist</strong> (n^{-4/5} for C² functions in 1D)</p></li>
<li><p><strong>Method choice matters</strong> but optimal methods achieve similar rates</p></li>
<li><p><strong>Diagnostics are essential</strong> to verify approximation quality</p></li>
</ol>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Jackson’s theorem</strong>: Verify numerically that polynomial approximation to sin(x) converges as O(n^{-k}) where k depends on which derivative you’re approximating.</p></li>
<li><p><strong>Spline convergence</strong>: Generate data from a known C² function. Fit splines with increasing numbers of knots. Verify O(h⁴) convergence for function, O(h³) for derivative.</p></li>
<li><p><strong>Optimal bandwidth</strong>: For local linear regression on sin(x) + noise, find the bandwidth that minimizes cross-validation error. Compare to theoretical n^{-1/5}.</p></li>
<li><p><strong>Curse of dimensionality</strong>: Repeat exercise 3 in 2D and 3D. How does optimal bandwidth and error scale with dimension?</p></li>
</ol>
<hr class="docutils" />
<p><em>Previous: <a class="reference internal" href="04_multivariate_derivatives.html"><span class="std std-doc">← Multivariate Derivatives</span></a> | Next: <a class="reference internal" href="06_differential_equations.html"><span class="std std-doc">Differential Equations →</span></a></em></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="04_multivariate_derivatives.html" class="btn btn-neutral float-left" title="Chapter 4: Multivariate Derivatives" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="06_differential_equations.html" class="btn btn-neutral float-right" title="Chapter 6: Differential Equations from Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Michael Harrison Lee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>