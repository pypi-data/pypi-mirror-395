

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Bibliography &mdash; pydelt 0.7.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5a057da9"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Mathematical Theory" href="../theory.html" />
    <link rel="prev" title="Chapter 8: Applications to Machine Learning" href="08_applications_to_ml.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            pydelt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Start Here:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematical Foundations:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../theory_index.html">Theory: Calculus for ML</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../theory_index.html#chapters">Chapters</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="00_why_calculus.html">Why Calculus? A Guide for ML Practitioners</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_functions_and_limits.html">Chapter 1: Functions and Limits</a></li>
<li class="toctree-l3"><a class="reference internal" href="02_derivatives_intuition.html">Chapter 2: Derivatives Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="03_differentiation_rules.html">Chapter 3: Differentiation Rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="04_integration_intuition.html">Chapter 4: Integration Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="05_approximation_theory.html">Chapter 5: Approximation Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="06_multivariate_calculus.html">Chapter 6: Multivariate Calculus</a></li>
<li class="toctree-l3"><a class="reference internal" href="07_complex_analysis.html">Chapter 7: Complex Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="08_applications_to_ml.html">Chapter 8: Applications to Machine Learning</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Bibliography</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#foundational-calculus-texts">Foundational Calculus Texts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-methods">Numerical Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#machine-learning-and-deep-learning">Machine Learning and Deep Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#functional-data-analysis">Functional Data Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#splines-and-interpolation">Splines and Interpolation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#local-regression-methods">Local Regression Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="#physics-informed-machine-learning">Physics-Informed Machine Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stochastic-calculus">Stochastic Calculus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#complex-analysis">Complex Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multivariate-calculus">Multivariate Calculus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#online-resources">Online Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="#how-to-use-this-bibliography">How to Use This Bibliography</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#learning-paths">Learning Paths</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#who-this-is-for">Who This Is For</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../theory.html">Mathematical Theory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Master the Methods:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic_interpolation.html">Basic Interpolation &amp; Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neural_networks.html">Neural Networks &amp; Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate_calculus.html">Multivariate Calculus &amp; Vector Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stochastic_computing.html">Stochastic Computing &amp; Probabilistic Derivatives</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference &amp; Help:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_examples.html">Visual Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature_comparison.html">Feature Comparison Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pydelt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../theory_index.html">Theory: Calculus for ML</a></li>
      <li class="breadcrumb-item active">Bibliography</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/theory/bibliography.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="bibliography">
<h1>Bibliography<a class="headerlink" href="#bibliography" title="Link to this heading"></a></h1>
<p>This bibliography provides references for deeper study of the mathematical concepts covered in PyDelt’s theory documentation.</p>
<hr class="docutils" />
<section id="foundational-calculus-texts">
<h2>Foundational Calculus Texts<a class="headerlink" href="#foundational-calculus-texts" title="Link to this heading"></a></h2>
<section id="for-intuition-and-accessibility">
<h3>For Intuition and Accessibility<a class="headerlink" href="#for-intuition-and-accessibility" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p><strong>Strang, G.</strong> (2010). <em>Calculus</em>. Wellesley-Cambridge Press.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Free online, excellent visualizations, focuses on understanding over formalism.</p></li>
<li><p><strong>Best for</strong>: Building intuition, seeing connections between concepts.</p></li>
<li><p><strong>Access</strong>: <a class="reference external" href="https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/">MIT OpenCourseWare</a></p></li>
</ul>
</li>
<li><p><strong>Thompson, S. P.</strong> (1914). <em>Calculus Made Easy</em>. Macmillan.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Classic text, remarkably accessible, still relevant after 100+ years.</p></li>
<li><p><strong>Best for</strong>: Absolute beginners, those intimidated by math.</p></li>
<li><p><strong>Access</strong>: Public domain, freely available online.</p></li>
</ul>
</li>
</ol>
</section>
<section id="for-rigor">
<h3>For Rigor<a class="headerlink" href="#for-rigor" title="Link to this heading"></a></h3>
<ol class="arabic simple" start="3">
<li><p><strong>Spivak, M.</strong> (2008). <em>Calculus</em> (4th ed.). Publish or Perish.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Rigorous but readable, beautiful proofs, develops mathematical maturity.</p></li>
<li><p><strong>Best for</strong>: Those wanting deep understanding, future mathematicians.</p></li>
</ul>
</li>
<li><p><strong>Apostol, T. M.</strong> (1967). <em>Calculus, Vol. 1 &amp; 2</em>. Wiley.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Comprehensive reference, integrates linear algebra with calculus.</p></li>
<li><p><strong>Best for</strong>: Complete coverage, reference work.</p></li>
</ul>
</li>
<li><p><strong>Rudin, W.</strong> (1976). <em>Principles of Mathematical Analysis</em> (3rd ed.). McGraw-Hill.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: The standard for rigorous real analysis.</p></li>
<li><p><strong>Best for</strong>: Graduate-level understanding, proving theorems.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="numerical-methods">
<h2>Numerical Methods<a class="headerlink" href="#numerical-methods" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="6">
<li><p><strong>Press, W. H., Teukolsky, S. A., Vetterling, W. T., &amp; Flannery, B. P.</strong> (2007). <em>Numerical Recipes: The Art of Scientific Computing</em> (3rd ed.). Cambridge University Press.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Practical algorithms with code, covers everything.</p></li>
<li><p><strong>Best for</strong>: Implementation, understanding trade-offs.</p></li>
</ul>
</li>
<li><p><strong>Trefethen, L. N.</strong> (2013). <em>Approximation Theory and Approximation Practice</em>. SIAM.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Modern treatment, connects theory to computation.</p></li>
<li><p><strong>Best for</strong>: Understanding interpolation, polynomial approximation.</p></li>
</ul>
</li>
<li><p><strong>Fornberg, B.</strong> (1988). “Generation of Finite Difference Formulas on Arbitrarily Spaced Grids.” <em>Mathematics of Computation</em>, 51(184), 699-706.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Foundational paper for finite difference methods.</p></li>
<li><p><strong>Best for</strong>: Understanding numerical differentiation.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="machine-learning-and-deep-learning">
<h2>Machine Learning and Deep Learning<a class="headerlink" href="#machine-learning-and-deep-learning" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="9">
<li><p><strong>Goodfellow, I., Bengio, Y., &amp; Courville, A.</strong> (2016). <em>Deep Learning</em>. MIT Press.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: The standard deep learning textbook, Chapter 4 covers numerical computation.</p></li>
<li><p><strong>Best for</strong>: Connecting calculus to neural networks.</p></li>
<li><p><strong>Access</strong>: <a class="reference external" href="https://www.deeplearningbook.org/">deeplearningbook.org</a></p></li>
</ul>
</li>
<li><p><strong>Boyd, S., &amp; Vandenberghe, L.</strong> (2004). <em>Convex Optimization</em>. Cambridge University Press.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Foundational for understanding optimization, gradients, Hessians.</p></li>
<li><p><strong>Best for</strong>: Optimization theory, understanding gradient descent.</p></li>
<li><p><strong>Access</strong>: <a class="reference external" href="https://web.stanford.edu/~boyd/cvxbook/">stanford.edu/~boyd/cvxbook</a></p></li>
</ul>
</li>
<li><p><strong>Baydin, A. G., Pearlmutter, B. A., Radul, A. A., &amp; Siskind, J. M.</strong> (2018). “Automatic Differentiation in Machine Learning: A Survey.” <em>Journal of Machine Learning Research</em>, 18(153), 1-43.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Comprehensive survey of autodiff, the foundation of modern deep learning.</p></li>
<li><p><strong>Best for</strong>: Understanding how PyTorch/TensorFlow compute gradients.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="functional-data-analysis">
<h2>Functional Data Analysis<a class="headerlink" href="#functional-data-analysis" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="12">
<li><p><strong>Ramsay, J. O., &amp; Silverman, B. W.</strong> (2005). <em>Functional Data Analysis</em> (2nd ed.). Springer.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: The definitive text on FDA, basis for PyDelt’s FDA methods.</p></li>
<li><p><strong>Best for</strong>: Understanding spline smoothing, functional representations.</p></li>
</ul>
</li>
<li><p><strong>Ramsay, J. O., Hooker, G., &amp; Graves, S.</strong> (2009). <em>Functional Data Analysis with R and MATLAB</em>. Springer.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Practical implementation of FDA concepts.</p></li>
<li><p><strong>Best for</strong>: Hands-on FDA work.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="splines-and-interpolation">
<h2>Splines and Interpolation<a class="headerlink" href="#splines-and-interpolation" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="14">
<li><p><strong>de Boor, C.</strong> (2001). <em>A Practical Guide to Splines</em> (Revised ed.). Springer.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: The authoritative reference on splines.</p></li>
<li><p><strong>Best for</strong>: Deep understanding of spline mathematics.</p></li>
</ul>
</li>
<li><p><strong>Wahba, G.</strong> (1990). <em>Spline Models for Observational Data</em>. SIAM.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Connects splines to statistics, optimal smoothing.</p></li>
<li><p><strong>Best for</strong>: Understanding smoothing parameter selection.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="local-regression-methods">
<h2>Local Regression Methods<a class="headerlink" href="#local-regression-methods" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="16">
<li><p><strong>Cleveland, W. S.</strong> (1979). “Robust Locally Weighted Regression and Smoothing Scatterplots.” <em>Journal of the American Statistical Association</em>, 74(368), 829-836.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Original LOWESS paper.</p></li>
<li><p><strong>Best for</strong>: Understanding the method PyDelt implements.</p></li>
</ul>
</li>
<li><p><strong>Fan, J., &amp; Gijbels, I.</strong> (1996). <em>Local Polynomial Modelling and Its Applications</em>. Chapman &amp; Hall.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Comprehensive treatment of local polynomial regression.</p></li>
<li><p><strong>Best for</strong>: Understanding LLA-type methods.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="physics-informed-machine-learning">
<h2>Physics-Informed Machine Learning<a class="headerlink" href="#physics-informed-machine-learning" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="18">
<li><p><strong>Raissi, M., Perdikaris, P., &amp; Karniadakis, G. E.</strong> (2019). “Physics-Informed Neural Networks: A Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations.” <em>Journal of Computational Physics</em>, 378, 686-707.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Foundational PINN paper.</p></li>
<li><p><strong>Best for</strong>: Understanding physics-informed approaches.</p></li>
</ul>
</li>
<li><p><strong>Chen, R. T. Q., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D.</strong> (2018). “Neural Ordinary Differential Equations.” <em>Advances in Neural Information Processing Systems</em>, 31.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Introduced Neural ODEs.</p></li>
<li><p><strong>Best for</strong>: Understanding continuous-depth networks.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="stochastic-calculus">
<h2>Stochastic Calculus<a class="headerlink" href="#stochastic-calculus" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="20">
<li><p><strong>Øksendal, B.</strong> (2003). <em>Stochastic Differential Equations: An Introduction with Applications</em> (6th ed.). Springer.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Accessible introduction to SDEs.</p></li>
<li><p><strong>Best for</strong>: Understanding Itô calculus, financial applications.</p></li>
</ul>
</li>
<li><p><strong>Kloeden, P. E., &amp; Platen, E.</strong> (1992). <em>Numerical Solution of Stochastic Differential Equations</em>. Springer.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Comprehensive treatment of numerical methods for SDEs.</p></li>
<li><p><strong>Best for</strong>: Implementation of stochastic methods.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="complex-analysis">
<h2>Complex Analysis<a class="headerlink" href="#complex-analysis" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="22">
<li><p><strong>Needham, T.</strong> (1997). <em>Visual Complex Analysis</em>. Oxford University Press.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Beautiful, geometric approach to complex analysis.</p></li>
<li><p><strong>Best for</strong>: Building intuition, seeing the geometry.</p></li>
</ul>
</li>
<li><p><strong>Ahlfors, L. V.</strong> (1979). <em>Complex Analysis</em> (3rd ed.). McGraw-Hill.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Standard graduate text.</p></li>
<li><p><strong>Best for</strong>: Rigorous treatment.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="multivariate-calculus">
<h2>Multivariate Calculus<a class="headerlink" href="#multivariate-calculus" title="Link to this heading"></a></h2>
<ol class="arabic simple" start="24">
<li><p><strong>Marsden, J. E., &amp; Tromba, A.</strong> (2011). <em>Vector Calculus</em> (6th ed.). W. H. Freeman.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Clear treatment of gradients, Jacobians, Hessians.</p></li>
<li><p><strong>Best for</strong>: Multivariate calculus foundations.</p></li>
</ul>
</li>
<li><p><strong>Hubbard, J. H., &amp; Hubbard, B. B.</strong> (2015). <em>Vector Calculus, Linear Algebra, and Differential Forms</em> (5th ed.). Matrix Editions.</p>
<ul class="simple">
<li><p><strong>Why read it</strong>: Unified treatment, connects to differential geometry.</p></li>
<li><p><strong>Best for</strong>: Deeper understanding of multivariate calculus.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="online-resources">
<h2>Online Resources<a class="headerlink" href="#online-resources" title="Link to this heading"></a></h2>
<section id="video-courses">
<h3>Video Courses<a class="headerlink" href="#video-courses" title="Link to this heading"></a></h3>
<ol class="arabic simple" start="26">
<li><p><strong>3Blue1Brown</strong> - “Essence of Calculus” (YouTube)</p>
<ul class="simple">
<li><p>Beautiful visualizations, builds intuition.</p></li>
<li><p><a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr">youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr</a></p></li>
</ul>
</li>
<li><p><strong>MIT OpenCourseWare</strong> - 18.01 Single Variable Calculus</p>
<ul class="simple">
<li><p>Full course with lectures, notes, problem sets.</p></li>
<li><p><a class="reference external" href="https://ocw.mit.edu/courses/mathematics/18-01sc-single-variable-calculus-fall-2010/">ocw.mit.edu</a></p></li>
</ul>
</li>
<li><p><strong>Stanford CS231n</strong> - Convolutional Neural Networks</p>
<ul class="simple">
<li><p>Excellent backpropagation explanation.</p></li>
<li><p><a class="reference external" href="https://cs231n.github.io/">cs231n.github.io</a></p></li>
</ul>
</li>
</ol>
</section>
<section id="interactive-tools">
<h3>Interactive Tools<a class="headerlink" href="#interactive-tools" title="Link to this heading"></a></h3>
<ol class="arabic simple" start="29">
<li><p><strong>Desmos</strong> - Graphing Calculator</p>
<ul class="simple">
<li><p>Visualize functions and derivatives.</p></li>
<li><p><a class="reference external" href="https://www.desmos.com/calculator">desmos.com/calculator</a></p></li>
</ul>
</li>
<li><p><strong>GeoGebra</strong> - Dynamic Mathematics</p>
<ul class="simple">
<li><p>Interactive calculus visualizations.</p></li>
<li><p><a class="reference external" href="https://www.geogebra.org/">geogebra.org</a></p></li>
</ul>
</li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="how-to-use-this-bibliography">
<h2>How to Use This Bibliography<a class="headerlink" href="#how-to-use-this-bibliography" title="Link to this heading"></a></h2>
<section id="if-you-re-new-to-calculus">
<h3>If you’re new to calculus:<a class="headerlink" href="#if-you-re-new-to-calculus" title="Link to this heading"></a></h3>
<p>Start with Thompson’s <em>Calculus Made Easy</em> or 3Blue1Brown videos, then move to Strang.</p>
</section>
<section id="if-you-want-rigorous-foundations">
<h3>If you want rigorous foundations:<a class="headerlink" href="#if-you-want-rigorous-foundations" title="Link to this heading"></a></h3>
<p>Work through Spivak, then Rudin for analysis.</p>
</section>
<section id="if-you-re-focused-on-ml-applications">
<h3>If you’re focused on ML applications:<a class="headerlink" href="#if-you-re-focused-on-ml-applications" title="Link to this heading"></a></h3>
<p>Read Goodfellow et al. Chapter 4, then Baydin et al. on autodiff.</p>
</section>
<section id="if-you-re-implementing-numerical-methods">
<h3>If you’re implementing numerical methods:<a class="headerlink" href="#if-you-re-implementing-numerical-methods" title="Link to this heading"></a></h3>
<p><em>Numerical Recipes</em> is your reference, supplemented by Trefethen.</p>
</section>
<section id="if-you-re-working-with-time-series">
<h3>If you’re working with time series:<a class="headerlink" href="#if-you-re-working-with-time-series" title="Link to this heading"></a></h3>
<p>Ramsay &amp; Silverman for FDA, Cleveland for LOWESS.</p>
</section>
<section id="if-you-re-doing-physics-informed-ml">
<h3>If you’re doing physics-informed ML:<a class="headerlink" href="#if-you-re-doing-physics-informed-ml" title="Link to this heading"></a></h3>
<p>Start with Raissi et al., then Chen et al. for Neural ODEs.</p>
<hr class="docutils" />
<p><em>Back to: <a class="reference internal" href="index.html"><span class="std std-doc">Theory Index</span></a></em></p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="08_applications_to_ml.html" class="btn btn-neutral float-left" title="Chapter 8: Applications to Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../theory.html" class="btn btn-neutral float-right" title="Mathematical Theory" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Michael Harrison Lee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>