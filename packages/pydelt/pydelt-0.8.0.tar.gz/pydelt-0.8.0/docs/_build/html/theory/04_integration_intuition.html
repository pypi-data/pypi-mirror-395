

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chapter 4: Integration Intuition &mdash; pydelt 0.7.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5a057da9"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 5: Approximation Theory" href="05_approximation_theory.html" />
    <link rel="prev" title="Chapter 3: Differentiation Rules" href="03_differentiation_rules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            pydelt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Start Here:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematical Foundations:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../theory_index.html">Theory: Calculus for ML</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../theory_index.html#chapters">Chapters</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="00_why_calculus.html">Why Calculus? A Guide for ML Practitioners</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_functions_and_limits.html">Chapter 1: Functions and Limits</a></li>
<li class="toctree-l3"><a class="reference internal" href="02_derivatives_intuition.html">Chapter 2: Derivatives Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="03_differentiation_rules.html">Chapter 3: Differentiation Rules</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Chapter 4: Integration Intuition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-two-faces-of-integration">The Two Faces of Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-as-accumulation">Integration as Accumulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-as-area">Integration as Area</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-fundamental-theorem-of-calculus">The Fundamental Theorem of Calculus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-integrals">Common Integrals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-integration">Numerical Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-in-machine-learning">Integration in Machine Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#monte-carlo-integration">Monte Carlo Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#integration-challenges">Integration Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="#connecting-derivatives-and-integrals-in-pydelt">Connecting Derivatives and Integrals in PyDelt</a></li>
<li class="toctree-l4"><a class="reference internal" href="#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="05_approximation_theory.html">Chapter 5: Approximation Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="06_multivariate_calculus.html">Chapter 6: Multivariate Calculus</a></li>
<li class="toctree-l3"><a class="reference internal" href="07_complex_analysis.html">Chapter 7: Complex Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="08_applications_to_ml.html">Chapter 8: Applications to Machine Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#learning-paths">Learning Paths</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#who-this-is-for">Who This Is For</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../theory.html">Mathematical Theory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Master the Methods:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic_interpolation.html">Basic Interpolation &amp; Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neural_networks.html">Neural Networks &amp; Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate_calculus.html">Multivariate Calculus &amp; Vector Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stochastic_computing.html">Stochastic Computing &amp; Probabilistic Derivatives</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference &amp; Help:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_examples.html">Visual Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature_comparison.html">Feature Comparison Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pydelt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../theory_index.html">Theory: Calculus for ML</a></li>
      <li class="breadcrumb-item active">Chapter 4: Integration Intuition</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/theory/04_integration_intuition.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-4-integration-intuition">
<h1>Chapter 4: Integration Intuition<a class="headerlink" href="#chapter-4-integration-intuition" title="Link to this heading"></a></h1>
<blockquote>
<div><p><em>“Integration is the inverse of differentiation. It accumulates change over time. It computes areas, volumes, and expectations.”</em></p>
</div></blockquote>
<section id="the-two-faces-of-integration">
<h2>The Two Faces of Integration<a class="headerlink" href="#the-two-faces-of-integration" title="Link to this heading"></a></h2>
<p>Integration has two complementary interpretations:</p>
<ol class="arabic simple">
<li><p><strong>Geometric</strong>: The area under a curve</p></li>
<li><p><strong>Analytical</strong>: The reverse of differentiation (antiderivative)</p></li>
</ol>
<p>Both are connected by the <strong>Fundamental Theorem of Calculus</strong>—one of the most important results in mathematics.</p>
</section>
<section id="integration-as-accumulation">
<h2>Integration as Accumulation<a class="headerlink" href="#integration-as-accumulation" title="Link to this heading"></a></h2>
<section id="the-intuition">
<h3>The Intuition<a class="headerlink" href="#the-intuition" title="Link to this heading"></a></h3>
<p>If the derivative tells you the <em>rate</em> of change, the integral tells you the <em>total</em> change.</p>
<p><strong>Example: Velocity and Distance</strong></p>
<ul class="simple">
<li><p>If v(t) is your velocity at time t</p></li>
<li><p>Then ∫v(t)dt is the total distance traveled</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Velocity (rate) ──derivative──&gt; Acceleration
                &lt;──integral───
                
Position ──derivative──&gt; Velocity ──derivative──&gt; Acceleration
         &lt;──integral───          &lt;──integral───
</pre></div>
</div>
</section>
<section id="in-code">
<h3>In Code<a class="headerlink" href="#in-code" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydelt.integrals</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate_derivative</span>

<span class="c1"># Velocity data (e.g., from a sensor)</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">velocity</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">time</span>  <span class="c1"># Constant acceleration: v = 2t</span>

<span class="c1"># Integrate to get position</span>
<span class="c1"># If v = 2t, then position = t² (plus initial position)</span>
<span class="n">position</span> <span class="o">=</span> <span class="n">integrate_derivative</span><span class="p">(</span><span class="n">velocity</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Check: position should be approximately t²</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Position at t=5: </span><span class="si">{</span><span class="n">position</span><span class="p">[</span><span class="mi">50</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be ~25</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exact: </span><span class="si">{</span><span class="mi">5</span><span class="o">**</span><span class="mi">2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># 25</span>
</pre></div>
</div>
</section>
</section>
<section id="integration-as-area">
<h2>Integration as Area<a class="headerlink" href="#integration-as-area" title="Link to this heading"></a></h2>
<section id="the-geometric-view">
<h3>The Geometric View<a class="headerlink" href="#the-geometric-view" title="Link to this heading"></a></h3>
<p>The <strong>definite integral</strong> ∫ₐᵇ f(x)dx represents the signed area between the curve f(x) and the x-axis, from x=a to x=b.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    f(x)
    │    ╱╲
    │   ╱  ╲    ← Area above x-axis (positive)
    │  ╱    ╲
────┼─╱──────╲────── x
    │         ╲
    │          ╲  ← Area below x-axis (negative)
    a          b
</pre></div>
</div>
<ul class="simple">
<li><p>Area above the x-axis counts as <strong>positive</strong></p></li>
<li><p>Area below the x-axis counts as <strong>negative</strong></p></li>
</ul>
</section>
<section id="why-this-matters-for-ml">
<h3>Why This Matters for ML<a class="headerlink" href="#why-this-matters-for-ml" title="Link to this heading"></a></h3>
<p><strong>Probability distributions</strong> are defined by integrals:</p>
<div class="math notranslate nohighlight">
\[P(a \leq X \leq b) = \int_a^b p(x) dx\]</div>
<p>The total probability must equal 1:</p>
<div class="math notranslate nohighlight">
\[\int_{-\infty}^{\infty} p(x) dx = 1\]</div>
</section>
</section>
<section id="the-fundamental-theorem-of-calculus">
<h2>The Fundamental Theorem of Calculus<a class="headerlink" href="#the-fundamental-theorem-of-calculus" title="Link to this heading"></a></h2>
<p>This theorem connects derivatives and integrals:</p>
<section id="part-1-differentiation-undoes-integration">
<h3>Part 1: Differentiation Undoes Integration<a class="headerlink" href="#part-1-differentiation-undoes-integration" title="Link to this heading"></a></h3>
<p>If F(x) = ∫ₐˣ f(t)dt, then F’(x) = f(x).</p>
<p><em>Translation</em>: If you integrate a function and then differentiate the result, you get back the original function.</p>
</section>
<section id="part-2-integration-undoes-differentiation">
<h3>Part 2: Integration Undoes Differentiation<a class="headerlink" href="#part-2-integration-undoes-differentiation" title="Link to this heading"></a></h3>
<p>If F’(x) = f(x), then ∫ₐᵇ f(x)dx = F(b) - F(a).</p>
<p><em>Translation</em>: To compute a definite integral, find an antiderivative and evaluate at the endpoints.</p>
</section>
<section id="example">
<h3>Example<a class="headerlink" href="#example" title="Link to this heading"></a></h3>
<div class="math notranslate nohighlight">
\[\int_0^2 x^2 dx = \left[\frac{x^3}{3}\right]_0^2 = \frac{8}{3} - 0 = \frac{8}{3}\]</div>
</section>
</section>
<section id="common-integrals">
<h2>Common Integrals<a class="headerlink" href="#common-integrals" title="Link to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Integral (Antiderivative)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>xⁿ</p></td>
<td><p>xⁿ⁺¹/(n+1) + C (n ≠ -1)</p></td>
</tr>
<tr class="row-odd"><td><p>1/x</p></td>
<td><p>ln|x| + C</p></td>
</tr>
<tr class="row-even"><td><p>eˣ</p></td>
<td><p>eˣ + C</p></td>
</tr>
<tr class="row-odd"><td><p>sin(x)</p></td>
<td><p>-cos(x) + C</p></td>
</tr>
<tr class="row-even"><td><p>cos(x)</p></td>
<td><p>sin(x) + C</p></td>
</tr>
<tr class="row-odd"><td><p>1/(1+x²)</p></td>
<td><p>arctan(x) + C</p></td>
</tr>
</tbody>
</table>
<p>The “+ C” is the <strong>constant of integration</strong>—since the derivative of a constant is zero, any constant could have been there.</p>
</section>
<section id="numerical-integration">
<h2>Numerical Integration<a class="headerlink" href="#numerical-integration" title="Link to this heading"></a></h2>
<p>When you don’t have a formula, you approximate the integral numerically.</p>
<section id="the-trapezoidal-rule">
<h3>The Trapezoidal Rule<a class="headerlink" href="#the-trapezoidal-rule" title="Link to this heading"></a></h3>
<p>Approximate the area using trapezoids:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x)dx \approx \sum_{i=0}^{n-1} \frac{f(x_i) + f(x_{i+1})}{2} \cdot \Delta x\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trapezoidal_integrate</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Integrate y with respect to x using trapezoidal rule.&quot;&quot;&quot;</span>
    <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dx</span><span class="p">)</span>

<span class="c1"># Example</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">area</span> <span class="o">=</span> <span class="n">trapezoidal_integrate</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;∫sin(x)dx from 0 to π = </span><span class="si">{</span><span class="n">area</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Should be 2.0</span>
</pre></div>
</div>
</section>
<section id="simpson-s-rule">
<h3>Simpson’s Rule<a class="headerlink" href="#simpson-s-rule" title="Link to this heading"></a></h3>
<p>Uses parabolas instead of lines—more accurate:</p>
<div class="math notranslate nohighlight">
\[\int_a^b f(x)dx \approx \frac{\Delta x}{3}\left[f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + ... + f(x_n)\right]\]</div>
</section>
<section id="pydelt-s-integration">
<h3>PyDelt’s Integration<a class="headerlink" href="#pydelt-s-integration" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pydelt.integrals</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate_derivative</span><span class="p">,</span> <span class="n">integrate_derivative_with_error</span>

<span class="c1"># With error estimation</span>
<span class="n">result</span><span class="p">,</span> <span class="n">error</span> <span class="o">=</span> <span class="n">integrate_derivative_with_error</span><span class="p">(</span>
    <span class="n">derivative_signal</span><span class="o">=</span><span class="n">velocity</span><span class="p">,</span>
    <span class="n">time</span><span class="o">=</span><span class="n">time</span><span class="p">,</span>
    <span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Integrated value: </span><span class="si">{</span><span class="n">result</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> ± </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="integration-in-machine-learning">
<h2>Integration in Machine Learning<a class="headerlink" href="#integration-in-machine-learning" title="Link to this heading"></a></h2>
<section id="probability-and-expectations">
<h3>1. Probability and Expectations<a class="headerlink" href="#probability-and-expectations" title="Link to this heading"></a></h3>
<p>The <strong>expected value</strong> of a random variable:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot p(x) dx\]</div>
<p>The <strong>variance</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{Var}(X) = \int_{-\infty}^{\infty} (x - \mu)^2 \cdot p(x) dx\]</div>
</section>
<section id="loss-functions-as-integrals">
<h3>2. Loss Functions as Integrals<a class="headerlink" href="#loss-functions-as-integrals" title="Link to this heading"></a></h3>
<p>Many loss functions are integrals in disguise:</p>
<p><strong>Cross-entropy</strong> (discrete version of KL divergence):
$<span class="math notranslate nohighlight">\(H(p, q) = -\sum_x p(x) \log q(x) \approx -\int p(x) \log q(x) dx\)</span>$</p>
</section>
<section id="cumulative-distribution-functions">
<h3>3. Cumulative Distribution Functions<a class="headerlink" href="#cumulative-distribution-functions" title="Link to this heading"></a></h3>
<p>The CDF is the integral of the PDF:</p>
<div class="math notranslate nohighlight">
\[F(x) = P(X \leq x) = \int_{-\infty}^{x} p(t) dt\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example: Standard normal CDF</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">pdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Probability density</span>
<span class="n">cdf</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Cumulative (integral of pdf)</span>

<span class="c1"># Verify: numerical integration of PDF ≈ CDF</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.integrate</span><span class="w"> </span><span class="kn">import</span> <span class="n">cumulative_trapezoid</span>
<span class="n">numerical_cdf</span> <span class="o">=</span> <span class="n">cumulative_trapezoid</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">initial</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">numerical_cdf</span> <span class="o">=</span> <span class="n">numerical_cdf</span> <span class="o">/</span> <span class="n">numerical_cdf</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Normalize</span>
</pre></div>
</div>
</section>
<section id="neural-odes">
<h3>4. Neural ODEs<a class="headerlink" href="#neural-odes" title="Link to this heading"></a></h3>
<p>Neural ODEs define the network as an integral:</p>
<div class="math notranslate nohighlight">
\[h(T) = h(0) + \int_0^T f(h(t), t, \theta) dt\]</div>
<p>The hidden state evolves continuously, and the integral is computed numerically.</p>
</section>
<section id="normalizing-flows">
<h3>5. Normalizing Flows<a class="headerlink" href="#normalizing-flows" title="Link to this heading"></a></h3>
<p>Change of variables requires integrating the Jacobian:</p>
<div class="math notranslate nohighlight">
\[p_Y(y) = p_X(f^{-1}(y)) \cdot \left|\det\left(\frac{\partial f^{-1}}{\partial y}\right)\right|\]</div>
</section>
</section>
<section id="monte-carlo-integration">
<h2>Monte Carlo Integration<a class="headerlink" href="#monte-carlo-integration" title="Link to this heading"></a></h2>
<p>For high-dimensional integrals, random sampling often works better than grid-based methods:</p>
<div class="math notranslate nohighlight">
\[\int f(x) dx \approx \frac{1}{N} \sum_{i=1}^{N} f(x_i)\]</div>
<p>where x_i are random samples.</p>
<section id="why-this-works">
<h3>Why This Works<a class="headerlink" href="#why-this-works" title="Link to this heading"></a></h3>
<p>By the law of large numbers, the sample mean converges to the expected value:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \sum_{i=1}^{N} f(x_i) \xrightarrow{N \to \infty} \mathbb{E}[f(X)]\]</div>
</section>
<section id="ml-connection">
<h3>ML Connection<a class="headerlink" href="#ml-connection" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Stochastic gradient descent</strong> is Monte Carlo estimation of the full gradient</p></li>
<li><p><strong>Variational inference</strong> uses Monte Carlo to estimate intractable integrals</p></li>
<li><p><strong>Reinforcement learning</strong> uses Monte Carlo returns</p></li>
</ul>
</section>
</section>
<section id="integration-challenges">
<h2>Integration Challenges<a class="headerlink" href="#integration-challenges" title="Link to this heading"></a></h2>
<section id="no-closed-form">
<h3>1. No Closed Form<a class="headerlink" href="#no-closed-form" title="Link to this heading"></a></h3>
<p>Many integrals have no analytical solution:</p>
<div class="math notranslate nohighlight">
\[\int e^{-x^2} dx = \frac{\sqrt{\pi}}{2} \text{erf}(x) + C\]</div>
<p>The error function erf(x) is defined by this integral—it has no simpler form.</p>
</section>
<section id="improper-integrals">
<h3>2. Improper Integrals<a class="headerlink" href="#improper-integrals" title="Link to this heading"></a></h3>
<p>Integrals over infinite domains or with singularities:</p>
<div class="math notranslate nohighlight">
\[\int_0^{\infty} e^{-x} dx = 1\]</div>
<p>These require careful handling (limits, convergence tests).</p>
</section>
<section id="high-dimensions">
<h3>3. High Dimensions<a class="headerlink" href="#high-dimensions" title="Link to this heading"></a></h3>
<p>The “curse of dimensionality” makes grid-based integration impractical:</p>
<ul class="simple">
<li><p>10 points per dimension</p></li>
<li><p>10 dimensions</p></li>
<li><p>= 10¹⁰ = 10 billion points!</p></li>
</ul>
<p>Monte Carlo methods scale much better.</p>
</section>
</section>
<section id="connecting-derivatives-and-integrals-in-pydelt">
<h2>Connecting Derivatives and Integrals in PyDelt<a class="headerlink" href="#connecting-derivatives-and-integrals-in-pydelt" title="Link to this heading"></a></h2>
<p>PyDelt lets you go both directions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydelt.interpolation</span><span class="w"> </span><span class="kn">import</span> <span class="n">SplineInterpolator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydelt.integrals</span><span class="w"> </span><span class="kn">import</span> <span class="n">integrate_derivative</span>

<span class="c1"># Original function</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Differentiate</span>
<span class="n">interp</span> <span class="o">=</span> <span class="n">SplineInterpolator</span><span class="p">(</span><span class="n">smoothing</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">interp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">derivative</span> <span class="o">=</span> <span class="n">interp</span><span class="o">.</span><span class="n">differentiate</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Should be cos(x)</span>

<span class="c1"># Integrate the derivative back</span>
<span class="n">reconstructed</span> <span class="o">=</span> <span class="n">integrate_derivative</span><span class="p">(</span><span class="n">derivative</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">initial_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Should recover sin(x) (up to numerical error)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Max reconstruction error: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">reconstructed</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="p">))</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Integration accumulates change</strong>—the inverse of differentiation</p></li>
<li><p><strong>Geometrically, it’s the area</strong> under a curve</p></li>
<li><p><strong>The Fundamental Theorem</strong> connects derivatives and integrals</p></li>
<li><p><strong>Numerical methods</strong> (trapezoidal, Simpson’s, Monte Carlo) handle real data</p></li>
<li><p><strong>ML uses integrals everywhere</strong>: probability, expectations, loss functions, Neural ODEs</p></li>
</ol>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Compute by hand</strong>: ∫₀¹ x² dx (use the power rule for antiderivatives)</p></li>
<li><p><strong>Verify numerically</strong>: Use PyDelt’s <code class="docutils literal notranslate"><span class="pre">integrate_derivative</span></code> to verify your answer.</p></li>
<li><p><strong>Probability</strong>: If X ~ Uniform(0, 1), compute E[X²] = ∫₀¹ x² dx. What is it?</p></li>
<li><p><strong>Round trip</strong>: Generate data from f(x) = x³, differentiate it, then integrate the derivative. How close do you get to the original?</p></li>
</ol>
<hr class="docutils" />
<p><em>Previous: <a class="reference internal" href="03_differentiation_rules.html"><span class="std std-doc">← Differentiation Rules</span></a> | Next: <a class="reference internal" href="05_approximation_theory.html"><span class="std std-doc">Approximation Theory →</span></a></em></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="03_differentiation_rules.html" class="btn btn-neutral float-left" title="Chapter 3: Differentiation Rules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="05_approximation_theory.html" class="btn btn-neutral float-right" title="Chapter 5: Approximation Theory" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Michael Harrison Lee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>