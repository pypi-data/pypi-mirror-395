

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Chapter 2: Derivatives Intuition &mdash; pydelt 0.7.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5a057da9"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 3: Differentiation Rules" href="03_differentiation_rules.html" />
    <link rel="prev" title="Chapter 1: Functions and Limits" href="01_functions_and_limits.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            pydelt
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Start Here:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quick Start Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Mathematical Foundations:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../theory_index.html">Theory: Calculus for ML</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../theory_index.html#chapters">Chapters</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="00_why_calculus.html">Why Calculus? A Guide for ML Practitioners</a></li>
<li class="toctree-l3"><a class="reference internal" href="01_functions_and_limits.html">Chapter 1: Functions and Limits</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Chapter 2: Derivatives Intuition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#three-ways-to-think-about-derivatives">Three Ways to Think About Derivatives</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-derivative-as-a-function">The Derivative as a Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="#computing-derivatives-with-pydelt">Computing Derivatives with PyDelt</a></li>
<li class="toctree-l4"><a class="reference internal" href="#higher-order-derivatives">Higher-Order Derivatives</a></li>
<li class="toctree-l4"><a class="reference internal" href="#notation-a-quick-guide">Notation: A Quick Guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="#derivatives-of-common-functions">Derivatives of Common Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-derivative-doesn-t-always-exist">The Derivative Doesnâ€™t Always Exist</a></li>
<li class="toctree-l4"><a class="reference internal" href="#numerical-vs-analytical-derivatives">Numerical vs. Analytical Derivatives</a></li>
<li class="toctree-l4"><a class="reference internal" href="#visualizing-derivatives">Visualizing Derivatives</a></li>
<li class="toctree-l4"><a class="reference internal" href="#key-takeaways">Key Takeaways</a></li>
<li class="toctree-l4"><a class="reference internal" href="#exercises">Exercises</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="03_differentiation_rules.html">Chapter 3: Differentiation Rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="04_integration_intuition.html">Chapter 4: Integration Intuition</a></li>
<li class="toctree-l3"><a class="reference internal" href="05_approximation_theory.html">Chapter 5: Approximation Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="06_multivariate_calculus.html">Chapter 6: Multivariate Calculus</a></li>
<li class="toctree-l3"><a class="reference internal" href="07_complex_analysis.html">Chapter 7: Complex Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="08_applications_to_ml.html">Chapter 8: Applications to Machine Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#learning-paths">Learning Paths</a></li>
<li class="toctree-l2"><a class="reference internal" href="../theory_index.html#who-this-is-for">Who This Is For</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../theory.html">Mathematical Theory</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Master the Methods:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic_interpolation.html">Basic Interpolation &amp; Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="../neural_networks.html">Neural Networks &amp; Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multivariate_calculus.html">Multivariate Calculus &amp; Vector Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../stochastic_computing.html">Stochastic Computing &amp; Probabilistic Derivatives</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference &amp; Help:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visual_examples.html">Visual Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../feature_comparison.html">Feature Comparison Matrix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Changelog</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">pydelt</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../theory_index.html">Theory: Calculus for ML</a></li>
      <li class="breadcrumb-item active">Chapter 2: Derivatives Intuition</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/theory/02_derivatives_intuition.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-2-derivatives-intuition">
<h1>Chapter 2: Derivatives Intuition<a class="headerlink" href="#chapter-2-derivatives-intuition" title="Link to this heading">ïƒ</a></h1>
<blockquote>
<div><p><em>â€œThe derivative measures instantaneous rate of change. Itâ€™s the slope of the tangent line. It tells you: if I nudge the input, how much does the output change?â€</em></p>
</div></blockquote>
<section id="three-ways-to-think-about-derivatives">
<h2>Three Ways to Think About Derivatives<a class="headerlink" href="#three-ways-to-think-about-derivatives" title="Link to this heading">ïƒ</a></h2>
<p>The derivative is one of the most important concepts in all of mathematicsâ€”and it has multiple interpretations that are all equally valid.</p>
<section id="interpretation-1-instantaneous-rate-of-change">
<h3>Interpretation 1: Instantaneous Rate of Change<a class="headerlink" href="#interpretation-1-instantaneous-rate-of-change" title="Link to this heading">ïƒ</a></h3>
<p>Imagine youâ€™re driving a car. Your speedometer shows your <strong>instantaneous speed</strong>â€”how fast youâ€™re going <em>right now</em>, not your average speed over the trip.</p>
<p>If your position is p(t) at time t, then:</p>
<ul class="simple">
<li><p><strong>Average speed</strong> over interval [tâ‚, tâ‚‚]: (p(tâ‚‚) - p(tâ‚)) / (tâ‚‚ - tâ‚)</p></li>
<li><p><strong>Instantaneous speed</strong> at time t: pâ€™(t) = lim[hâ†’0] (p(t+h) - p(t)) / h</p></li>
</ul>
<p>The derivative gives you the instantaneous rate of change.</p>
</section>
<section id="interpretation-2-slope-of-the-tangent-line">
<h3>Interpretation 2: Slope of the Tangent Line<a class="headerlink" href="#interpretation-2-slope-of-the-tangent-line" title="Link to this heading">ïƒ</a></h3>
<p>Geometrically, the derivative at a point is the <strong>slope of the line that just touches the curve</strong> at that point.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>        /
       /  â† tangent line (slope = derivative)
      â€¢
     /|
    / |
   /  |
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
</pre></div>
</div>
<ul class="simple">
<li><p>If the tangent line goes up (positive slope), the function is increasing</p></li>
<li><p>If the tangent line goes down (negative slope), the function is decreasing</p></li>
<li><p>If the tangent line is flat (zero slope), youâ€™re at a local maximum or minimum</p></li>
</ul>
</section>
<section id="interpretation-3-sensitivity-the-ml-interpretation">
<h3>Interpretation 3: Sensitivity (The ML Interpretation)<a class="headerlink" href="#interpretation-3-sensitivity-the-ml-interpretation" title="Link to this heading">ïƒ</a></h3>
<p>This is the most useful interpretation for machine learning:</p>
<blockquote>
<div><p><strong>The derivative tells you how sensitive the output is to changes in the input.</strong></p>
</div></blockquote>
<p>If fâ€™(x) = 3, then a small change Î”x in the input produces approximately 3Î”x change in the output.</p>
<p>This is exactly what gradients tell you during training:</p>
<ul class="simple">
<li><p>Large gradient â†’ weight has big impact on loss</p></li>
<li><p>Small gradient â†’ weight has little impact on loss</p></li>
<li><p>Zero gradient â†’ changing this weight doesnâ€™t affect loss (at this point)</p></li>
</ul>
</section>
</section>
<section id="the-derivative-as-a-function">
<h2>The Derivative as a Function<a class="headerlink" href="#the-derivative-as-a-function" title="Link to this heading">ïƒ</a></h2>
<p>The derivative of f(x) is itself a function, fâ€™(x), that tells you the slope at every point.</p>
<section id="example-f-x-x2">
<h3>Example: f(x) = xÂ²<a class="headerlink" href="#example-f-x-x2" title="Link to this heading">ïƒ</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>x</p></th>
<th class="head"><p>f(x) = xÂ²</p></th>
<th class="head"><p>fâ€™(x) = 2x</p></th>
<th class="head"><p>Interpretation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-2</p></td>
<td><p>4</p></td>
<td><p>-4</p></td>
<td><p>Steeply decreasing</p></td>
</tr>
<tr class="row-odd"><td><p>-1</p></td>
<td><p>1</p></td>
<td><p>-2</p></td>
<td><p>Decreasing</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>Flat (minimum!)</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>Increasing</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>4</p></td>
<td><p>4</p></td>
<td><p>Steeply increasing</p></td>
</tr>
</tbody>
</table>
<p>Notice: the derivative is zero exactly where the function has its minimum. This is the foundation of optimization!</p>
</section>
</section>
<section id="computing-derivatives-with-pydelt">
<h2>Computing Derivatives with PyDelt<a class="headerlink" href="#computing-derivatives-with-pydelt" title="Link to this heading">ïƒ</a></h2>
<p>When you have data instead of formulas, PyDelt computes derivatives numerically:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydelt.interpolation</span><span class="w"> </span><span class="kn">import</span> <span class="n">SplineInterpolator</span>

<span class="c1"># Generate data from f(x) = xÂ²</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="c1"># Fit and differentiate</span>
<span class="n">interpolator</span> <span class="o">=</span> <span class="n">SplineInterpolator</span><span class="p">(</span><span class="n">smoothing</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">interpolator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">derivative_func</span> <span class="o">=</span> <span class="n">interpolator</span><span class="o">.</span><span class="n">differentiate</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Evaluate derivative</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">derivatives</span> <span class="o">=</span> <span class="n">derivative_func</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Computed: </span><span class="si">{</span><span class="n">derivatives</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exact:    </span><span class="si">{</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x_test</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># Output: Computed: [-4. -2.  0.  2.  4.]</span>
<span class="c1">#         Exact:    [-4 -2  0  2  4]</span>
</pre></div>
</div>
</section>
<section id="higher-order-derivatives">
<h2>Higher-Order Derivatives<a class="headerlink" href="#higher-order-derivatives" title="Link to this heading">ïƒ</a></h2>
<p>You can differentiate a derivative to get the <strong>second derivative</strong>, and so on:</p>
<ul class="simple">
<li><p><strong>f(x)</strong>: Position</p></li>
<li><p><strong>fâ€™(x)</strong>: Velocity (first derivative)</p></li>
<li><p><strong>fâ€™â€™(x)</strong>: Acceleration (second derivative)</p></li>
<li><p><strong>fâ€™â€™â€™(x)</strong>: Jerk (third derivative)</p></li>
</ul>
<section id="what-second-derivatives-tell-you">
<h3>What Second Derivatives Tell You<a class="headerlink" href="#what-second-derivatives-tell-you" title="Link to this heading">ïƒ</a></h3>
<p>The second derivative measures <strong>curvature</strong>â€”how the slope itself is changing:</p>
<ul class="simple">
<li><p><strong>fâ€™â€™(x) &gt; 0</strong>: Curve is concave up (like a smile ğŸ˜Š), slope is increasing</p></li>
<li><p><strong>fâ€™â€™(x) &lt; 0</strong>: Curve is concave down (like a frown ğŸ˜), slope is decreasing</p></li>
<li><p><strong>fâ€™â€™(x) = 0</strong>: Inflection point (curvature changes sign)</p></li>
</ul>
</section>
<section id="ml-connection-the-hessian">
<h3>ML Connection: The Hessian<a class="headerlink" href="#ml-connection-the-hessian" title="Link to this heading">ïƒ</a></h3>
<p>In optimization, the second derivative (or its multidimensional analog, the <strong>Hessian</strong>) tells you about the curvature of your loss landscape:</p>
<ul class="simple">
<li><p><strong>Positive curvature</strong>: Youâ€™re in a valley (good for optimization)</p></li>
<li><p><strong>Negative curvature</strong>: Youâ€™re on a ridge (saddle point or maximum)</p></li>
<li><p><strong>Mixed curvature</strong>: Saddle point (common in high dimensions)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Second derivative with PyDelt</span>
<span class="n">second_derivative_func</span> <span class="o">=</span> <span class="n">interpolator</span><span class="o">.</span><span class="n">differentiate</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">curvature</span> <span class="o">=</span> <span class="n">second_derivative_func</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Curvature at all points: </span><span class="si">{</span><span class="n">curvature</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># For f(x) = xÂ², f&#39;&#39;(x) = 2 everywhere</span>
</pre></div>
</div>
</section>
</section>
<section id="notation-a-quick-guide">
<h2>Notation: A Quick Guide<a class="headerlink" href="#notation-a-quick-guide" title="Link to this heading">ïƒ</a></h2>
<p>Different fields use different notation for derivatives:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Notation</p></th>
<th class="head"><p>Read as</p></th>
<th class="head"><p>Common in</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>fâ€™(x)</p></td>
<td><p>â€œf prime of xâ€</p></td>
<td><p>Mathematics</p></td>
</tr>
<tr class="row-odd"><td><p>df/dx</p></td>
<td><p>â€œd f d xâ€</p></td>
<td><p>Physics, engineering</p></td>
</tr>
<tr class="row-even"><td><p>âˆ‚f/âˆ‚x</p></td>
<td><p>â€œpartial f partial xâ€</p></td>
<td><p>Multivariate calculus</p></td>
</tr>
<tr class="row-odd"><td><p>âˆ‡f</p></td>
<td><p>â€œgradient of fâ€</p></td>
<td><p>Machine learning</p></td>
</tr>
<tr class="row-even"><td><p>Df</p></td>
<td><p>â€œD fâ€</p></td>
<td><p>Functional analysis</p></td>
</tr>
</tbody>
</table>
<p>They all mean the same thing: the derivative of f with respect to x.</p>
</section>
<section id="derivatives-of-common-functions">
<h2>Derivatives of Common Functions<a class="headerlink" href="#derivatives-of-common-functions" title="Link to this heading">ïƒ</a></h2>
<p>Here are derivatives youâ€™ll encounter constantly:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Derivative</p></th>
<th class="head"><p>Why It Matters</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>xâ¿</p></td>
<td><p>nÂ·xâ¿â»Â¹</p></td>
<td><p>Polynomial layers</p></td>
</tr>
<tr class="row-odd"><td><p>eË£</p></td>
<td><p>eË£</p></td>
<td><p>Softmax, exponential families</p></td>
</tr>
<tr class="row-even"><td><p>ln(x)</p></td>
<td><p>1/x</p></td>
<td><p>Log-likelihood, cross-entropy</p></td>
</tr>
<tr class="row-odd"><td><p>sin(x)</p></td>
<td><p>cos(x)</p></td>
<td><p>Positional encodings, Fourier</p></td>
</tr>
<tr class="row-even"><td><p>cos(x)</p></td>
<td><p>-sin(x)</p></td>
<td><p>Positional encodings, Fourier</p></td>
</tr>
<tr class="row-odd"><td><p>Ïƒ(x) = 1/(1+eâ»Ë£)</p></td>
<td><p>Ïƒ(x)(1-Ïƒ(x))</p></td>
<td><p>Sigmoid activation</p></td>
</tr>
<tr class="row-even"><td><p>tanh(x)</p></td>
<td><p>1 - tanhÂ²(x)</p></td>
<td><p>Tanh activation</p></td>
</tr>
<tr class="row-odd"><td><p>max(0,x)</p></td>
<td><p>1 if x&gt;0, 0 if x&lt;0</p></td>
<td><p>ReLU activation</p></td>
</tr>
</tbody>
</table>
</section>
<section id="the-derivative-doesn-t-always-exist">
<h2>The Derivative Doesnâ€™t Always Exist<a class="headerlink" href="#the-derivative-doesn-t-always-exist" title="Link to this heading">ïƒ</a></h2>
<p>Some functions have points where the derivative is undefined:</p>
<section id="corners-non-differentiable-points">
<h3>1. Corners (Non-Differentiable Points)<a class="headerlink" href="#corners-non-differentiable-points" title="Link to this heading">ïƒ</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>At x = 0, ReLU has a â€œcornerâ€â€”the left slope is 0, the right slope is 1. The derivative doesnâ€™t exist (though we define it as 0 by convention).</p>
</section>
<section id="vertical-tangents">
<h3>2. Vertical Tangents<a class="headerlink" href="#vertical-tangents" title="Link to this heading">ïƒ</a></h3>
<p>If the tangent line is vertical, the slope is infiniteâ€”the derivative doesnâ€™t exist as a finite number.</p>
</section>
<section id="discontinuities">
<h3>3. Discontinuities<a class="headerlink" href="#discontinuities" title="Link to this heading">ïƒ</a></h3>
<p>If the function jumps, thereâ€™s no tangent line at the jump point.</p>
</section>
<section id="why-this-matters-for-ml">
<h3>Why This Matters for ML<a class="headerlink" href="#why-this-matters-for-ml" title="Link to this heading">ïƒ</a></h3>
<ul class="simple">
<li><p><strong>ReLU</strong> is not differentiable at 0, but we use it anyway (subgradients)</p></li>
<li><p><strong>Discrete operations</strong> (argmax, sampling) have no gradients (use Gumbel-Softmax, REINFORCE)</p></li>
<li><p><strong>Quantization</strong> breaks differentiability (use straight-through estimators)</p></li>
</ul>
</section>
</section>
<section id="numerical-vs-analytical-derivatives">
<h2>Numerical vs. Analytical Derivatives<a class="headerlink" href="#numerical-vs-analytical-derivatives" title="Link to this heading">ïƒ</a></h2>
<section id="analytical-derivatives">
<h3>Analytical Derivatives<a class="headerlink" href="#analytical-derivatives" title="Link to this heading">ïƒ</a></h3>
<p>When you have a formula, you can compute the derivative exactly using rules (next chapter).</p>
<p><strong>Pros</strong>: Exact, fast to evaluate
<strong>Cons</strong>: Requires knowing the formula</p>
</section>
<section id="numerical-derivatives">
<h3>Numerical Derivatives<a class="headerlink" href="#numerical-derivatives" title="Link to this heading">ïƒ</a></h3>
<p>When you only have data, you approximate:</p>
<div class="math notranslate nohighlight">
\[f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}\]</div>
<p><strong>Pros</strong>: Works with any data
<strong>Cons</strong>: Approximate, sensitive to noise, amplifies high-frequency errors</p>
</section>
<section id="automatic-differentiation-autodiff">
<h3>Automatic Differentiation (Autodiff)<a class="headerlink" href="#automatic-differentiation-autodiff" title="Link to this heading">ïƒ</a></h3>
<p>The best of both worldsâ€”used by PyTorch and TensorFlow:</p>
<ul class="simple">
<li><p>Computes exact derivatives</p></li>
<li><p>Works with complex compositions</p></li>
<li><p>No need to derive formulas by hand</p></li>
</ul>
<p>PyDeltâ€™s <code class="docutils literal notranslate"><span class="pre">NeuralNetworkInterpolator</span></code> uses autodiff internally.</p>
</section>
</section>
<section id="visualizing-derivatives">
<h2>Visualizing Derivatives<a class="headerlink" href="#visualizing-derivatives" title="Link to this heading">ïƒ</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydelt.interpolation</span><span class="w"> </span><span class="kn">import</span> <span class="n">SplineInterpolator</span>

<span class="c1"># Create data</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Fit and differentiate</span>
<span class="n">interp</span> <span class="o">=</span> <span class="n">SplineInterpolator</span><span class="p">(</span><span class="n">smoothing</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">interp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">deriv</span> <span class="o">=</span> <span class="n">interp</span><span class="o">.</span><span class="n">differentiate</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f(x) = sin(x)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">deriv</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f&#39;(x) = cos(x)&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exact cos(x)&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;f&#39;(x)&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">ïƒ</a></h2>
<ol class="arabic simple">
<li><p><strong>Derivatives measure instantaneous rate of change</strong></p></li>
<li><p><strong>Geometrically, itâ€™s the slope of the tangent line</strong></p></li>
<li><p><strong>For ML, itâ€™s sensitivity: how outputs respond to input changes</strong></p></li>
<li><p><strong>Second derivatives measure curvature</strong> (important for optimization)</p></li>
<li><p><strong>Derivatives donâ€™t always exist</strong> (corners, jumps, vertical tangents)</p></li>
<li><p><strong>PyDelt computes derivatives from data</strong> when you donâ€™t have formulas</p></li>
</ol>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">ïƒ</a></h2>
<ol class="arabic simple">
<li><p><strong>Intuition check</strong>: If fâ€™(3) = -2, is f increasing or decreasing at x = 3? By approximately how much does f change if x increases from 3 to 3.1?</p></li>
<li><p><strong>Find the minimum</strong>: For f(x) = xÂ² - 4x + 5, find where fâ€™(x) = 0. Verify this is a minimum by checking fâ€™â€™(x).</p></li>
<li><p><strong>Code it</strong>: Use PyDelt to compute the derivative of f(x) = e^(-xÂ²) and plot both the function and its derivative.</p></li>
</ol>
<hr class="docutils" />
<p><em>Previous: <a class="reference internal" href="01_functions_and_limits.html"><span class="std std-doc">â† Functions and Limits</span></a> | Next: <a class="reference internal" href="03_differentiation_rules.html"><span class="std std-doc">Differentiation Rules â†’</span></a></em></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="01_functions_and_limits.html" class="btn btn-neutral float-left" title="Chapter 1: Functions and Limits" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="03_differentiation_rules.html" class="btn btn-neutral float-right" title="Chapter 3: Differentiation Rules" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Michael Harrison Lee.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>