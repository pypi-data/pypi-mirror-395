# Privacy Architecture - Local LLM Operation

## Executive Summary

This document establishes the privacy architecture for Review Bot Automator's LLM integration, with a focus on reducing third-party data exposure through local LLM operation using Ollama.

### Purpose

This document provides:

* Foundation for privacy-preserving LLM operation
* Data flow analysis for local vs. API-based providers
* Compliance guidance for regulated industries
* Privacy verification procedures
* Risk assessment for different deployment scenarios

### Privacy-First Approach Rationale

Review Bot Automator processes source code and review comments that may contain:

* Proprietary business logic
* Security-sensitive implementations
* Personal Identifiable Information (PII)
* Protected Health Information (PHI)
* Trade secrets and intellectual property

**Important Context**: This tool works with GitHub pull requests, which means your code is already on GitHub and accessible to CodeRabbit (or other review bots). The privacy benefit of using Ollama is **reducing third-party LLM vendor exposure**, not achieving complete isolation.

When using cloud-based LLM providers (OpenAI, Anthropic), your code is exposed to:

* GitHub (required for PR workflow)
* CodeRabbit (required for review comments)
* LLM vendor (OpenAI/Anthropic)

**Local operation with Ollama reduces this to**:

* GitHub (required for PR workflow)
* CodeRabbit (required for review comments)
* ~~LLM vendor~~ (eliminated - processed locally)

### Key Stakeholders

* **Developers**: Primary users who require code privacy
* **Security Team**: Ensures data protection policies are enforced
* **Compliance Team**: Ensures adherence to GDPR, HIPAA, SOC2, etc.
* **Legal Team**: Manages intellectual property and data residency requirements

---

## Table of Contents

* [Privacy Principles](#privacy-principles)
* [Data Flow Comparison](#data-flow-comparison)
* [Provider Comparison Matrix](#provider-comparison-matrix)
* [Compliance & Regulations](#compliance--regulations)
* [Privacy Guarantees](#privacy-guarantees)
* [Threat Model for Privacy](#threat-model-for-privacy)
* [Security Controls for Local Models](#security-controls-for-local-models)
* [Privacy Verification](#privacy-verification)
* [Related Documentation](#related-documentation)

---

## Privacy Principles

The following privacy principles guide our architecture and provider recommendations:

### 1. Data Minimization

**Principle**: Only process data that is strictly necessary for the operation.

**Implementation**:

* LLM providers only receive review comments and relevant code context
* No full repository access
* No user authentication data sent to LLMs
* Minimal metadata in requests

**Local vs API**:

* **Ollama (Local)**: Review comments processed locally, no transmission to LLM vendor
* **API Providers**: Review comments sent to third-party LLM servers (OpenAI/Anthropic)

**Note**: GitHub API access is required for both options to fetch PR review comments.

### 2. Data Sovereignty

**Principle**: Minimize data processing in third-party data centers.

**Implementation**:

* **Ollama**: LLM inference on user's hardware (review comments processed locally)
* **API Providers**: LLM inference in provider's data centers (US, EU, etc.)

**Rationale**: Regulatory compliance (GDPR, data residency laws) often benefits from reducing the number of third-party processors.

**Important**: Your code is already on GitHub (required for PR workflow), so complete data sovereignty is not possible with this tool.

### 3. Third-Party Exposure Reduction

**Principle**: Minimize the number of third parties with access to sensitive code and review comments.

**Reality Check**:

* **GitHub**: Has access (required - your code lives here)
* **CodeRabbit**: Has access (required - generates review comments)
* **LLM Vendor**: This is what we can control

**Implementation**:

* **Ollama**: Eliminates LLM vendor from the access chain
* **API Providers**: Adds OpenAI/Anthropic to the access chain

**Rationale**: Every additional third party increases the risk of data breaches, unauthorized access, and compliance complexity. Ollama removes one third party (LLM vendor) from the chain.

### 4. Transparency

**Principle**: Users should know exactly where their data goes and how it's processed.

**Implementation**:

* Clear documentation of data flows for each provider
* Privacy verification tooling (`scripts/verify_privacy.sh`)
* No hidden telemetry or analytics
* **Honest disclosure**: GitHub and CodeRabbit have access (required for PR workflow)

**Rationale**: Informed consent requires transparency about data handling practices.

### 5. User Control

**Principle**: Users choose their privacy/performance trade-off.

**Implementation**:

* 5 provider options with varying privacy levels
* Easy switching between providers via presets
* Clear privacy comparison matrix (see below)

**Rationale**: Different use cases have different privacy requirements. We empower users to make informed decisions.

---

## Data Flow Comparison

### Local Model (Ollama) - Reduced Third-Party Exposure

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Internet (GitHub API - Required)                                â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  GitHub PR   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  CodeRabbit     â”‚                   â”‚
â”‚  â”‚  (Your Code) â”‚  Review â”‚  (Review Bot)   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTPS (Fetch PR comments)
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Machine (localhost)                                         â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  pr-resolve  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  GitHub API     â”‚                    â”‚
â”‚  â”‚  (Fetch)     â”‚         â”‚  Client         â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚                                                          â”‚
â”‚         â”‚ Review Comments                                         â”‚
â”‚         â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  pr-resolve  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Ollama Server  â”‚                    â”‚
â”‚  â”‚  (Process)   â”‚  HTTP   â”‚  (Local LLM)    â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  :11434 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                                    â”‚
â”‚  âœ… LLM inference stays local (no OpenAI/Anthropic)               â”‚
â”‚  âœ… No LLM vendor API keys required                               â”‚
â”‚  âœ… No per-request LLM costs                                      â”‚
â”‚  âš ï¸  GitHub API access required (code already on GitHub)          â”‚
â”‚  âš ï¸  CodeRabbit has access (generates review comments)            â”‚
â”‚  âš ï¸  Internet required to fetch PR comments                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### API-Based Models - Additional Third-Party Exposure

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Internet (GitHub API - Required)                                â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  GitHub PR   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  CodeRabbit     â”‚                   â”‚
â”‚  â”‚  (Your Code) â”‚  Review â”‚  (Review Bot)   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚         â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ HTTPS (Fetch PR comments)
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Machine (localhost)                                         â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  pr-resolve  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  GitHub API     â”‚                    â”‚
â”‚  â”‚  (Fetch)     â”‚         â”‚  Client         â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚         â”‚                                                          â”‚
â”‚         â”‚ Review Comments                                         â”‚
â”‚         â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚  â”‚  pr-resolve  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”
â”‚  â”‚  (Process)   â”‚  HTTPS (API key, comments)                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚  â”‚
â”‚                                                                    â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                                         â”‚
                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•
                          Internet (TLS Encrypted to LLM Vendor)
                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•
                                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚  LLM Provider Data Center (OpenAI/Anthropic - US, EU, etc.)               â”‚
â”‚                                                                             â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                           â”‚  API Gateway    â”‚                             â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                    â”‚                                       â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                           â”‚  LLM Service    â”‚                             â”‚
â”‚                           â”‚  (GPT-4/Claude) â”‚                             â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                    â”‚ Response                             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                          Internet (TLS Encrypted)
                     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Machine                                                         â”‚
â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚                           â”‚  pr-resolve     â”‚                        â”‚
â”‚                           â”‚  (Apply fixes)  â”‚                        â”‚
â”‚                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                       â”‚
â”‚  âš ï¸  GitHub API access required (code already on GitHub)             â”‚
â”‚  âš ï¸  CodeRabbit has access (generates review comments)               â”‚
â”‚  âš ï¸  Internet required to fetch PR comments                          â”‚
â”‚  âŒ ADDITIONAL: Review comments sent to LLM vendor                   â”‚
â”‚  âŒ ADDITIONAL: Stored on LLM vendor servers (temp/permanent)        â”‚
â”‚  âŒ ADDITIONAL: Subject to LLM vendor data retention policies        â”‚
â”‚  âŒ Requires LLM vendor API key management                           â”‚
â”‚  âŒ Subject to rate limits                                           â”‚
â”‚  ğŸ’° Costs per LLM request                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

### Key Differences

| Aspect | Ollama (Local) | API Providers |
| -------- | --------------- | --------------- |
| **LLM Inference Location** | Your machine (localhost) | LLM vendor servers |
| **Third-Party LLM Vendor** | âŒ None | âœ… OpenAI/Anthropic |
| **GitHub/CodeRabbit Access** | âš ï¸ Yes (required) | âš ï¸ Yes (required) |
| **Internet Required** | âœ… Yes (to fetch PRs) | âœ… Yes (PRs + LLM API) |
| **Data Retention (LLM)** | You control | Vendor policy (30-90 days) |
| **Regulatory Compliance** | Simpler (one fewer processor) | More complex (additional processor) |
| **Cost** | Hardware only | Hardware + per-request fees |
| **Privacy Benefit** | Removes LLM vendor exposure | LLM vendor sees all comments |

---

## Provider Comparison Matrix

Comprehensive comparison of all 5 supported LLM providers across privacy dimensions:

| Provider | LLM Vendor Exposure | GitHub API Required | Cost | Best For |
| ---------- | --------------------- | --------------------- | ------ | ---------- |
| **Ollama** | âœ… **None** (localhost) | âœ… Yes | âœ… **Free** | Minimizing third-party exposure, compliance, cost savings |
| **OpenAI API** | âŒ OpenAI (US) | âœ… Yes | ğŸ’° Low (~$0.01/PR) | Production, budget-conscious |
| **Anthropic API** | âŒ Anthropic (US) | âœ… Yes | ğŸ’° Medium | Quality, caching benefits |
| **Claude CLI** | âŒ Anthropic (US) | âœ… Yes | ğŸ’° Subscription | Interactive, convenience |
| **Codex CLI** | âŒ GitHub/OpenAI | âœ… Yes | ğŸ’° Subscription (Copilot) | GitHub integration, free with Copilot |

### Privacy Ranking (by Third-Party Exposure)

1. **ğŸ¥‡ Ollama** - Best Privacy (GitHub + CodeRabbit only)
2. **ğŸ¥ˆ OpenAI/Anthropic API** - Moderate Privacy (GitHub + CodeRabbit + LLM vendor)
3. **ğŸ¥‰ Claude CLI/Codex CLI** - Moderate Privacy (GitHub + CodeRabbit + LLM vendor)

**Note**: All options require GitHub API access and CodeRabbit has access to your code. The privacy difference is whether an additional LLM vendor (OpenAI/Anthropic) also gets access to review comments.

### Data Retention Policies (API Providers)

**OpenAI**:

* API requests: 30 days retention (for abuse monitoring)
* Can opt out of training data usage
* See: <https://openai.com/policies/api-data-usage-policies>

**Anthropic**:

* API requests: Not used for training by default
* 90 days retention for Trust & Safety
* See: <https://www.anthropic.com/legal/commercial-terms>

**GitHub (Codex CLI)**:

* Subject to GitHub's Privacy Statement
* Integrated with Copilot subscription
* See: <https://docs.github.com/en/site-policy/privacy-policies/github-privacy-statement>

**Important**: These policies may change. Always review current terms before use in regulated environments.

---

## Compliance & Regulations

### GDPR (General Data Protection Regulation)

**Requirements**:

* Personal data must be processed lawfully, fairly, and transparently
* Data minimization principle
* Right to erasure ("right to be forgotten")
* Data sovereignty (EU data stays in EU)

**Reality for This Tool**:

* âš ï¸ **Code is on GitHub** - Already accessible to GitHub (US-based)
* âš ï¸ **CodeRabbit processes code** - Review bot has access
* âš ï¸ **Data Processing Agreements needed** - For GitHub, CodeRabbit

**Ollama Additional Benefits**:

* âœ… **Reduces processors** - Eliminates one additional data processor (LLM vendor)
* âœ… **Simplifies DPA chain** - No additional agreement for LLM vendor
* âœ… **Reduces cross-border transfers** - LLM processing stays local

**API Provider Additional Considerations**:

* âš ï¸ **Adds another data processor** - OpenAI/Anthropic to DPA chain
* âš ï¸ **Additional cross-border transfer** - Review comments to LLM vendor
* âš ï¸ **Check provider's GDPR compliance** - Requires additional legal review

### HIPAA (Health Insurance Portability and Accountability Act)

**Requirements**:

* Protected Health Information (PHI) must remain secure
* Business Associate Agreements (BAA) required for third parties
* Audit trails and access controls

**Reality for This Tool**:

* âš ï¸ **Code on GitHub** - BAA required with GitHub if PHI in code
* âš ï¸ **CodeRabbit processes code** - BAA required with CodeRabbit
* âš ï¸ **If PHI in code, already exposed** - GitHub and CodeRabbit have access

**Ollama Additional Benefits**:

* âœ… **Reduces BAA requirements** - No additional BAA for LLM vendor
* âœ… **Simpler compliance chain** - One fewer business associate

**API Provider Additional Considerations**:

* âš ï¸ **Another BAA required** - Must sign BAA with OpenAI/Anthropic
* âš ï¸ **Check HIPAA-eligible services** - Not all API tiers support HIPAA
* âš ï¸ **Additional costs** - HIPAA-compliant tiers often more expensive
* âŒ **Verify current HIPAA support** - OpenAI/Anthropic support varies

### SOC 2 (Service Organization Control)

**Requirements**:

* Security, availability, processing integrity, confidentiality, privacy
* Third-party service providers must be audited

**Reality for This Tool**:

* âš ï¸ **GitHub assessment required** - Vendor risk for GitHub
* âš ï¸ **CodeRabbit assessment required** - Vendor risk for review bot

**Ollama Additional Benefits**:

* âœ… **Reduces vendor assessments** - One fewer vendor (no LLM vendor)
* âœ… **Simpler SOC 2 scope** - LLM processing under your control

**API Provider Additional Considerations**:

* âš ï¸ **Another vendor assessment** - OpenAI/Anthropic SOC 2 review needed
* âš ï¸ **SOC 2 reports must be reviewed** - Ensure Type II reports available
* âš ï¸ **Continuous monitoring** - Provider's compliance status may change

---

## Privacy Guarantees

### Ollama Local Model Guarantees

When using Ollama with Review Bot Automator, you have the following privacy guarantees **for LLM inference**:

**Important Context**: This tool requires GitHub API access to fetch PR comments. Your code is already on GitHub. These guarantees apply to the LLM processing step only.

#### 1. LLM Inference Isolation

* **All LLM communication occurs on localhost** (127.0.0.1 / ::1)
* No external network connections initiated by Ollama during inference
* Can be verified with `scripts/verify_privacy.sh`
* âš ï¸ **GitHub API calls still occur** (required to fetch PR comments)

#### 2. LLM Data Residency

* **Review comments processed locally** on your machine
* Model weights stored locally (`~/.ollama/models/`)
* No cloud synchronization or telemetry for LLM inference
* âš ï¸ **Code already on GitHub** (required for PR workflow)

#### 3. No LLM Vendor Dependencies

* **Direct HTTP communication** with local Ollama server
* No LLM vendor intermediary services (OpenAI/Anthropic)
* No LLM vendor analytics or tracking
* âš ï¸ **GitHub and CodeRabbit still involved** (required)

#### 4. User Control (LLM Models)

* **You control when models download** (explicit `ollama pull` required)
* **You control when models update** (no automatic updates)
* **You control model data deletion** (standard file system operations)

#### 5. Encryption at Rest (Optional)

* **Use encrypted filesystems** for model storage
* **Standard OS-level encryption** (LUKS, FileVault, BitLocker)
* **No special Ollama configuration required**

#### 6. Access Control

* **Standard OS permissions** apply to Ollama process and files
* **User-level isolation** via Unix permissions
* **Optional: Run in Docker** for additional containerization

### API Provider Considerations

When using API-based providers, understand the privacy limitations:

#### Data in Transit

* âœ… **Encrypted via TLS** (HTTPS)
* âš ï¸ **Provider can decrypt** (they control the endpoint)
* âš ï¸ **Vulnerable to MitM** (if certificate verification bypassed)

#### Data at Rest (Provider's Servers)

* âš ï¸ **Temporary storage** for request processing
* âš ï¸ **Retention period varies** (30-90 days typical)
* âš ï¸ **Used for abuse monitoring** and potentially training
* âš ï¸ **Subject to provider's security** (data breaches possible)

#### Third-Party Subprocessors

* âš ï¸ **Providers may use subprocessors** (cloud hosting, monitoring)
* âš ï¸ **Review provider's subprocessor list**
* âš ï¸ **Additional parties may have access**

---

## Threat Model for Privacy

### Threats Mitigated by Local Operation (Ollama)

| Threat | Risk with API | Risk with Ollama |
| -------- | -------------- | ------------------ |
| **Data Breach at Provider** | High - All customer data exposed | None - No data at provider |
| **Unauthorized Access** | Medium - Provider employees, hackers | Low - OS-level controls |
| **Man-in-the-Middle Attack** | Medium - Network interception | None - Localhost only |
| **Data Retention Abuse** | High - Provider keeps data indefinitely | None - You control retention |
| **Regulatory Non-Compliance** | Medium-High - Depends on provider | Low - Simplified compliance |
| **Subpoena/Legal Disclosure** | High - Provider must comply | Low - Only you can be compelled |
| **Insider Threats (Provider)** | Medium - Malicious employees | None - Not applicable |
| **Supply Chain Attacks** | Medium - Compromised provider | Low - Limited attack surface |

### Threats NOT Mitigated by Local Operation

| Threat | Mitigation |
| -------- | ----------- |
| **Local Machine Compromise** | Strong endpoint security, EDR, regular patching |
| **Malicious Model Weights** | Download models from trusted sources only (official Ollama registry) |
| **Physical Access Attacks** | Encrypted storage, physical security controls |
| **Insider Threats (Your Org)** | Access controls, audit logging, separation of duties |
| **Code Injection via Review Comments** | Already mitigated by input validation in pr-resolve |

### Privacy Risk Assessment

**High Privacy Requirements** (Healthcare, Finance, Defense):

* âœ… **Recommended**: Ollama (local operation)
* âš ï¸ **Acceptable with review**: API providers with BAA/DPA and compliance verification
* âŒ **Not recommended**: Free API tiers without enterprise agreements

**Medium Privacy Requirements** (Most Enterprises):

* âœ… **Recommended**: Ollama or Anthropic/OpenAI with enterprise agreements
* âœ… **Acceptable**: Claude CLI/Codex CLI with subscription

**Low Privacy Requirements** (Open Source, Public Code):

* âœ… **Recommended**: Any provider based on cost/performance trade-offs
* âœ… **Acceptable**: Free API tiers

---

## Security Controls for Local Models

While Ollama provides excellent privacy guarantees, follow these security best practices:

### 1. Model Provenance

**Risk**: Malicious or compromised model weights

**Controls**:

* âœ… Download models only from official Ollama registry
* âœ… Verify model checksums when available
* âœ… Use well-known, popular models (qwen2.5-coder, codellama)
* âŒ Avoid importing models from untrusted sources

### 2. Network Segmentation

**Risk**: Ollama server exposed to network

**Controls**:

* âœ… Default configuration binds to localhost only (127.0.0.1)
* âœ… Firewall rules to block external access
* âš ï¸ If you need remote access, use VPN or SSH tunneling
* âŒ Do NOT expose Ollama directly to the internet

### 3. Access Control

**Risk**: Unauthorized access to Ollama service

**Controls**:

* âœ… Run Ollama under dedicated user account
* âœ… Restrict file permissions on `~/.ollama/` directory
* âœ… Use OS-level access controls (AppArmor, SELinux)
* âœ… Consider Docker containerization for additional isolation

### 4. Resource Limits

**Risk**: Denial of service via resource exhaustion

**Controls**:

* âœ… Set memory limits for Ollama process (Docker, systemd)
* âœ… Monitor resource usage (`ollama ps`, `htop`)
* âœ… Configure max concurrent requests if needed

### 5. Audit Logging

**Risk**: Unauthorized usage or configuration changes

**Controls**:

* âœ… Enable system logs for Ollama service (journalctl, syslog)
* âœ… Monitor Ollama logs for errors: `~/.ollama/logs/`
* âœ… Track model downloads and updates
* âœ… Integrate with SIEM if available

### 6. Encryption at Rest

**Risk**: Physical theft or unauthorized access to storage

**Controls**:

* âœ… Use full-disk encryption (LUKS, FileVault, BitLocker)
* âœ… Encrypt model storage directory specifically if needed
* âœ… Secure backup procedures for encrypted data

---

## Privacy Verification

### Automated Verification Script

Use the provided privacy verification script to confirm local-only operation:

```bash
# Run privacy verification test
./scripts/verify_privacy.sh

# Expected output
# âœ… Privacy Verification: PASSED
# âœ… No external network connections detected
# âœ… Report: privacy-verification-report.md

```

The script:

1. Monitors network traffic during Ollama inference
2. Verifies no connections to external IPs (for LLM inference only)
3. Generates detailed report with timestamps
4. Exit code 0 (success) or 1 (external connections detected)

**Note**: This script verifies Ollama's localhost-only operation. It does not prevent or monitor GitHub API calls, which are required for the tool to function.

See [Privacy Verification Script Documentation](local-llm-operation-guide.md#privacy-verification) for details.

### Manual Verification

You can also manually verify privacy using standard network monitoring tools:

#### Linux

```bash
# Monitor network connections while running inference
sudo tcpdump -i any port not 11434 and host not 127.0.0.1 &
pr-resolve apply 123 --llm-preset ollama-local
sudo pkill tcpdump

# Should see no packets captured (only localhost traffic)

```

#### macOS

```bash
# Monitor network connections
sudo lsof -i -n -P | grep -v "127.0.0.1"

# Run inference
pr-resolve apply 123 --llm-preset ollama-local

# Check lsof again - should see no new external connections

```

#### Docker Network Isolation

```bash
# Run Ollama in Docker with no external network
docker run -d --name ollama \
  --network none \
  -v ollama:/root/.ollama \
  ollama/ollama

# This will FAIL to download models (no network)
# But inference works fine after models are pre-loaded

```

---

## Related Documentation

### Privacy & Local LLM Operation

* [Local LLM Operation Guide](local-llm-operation-guide.md) - Local LLM setup with Ollama
* [Privacy FAQ](privacy-faq.md) - Common privacy questions answered
* [Ollama Setup Guide](ollama-setup.md) - Installation and configuration

### Security

* [Security Architecture](security-architecture.md) - Overall security design
* [API Key Security](llm-configuration.md#api-key-security) - Secure API key management

### Configuration

* [LLM Configuration Guide](llm-configuration.md) - Provider setup and presets
* [Configuration Guide](configuration.md) - General configuration options

### Performance

* [Performance Benchmarks](performance-benchmarks.md) - Provider performance comparison

---

## Conclusion

**Ollama reduces third-party exposure** by keeping LLM inference local to your machine. This architecture:

âœ… **Eliminates LLM vendor exposure** - OpenAI/Anthropic never see your review comments
âœ… **Simplifies compliance** - One fewer data processor (no LLM vendor BAA/DPA)
âœ… **Reduces attack surface** - Fewer third parties with access
âœ… **Gives you control over LLM** - Local model management
âœ… **Costs nothing for LLM** - Free after initial hardware investment

âš ï¸ **Important limitations**:

* âŒ **Not air-gapped** - Requires internet to fetch PR comments from GitHub
* âš ï¸ **GitHub has access** - Your code is on GitHub (required for PR workflow)
* âš ï¸ **CodeRabbit has access** - Review bot processes your code (required)

**When to use Ollama**:

* Want to minimize third-party LLM vendor exposure
* Regulated industries wanting to reduce data processor chain (GDPR, HIPAA, SOC2)
* Cost-conscious usage (no per-request LLM fees)
* Organizations with policies against cloud LLM services

**When API providers may be acceptable**:

* Open source / public code
* Enterprise agreements with BAA/DPA already in place
* Need for highest quality models (GPT-4, Claude Sonnet 4.5)
* Budget available for per-request costs
* Comfortable with additional third-party exposure

**The honest trade-off**: Ollama eliminates LLM vendor exposure at the cost of local hardware requirements and potentially lower model quality. Your code is still on GitHub and accessible to CodeRabbitâ€”Ollama just prevents one additional third party (the LLM vendor) from accessing your review comments.

For step-by-step local LLM setup, see the [Local LLM Operation Guide](local-llm-operation-guide.md).
