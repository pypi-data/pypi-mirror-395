# ==============================================================================
# PR Conflict Resolver - Environment Configuration Template
# ==============================================================================
# Copy this file to .env and configure your settings.
# Environment variables override config file settings but are overridden by CLI flags.
# Configuration precedence: CLI flags > env vars > config file > defaults
#
# Documentation: docs/configuration.md

# ==============================================================================
# Application Mode
# ==============================================================================
# Determines which changes are applied during conflict resolution.
#
# Options:
#   - all: Apply both conflicting and non-conflicting changes (default)
#   - conflicts-only: Apply only changes with conflicts (after resolution)
#   - non-conflicts-only: Apply only non-conflicting changes
#   - dry-run: Analyze conflicts without applying any changes
#
CR_MODE=all

# ==============================================================================
# Safety Features
# ==============================================================================
# These features help prevent data loss and ensure safe conflict resolution.

# Enable automatic rollback on failure using git stash (recommended: true)
# When enabled, creates a checkpoint before applying changes and rolls back on error.
CR_ENABLE_ROLLBACK=true

# Enable pre-application validation (recommended: true)
# Validates changes before applying to catch errors early.
CR_VALIDATE=true

# ==============================================================================
# Parallel Processing (Experimental)
# ==============================================================================
# Enable parallel processing of changes for improved performance.
# Note: Parallel processing is thread-safe but may affect logging order.

# Enable parallel processing (default: false)
CR_PARALLEL=false

# Maximum number of worker threads (default: 4, recommended: 4-8)
# Higher values may improve performance for large PRs but increase resource usage.
CR_MAX_WORKERS=4

# ==============================================================================
# Logging Configuration
# ==============================================================================
# Configure logging level and output destination.

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
# Default: INFO
CR_LOG_LEVEL=INFO

# Log file path (optional)
# If not set or empty, logs only to stdout/stderr.
# Example: CR_LOG_FILE=/var/log/pr-resolver/resolver.log
CR_LOG_FILE=

# ==============================================================================
# GitHub Integration
# ==============================================================================
# GitHub Personal Access Token for API access.
# Required for fetching PR data and posting comments.
#
# Permissions needed:
#   - repo (full control)
#   - read:org (if working with organization repos)
#
# Create token at: https://github.com/settings/tokens
GITHUB_PERSONAL_ACCESS_TOKEN=your_token_here

# ==============================================================================
# LLM Configuration (AI-Powered Parsing)
# ==============================================================================
# Enable LLM-based parsing for higher coverage of CodeRabbit comment formats.
# LLM features are optional and provide 95%+ parsing coverage (vs. 20% regex-only).
#
# Documentation: docs/llm-configuration.md

# Enable LLM-based parsing (default: false)
# When enabled, uses AI models to parse complex CodeRabbit comment formats.
CR_LLM_ENABLED=false

# LLM Provider Selection
# ==============================================================================
# Choose from 5 supported providers:
#   - claude-cli: Claude via CLI (no API key, requires Claude subscription)
#   - codex-cli: GitHub Codex via CLI (no API key, requires Copilot subscription)
#   - ollama: Local Ollama models (no API key, free, private, offline)
#   - openai: OpenAI API (requires API key, pay-per-use)
#   - anthropic: Anthropic API (requires API key, pay-per-use, prompt caching)
#
# Default: claude-cli
CR_LLM_PROVIDER=claude-cli

# LLM Model Selection
# ==============================================================================
# Model identifier for the selected provider:
#   Claude CLI: claude-sonnet-4-5, claude-opus-4
#   Codex CLI: codex
#   Ollama: qwen2.5-coder:7b, llama3.3:70b, codestral:22b
#   OpenAI: gpt-4, gpt-4-turbo, gpt-4o, gpt-4o-mini
#   Anthropic: claude-sonnet-4-5-20250929, claude-haiku-4-20250514
#
# Default: claude-sonnet-4-5
CR_LLM_MODEL=claude-sonnet-4-5

# API Keys (API-Based Providers Only)
# ==============================================================================
# ⚠️  SECURITY WARNING: Never commit API keys to version control!
# ⚠️  API keys must ONLY be set via environment variables, never in config files.
#
# For OpenAI (required if CR_LLM_PROVIDER=openai):
#   Get key from: https://platform.openai.com/api-keys
#   Format: sk-...
# CR_LLM_API_KEY=sk-your-openai-api-key-here
#
# For Anthropic (required if CR_LLM_PROVIDER=anthropic):
#   Get key from: https://console.anthropic.com/
#   Format: sk-ant-...
#   Benefits: 50-90% cost savings with prompt caching
# CR_LLM_API_KEY=sk-ant-your-anthropic-api-key-here
#
# Note: CLI-based providers (claude-cli, codex-cli) and Ollama do NOT require API keys

# LLM Behavior Settings
# ==============================================================================
# Fall back to regex parsing if LLM fails (recommended: true)
# Ensures backward compatibility and resilience.
CR_LLM_FALLBACK_TO_REGEX=true

# Enable response caching to reduce cost and latency (recommended: true)
# Caches identical prompts to avoid redundant API calls.
# Anthropic provider: Enables prompt caching for 50-90% cost savings.
CR_LLM_CACHE_ENABLED=true

# Maximum tokens per LLM request (default: 2000)
# Higher values allow parsing larger comment blocks but increase cost.
# Recommended: 1000-4000 depending on PR size.
CR_LLM_MAX_TOKENS=2000

# Maximum cost budget per run in USD (optional)
# Prevents runaway costs for large PRs. Execution stops when budget exceeded.
# Example: CR_LLM_COST_BUDGET=5.0 (limits to $5 per run)
# Default: unlimited (None)
CR_LLM_COST_BUDGET=

# LLM Confidence Threshold
# ==============================================================================
# Minimum confidence score (0.0-1.0) to accept LLM-parsed changes.
# Changes below this threshold fall back to regex or are rejected.
# Default: 0.5
CR_LLM_CONFIDENCE_THRESHOLD=0.5

# LLM Retry Configuration (Phase 5)
# ==============================================================================
# Configure automatic retry behavior for rate limit and transient errors.

# Enable retry on rate limit errors (recommended: true)
CR_LLM_RETRY_ON_RATE_LIMIT=true

# Maximum retry attempts (default: 3, minimum: 1)
CR_LLM_RETRY_MAX_ATTEMPTS=3

# Base delay in seconds for exponential backoff (default: 2.0)
# Actual delay: base_delay * 2^attempt + jitter
CR_LLM_RETRY_BASE_DELAY=2.0

# LLM Circuit Breaker Configuration
# ==============================================================================
# Prevents cascading failures by temporarily disabling failing providers.

# Enable circuit breaker (recommended: true)
CR_LLM_CIRCUIT_BREAKER_ENABLED=true

# Consecutive failures before circuit opens (default: 5)
CR_LLM_CIRCUIT_BREAKER_THRESHOLD=5

# Cooldown seconds before attempting recovery (default: 60.0)
CR_LLM_CIRCUIT_BREAKER_COOLDOWN=60.0

# Ollama Configuration (Local Models Only)
# ==============================================================================
# Ollama base URL for local model inference (default: http://localhost:11434)
# Only used when CR_LLM_PROVIDER=ollama
# CR_OLLAMA_BASE_URL=http://localhost:11434
#
# Ollama model recommendations:
#   - qwen2.5-coder:7b (fast, good quality, 7B params)
#   - llama3.3:70b (highest quality, 70B params, slower)
#   - codestral:22b (balanced, 22B params)
#
# Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh
# Pull model: ollama pull qwen2.5-coder:7b

# LLM Quick Setup Examples
# ==============================================================================
# 1. Free - Codex CLI (GitHub Copilot subscription required):
#    CR_LLM_ENABLED=true
#    CR_LLM_PROVIDER=codex-cli
#    CR_LLM_MODEL=codex
#
# 2. Free - Ollama Local (no subscription, fully private):
#    CR_LLM_ENABLED=true
#    CR_LLM_PROVIDER=ollama
#    CR_LLM_MODEL=qwen2.5-coder:7b
#
# 3. Paid - OpenAI API (pay-per-use):
#    CR_LLM_ENABLED=true
#    CR_LLM_PROVIDER=openai
#    CR_LLM_MODEL=gpt-4o-mini
#    CR_LLM_API_KEY=sk-...
#    CR_LLM_COST_BUDGET=5.0
#
# 4. Paid - Anthropic API (best cost/performance with caching):
#    CR_LLM_ENABLED=true
#    CR_LLM_PROVIDER=anthropic
#    CR_LLM_MODEL=claude-haiku-4-20250514
#    CR_LLM_API_KEY=sk-ant-...
#    CR_LLM_COST_BUDGET=5.0

# ==============================================================================
# LLM Presets (Zero-Config Setup)
# ==============================================================================
# Use presets for instant LLM setup with sensible defaults.
# Presets configure provider, model, and settings automatically.
#
# Available presets:
#   - codex-cli-free: Free Codex CLI (GitHub Copilot subscription)
#   - ollama-local: Local Ollama (qwen2.5-coder:7b, free, private)
#   - claude-cli-sonnet: Claude CLI (claude-sonnet-4-5, subscription)
#   - openai-api-mini: OpenAI GPT-4o-mini (low-cost API, $5 budget)
#   - anthropic-api-balanced: Anthropic Claude Haiku 4 (balanced, $5 budget)
#
# CLI usage (preset + API key):
#   pr-resolve apply 123 --llm-preset codex-cli-free
#   pr-resolve apply 123 --llm-preset openai-api-mini --llm-api-key sk-...
#   pr-resolve apply 123 --llm-preset anthropic-api-balanced --llm-api-key sk-ant-...
#
# See: docs/llm-presets.md

# ==============================================================================
# Advanced Configuration
# ==============================================================================
# For advanced use cases, consider using a config file (config.yaml or config.toml)
# instead of environment variables. See docs/configuration.md for details.
#
# Example config file usage:
#   pr-resolve apply 123 --config /path/to/config.yaml
#
# Configuration precedence (highest to lowest):
#   1. CLI flags (--mode, --parallel, etc.)
#   2. Environment variables (CR_* variables)
#   3. Configuration file (config.yaml or config.toml)
#   4. LLM presets (--llm-preset flag)
#   5. Default values
