---
title: Testing Guidelines
description: Test organization, pytest conventions, and coverage requirements
alwaysApply: false
globs:
  - "tests/**/*.py"
  - "**/test_*.py"
  - "tests/conftest.py"
---

# Testing Guidelines

## Test Organization

```
tests/
├── unit/           # Unit tests for individual components
├── integration/    # Integration tests for workflows
├── fixtures/       # Test data and fixtures
└── conftest.py     # Shared test configuration
```

## Test Structure

### Unit Tests

Test individual components in isolation:

- Mock external dependencies
- Test edge cases and error conditions
- Verify expected behavior with various inputs
- Test all public methods and properties

### Integration Tests

Test component interactions:

- End-to-end workflows
- Real GitHub API interactions (with test repos)
- File system operations
- Configuration loading and validation

## Pytest Conventions

### Test Naming

- Files: `test_*.py`
- Classes: `Test*`
- Functions: `test_*`
- Use descriptive names: `test_apply_change_success`, `test_detect_conflicts_multiple_files`

### Fixtures

Use fixtures from `tests/conftest.py`:

- `sample_pr_comments`: Mock GitHub comments
- `temp_workspace`: Temporary directory for file operations
- `sample_json_file`: Test JSON file
- `sample_yaml_file`: Test YAML file

### Test Markers

```python
@pytest.mark.slow
def test_large_pr_processing():
    """Test processing of large PRs with many comments."""
    pass

@pytest.mark.integration
def test_github_api_integration():
    """Test real GitHub API integration."""
    pass
```

## Coverage Requirements

- **Minimum**: 80% test coverage
- **Target**: 90%+ for critical components
- **Exclude**: Test files, fixtures, and configuration files
- **Focus**: Core business logic and error handling

## Test Data

### Fixtures

Create reusable test data:

```python
@pytest.fixture
def sample_conflict():
    """Create a sample conflict for testing."""
    return Conflict(
        file_path="test.json",
        line_range=(1, 3),
        changes=[change1, change2],
        conflict_type="exact",
        severity="medium",
        overlap_percentage=100.0
    )
```

### Mocking

Mock external dependencies:

```python
@patch('review_bot_automator.integrations.github.GitHubCommentExtractor.fetch_pr_comments')
def test_resolve_pr_conflicts(mock_fetch):
    """Test PR conflict resolution with mocked GitHub API."""
    mock_fetch.return_value = sample_comments
    # Test implementation
```

## Assertions

Use specific assertions:

```python
# Good
assert result.applied_count == 5
assert result.success_rate == 100.0
assert "error" in str(excinfo.value)

# Avoid
assert result  # Too generic
assert True    # Meaningless
```

## Test Isolation

- Each test should be independent
- Use fresh fixtures for each test
- Clean up temporary files
- Don't rely on test execution order

## Performance Testing

Test performance characteristics:

```python
@pytest.mark.slow
def test_large_file_processing():
    """Test processing of large files."""
    start_time = time.time()
    result = process_large_file()
    duration = time.time() - start_time
    assert duration < 5.0  # Should complete within 5 seconds
```

## Error Testing

Test error conditions:

```python
def test_invalid_file_path():
    """Test handling of invalid file paths."""
    with pytest.raises(FileNotFoundError):
        handler.apply_change("/nonexistent/file.json", "content", 1, 1)

def test_malformed_json():
    """Test handling of malformed JSON."""
    with pytest.raises(ValueError, match="Invalid JSON"):
        handler.apply_change("test.json", "{ invalid json", 1, 1)
```

## Parametrized Tests

Use parametrized tests for multiple scenarios:

```python
@pytest.mark.parametrize("file_type,expected_handler", [
    (".json", JsonHandler),
    (".yaml", YamlHandler),
    (".toml", TomlHandler),
])
def test_handler_selection(file_type, expected_handler):
    """Test correct handler selection for file types."""
    handler = resolver.get_handler(f"test{file_type}")
    assert isinstance(handler, expected_handler)
```

## Test Documentation

Document test purpose and expected behavior:

```python
def test_priority_based_resolution():
    """Test that user selections override other changes.

    When multiple changes conflict, user selections should
    take priority over security fixes, which take priority
    over regular suggestions.
    """
    # Test implementation
```

## Continuous Integration

Tests must pass in CI environment:

- Use GitHub Actions for automated testing
- Test on Python 3.12.x
- Include coverage reporting
- Fail on coverage below 80%

## LLM Provider Testing Patterns

### Mocking LLM Providers

Mock LLM providers in unit tests:

```python
from unittest.mock import MagicMock, patch

@patch("review_bot_automator.llm.providers.openai_api.OpenAIAPIProvider")
def test_parser_with_mocked_provider(mock_provider_class):
    """Test parser with mocked OpenAI provider."""
    mock_provider = MagicMock()
    mock_provider.generate.return_value = '{"changes": []}'
    mock_provider.count_tokens.return_value = 10
    mock_provider.get_total_cost.return_value = 0.001
    mock_provider_class.return_value = mock_provider

    parser = UniversalLLMParser(provider=mock_provider)
    changes = parser.parse_comment("test comment")
    assert len(changes) == 0
```

### Integration Test Markers

Mark integration tests for real LLM providers:

```python
@pytest.mark.integration
@pytest.mark.llm
def test_openai_provider_integration():
    """Test OpenAI provider with real API (skip if no key)."""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        pytest.skip("OPENAI_API_KEY not set")

    provider = OpenAIAPIProvider(api_key=api_key)
    response = provider.generate("Say hello", max_tokens=10)
    assert len(response) > 0
```

### Testing Retry Logic

Test retry behavior:

```python
from unittest.mock import patch, side_effect
from review_bot_automator.llm.exceptions import LLMTimeoutError

def test_retry_on_timeout():
    """Test that provider retries on timeout."""
    provider = OpenAIAPIProvider(api_key="test-key")

    # First two calls fail, third succeeds
    with patch.object(provider.client.chat.completions, "create") as mock_create:
        mock_create.side_effect = [
            APITimeoutError("Timeout"),
            APITimeoutError("Timeout"),
            MagicMock(choices=[MagicMock(message=MagicMock(content="Success"))]),
        ]

        response = provider.generate("test")
        assert response == "Success"
        assert mock_create.call_count == 3
```

### Testing Cost Tracking

Test cost calculation:

```python
def test_cost_tracking():
    """Test that costs are tracked correctly."""
    provider = OpenAIAPIProvider(api_key="test-key", model="gpt-4o-mini")

    # Mock token counts
    with patch.object(provider, "count_tokens", return_value=100):
        provider.generate("test prompt")

    assert provider.total_input_tokens > 0
    assert provider.total_output_tokens > 0
    assert provider.total_cost > 0.0
```

### Testing Cache Behavior

Test prompt caching:

```python
def test_cache_hit():
    """Test that cached responses are returned."""
    cache = PromptCache(cache_dir=temp_dir)
    cache_key = cache._compute_cache_key("test prompt", "openai", "gpt-4")

    # Set cache entry
    cache.set(cache_key, "cached response")

    # Should return cached value
    cached = cache.get(cache_key)
    assert cached == "cached response"
```

### Mocking Subprocess Calls (CLI Providers)

Mock subprocess for CLI providers:

```python
from unittest.mock import patch, MagicMock

@patch("subprocess.run")
def test_claude_cli_provider(mock_subprocess):
    """Test Claude CLI provider with mocked subprocess."""
    mock_result = MagicMock()
    mock_result.stdout = '{"changes": []}'
    mock_result.returncode = 0
    mock_subprocess.return_value = mock_result

    provider = ClaudeCLIProvider()
    response = provider.generate("test prompt")
    assert response == '{"changes": []}'
```

## Related Rules

- See `.cursor/rules/llm-integration.mdc` for LLM provider patterns
- See `.cursor/rules/cost-optimization.mdc` for cost tracking tests
