---
title: Metrics & Monitoring Guidelines
description: Metrics collection patterns, success rate tracking, latency tracking, cost tracking, and export formats
globs: ["src/review_bot_automator/core/**/*.py", "src/review_bot_automator/llm/**/*.py"]
alwaysApply: false
---

# Metrics & Monitoring Guidelines

> **Implementation Status**: Basic metrics aggregation exists (`_aggregate_llm_metrics()` in `resolver.py`, `LLMMetrics` dataclass). Metrics export (JSON/CSV) and latency percentiles (p95/p99) tracking are NOT yet implemented (Issue #224). The patterns below are for future implementation.


## Metrics Collection Patterns

### Metrics Dataclass

Use dataclass for metrics structure:

```python
@dataclass
class ParsingMetrics:
    """Metrics for LLM parsing operations."""
    total_comments: int
    llm_parsed: int
    regex_parsed: int
    total_cost: float
    cache_hits: int
    cache_misses: int
    cache_hit_rate: float
    average_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    success_rate: float
    error_count: int
```

### Metrics Collection

Collect metrics during operations:

```python
class MetricsCollector:
    def __init__(self):
        self.metrics = ParsingMetrics(
            total_comments=0,
            llm_parsed=0,
            regex_parsed=0,
            total_cost=0.0,
            cache_hits=0,
            cache_misses=0,
            cache_hit_rate=0.0,
            average_latency_ms=0.0,
            p95_latency_ms=0.0,
            p99_latency_ms=0.0,
            success_rate=0.0,
            error_count=0,
        )
        self.latencies: list[float] = []

    def record_parsing(self, method: str, latency_ms: float, cost: float = 0.0):
        """Record parsing operation."""
        self.metrics.total_comments += 1
        self.latencies.append(latency_ms)

        if method == "llm":
            self.metrics.llm_parsed += 1
            self.metrics.total_cost += cost
        else:
            self.metrics.regex_parsed += 1

    def record_cache_hit(self):
        """Record cache hit."""
        self.metrics.cache_hits += 1

    def record_cache_miss(self):
        """Record cache miss."""
        self.metrics.cache_misses += 1

    def record_error(self):
        """Record error."""
        self.metrics.error_count += 1

    def finalize(self) -> ParsingMetrics:
        """Calculate final metrics."""
        total_requests = self.metrics.cache_hits + self.metrics.cache_misses
        if total_requests > 0:
            self.metrics.cache_hit_rate = self.metrics.cache_hits / total_requests

        if self.latencies:
            sorted_latencies = sorted(self.latencies)
            self.metrics.average_latency_ms = sum(self.latencies) / len(self.latencies)
            self.metrics.p95_latency_ms = sorted_latencies[int(len(sorted_latencies) * 0.95)]
            self.metrics.p99_latency_ms = sorted_latencies[int(len(sorted_latencies) * 0.99)]

        if self.metrics.total_comments > 0:
            self.metrics.success_rate = (
                (self.metrics.total_comments - self.metrics.error_count)
                / self.metrics.total_comments
            )

        return self.metrics
```

## Success Rate Tracking

### Track Success Per Provider

Track success rate per LLM provider:

```python
@dataclass
class ProviderMetrics:
    """Metrics per LLM provider."""
    provider: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    success_rate: float
    average_latency_ms: float
    total_cost: float

class ProviderMetricsCollector:
    def __init__(self):
        self.providers: dict[str, ProviderMetrics] = {}

    def record_request(self, provider: str, success: bool, latency_ms: float, cost: float = 0.0):
        """Record provider request."""
        if provider not in self.providers:
            self.providers[provider] = ProviderMetrics(
                provider=provider,
                total_requests=0,
                successful_requests=0,
                failed_requests=0,
                success_rate=0.0,
                average_latency_ms=0.0,
                total_cost=0.0,
            )

        metrics = self.providers[provider]
        metrics.total_requests += 1
        if success:
            metrics.successful_requests += 1
        else:
            metrics.failed_requests += 1

        # Update average latency
        metrics.average_latency_ms = (
            (metrics.average_latency_ms * (metrics.total_requests - 1) + latency_ms)
            / metrics.total_requests
        )

        metrics.total_cost += cost
        metrics.success_rate = metrics.successful_requests / metrics.total_requests
```

## Latency Tracking (p95/p99)

### Percentile Calculation

Calculate latency percentiles:

```python
def calculate_percentiles(latencies: list[float]) -> dict[str, float]:
    """Calculate latency percentiles.

    Args:
        latencies: List of latency measurements in milliseconds

    Returns:
        Dictionary with p50, p95, p99 percentiles
    """
    if not latencies:
        return {"p50": 0.0, "p95": 0.0, "p99": 0.0}

    sorted_latencies = sorted(latencies)
    n = len(sorted_latencies)

    return {
        "p50": sorted_latencies[int(n * 0.50)],
        "p95": sorted_latencies[int(n * 0.95)],
        "p99": sorted_latencies[int(n * 0.99)],
    }
```

### Track Latency Per Operation

Track latency for each operation:

```python
import time

def parse_with_latency_tracking(parser, comment_body: str) -> tuple[list[ParsedChange], float]:
    """Parse comment and track latency.

    Returns:
        Tuple of (parsed_changes, latency_ms)
    """
    start_time = time.perf_counter()
    changes = parser.parse_comment(comment_body)
    end_time = time.perf_counter()

    latency_ms = (end_time - start_time) * 1000
    return changes, latency_ms
```

## Cost Tracking

### Track Cost Per Provider

Track costs separately per provider:

```python
class CostTracker:
    def __init__(self):
        self.costs: dict[str, float] = {}
        self.token_usage: dict[str, dict[str, int]] = {}

    def record_cost(self, provider: str, cost: float, input_tokens: int, output_tokens: int):
        """Record cost and token usage."""
        if provider not in self.costs:
            self.costs[provider] = 0.0
            self.token_usage[provider] = {"input": 0, "output": 0}

        self.costs[provider] += cost
        self.token_usage[provider]["input"] += input_tokens
        self.token_usage[provider]["output"] += output_tokens

    def get_total_cost(self) -> float:
        """Get total cost across all providers."""
        return sum(self.costs.values())

    def get_cost_summary(self) -> dict[str, Any]:
        """Get cost summary."""
        return {
            "total_cost": self.get_total_cost(),
            "by_provider": self.costs,
            "token_usage": self.token_usage,
        }
```

## Cache Hit Rate Tracking

### Track Cache Performance

Monitor cache hit rates:

```python
class CacheMetrics:
    def __init__(self):
        self.hits = 0
        self.misses = 0

    def record_hit(self):
        """Record cache hit."""
        self.hits += 1

    def record_miss(self):
        """Record cache miss."""
        self.misses += 1

    @property
    def hit_rate(self) -> float:
        """Calculate cache hit rate."""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    def get_stats(self) -> dict[str, Any]:
        """Get cache statistics."""
        return {
            "hits": self.hits,
            "misses": self.misses,
            "total_requests": self.hits + self.misses,
            "hit_rate": self.hit_rate,
        }
```

## Export Formats (JSON/CSV)

### JSON Export

Export metrics to JSON:

```python
import json
from pathlib import Path

def export_metrics_json(metrics: ParsingMetrics, output_path: Path) -> None:
    """Export metrics to JSON file.

    Args:
        metrics: Metrics to export
        output_path: Path to output JSON file
    """
    metrics_dict = {
        "total_comments": metrics.total_comments,
        "llm_parsed": metrics.llm_parsed,
        "regex_parsed": metrics.regex_parsed,
        "total_cost": metrics.total_cost,
        "cache_hits": metrics.cache_hits,
        "cache_misses": metrics.cache_misses,
        "cache_hit_rate": metrics.cache_hit_rate,
        "average_latency_ms": metrics.average_latency_ms,
        "p95_latency_ms": metrics.p95_latency_ms,
        "p99_latency_ms": metrics.p99_latency_ms,
        "success_rate": metrics.success_rate,
        "error_count": metrics.error_count,
    }

    with open(output_path, "w") as f:
        json.dump(metrics_dict, f, indent=2)
```

### CSV Export

Export metrics to CSV:

```python
import csv

def export_metrics_csv(metrics_list: list[ParsingMetrics], output_path: Path) -> None:
    """Export metrics list to CSV file.

    Args:
        metrics_list: List of metrics to export
        output_path: Path to output CSV file
    """
    fieldnames = [
        "total_comments", "llm_parsed", "regex_parsed", "total_cost",
        "cache_hits", "cache_misses", "cache_hit_rate",
        "average_latency_ms", "p95_latency_ms", "p99_latency_ms",
        "success_rate", "error_count",
    ]

    with open(output_path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for metrics in metrics_list:
            writer.writerow({
                "total_comments": metrics.total_comments,
                "llm_parsed": metrics.llm_parsed,
                "regex_parsed": metrics.regex_parsed,
                "total_cost": metrics.total_cost,
                "cache_hits": metrics.cache_hits,
                "cache_misses": metrics.cache_misses,
                "cache_hit_rate": metrics.cache_hit_rate,
                "average_latency_ms": metrics.average_latency_ms,
                "p95_latency_ms": metrics.p95_latency_ms,
                "p99_latency_ms": metrics.p99_latency_ms,
                "success_rate": metrics.success_rate,
                "error_count": metrics.error_count,
            })
```

## Related Rules

- See `.cursor/rules/cost-optimization.mdc` for cost tracking details
- See `.cursor/rules/llm-integration.mdc` for LLM metrics
