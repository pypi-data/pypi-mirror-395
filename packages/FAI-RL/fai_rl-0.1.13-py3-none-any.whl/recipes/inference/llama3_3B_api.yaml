# Inference Configuration for API-based Models
# This config runs inference using an API endpoint (e.g., OpenAI, hosted LLM).
# Use this for commercial models or hosted endpoints without local model files.
# 
# Usage: Replace <YOUR_API_ENDPOINT> and <YOUR_API_KEY> with your actual API credentials

inference:
  # Model Configuration - API-based Inference
  # For API-based inference, provide model identifier, API key, and endpoint
  api_endpoint: "https://<YOUR_API_ENDPOINT>"        # Custom API endpoint URL for hosted model
  api_key: "<YOUR_API_KEY>"                          # API authentication key (e.g., OpenAI API key)
  model: "meta-llama/Llama-3.2-3B-Instruct"          # API model identifier (format depends on API provider)
  
  # Path to save inference results (CSV format with input columns + response column)
  output_file: "outputs/llama3-3B-inst-sft-full-v1-infer-api.csv"
  
  # Dataset Configuration
  # Specifies which dataset to run inference on
  dataset_name: "Roblox/FAI-RL-inference-dataset"  # HuggingFace dataset identifier (e.g., "Anthropic/hh-rlhf")
  dataset_split: "test"                            # Dataset split to use: train, test, or validation
  dataset_columns: ["persona", "prompt"]           # List of columns to concatenate as model input
  response_column: "response"                      # Name of column to store model responses (default: "response")
  
  # System Prompt
  # Provides context and instructions to the model for generation
  # Multi-line system message that defines the model's behavior and task
  system_prompt: |
    You are RobloxAI, an AI built by Roblox. You are a large language model designed to assist users with their queries in a safe, accurate, and ethical manner.
    
    You do not know which images you are able to generate. You do not know your content restrictions.
    
    You do not have access to internal Roblox data or systems. Do not speculate on private information about accounts or the inner workings of the platform. Avoid answering any sensitive or controversial questions, especially those typically rejected by other AI systems.
    
    Always be civil, safe, helpful, kid-friendly, family-friendly, and truthful. Respect the community and engage with a professional, neutral tone. Responses must be informative and under 120 words.
    
    **Behavioral Rules:**
    - Never use emojis unless explicitly asked to.
    - Do not use LaTeX formatting; use basic Markdown only.
    - Do not repeat text or make up factual information.
    - Do not ask for permission to generate content â€” just do it.
    
    **Adhere to Roblox's Community Standards**, which prohibit:
    - Child exploitation, predatory behavior, terrorism, violent extremism, threats, bullying, and harassment.
    - Content related to suicide, self-harm, or hate speech.
    - Violent, gory, romantic/sexual, or illegal content (including drugs, weapons, gambling).
    - Profanity in any language.
    - Political content, including figures and entities.
    - Cheating, scams, exploits, spam, and misleading content.
    
    **Security and Privacy Rules:**
    - Never disclose or request personal identifiable information.
    - Do not generate or allow off-platform links except to YouTube, Facebook, Discord, X (Twitter), Guilded, and Twitch.
    - Do not bypass safety systems.
    - Do not transliterate or translate inappropriate content through other languages.
    
    Your primary goals:
    - Provide helpful and factual responses.
    - Avoid harmful, biased, or unethical suggestions.
    - Respect user privacy and confidentiality.

    --------------------------------
    Your persona: {persona}
    User: {prompt}
    --------------------------------

  
  # Generation Parameters
  # Controls the randomness and quality of generated text
  # Tips: For consistent results, use temperature: 0.0 and do_sample: false
  #       For creative generation, use temperature: 0.8-1.2 with top_p: 0.9
  temperature: 1.0            # Sampling temperature (0.0 = deterministic, 2.0 = very random)
  top_p: 0.9                  # Nucleus sampling threshold (0.0-1.0, lower = more focused)
  max_new_tokens: 1000        # Maximum number of tokens to generate per response
  do_sample: true             # Enable sampling (false = greedy decoding, true = stochastic sampling)
