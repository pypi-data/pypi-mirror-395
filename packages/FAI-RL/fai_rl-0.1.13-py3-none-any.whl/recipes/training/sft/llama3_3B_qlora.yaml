# Model Configuration
# Defines the base model and its loading parameters
model:
  base_model_name: "meta-llama/Llama-3.2-3B-Instruct"   # HuggingFace model name or local path
  torch_dtype: "bfloat16"                               # Data type for model weights (bfloat16/float16/float32)
  low_cpu_mem_usage: true                               # Reduce CPU memory usage during model loading
  load_in_8bit: false                                   # Enable 8-bit quantization
  load_in_4bit: true                                    # Enable 4-bit quantization (for QLoRA)
  use_flash_attention: false                            # Use Flash Attention for faster training (if supported)
  
  # QLoRA-specific quantization parameters (only when load_in_4bit: true)
  bnb_4bit_compute_dtype: "bfloat16"                    # Compute dtype for 4-bit quantization
  bnb_4bit_quant_type: "nf4"                            # Quantization type: "nf4" or "fp4"
  bnb_4bit_use_double_quant: true                       # Use double quantization for better accuracy
  
  # LoRA configuration (for parameter-efficient fine-tuning)
  use_lora: true                                        # Enable LoRA
  lora_r: 16                                            # LoRA rank (8 for LoRA, 16 for QLoRA)
  lora_alpha: 32                                        # LoRA alpha scaling factor (typically 2x rank)
  lora_dropout: 0.05                                    # LoRA dropout rate
  lora_target_modules:                                  # Target modules for LoRA
    - q_proj
    - v_proj
    - k_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  lora_bias: "none"                                     # Bias handling: "none", "all", or "lora_only"

# Data Configuration
# Specifies datasets and preprocessing settings
data:
  datasets: # YOUR TRAINING DATASET
    # Supports multiple datasets.
    # Add additional entries here for combined training across datasets.
    - name: "nvidia/Aegis-AI-Content-Safety-Dataset-2.0"  # HuggingFace dataset name/path (required)
      split: "train"                                      # Dataset split to use (default: "train")
      dataset_columns: ["prompt", "response", "prompt_label", "response_label"]  # Specify exact columns to keep
  
  # Text processing settings
  max_length: 2048                                        # Maximum sequence length for model input
  max_prompt_length: 1024                                 # Maximum length for prompts (rest reserved for responses)
  remove_unused_columns: false                            # Keep all dataset columns (set true to save memory)

  system_prompt: |
    You are a skilled content moderator responsible for evaluating the dialog below.
      
    Assess the dialogue between the `user` and the `response`.

    <BEGIN CONVERSATION>
    User: {prompt}
    Response: {response}
    <END CONVERSATION>

    Provide your safety assessment for the above conversation in the following JSON format. Include only the specified fields.
    {{
      "User Safety": "safe" or "unsafe",
      "Response Safety": "safe" or "unsafe"
    }}  

    Do not include anything other than the output JSON in your response.
    Output JSON:
    {{
      "User Safety": "{prompt_label}",
      "Response Safety": "{response_label}"
    }}

# Training Configuration
# Controls the training process and optimization settings
training:
  algorithm: "sft"                                        # Training algorithm: sft, dpo, ppo, grpo, gspo
  output_dir: "models/llama3-3B-inst-sft-qlora-v1"        # Directory to save trained model and checkpoints
  
  # Core training hyperparameters
  per_device_train_batch_size: 1                          # Batch size per GPU (adjust based on GPU memory)
  gradient_accumulation_steps: 16                         # Steps to accumulate gradients (effective batch = batch_size × accum_steps × num_gpus)
  learning_rate: 2.0e-4                                   # Learning rate (1e-5 for full/LoRA, 1e-4 to 2e-4 for QLoRA)
  num_train_epochs: 1                                     # Number of complete passes through the dataset
  max_steps: -1                                           # Maximum number of training steps (-1 = train for num_train_epochs)
  warmup_steps: 100                                       # Linear warmup steps for learning rate scheduler
  
  # Logging and checkpointing
  logging_steps: 5                                        # Log training metrics every N steps
  save_steps: 50                                          # Save model checkpoint every N steps
  eval_steps: 50                                          # Evaluate model every N steps (optional, used by DPO/GRPO/GSPO/SFT)
  
  # Memory and precision optimization
  bf16: true                                              # Use bfloat16 precision (recommended for modern GPUs)
  fp16: false                                             # Use float16 precision (alternative to bf16)
  gradient_checkpointing: true                            # Trade compute for memory (enables larger models/batch sizes)
  
  # Data loading optimization
  dataloader_num_workers: 0                               # Number of CPU workers for data loading (0 = main process only)
  dataloader_pin_memory: false                            # Pin memory for faster GPU transfer (set true if sufficient RAM)
  dataloader_drop_last: true                              # Drop last incomplete batch to ensure consistent batch sizes

# Weights & Biases Integration
# Optional experiment tracking and monitoring
wandb:
  enabled: false                                          # Enable W&B logging
  project: "rl"                                           # W&B project name
  entity: "fai-llmaas"                                    # W&B username or team name
  name: "Llama-3.2-3B-Inst-QLoRA-SFT"                     # Experiment name in W&B
  tags: ["SFT", "llama3", "3B-Inst", "QLoRA", "4-bit"]    # Tags for organizing experiments