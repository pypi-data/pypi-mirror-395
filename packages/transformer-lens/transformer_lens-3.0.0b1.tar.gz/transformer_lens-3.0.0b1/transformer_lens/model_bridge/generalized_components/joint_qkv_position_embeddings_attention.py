"""Joint QKV attention bridge with position embeddings support.

This module provides an attention bridge for models that use both:
1. Fused QKV matrices (like Pythia)
2. Position embeddings like RoPE (Rotary Position Embeddings)
"""
from __future__ import annotations

from typing import Any, Callable, Dict, Optional

import torch

from transformer_lens.hook_points import HookPoint
from transformer_lens.model_bridge.generalized_components.joint_qkv_attention import (
    JointQKVAttentionBridge,
)


class JointQKVPositionEmbeddingsAttentionBridge(JointQKVAttentionBridge):
    """Attention bridge for models with fused QKV and position embeddings (e.g., Pythia).

    This combines the functionality of JointQKVAttentionBridge (splitting fused QKV matrices)
    with position embeddings support (for models using RoPE).

    The position_embeddings are generated by calling the model's rotary_emb
    component with dummy Q/K tensors and position_ids.
    """

    def __init__(
        self,
        name: str,
        config: Any,
        split_qkv_matrix: Optional[Callable] = None,
        submodules: Optional[Dict[str, Any]] = None,
        **kwargs,
    ):
        """Initialize Joint QKV Position Embeddings attention bridge.

        Args:
            name: Component name
            config: Model configuration
            split_qkv_matrix: Optional function to split the qkv matrix
            submodules: Dictionary of subcomponents
            **kwargs: Additional arguments passed to JointQKVAttentionBridge
        """
        # Ensure position embeddings are required
        kwargs["requires_position_embeddings"] = True
        super().__init__(
            name=name,
            config=config,
            split_qkv_matrix=split_qkv_matrix,
            submodules=submodules,
            **kwargs,
        )
        self._rotary_emb = None
        # Add hooks for cos and sin to match HookedTransformer pattern
        self.hook_cos = HookPoint()
        self.hook_sin = HookPoint()

    def set_rotary_emb(self, rotary_emb: Any) -> None:
        """Set reference to the model's rotary embedding component.

        Args:
            rotary_emb: The model's rotary_emb component (from model.gpt_neox.layers[0].attention.rotary_emb)
        """
        self._rotary_emb = rotary_emb

    def _apply_position_embedding_hooks(self, position_embeddings):
        """Apply hooks to position embeddings (cos, sin tuple).

        Extracts cos and sin from the position_embeddings tuple and passes them
        through hook_cos and hook_sin to match HookedTransformer's behavior.

        Args:
            position_embeddings: Tuple of (cos, sin) tensors

        Returns:
            Tuple of (hooked_cos, hooked_sin) tensors
        """
        if isinstance(position_embeddings, tuple) and len(position_embeddings) == 2:
            cos, sin = position_embeddings
            # Apply hooks to match HookedTransformer's rotary_cos/rotary_sin pattern
            hooked_cos = self.hook_cos(cos)
            hooked_sin = self.hook_sin(sin)
            return (hooked_cos, hooked_sin)
        return position_embeddings

    def get_random_inputs(
        self,
        batch_size: int = 2,
        seq_len: int = 8,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> Dict[str, Any]:
        """Generate random inputs for component testing.

        For models using RoPE, position_embeddings are generated by calling rotary_emb
        which returns a tuple of (cos, sin) tensors.

        Args:
            batch_size: Batch size for generated inputs
            seq_len: Sequence length for generated inputs
            device: Device to place tensors on
            dtype: Dtype for generated tensors

        Returns:
            Dictionary with keys: hidden_states, position_embeddings, attention_mask
        """
        if device is None:
            device = torch.device("cpu")
        if dtype is None:
            dtype = torch.float32

        d_model = self.config.d_model if self.config and hasattr(self.config, "d_model") else 512
        inputs: Dict[str, Any] = {
            "hidden_states": torch.randn(batch_size, seq_len, d_model, device=device, dtype=dtype)
        }

        num_heads = (
            self.config.num_attention_heads
            if self.config and hasattr(self.config, "num_attention_heads")
            else self.config.n_heads
            if self.config and hasattr(self.config, "n_heads")
            else 8
        )
        head_dim = (
            self.config.head_dim
            if self.config and hasattr(self.config, "head_dim")
            else (d_model // num_heads)
        )

        # Generate position_embeddings using rotary_emb if available
        if self._rotary_emb is not None:
            try:
                # For GPT-NeoX/Pythia rotary embeddings
                # rotary_emb expects (seq_len, device) and returns (cos, sin)
                position_embeddings = self._rotary_emb(seq_len, device=device)
                inputs["position_embeddings"] = position_embeddings
            except Exception:
                # Fallback: create dummy cos/sin tensors
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                inputs["position_embeddings"] = (cos, sin)
        else:
            # Fallback: create dummy cos/sin tensors
            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
            inputs["position_embeddings"] = (cos, sin)

        inputs["attention_mask"] = None
        return inputs

    def _apply_rotary_pos_emb(
        self, q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Apply rotary position embeddings to query and key tensors.

        This implements the same logic as HuggingFace's apply_rotary_pos_emb.

        Args:
            q: Query tensor [batch, heads, seq_len, head_dim]
            k: Key tensor [batch, heads, seq_len, head_dim]
            cos: Cosine values [batch, seq_len, head_dim] or [1, seq_len, head_dim]
            sin: Sine values [batch, seq_len, head_dim] or [1, seq_len, head_dim]

        Returns:
            Tuple of (rotated_q, rotated_k)
        """
        # Add head dimension for broadcasting: [batch, 1, seq_len, head_dim]
        cos = cos.unsqueeze(1)
        sin = sin.unsqueeze(1)

        # Apply rotary embeddings
        # Split into rotary and passthrough dimensions
        rotary_dim = cos.shape[-1]
        q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]
        k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]

        # Apply rotation: q_embed = (q * cos) + (rotate_half(q) * sin)
        def rotate_half(x):
            """Rotates half the hidden dims of the input."""
            x1 = x[..., : x.shape[-1] // 2]
            x2 = x[..., x.shape[-1] // 2 :]
            return torch.cat((-x2, x1), dim=-1)

        q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)
        k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)

        # Concatenate with passthrough dimensions
        q_final = torch.cat([q_embed, q_pass], dim=-1) if q_pass.numel() > 0 else q_embed
        k_final = torch.cat([k_embed, k_pass], dim=-1) if k_pass.numel() > 0 else k_embed

        return q_final, k_final

    def _reconstruct_attention(
        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, **kwargs
    ) -> tuple:
        """Manual attention computation with rotary position embeddings.

        This overrides the parent class to apply rotary embeddings to Q and K
        before computing attention scores.
        """
        original_component = self.original_component
        assert original_component is not None
        assert self.config is not None
        num_heads = self.config.n_heads

        # Reshape Q, K, V to [batch, heads, seq_len, head_dim]
        if len(q.shape) == 3:
            batch_size, seq_len, hidden_size = q.shape
            head_dim: int = hidden_size // num_heads
            q = q.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
            k = k.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
            v = v.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)
        elif len(q.shape) == 4:
            batch_size, seq_len, num_heads_tensor, head_dim = q.shape
            assert (
                num_heads_tensor == num_heads
            ), f"Expected {num_heads} heads, got {num_heads_tensor}"
            q = q.transpose(1, 2)
            k = k.transpose(1, 2)
            v = v.transpose(1, 2)
            seq_len = q.shape[2]  # Update seq_len from transposed tensor
        else:
            raise ValueError(f"Unexpected Q tensor shape: {q.shape}. Expected 3D or 4D tensor.")

        # Apply rotary position embeddings if provided
        position_embeddings = kwargs.get("position_embeddings", None)
        if position_embeddings is not None and isinstance(position_embeddings, tuple):
            cos, sin = position_embeddings
            # Apply hooks to position embeddings
            hooked_position_embeddings = self._apply_position_embedding_hooks(position_embeddings)
            cos, sin = hooked_position_embeddings
            # Apply rotary embeddings to Q and K
            q, k = self._apply_rotary_pos_emb(q, k, cos, sin)

        # Compute attention scores
        scale = head_dim ** (-0.5)
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale

        # Apply causal mask
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=q.device))
        attn_scores = attn_scores.masked_fill(causal_mask == 0, float("-inf"))

        # Apply attention mask if provided
        attention_mask = kwargs.get("attention_mask", None)
        if attention_mask is not None:
            if attention_mask.shape[-1] != seq_len:
                attention_mask = attention_mask[..., :seq_len]
            if attention_mask.shape[-2] != seq_len:
                attention_mask = attention_mask[..., :seq_len, :]
            attn_scores = attn_scores + attention_mask

        # Apply hook to attention scores
        attn_scores = self.hook_attn_scores(attn_scores)

        # Compute attention weights
        attn_weights = torch.nn.functional.softmax(attn_scores, dim=-1)

        # Apply dropout if present
        if hasattr(original_component, "attn_dropout"):
            attn_weights = original_component.attn_dropout(attn_weights)  # type: ignore[operator]

        # Apply hook to attention pattern
        attn_weights = self.hook_pattern(attn_weights)

        # Compute attention output
        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(1, 2).contiguous()
        final_hidden_size: int = num_heads * head_dim
        attn_output = attn_output.view(batch_size, seq_len, final_hidden_size)

        # Apply output projection if present
        if hasattr(self, "o") and self.o is not None:
            attn_output = self.o(attn_output)

        return (attn_output, attn_weights)
