"""Gemma-2/3 specialized attention bridge.

Gemma-2/3 use Rotary Position Embeddings (RoPE) which requires special handling
to fire hook_rot_q and hook_rot_k with the correct post-rotation Q/K values.

This is achieved by wrapping HuggingFace's eager_attention_forward function
to intercept the query and key tensors after rotary embeddings have been applied.
"""
from __future__ import annotations

import weakref
from typing import Any, Callable, Dict, Optional

import torch
import transformers.models.gemma2.modeling_gemma2 as gemma2_module

from transformer_lens.hook_points import HookPoint
from transformer_lens.model_bridge.generalized_components.attention import (
    AttentionBridge,
)

# Global registry mapping HF attention modules to their bridge instances
# Uses WeakValueDictionary to avoid preventing garbage collection of bridges
_ATTENTION_BRIDGE_REGISTRY: weakref.WeakValueDictionary = weakref.WeakValueDictionary()

# Track whether we've already wrapped eager_attention_forward
_EAGER_ATTENTION_WRAPPED = False

# Store the original function for restoration
_ORIGINAL_EAGER_ATTENTION_FORWARD: Optional[Callable] = None


def _setup_eager_attention_hook_wrapper() -> None:
    """Wrap gemma2's eager_attention_forward to fire hook_rot_q and hook_rot_k.

    This function monkey-patches the module-level eager_attention_forward function
    to intercept query and key tensors (which have already had rotary embeddings applied)
    and fire the corresponding hooks on the registered bridge instance.

    This is safe to call multiple times - it will only wrap once.
    """
    global _EAGER_ATTENTION_WRAPPED, _ORIGINAL_EAGER_ATTENTION_FORWARD

    if _EAGER_ATTENTION_WRAPPED:
        return

    # Store the original function
    _ORIGINAL_EAGER_ATTENTION_FORWARD = gemma2_module.eager_attention_forward

    def hooked_eager_attention_forward(
        module: torch.nn.Module,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attention_mask: Optional[torch.Tensor],
        **kwargs: Any,
    ) -> tuple:
        """Wrapped eager_attention_forward that fires rotary hooks.

        Args:
            module: The HF attention module (used to look up the bridge)
            query: Query tensor AFTER rotary embeddings applied
            key: Key tensor AFTER rotary embeddings applied
            value: Value tensor
            attention_mask: Attention mask
            **kwargs: Additional arguments (dropout, scaling, etc.)

        Returns:
            Tuple of (attn_output, attn_weights)
        """
        # Look up the bridge instance for this attention module
        bridge = _ATTENTION_BRIDGE_REGISTRY.get(id(module))

        if bridge is not None:
            # Fire hook_rot_q and hook_rot_k with the post-rotary Q/K
            if hasattr(bridge, "hook_rot_q"):
                query = bridge.hook_rot_q(query)
            if hasattr(bridge, "hook_rot_k"):
                key = bridge.hook_rot_k(key)

        # Call the original function
        return _ORIGINAL_EAGER_ATTENTION_FORWARD(
            module, query, key, value, attention_mask, **kwargs
        )

    # Replace the module-level function
    gemma2_module.eager_attention_forward = hooked_eager_attention_forward  # type: ignore[assignment]
    _EAGER_ATTENTION_WRAPPED = True


class PositionEmbeddingsAttentionBridge(AttentionBridge):
    """Attention bridge for models that require position embeddings (e.g., Gemma-3).

    Some models use specialized position embedding systems (like Gemma-3's dual RoPE)
    which require position_embeddings to be generated in a specific format that differs
    from standard RoPE models.

    The position_embeddings are generated by calling the model's rotary_emb
    component with dummy Q/K tensors and position_ids.
    """

    def __init__(
        self, name: str, config: Any, submodules: Optional[Dict[str, Any]] = None, **kwargs
    ):
        """Initialize Gemma-3 attention bridge.

        Args:
            name: Component name
            config: Model configuration
            submodules: Dictionary of subcomponents
            **kwargs: Additional arguments passed to AttentionBridge
        """
        kwargs["requires_position_embeddings"] = True
        kwargs["requires_attention_mask"] = True
        kwargs["maintain_native_attention"] = True
        super().__init__(name, config, submodules, **kwargs)
        self._rotary_emb = None
        # Add hooks for cos and sin to match HookedTransformer pattern
        self.hook_cos = HookPoint()
        self.hook_sin = HookPoint()

    def set_rotary_emb(self, rotary_emb: Any) -> None:
        """Set reference to the model's rotary embedding component.

        Args:
            rotary_emb: The model's rotary_emb component (from model.model.rotary_emb)
        """
        self._rotary_emb = rotary_emb

    def set_original_component(self, component: torch.nn.Module) -> None:
        """Set the original HF component and register for rotary hook firing.

        This overrides the base class method to also:
        1. Register this bridge in the global registry (for hook_rot_q/hook_rot_k)
        2. Set up the eager_attention_forward wrapper if not already done

        Args:
            component: The HuggingFace attention module
        """
        super().set_original_component(component)

        # Register this bridge instance so the wrapped eager_attention_forward can find it
        _ATTENTION_BRIDGE_REGISTRY[id(component)] = self

        # Ensure the wrapper is set up
        _setup_eager_attention_hook_wrapper()

    def _apply_position_embedding_hooks(self, position_embeddings):
        """Apply hooks to position embeddings (cos, sin tuple).

        Extracts cos and sin from the position_embeddings tuple and passes them
        through hook_cos and hook_sin to match HookedTransformer's behavior.

        Args:
            position_embeddings: Tuple of (cos, sin) tensors

        Returns:
            Tuple of (hooked_cos, hooked_sin) tensors
        """
        if isinstance(position_embeddings, tuple) and len(position_embeddings) == 2:
            cos, sin = position_embeddings
            # Apply hooks to match HookedTransformer's rotary_cos/rotary_sin pattern
            hooked_cos = self.hook_cos(cos)
            hooked_sin = self.hook_sin(sin)
            return (hooked_cos, hooked_sin)
        return position_embeddings

    def get_random_inputs(
        self,
        batch_size: int = 2,
        seq_len: int = 8,
        device: Optional[torch.device] = None,
        dtype: Optional[torch.dtype] = None,
    ) -> Dict[str, Any]:
        """Generate random inputs for Gemma-3 attention testing.

        Gemma-3's position_embeddings are generated by calling rotary_emb(seq_len, device)
        which returns a tuple of (cos, sin) tensors with shape [seq_len, head_dim].

        Args:
            batch_size: Batch size for generated inputs
            seq_len: Sequence length for generated inputs
            device: Device to place tensors on
            dtype: Dtype for generated tensors

        Returns:
            Dictionary with keys: hidden_states, position_embeddings, attention_mask
        """
        if device is None:
            device = torch.device("cpu")
        if dtype is None:
            dtype = torch.float32
        d_model = self.config.d_model if self.config and hasattr(self.config, "d_model") else 1152
        inputs: Dict[str, Any] = {
            "hidden_states": torch.randn(batch_size, seq_len, d_model, device=device, dtype=dtype)
        }
        num_heads = (
            self.config.num_attention_heads
            if self.config and hasattr(self.config, "num_attention_heads")
            else 4
        )
        head_dim = self.config.head_dim if self.config and hasattr(self.config, "head_dim") else 256
        dummy_qk = torch.randn(1, seq_len, num_heads, head_dim, device=device, dtype=dtype)
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0)
        if self._rotary_emb is not None:
            try:
                position_embeddings = self._rotary_emb(dummy_qk, position_ids)
                inputs["position_embeddings"] = position_embeddings
            except Exception as e:
                cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
                sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
                inputs["position_embeddings"] = (cos, sin)
        else:
            cos = torch.ones(1, seq_len, head_dim, device=device, dtype=dtype)
            sin = torch.zeros(1, seq_len, head_dim, device=device, dtype=dtype)
            inputs["position_embeddings"] = (cos, sin)
        inputs["attention_mask"] = None
        return inputs
