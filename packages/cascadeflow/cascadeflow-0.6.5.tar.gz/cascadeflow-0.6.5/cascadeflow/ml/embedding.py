"""
Unified Embedding Service for cascadeflow

Provides a single embedding model (BGE-small-en-v1.5 via FastEmbed) for all
semantic tasks: quality validation, domain detection, complexity analysis, and
alignment scoring.

Features:
- Lazy initialization (only loads when first needed)
- Optional dependency (graceful degradation without FastEmbed)
- Request-scoped caching (50% latency reduction)
- ONNX-optimized inference (4x faster than PyTorch)
- ~40MB model size, <50ms latency per embedding

Example:
    >>> from cascadeflow.ml import UnifiedEmbeddingService, EmbeddingCache
    >>>
    >>> # Initialize service (lazy loads model)
    >>> embedder = UnifiedEmbeddingService()
    >>>
    >>> if embedder.is_available:
    ...     # Get similarity between two texts
    ...     similarity = embedder.similarity("query", "response")
    ...     print(f"Similarity: {similarity:.2%}")
    ...
    ...     # Use caching for multiple operations
    ...     cache = EmbeddingCache(embedder)
    ...     emb1 = cache.get_or_embed("query")
    ...     emb2 = cache.get_or_embed("query")  # Cached, no re-computation
"""

import logging
from typing import Any, Optional

# Optional dependency - numpy comes with FastEmbed
try:
    import numpy as np

    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False
    np = None

logger = logging.getLogger(__name__)


class UnifiedEmbeddingService:
    """
    Single embedding model for all semantic tasks in cascadeflow.

    Uses FastEmbed with BGE-small-en-v1.5 (ONNX optimized):
    - 45M parameters, 384 dimensions
    - ~40MB model size
    - ~20-30ms per embedding (CPU)
    - 91.8% MTEB score

    Lazy-loaded and optional - gracefully degrades if FastEmbed not available.
    """

    def __init__(self, model_name: str = "BAAI/bge-small-en-v1.5"):
        """
        Initialize embedding service (model loaded lazily on first use).

        Args:
            model_name: FastEmbed model name (default: BGE-small-en-v1.5)
        """
        self.model_name = model_name
        self._embedder = None
        self._is_available = None
        self._initialize_attempted = False

    @property
    def is_available(self) -> bool:
        """
        Check if embedding service is available.

        Returns:
            True if FastEmbed loaded successfully, False otherwise
        """
        if self._is_available is None and not self._initialize_attempted:
            self._lazy_initialize()
        return self._is_available or False

    def _lazy_initialize(self):
        """
        Lazy initialization - only loads model when first needed.

        This defers the ~200-500ms model load time until first use,
        and allows the service to remain available even if FastEmbed
        is not installed.
        """
        if self._initialize_attempted:
            return

        self._initialize_attempted = True

        try:
            from fastembed import TextEmbedding

            logger.info(f"Loading embedding model: {self.model_name}")
            self._embedder = TextEmbedding(model_name=self.model_name)
            self._is_available = True
            logger.info("Embedding service initialized successfully")

        except ImportError:
            logger.warning("FastEmbed not available. Install with: pip install fastembed")
            self._is_available = False

        except Exception as e:
            logger.error(f"Failed to initialize embedding service: {e}")
            self._is_available = False

    def embed(self, text: str) -> Optional[Any]:
        """
        Get embedding for a single text.

        Args:
            text: Text to embed

        Returns:
            384-dimensional embedding vector, or None if service unavailable
        """
        if not self.is_available or not HAS_NUMPY:
            return None

        try:
            # FastEmbed returns a generator, get first result
            embeddings = list(self._embedder.embed([text]))
            return embeddings[0] if embeddings else None

        except Exception as e:
            logger.error(f"Error generating embedding: {e}")
            return None

    def embed_batch(self, texts: list[str]) -> Optional[list[Any]]:
        """
        Get embeddings for multiple texts (batching for efficiency).

        Batching is ~30% faster than individual calls:
        - Single: 25ms Ã— 2 = 50ms
        - Batch: 35ms total

        Args:
            texts: List of texts to embed

        Returns:
            List of embedding vectors, or None if service unavailable
        """
        if not self.is_available or not HAS_NUMPY:
            return None

        try:
            embeddings = list(self._embedder.embed(texts))
            return embeddings

        except Exception as e:
            logger.error(f"Error generating batch embeddings: {e}")
            return None

    def similarity(self, text1: str, text2: str) -> Optional[float]:
        """
        Compute cosine similarity between two texts.

        Args:
            text1: First text
            text2: Second text

        Returns:
            Similarity score [0.0, 1.0], or None if service unavailable
        """
        if not self.is_available:
            return None

        # Use batch embedding for efficiency (35ms vs 50ms)
        embeddings = self.embed_batch([text1, text2])
        if embeddings is None or len(embeddings) != 2:
            return None

        return self._cosine_similarity(embeddings[0], embeddings[1])

    @staticmethod
    def _cosine_similarity(vec1: Any, vec2: Any) -> float:
        """
        Compute cosine similarity between two vectors.

        Args:
            vec1: First embedding vector
            vec2: Second embedding vector

        Returns:
            Similarity score [0.0, 1.0]
        """
        if not HAS_NUMPY:
            return 0.0

        # Normalize vectors
        vec1_norm = vec1 / (np.linalg.norm(vec1) + 1e-8)
        vec2_norm = vec2 / (np.linalg.norm(vec2) + 1e-8)

        # Compute cosine similarity
        similarity = np.dot(vec1_norm, vec2_norm)

        # Clamp to [0, 1] (cosine can be [-1, 1], but we only care about positive)
        return float(max(0.0, min(1.0, similarity)))


class EmbeddingCache:
    """
    Request-scoped cache for embeddings.

    Reduces latency by 50% when the same text is embedded multiple times
    within a single request (e.g., query embedded for domain detection,
    complexity analysis, and quality validation).

    Example:
        >>> cache = EmbeddingCache(embedder)
        >>>
        >>> # First call: computes embedding (~25ms)
        >>> emb1 = cache.get_or_embed("What is Python?")
        >>>
        >>> # Second call: returns cached (<1ms)
        >>> emb2 = cache.get_or_embed("What is Python?")
        >>>
        >>> # Different text: computes new embedding
        >>> emb3 = cache.get_or_embed("Explain quantum physics")
    """

    def __init__(self, embedder: UnifiedEmbeddingService):
        """
        Initialize cache with an embedding service.

        Args:
            embedder: UnifiedEmbeddingService instance
        """
        self.embedder = embedder
        self._cache: dict[str, Any] = {}

    def get_or_embed(self, text: str) -> Optional[Any]:
        """
        Get embedding from cache or compute if not cached.

        Args:
            text: Text to embed

        Returns:
            Embedding vector, or None if service unavailable
        """
        if text in self._cache:
            return self._cache[text]

        embedding = self.embedder.embed(text)
        if embedding is not None:
            self._cache[text] = embedding

        return embedding

    def similarity(self, text1: str, text2: str) -> Optional[float]:
        """
        Compute similarity with caching.

        Args:
            text1: First text
            text2: Second text

        Returns:
            Similarity score [0.0, 1.0], or None if service unavailable
        """
        emb1 = self.get_or_embed(text1)
        emb2 = self.get_or_embed(text2)

        if emb1 is None or emb2 is None:
            return None

        return self.embedder._cosine_similarity(emb1, emb2)

    def clear(self):
        """Clear the cache (e.g., at the end of a request)."""
        self._cache.clear()

    def cache_size(self) -> int:
        """Get number of cached embeddings."""
        return len(self._cache)

    def cache_info(self) -> dict[str, Any]:
        """Get cache statistics."""
        return {
            "size": len(self._cache),
            "texts": list(self._cache.keys())[:5],  # First 5 for debugging
        }
