"""
Proxy module for Binary Semantic Cache.

Provides an OpenAI-compatible HTTP proxy that transparently caches LLM responses.

Contains:
- server: FastAPI application factory
- routes: OpenAI-compatible endpoints
- middleware: Caching logic
"""

# Imports will be added by Engineer after implementation

