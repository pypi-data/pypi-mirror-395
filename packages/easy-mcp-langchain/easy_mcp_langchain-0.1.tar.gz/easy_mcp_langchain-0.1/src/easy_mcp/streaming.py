from langchain_core.messages import HumanMessage

async def run_agent_with_streaming(app,query:str):
    """
    é€šç”¨æµå¼è¿è¡Œå™¨ï¼Œè´Ÿè´£å°† LangGraph çš„è¿è¡Œè¿‡ç¨‹å¯è§†åŒ–è¾“å‡ºåˆ°æ§åˆ¶å°

    :param app: ç¼–è¯‘å¥½çš„ LangGraph åº”ç”¨ (workflow.compile())
    :param query: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
    """
    print(f'\nç”¨æˆ·:{query}\n')
    print("ğŸ¤– AI:",end="",flush=True)

    # æ„é€ è¾“å…¥æ¶ˆæ¯
    inputs = {"messages":[HumanMessage(content=query)]}

    # æ ¸å¿ƒ:ç›‘å¬v2ç‰ˆæœ¬çš„äº‹ä»¶æµ(ç›¸æ¯”v1æ›´å…¨é¢)
    async for event in app.astream_events(inputs,version="v2"):
        kind = event["event"]

        # 1.ç›‘å¬LLMçš„æµå¼åå­—(å˜´åœ¨åŠ¨)
        if kind == "on_chat_model_stream":
            chunk = event["data"]["chunk"]
            # è¿‡æ»¤æ‰ç©ºçš„chunk(æœ‰æ—¶å·¥å…·è°ƒç”¨ä¼šäº§ç”Ÿç©ºå†…å®¹)
            if chunk.content:
                print(chunk.content,end="",flush=True)

        # 2.ç›‘å¬å·¥å…·å¼€å§‹è°ƒç”¨(æ‰‹åœ¨åŠ¨)
        elif kind == "on_tool_start":
            tool_name = event["name"]
            # ä¸æ‰“å°å†…éƒ¨åŒ…è£…ï¼Œåªæ‰“å°è‡ªå®šä¹‰çš„å·¥å…·
            if not tool_name.startswith("_"):
                print(f"\n\nğŸ”¨ æ­£åœ¨è°ƒç”¨å·¥å…·: {tool_name} ...")

        # 3.ç›‘å¬å·¥å…·è°ƒç”¨ç»“æŸ(æ‹¿åˆ°ç»“æœ)
        elif kind == "on_tool_end":
            tool_name = event["name"]
            if not tool_name.startswith("_"):
                print(f"âœ… è°ƒç”¨å®Œæˆï¼Œç»§ç»­æ€è€ƒ...\n")
                print("ğŸ¤– AI: ", end="", flush=True)
    print("\n\nğŸ˜Š è¾“å‡ºç»“æŸ!")