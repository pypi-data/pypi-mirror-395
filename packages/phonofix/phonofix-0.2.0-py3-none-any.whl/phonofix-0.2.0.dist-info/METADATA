Metadata-Version: 2.4
Name: phonofix
Version: 0.2.0
Summary: Multilingual phonetic-similarity replacement engine â€” a proper-noun substitution tool based on phonetic similarity, supporting ASR/LLM post-processing.
Project-URL: Homepage, https://github.com/JonesHong/phonofix
Project-URL: Repository, https://github.com/JonesHong/phonofix
Project-URL: Documentation, https://github.com/JonesHong/phonofix#readme
Author-email: JonesHong <latte831104@gmail.com>
License: MIT
Keywords: asr,chinese,english,llm,nlp,phonetic,phonofix,speech-recognition,text-correction
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Text Processing :: Linguistic
Requires-Python: >=3.9
Requires-Dist: hanziconv>=0.3.2
Requires-Dist: phonemizer>=3.3.0
Requires-Dist: pinyin2hanzi>=0.1.1
Requires-Dist: pypinyin>=0.44.0
Requires-Dist: python-levenshtein>=0.12.2
Provides-Extra: all
Requires-Dist: hanziconv>=0.3.2; extra == 'all'
Requires-Dist: phonemizer>=3.3.0; extra == 'all'
Requires-Dist: pinyin2hanzi>=0.1.1; extra == 'all'
Requires-Dist: pypinyin>=0.44.0; extra == 'all'
Provides-Extra: ch
Requires-Dist: hanziconv>=0.3.2; extra == 'ch'
Requires-Dist: pinyin2hanzi>=0.1.1; extra == 'ch'
Requires-Dist: pypinyin>=0.44.0; extra == 'ch'
Provides-Extra: dev
Requires-Dist: mypy>=1.0.0; extra == 'dev'
Requires-Dist: pytest-cov>=4.0.0; extra == 'dev'
Requires-Dist: pytest>=7.0.0; extra == 'dev'
Requires-Dist: ruff>=0.1.0; extra == 'dev'
Provides-Extra: en
Requires-Dist: phonemizer>=3.3.0; extra == 'en'
Description-Content-Type: text/markdown

**English** | [ç¹é«”ä¸­æ–‡](README.zh-TW.md)

# Phonetic Substitution Engine

A multi-language phonetic similarity-based proper noun substitution tool. Supports ASR/LLM post-processing, regional vocabulary conversion, abbreviation expansion, and various other use cases.
Specially optimized for **Code-Switching** (mixed Chinese-English) scenarios.

## ğŸ’¡ Core Philosophy

**This package does not maintain any proper noun dictionaries; instead, it provides a substitution engine based on phonetic vector space.**

The core mechanism of this package is to uniformly map text from different languages into a **phonetic vector space** (composed of Pinyin and IPA phonetic symbols).

Whether it's spelling errors from ASR (Automatic Speech Recognition), LLM (Large Language Models), or other scenariosâ€”typically caused by rare proper nouns leading to incorrect character selectionâ€”this tool converts them to the **Pinyin/IPA dimension**.

The system then compares these converted phonetic features with **user-provided proper nouns** (plus system-generated fuzzy phonetic variants) to calculate probability and precisely replace spelling errors.

> âš ï¸ **Note**: This is not a full-text spell checker, but focuses on "phonetic similarity substitution for proper nouns."

Users must provide their own proper noun dictionary. This tool will:
1. **Automatically generate phonetic variants**:
   - **Chinese**: Automatically generate Taiwanese accent/fuzzy phonetic variants (e.g., "åŒ—è»Š" â†’ "å°åŒ—è»Šç«™")
   - **English**: Calculate phonetic similarity based on IPA (International Phonetic Alphabet) (e.g., "Ten so floor" â†’ "TensorFlow")
2. **Intelligent vocabulary substitution**: Automatically identify language segments and replace phonetically similar words with your specified standard proper nouns

**Use Cases**:
- **ASR Post-Processing**: Correct proper noun errors from speech-to-text (including mixed Chinese-English)
- **LLM Output Post-Processing**: Correct homophone/near-homophone errors when LLMs choose wrong characters for rare proper nouns
- **Proper Noun Standardization**: Restore colloquial/misspelled terms to their formal names
- **Regional Vocabulary Conversion**: Mainland China terms â†” Taiwan terms

## ğŸ“š Features

### 1. Multi-Language Support
- **Unified Corrector**: Single entry point, automatically handles mixed Chinese-English text
- **English Phonetic Substitution**: 
    - Uses IPA (International Phonetic Alphabet) for phonetic similarity matching
    - Supports phonetic restoration of acronyms
- **Chinese Phonetic Substitution**:
    - Uses Pinyin for fuzzy phonetic matching
    - Supports Taiwanese Mandarin-specific pronunciation confusion patterns

### 2. Automatic Phonetic Variant Generation
- **Chinese**: Automatically generates Taiwanese accent/fuzzy phonetic variants
  - Retroflex consonants (z/zh, c/ch, s/sh)
  - n/l confusion (Taiwanese Mandarin)
  - r/l confusion, f/h confusion
  - Final vowel confusion (in/ing, en/eng, ue/ie, etc.)
- **English**: Automatically generates common ASR/LLM error variants
  - Syllable split variants ("TensorFlow" â†’ "Ten so floor")
  - Acronym expansion variants ("AWS" â†’ "A W S")

### 3. Intelligent Substitution Engine
- Sliding window matching algorithm
- Context keyword weighting mechanism
- Dynamic tolerance rate adjustment

### 4. Streaming Support (ASR/LLM Streaming)
- **Accumulated Mode** (`StreamingCorrector`): For Realtime ASR
  - Supports continuous updates of accumulated text
  - Automatically detects new paragraphs and resets cache
- **Chunk Mode** (`ChunkStreamingCorrector`): For LLM Streaming
  - Incremental input, real-time output of confirmed corrections
  - Preserves overlap region to prevent word truncation

## ğŸ“¦ Installation

### Using uv (Recommended)

[uv](https://docs.astral.sh/uv/) is the next-generation Python package manager, fast and feature-complete.

```bash
# Install uv (Windows PowerShell)
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Install uv (macOS/Linux)
curl -LsSf https://astral.sh/uv/install.sh | sh
```

```bash
# Default installation (includes both Chinese and English support)
uv add phonofix

# Chinese support only
uv add "phonofix[ch]"

# English support only (requires espeak-ng installation, see below)
uv add "phonofix[en]"

# Full installation (same as default)
uv add "phonofix[all]"
```

### English Support (espeak-ng Installation)

English phonetic features depend on the [espeak-ng](https://github.com/espeak-ng/espeak-ng) system package.

**Using Built-in Installation Scripts (Recommended)**:

This project provides automated installation scripts that will download, install, and configure environment variables automatically:

```bash
# Windows PowerShell (recommended to run as Administrator)
.\scripts\setup_espeak.ps1

# Windows CMD (recommended to run as Administrator)
scripts\setup_espeak_windows.bat

# macOS / Linux
chmod +x scripts/setup_espeak.sh
./scripts/setup_espeak.sh
```

The scripts will automatically:
1. Check/install espeak-ng
2. Set the `PHONEMIZER_ESPEAK_LIBRARY` environment variable
3. Verify that phonemizer works correctly

**Manual Installation**:

```bash
# Windows: Download installer from GitHub
# https://github.com/espeak-ng/espeak-ng/releases

# macOS
brew install espeak-ng

# Debian/Ubuntu
sudo apt install espeak-ng

# Arch Linux
sudo pacman -S espeak-ng
```

### Development Environment Setup

```bash
# Install dependencies after cloning
uv sync

# Install dev dependencies
uv sync --dev

# Run examples
uv run python examples/chinese_examples.py

# Run tests
uv run pytest
```

## ğŸ§ª Development

```bash
# Run tests
uv run pytest

# Run tests with coverage
uv run pytest --cov

# Code formatting
uv run ruff format .

# Code linting
uv run ruff check .

# Type checking
uv run mypy src/phonofix
```

## ğŸš€ Quick Start

### 1. Mixed Language Substitution (Unified Corrector)

```python
from phonofix import UnifiedEngine

# Define your proper noun dictionary
terms = [
    "å°åŒ—è»Šç«™",      # Chinese word
    "TensorFlow",   # English proper noun
    "Python"
]

# Initialize engine (singleton pattern, recommended to initialize only once globally)
engine = UnifiedEngine()

# Create corrector
corrector = engine.create_corrector(terms)

# ASR output post-processing
asr_text = "æˆ‘åœ¨åŒ—è»Šç”¨Pytonå¯«Ten so floorçš„code"
result = corrector.correct(asr_text)
print(result)
# Output: "æˆ‘åœ¨å°åŒ—è»Šç«™ç”¨Pythonå¯«TensorFlowçš„code"

# LLM output post-processing (LLM may choose wrong homophones for rare words)
llm_text = "æˆ‘åœ¨åŒ—è»Šç”¨æ´¾æ£®å¯«code"  # LLM transliterated Python as "æ´¾æ£®"
result = corrector.correct(llm_text)
print(result)
# Output: "æˆ‘åœ¨å°åŒ—è»Šç«™ç”¨Pythonå¯«code"
```

### 2. Chinese Only (Chinese Engine)

**Important Note**: This tool does not provide a default dictionary. You need to create your own proper noun list based on your business scenario.

#### Recommended Usage - Auto-generate Aliases

Using `ChineseEngine`, **you only need to provide your proper noun list**, and the tool will automatically generate all possible fuzzy phonetic variants with Pinyin deduplication:

#### Simplest Format - Keyword List Only

```python
from phonofix import ChineseEngine

# Step 1: Provide your proper noun list (this is the dictionary you need to maintain)
my_terms = ["å°åŒ—è»Šç«™", "ç‰›å¥¶", "ç™¼æ®"]

# Step 2: Initialize engine and create corrector
# The tool will automatically generate all possible fuzzy phonetic variants
# For example: "å°åŒ—è»Šç«™" â†’ automatically generates "åŒ—è»Š", "è‡ºåŒ—è»Šç«™", etc.
engine = ChineseEngine()
corrector = engine.create_corrector(my_terms)

# Step 3: Automatically convert phonetically similar words to correct proper nouns
result = corrector.correct("æˆ‘åœ¨åŒ—è»Šè²·äº†æµå¥¶,ä»–èŠ±æ®äº†æ‰èƒ½")
# Result: 'æˆ‘åœ¨å°åŒ—è»Šç«™è²·äº†ç‰›å¥¶,ä»–ç™¼æ®äº†æ‰èƒ½'
# Explanation: "åŒ—è»Š" â†’ "å°åŒ—è»Šç«™", "æµå¥¶" â†’ "ç‰›å¥¶", "èŠ±æ®" â†’ "ç™¼æ®"
```

#### Full Format - Aliases + Keywords + Weights

When the same alias may correspond to multiple proper nouns, use context keywords and weights to improve accuracy:

```python
# Your proper noun dictionary (maintain based on your business scenario)
my_business_terms = {
    "æ°¸å’Œè±†æ¼¿": {
        "aliases": ["æ°¸è±†", "å‹‡è±†"],  # Manually provide common aliases or mispronunciations
        "keywords": ["åƒ", "å–", "è²·", "å®µå¤œ"],  # Context keywords help with judgment
        "weight": 0.3  # Matching weight
    },
    "å‹‡è€…é¬¥æƒ¡é¾": {
        "aliases": ["å‹‡é¬¥", "æ°¸é¬¥"],  # Homophones with different meanings
        "keywords": ["ç©", "éŠæˆ²", "æ”»ç•¥"],
        "weight": 0.2
    }
}

engine = ChineseEngine()
corrector = engine.create_corrector(my_business_terms)

result = corrector.correct("æˆ‘å»è²·å‹‡é¬¥ç•¶å®µå¤œ")
# Result: 'æˆ‘å»è²·æ°¸å’Œè±†æ¼¿ç•¶å®µå¤œ'
# Explanation: Matched "è²·" and "å®µå¤œ" keywords, determined to be "æ°¸å’Œè±†æ¼¿" instead of "å‹‡è€…é¬¥æƒ¡é¾"
```

**Advantages**:
- âœ… Automatically generates fuzzy phonetic variants, no manual maintenance required
- âœ… Automatically filters Pinyin-duplicate aliases (similar to Set behavior)
- âœ… Supports multiple input formats, flexible usage
- âœ… Reduces configuration effort, focus on core vocabulary

### Advanced Features

#### Context Keywords

```python
from phonofix import ChineseEngine

# Use context keywords to improve accuracy
engine = ChineseEngine()
corrector = engine.create_corrector({
    "æ°¸å’Œè±†æ¼¿": {
        "aliases": ["æ°¸è±†"],
        "keywords": ["åƒ", "å–", "è²·", "å®µå¤œ", "æ—©é¤"]
    },
    "å‹‡è€…é¬¥æƒ¡é¾": {
        "aliases": ["å‹‡é¬¥"],
        "keywords": ["ç©", "éŠæˆ²", "é›»å‹•", "æ”»ç•¥"]
    }
})

result = corrector.correct("æˆ‘å»è²·å‹‡é¬¥ç•¶å®µå¤œ")  # Matched "è²·" â†’ æ°¸å’Œè±†æ¼¿
result = corrector.correct("é€™æ¬¾æ°¸è±†çš„æ”»ç•¥å¾ˆé›£æ‰¾")  # Matched "æ”»ç•¥" â†’ å‹‡è€…é¬¥æƒ¡é¾
```

#### Weight System

```python
# Use weights to increase priority
engine = ChineseEngine()
corrector = engine.create_corrector({
    "æ©å…¸": {
        "aliases": ["å®‰é»"],
        "weight": 0.3  # High weight, priority matching
    },
    "ä¸Šå¸": {
        "aliases": ["ä¸Šå¸"],
        "weight": 0.1  # Low weight
    }
})
```

#### Protected Terms

```python
# Set protected terms list to prevent specific words from being corrected
engine = ChineseEngine()
corrector = engine.create_corrector(
    terms={
        "å°åŒ—è»Šç«™": ["åŒ—è»Š"]
    },
    protected_terms=["åŒ—å´", "å—å´"]  # These words will not be corrected
)

result = corrector.correct("æˆ‘åœ¨åŒ—å´ç­‰ä½ ")  # Will not be corrected to "å°åŒ—è»Šç«™å´"
```

### 3. Streaming Processing (ASR/LLM Streaming)

#### Realtime ASR Streaming

For real-time subtitle scenarios in speech recognition, passing accumulated complete recognition results each time:

```python
from phonofix import ChineseEngine, StreamingCorrector

engine = ChineseEngine()
corrector = engine.create_corrector(["å°åŒ—è»Šç«™", "ç‰›å¥¶"])

# Create streaming processor
streamer = StreamingCorrector(corrector, overlap_size=8)

# Simulate ASR accumulated input
asr_outputs = [
    "æˆ‘åœ¨èƒåŒ—",
    "æˆ‘åœ¨èƒåŒ—è»Šç«™",
    "æˆ‘åœ¨èƒåŒ—è»Šç«™è²·äº†æµ",
    "æˆ‘åœ¨èƒåŒ—è»Šç«™è²·äº†æµå¥¶",
]

for text in asr_outputs:
    result = streamer.feed(text)
    print(f"Confirmed: {result.confirmed} | Pending: {result.pending}")

# Get complete result at the end
final = streamer.finalize()
print(f"Final: {final}")
# Final: æˆ‘åœ¨å°åŒ—è»Šç«™è²·äº†ç‰›å¥¶
```

#### LLM Streaming Output

For LLM streaming output, passing new chunks each time:

```python
from phonofix import ChineseEngine, ChunkStreamingCorrector

engine = ChineseEngine()
corrector = engine.create_corrector(["è–éˆ", "è–ç¶“", "æ©å…¸"])

# Create chunk mode streaming processor
streamer = ChunkStreamingCorrector(corrector, overlap_size=6)

# Simulate LLM streaming output
llm_chunks = ["è–æ—", "å€Ÿè‘—é»˜æ°", "å¯«äº†é€™æœ¬", "ç”Ÿç¶“ï¼Œ", "æ˜¯å®‰é»ã€‚"]

for chunk in llm_chunks:
    result = streamer.feed_chunk(chunk)
    if result.confirmed:
        print(result.confirmed, end="", flush=True)  # Real-time output

# Output remaining part at the end
remaining = streamer.finalize()
print(remaining)
# Output: è–éˆå€Ÿè‘—é»˜æ°å¯«äº†é€™æœ¬è–ç¶“ï¼Œæ˜¯æ©å…¸ã€‚
```

#### WebSocket Real-World Application

```python
from phonofix import ChineseEngine, StreamingCorrector
import json

engine = ChineseEngine()
corrector = engine.create_corrector(my_terms)

async def handle_asr_websocket(websocket):
    streamer = StreamingCorrector(corrector, overlap_size=10)
    
    async for message in websocket:
        data = json.loads(message)
        
        if data["type"] == "partial":
            result = streamer.feed(data["text"])
            await websocket.send(json.dumps({
                "confirmed": result.confirmed,
                "pending": result.pending,
            }))
            
        elif data["type"] == "final":
            final = streamer.finalize()
            await websocket.send(json.dumps({"final": final}))
            streamer.reset()  # Reset, ready for next segment
```

## ğŸ“ Project Structure

```
phonofix/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ phonofix/                      # Main package (src layout)
â”‚       â”œâ”€â”€ __init__.py                # Main entry, exports UnifiedEngine, ChineseEngine, etc.
â”‚       â”‚
â”‚       â”œâ”€â”€ engine/                    # Engine layer (singleton pattern entry)
â”‚       â”‚   â”œâ”€â”€ base.py                # BaseEngine abstract class
â”‚       â”‚   â”œâ”€â”€ unified_engine.py      # UnifiedEngine - mixed language
â”‚       â”‚   â”œâ”€â”€ chinese_engine.py      # ChineseEngine - Chinese only
â”‚       â”‚   â””â”€â”€ english_engine.py      # EnglishEngine - English only
â”‚       â”‚
â”‚       â”œâ”€â”€ backend/                   # Phonetic backend (phonemizer/pypinyin wrapper)
â”‚       â”‚   â”œâ”€â”€ base.py                # PhoneticBackend abstract class
â”‚       â”‚   â”œâ”€â”€ chinese_backend.py     # Chinese Pinyin backend
â”‚       â”‚   â””â”€â”€ english_backend.py     # English IPA backend
â”‚       â”‚
â”‚       â”œâ”€â”€ correction/                # Corrector layer
â”‚       â”‚   â”œâ”€â”€ protocol.py            # CorrectorProtocol definition
â”‚       â”‚   â”œâ”€â”€ unified_corrector.py   # Mixed language corrector
â”‚       â”‚   â””â”€â”€ streaming_corrector.py # Streaming corrector (ASR/LLM)
â”‚       â”‚
â”‚       â”œâ”€â”€ languages/                 # Language-specific implementations
â”‚       â”‚   â”œâ”€â”€ chinese/               # Chinese module
â”‚       â”‚   â”‚   â”œâ”€â”€ config.py          # Pinyin config (initials/finals/fuzzy sounds)
â”‚       â”‚   â”‚   â”œâ”€â”€ corrector.py       # Chinese corrector
â”‚       â”‚   â”‚   â”œâ”€â”€ fuzzy_generator.py # Fuzzy phonetic variant generator
â”‚       â”‚   â”‚   â”œâ”€â”€ number_variants.py # Number variant handling
â”‚       â”‚   â”‚   â””â”€â”€ tokenizer.py       # Chinese tokenizer
â”‚       â”‚   â”‚
â”‚       â”‚   â””â”€â”€ english/               # English module
â”‚       â”‚       â”œâ”€â”€ config.py          # IPA config
â”‚       â”‚       â”œâ”€â”€ corrector.py       # English corrector
â”‚       â”‚       â”œâ”€â”€ fuzzy_generator.py # Syllable split variant generator
â”‚       â”‚       â””â”€â”€ tokenizer.py       # English tokenizer
â”‚       â”‚
â”‚       â”œâ”€â”€ router/                    # Language router
â”‚       â”‚   â””â”€â”€ language_router.py     # Auto-detect Chinese/English segments
â”‚       â”‚
â”‚       â””â”€â”€ utils/                     # Utility modules
â”‚           â”œâ”€â”€ lazy_imports.py        # Lazy imports (optional dependency management)
â”‚           â””â”€â”€ logger.py              # Logging utilities
â”‚
â”œâ”€â”€ scripts/                           # Installation scripts
â”‚   â”œâ”€â”€ setup_espeak.ps1               # Windows PowerShell espeak-ng installer
â”‚   â”œâ”€â”€ setup_espeak.sh                # macOS/Linux espeak-ng installer
â”‚   â””â”€â”€ setup_espeak_windows.bat       # Windows CMD espeak-ng installer
â”‚
â”œâ”€â”€ examples/                          # Usage examples
â”‚   â”œâ”€â”€ chinese_examples.py            # Chinese correction examples
â”‚   â”œâ”€â”€ english_examples.py            # English correction examples
â”‚   â”œâ”€â”€ mixed_language_examples.py     # Mixed language examples
â”‚   â”œâ”€â”€ streaming_demo.py              # Streaming processing examples
â”‚   â””â”€â”€ timing_demo.py                 # Performance timing examples
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”‚   â”œâ”€â”€ test_chinese_corrector.py
â”‚   â”œâ”€â”€ test_english_corrector.py
â”‚   â””â”€â”€ test_unified_corrector.py
â”‚
â”œâ”€â”€ pyproject.toml                     # Project configuration (phonofix)
â”œâ”€â”€ requirements.txt                   # Dependency list
â””â”€â”€ README.md
```

## ğŸ¯ Use Cases

The following examples demonstrate how to create your own proper noun dictionary for different business scenarios:

### 1. ASR Post-Processing

**Problem**: Speech recognition often mishears proper nouns as phonetically similar common words

```python
# Your proper noun dictionary
terms = ["ç‰›å¥¶", "ç™¼æ®", "ç„¶å¾Œ", "TensorFlow", "Kubernetes"]

engine = UnifiedEngine()
corrector = engine.create_corrector(terms)

# ASR output: proper nouns misheard
asr_output = "æˆ‘è²·äº†æµå¥¶ï¼Œè˜­å¾Œç”¨Ten so floorè¨“ç·´æ¨¡å‹"
result = corrector.correct(asr_output)
# Result: "æˆ‘è²·äº†ç‰›å¥¶ï¼Œç„¶å¾Œç”¨TensorFlowè¨“ç·´æ¨¡å‹"
```

### 2. LLM Output Post-Processing

**Problem**: LLMs may choose phonetically similar common characters for rare proper nouns

```python
# Your proper noun dictionary
terms = ["è€¶ç©Œ", "æ©å…¸", "PyTorch", "NumPy"]

engine = UnifiedEngine()
corrector = engine.create_corrector(terms)

# LLM output: rare proper nouns replaced with homophone common characters
llm_output = "è€¶ç©Œçš„æ©é»å¾ˆå¤§ï¼Œæˆ‘ç”¨æ’ç‚¬å’Œå—æ´¾åšæ©Ÿå™¨å­¸ç¿’"
result = corrector.correct(llm_output)
# Result: "è€¶ç©Œçš„æ©å…¸å¾ˆå¤§ï¼Œæˆ‘ç”¨PyTorchå’ŒNumPyåšæ©Ÿå™¨å­¸ç¿’"
```

### 3. Regional Vocabulary Conversion

**Your Dictionary**: Maintain regional mapping table (e.g., Mainland China â†” Taiwan terms)

```python
# Your regional vocabulary dictionary
region_terms = {
    "é¦¬éˆ´è–¯": {"aliases": ["åœŸè±†"], "weight": 0.0},
    "å½±ç‰‡": {"aliases": ["è¦–é »"], "weight": 0.0}
}

engine = ChineseEngine()
corrector = engine.create_corrector(region_terms)

result = corrector.correct("æˆ‘ç”¨åœŸè±†åšäº†è¦–é »")
# Result: "æˆ‘ç”¨é¦¬éˆ´è–¯åšäº†å½±ç‰‡"
```

### 4. Abbreviation Expansion

**Your Dictionary**: Maintain common abbreviations and full names mapping

```python
# Your abbreviation dictionary
abbreviation_terms = {
    "å°åŒ—è»Šç«™": {"aliases": ["åŒ—è»Š"], "weight": 0.0}
}

engine = ChineseEngine()
corrector = engine.create_corrector(abbreviation_terms)

result = corrector.correct("æˆ‘åœ¨åŒ—è»Šç­‰ä½ ")
# Result: "æˆ‘åœ¨å°åŒ—è»Šç«™ç­‰ä½ "
```

### 5. Professional Terminology Standardization

**Your Dictionary**: Maintain professional terminology for your business domain

```python
# Your medical terminology dictionary
medical_terms = {
    "é˜¿æ–¯åŒ¹éˆ": {"aliases": ["é˜¿æ–¯åŒ¹æ—", "äºŒå››æ‰¹æ—"], "weight": 0.2}
}

engine = ChineseEngine()
corrector = engine.create_corrector(medical_terms)

result = corrector.correct("é†«ç”Ÿé–‹äº†äºŒå››æ‰¹æ—çµ¦æˆ‘")
# Result: "é†«ç”Ÿé–‹äº†é˜¿æ–¯åŒ¹éˆçµ¦æˆ‘"
```

## ğŸ“– Complete Examples

Please refer to the `examples/` directory, which contains multiple usage examples:

| File | Description |
|------|-------------|
| `chinese_examples.py` | Chinese phonetic substitution examples |
| `english_examples.py` | English phonetic substitution examples |
| `mixed_language_examples.py` | Mixed Chinese-English substitution examples |
| `streaming_demo.py` | Basic streaming processing examples |
| `realtime_streaming_demo.py` | ASR/LLM real-time streaming examples |
| `timing_demo.py` | Performance timing examples |

```bash
# Run Chinese examples
uv run python examples/chinese_examples.py

# Run English examples (requires espeak-ng)
uv run python examples/english_examples.py

# Run streaming examples
uv run python examples/realtime_streaming_demo.py
```

## ğŸ”§ Technical Details

### Phonetic Matching Mechanism

#### Chinese: Pinyin Fuzzy Sound Rules

**Initial Consonant Fuzzy Groups**
| Group | Phonemes | Description |
|-------|----------|-------------|
| Retroflex | z â‡„ zh, c â‡„ ch, s â‡„ sh | Common in Taiwanese Mandarin |
| n/l confusion | n â‡„ l | Taiwanese Mandarin characteristic |
| r/l confusion | r â‡„ l | Common ASR error |
| f/h confusion | f â‡„ h | Dialect influence |

**Final Vowel Fuzzy Mapping**
- `in` â‡„ `ing`, `en` â‡„ `eng`, `an` â‡„ `ang`
- `ian` â‡„ `iang`, `uan` â‡„ `uang`, `uan` â‡„ `an`
- `ong` â‡„ `eng`, `uo` â‡„ `o`, `ue` â‡„ `ie`

**Special Syllable Mappings**
- `fa` â‡„ `hua` (ç™¼/èŠ±)
- `xue` â‡„ `xie` (å­¸/é‹)
- `ran` â‡„ `lan`, `yan` (ç„¶/è˜­/åš´)
- For more, please refer to `src/phonofix/languages/chinese/config.py`

#### English: IPA Phonetic Matching

Uses [phonemizer](https://github.com/bootphon/phonemizer) to convert English to IPA (International Phonetic Alphabet), then calculates Levenshtein edit distance.

**Common ASR/LLM Error Types**
| Error Type | Example | Description |
|------------|---------|-------------|
| Syllable splitting | "TensorFlow" â†’ "Ten so floor" | Speech recognition split error |
| Homophone | "Python" â†’ "Pyton" | Spelling error |
| Acronym expansion | "API" â†’ "A P I" | Letter-by-letter pronunciation |

### Keywords and exclude_when Mechanism

When the same alias may correspond to multiple proper nouns, use `keywords` and `exclude_when` for precise judgment:

```
Substitution Logic:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input text contains alias (e.g., "1kg")                â”‚
â”‚                    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Step 1: Check exclude_when (exclusion conditions)â”‚    â”‚
â”‚  â”‚   - If text contains any exclusion word â†’ No sub âŒâ”‚   â”‚
â”‚  â”‚   - e.g.: "1kgæ°´å¾ˆé‡" contains "æ°´" â†’ No sub to EKGâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“ (No exclusion match)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ Step 2: Check keywords (required conditions)     â”‚    â”‚
â”‚  â”‚   - If keywords set and none match â†’ No sub âŒ    â”‚    â”‚
â”‚  â”‚   - If keywords set and matched â†’ Substitute âœ…   â”‚    â”‚
â”‚  â”‚   - If no keywords set â†’ Substitute âœ…            â”‚    â”‚
â”‚  â”‚   - e.g.: "1kgè¨­å‚™" contains "è¨­å‚™" â†’ Sub to EKG  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Important Rule: exclude_when Takes Priority Over Keywords**

Even if keywords match, no substitution occurs if exclude_when matches:

```python
"EKG": {
    "aliases": ["1kg"],
    "keywords": ["è¨­å‚™", "é†«ç™‚"],      # Must contain one to substitute
    "exclude_when": ["é‡", "å…¬æ–¤"],    # Contains any = no substitution
}

# Examples:
"é€™å€‹è¨­å‚™æœ‰ 1kgé‡"  # keywords(è¨­å‚™) âœ“ + exclude_when(é‡) âœ“ â†’ No substitution
"é€™å€‹ 1kgè¨­å‚™"      # keywords(è¨­å‚™) âœ“ + exclude_when âœ— â†’ Substitute to EKG
"è²·äº† 1kgçš„æ±è¥¿"    # keywords âœ— â†’ No substitution
```

### Substitution Algorithm Flow

```
Input Text
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Build Protection Mask             â”‚
â”‚    Mark positions of protected_terms â”‚
â”‚    These positions skip substitution â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Language Block Detection          â”‚
â”‚    (UnifiedCorrector)                â”‚
â”‚    Chinese block â†’ ChineseCorrector  â”‚
â”‚    English block â†’ EnglishCorrector  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Sliding Window Scan               â”‚
â”‚    Traverse all possible word length â”‚
â”‚    combinations                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Phonetic Similarity Calculation   â”‚
â”‚    Chinese: Pinyin (specialâ†’finalâ†’   â”‚
â”‚             edit distance)           â”‚
â”‚    English: IPA edit distance        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Keywords/exclude_when Filtering   â”‚
â”‚    - exclude_when matched â†’ Skip     â”‚
â”‚    - No keywords matched â†’ Skip      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Calculate Final Score             â”‚
â”‚    Score = error_rate - weight -     â”‚
â”‚            context_bonus             â”‚
â”‚    (Lower score is better)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. Conflict Resolution               â”‚
â”‚    Sort by score, select best        â”‚
â”‚    non-overlapping candidates        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. Text Replacement                  â”‚
â”‚    Replace from back to front to     â”‚
â”‚    avoid index shifting              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Output Result
```

### Dynamic Tolerance Rate

| Word Length | Tolerance | Description |
|-------------|-----------|-------------|
| 2 chars/letters | 0.20 | Must be very accurate |
| 3 chars/letters | 0.30 | Moderately strict |
| 4+ chars/letters | 0.40 | Higher tolerance |
| English mixed | 0.45 | Higher tolerance |

## ğŸ¤ Contributing

Issues and Pull Requests are welcome!

## ğŸ“„ License

MIT License

## ğŸ‘¨â€ğŸ’» Author

JonesHong

## ğŸ™ Acknowledgments

Thanks to the following projects:
- [pypinyin](https://github.com/mozillazg/python-pinyin)
- [python-Levenshtein](https://github.com/maxbachmann/Levenshtein)
- [Pinyin2Hanzi](https://github.com/letiantian/Pinyin2Hanzi)
- [hanziconv](https://github.com/berniey/hanziconv)
