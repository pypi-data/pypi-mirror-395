{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291a2191",
   "metadata": {},
   "source": [
    "---\n",
    "title: Loading bed picks and layer data\n",
    "date: 2025-12-05\n",
    "---\n",
    "\n",
    "For workflows that primarily need surface, bed, or internal layer picks, this notebook demonstrates how to work with OPR layer picking information.\n",
    "\n",
    "This example shows how to load bed picks for a region and grid them onto a regular grid using Verde.\n",
    "\n",
    "(If your primary use case is plotting layers on top of a radargram you've already loaded, the `demo_notebook.ipynb` may be more useful to you.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe8678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xopr\n",
    "from xopr.bedmap import query_bedmap, query_bedmap_catalog, fetch_bedmap\n",
    "\n",
    "import holoviews as hv\n",
    "import xarray as xr\n",
    "import hvplot\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import geoviews.feature as gf\n",
    "import cartopy.crs as ccrs\n",
    "import rioxarray\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import verde as vd\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import box\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0a671",
   "metadata": {},
   "outputs": [],
   "source": [
    "opr = xopr.OPRConnection(cache_dir='radar_cache')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06d43d",
   "metadata": {},
   "source": [
    "We'll setup some useful backgrounds for context on our maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65507af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsg_3031 = ccrs.Stereographic(central_latitude=-90, true_scale_latitude=-71)\n",
    "coastline = gf.coastline.options(scale='50m').opts(projection=epsg_3031)\n",
    "velocity = rioxarray.open_rasterio(\n",
    "    \"https://its-live-data.s3.amazonaws.com/velocity_mosaic/v2/static/cog/ITS_LIVE_velocity_120m_RGI19A_0000_v02_v.tif\",\n",
    "    chunks='auto', overview_level=4, cache=False\n",
    ").squeeze().drop_vars(['spatial_ref', 'band']).rename('velocity (m/year)')\n",
    "velocity_map = velocity.hvplot.image(x='x', y='y', cmap='gray_r').opts(clim=(0,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c8df5",
   "metadata": {},
   "source": [
    "For this example, we'll focus on a specific region. Feel free to try swapping this region out for any other, of course.\n",
    "\n",
    "The Vincennes Bay area has some deep troughs that run across flow, making it an interesting area to loop at bed topography. This region is covered more extensively by UTIG data, which has only recently become available through OPR. Keep in mind that not all of this data yet has bed picks, as we'll see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb42795",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = xopr.geometry.get_antarctic_regions(name=[\"Vincennes_Bay\", \"Underwood\"], merge_regions=True, simplify_tolerance=100)\n",
    "region_projected = xopr.geometry.project_geojson(region, source_crs='EPSG:4326', target_crs=\"EPSG:3031\")\n",
    "\n",
    "region_hv = hv.Polygons([region_projected]).opts(\n",
    "    color='green',\n",
    "    line_color='black',\n",
    "    fill_alpha=0.3)\n",
    "\n",
    "(velocity_map * coastline * region_hv).opts(aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c334c",
   "metadata": {},
   "source": [
    "Querying for bed picks starts the same as any other radar query. We'll begin by fetching the STAC items corresponding to the radar data we want. As a reminder, this step doesn't load any actual radar data yet -- we're just getting the paths along which the radar data was collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675238b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = opr.query_frames(geometry=region).to_crs('EPSG:3031')\n",
    "print(f\"Found {len(gdf)} radar frames in the selected region.\")\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde48ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_frames_hv = gdf.hvplot(by='collection', hover_cols=['id'])\n",
    "(velocity_map * coastline * region_hv * radar_frames_hv).opts(aspect='equal', legend_position='top_left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34178f",
   "metadata": {},
   "source": [
    "### Getting bed picks\n",
    "\n",
    "Now that we've got our frames selected, we can load layer information for them. Layer information includes surface and bed and might come from the OPS API or from layerdata files hosted on OPR servers and indexed in the STAC catalog.\n",
    "\n",
    "See https://gitlab.com/openpolarradar/opr/-/wikis/Layer-File-Guide\n",
    "\n",
    "xOPR tries to abstract away the difference between the layer database and the layer files by formatting both to look like the layer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_ds_list = []\n",
    "\n",
    "for id, frame in tqdm(gdf.iterrows(), total=len(gdf), mininterval=1.0, desc=\"Loading layers\"):\n",
    "    layers = opr.get_layers(frame)\n",
    "    bed_layer_name = None\n",
    "    if 'standard:bottom' in layers: # Generally, the picked bed should be in group \"standard\" with layer name \"bottom\"\n",
    "        bed_layer_name = 'standard:bottom'\n",
    "    elif ':bottom' in layers:\n",
    "        bed_layer_name = ':bottom' # But occasionally it seems to be missing the group\n",
    "    else:\n",
    "        continue  # No bed layer found\n",
    "    # Layers are stored in terms of two-way travel time to avoid any questions about travel speed within ice\n",
    "    # This is different from how BedMap layers are stored, but it does make more sense when the radar data is availble to use twtt\n",
    "    layer_wgs84 = xopr.radar_util.layer_twtt_to_range(layers[bed_layer_name], layers[\"standard:surface\"], vertical_coordinate='wgs84').rename({'lat': 'Latitude', 'lon': 'Longitude'})\n",
    "    layer_wgs84 = xopr.geometry.project_dataset(layer_wgs84, target_crs='EPSG:3031')\n",
    "    layer_wgs84 = layer_wgs84.dropna('slow_time', subset=['wgs84'])\n",
    "    layer_wgs84['source'] = id\n",
    "    layer_ds_list.append(layer_wgs84)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326cee7",
   "metadata": {},
   "source": [
    "We can now combine all of the layers to get a pointwise list of bed picks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce35862",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_merged = xr.concat(layer_ds_list, dim='slow_time')\n",
    "\n",
    "# Get extent of radar data for plots and bedmap comparison\n",
    "xlim = (bed_merged.x.min().item(), bed_merged.x.max().item())\n",
    "ylim = (bed_merged.y.min().item(), bed_merged.y.max().item())\n",
    "\n",
    "bed_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1823f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_hv = bed_merged.hvplot.scatter(x='x', y='y', c='wgs84', cmap='turbo', s=2).opts(clabel='Bed Elevation WGS84 (m)')\n",
    "(velocity_map.opts(colorbar=False) * coastline * region_hv * radar_frames_hv * bed_hv).opts(aspect='equal', legend_position='top_left', xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60423f7f",
   "metadata": {},
   "source": [
    "As you can see, many of the radar lines are (as of the time of writing) missing bed picks.\n",
    "\n",
    "If you're looking at the Vincennes Bay region, you'll see the very deep trough running roughly grid top to bottom in the radar bed picks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513725ef",
   "metadata": {},
   "source": [
    "### Gridding\n",
    "\n",
    "What you want to do with the bed picks is, of course, up to you. One use case might be to aggregate these picks onto a common grid. We'll show an example of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_dataarray(d: xr.DataArray, spacing=1000, aggregation_fns={'median': \"median\", 'std': 'std', 'count': \"count\"}):\n",
    "    \"\"\"\n",
    "    Grid a DataArray with x,y coordinates into a regular grid using block aggregation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : xr.DataArray\n",
    "        Input DataArray with 'x' and 'y' coordinates\n",
    "    spacing : float\n",
    "        Grid spacing in the same units as x,y coordinates\n",
    "    aggregation_fns : dict\n",
    "        Dictionary mapping aggregation function names to functions (e.g., {'median': np.median, 'std': np.std})\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Dataset with variables named {d.name}_{fn_name} for each aggregation function\n",
    "    \"\"\"\n",
    "    # Get data extent\n",
    "    x_min = d['x'].min().values\n",
    "    x_max = d['x'].max().values\n",
    "    y_min = d['y'].min().values\n",
    "    y_max = d['y'].max().values\n",
    "    \n",
    "    # Extract coordinate and data values\n",
    "    x_data = d['x'].values\n",
    "    y_data = d['y'].values\n",
    "    data_values = d.values\n",
    "    \n",
    "    # Create grid coordinates\n",
    "    grid_x, grid_y = vd.grid_coordinates(\n",
    "        region=(x_min, x_max, y_min, y_max),\n",
    "        spacing=spacing\n",
    "    )\n",
    "    \n",
    "    # Dictionary to store gridded results for each aggregation function\n",
    "    data_vars = {}\n",
    "    \n",
    "    for fn_name, fn in aggregation_fns.items():\n",
    "        # Use Verde's BlockReduce with the specified aggregation function\n",
    "        gridder = vd.BlockReduce(\n",
    "            reduction=fn, \n",
    "            spacing=spacing, \n",
    "            region=(x_min, x_max, y_min, y_max),\n",
    "            center_coordinates=True\n",
    "        )\n",
    "        block_coords, block_values = gridder.filter(\n",
    "            coordinates=(x_data, y_data), \n",
    "            data=data_values\n",
    "        )\n",
    "        \n",
    "        # Initialize grid with NaN\n",
    "        grid_data = np.full(grid_x.shape, np.nan)\n",
    "        \n",
    "        # Vectorized approach: compute indices directly from coordinates\n",
    "        x_indices = np.floor((block_coords[0] - x_min) / spacing).astype(int)\n",
    "        y_indices = np.floor((block_coords[1] - y_min) / spacing).astype(int)\n",
    "        \n",
    "        for x_idx, y_idx, value in zip(x_indices.flatten(), y_indices.flatten(), block_values.flatten()):\n",
    "            grid_data[y_idx, x_idx] = value\n",
    "        \n",
    "        # Store in dictionary with name pattern\n",
    "        var_name = f\"{d.name}_{fn_name}\" if d.name else f\"data_{fn_name}\"\n",
    "        data_vars[var_name] = (['y', 'x'], grid_data)\n",
    "    \n",
    "    # Create Dataset with all aggregated variables\n",
    "    return xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords={\n",
    "            'y': grid_y[:, 0],\n",
    "            'x': grid_x[0, :]\n",
    "        }\n",
    "    )\n",
    "\n",
    "gridded = grid_dataarray(bed_merged['wgs84'], spacing=5000)\n",
    "\n",
    "gridded_median_hv = hv.Image(gridded, kdims=['x', 'y'], vdims=['wgs84_median', 'wgs84_std', 'wgs84_count']).opts(\n",
    "    cmap='turbo',\n",
    "    aspect='equal',\n",
    "    tools=['hover'],\n",
    "    colorbar=True,\n",
    "    clabel='WGS84 Elevation (m)'\n",
    ")\n",
    "\n",
    "gridded_std_hv = hv.Image(gridded, kdims=['x', 'y'], vdims=['wgs84_std', 'wgs84_median', 'wgs84_count']).opts(\n",
    "    cmap='inferno',\n",
    "    aspect='equal',\n",
    "    tools=['hover'],\n",
    "    colorbar=True,\n",
    "    clabel='Std of WGS84 Elevation (m)'\n",
    ")\n",
    "\n",
    "(velocity_map * region_hv * coastline * gridded_median_hv).opts(width=500, aspect='equal', xlim=xlim, ylim=ylim) + \\\n",
    "    (velocity_map * region_hv * coastline * gridded_std_hv).opts(width=500, aspect='equal', xlim=xlim, ylim=ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1f859",
   "metadata": {},
   "source": [
    "## Bedmap Data Integration\n",
    "\n",
    "The BedMap(1/2/3) datasets contain an enormous catalog of surface and bed picks. BedMap includes data from surveys that aren't yet in the OPR catalog, while OPR has some radar data that hasn't made it into BedMap. xopr provides unified access to bed picks from both sources.\n",
    "\n",
    "The bedmap data is hosted on Google Cloud Storage at `gs://opr_stac/bedmap/`. The query process works in two stages:\n",
    "1. **STAC Catalog Query**: Find GeoParquet files that intersect with the query geometry/time\n",
    "2. **DuckDB Partial Reads**: Fetch only relevant rows from those files using SQL pushdown\n",
    "\n",
    "This approach minimizes data transfer - only the data you need is downloaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b434d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polygon from radar extent (EPSG:3031) and convert to WGS84 for bedmap query\n",
    "extent_box_3031 = box(xlim[0], ylim[0], xlim[1], ylim[1])\n",
    "radar_extent = xopr.geometry.project_geojson(extent_box_3031, source_crs='EPSG:3031', target_crs='EPSG:4326')\n",
    "\n",
    "# Query the catalog to see what bedmap files match our region\n",
    "print(\"Querying STAC catalog for matching bedmap files...\")\n",
    "\n",
    "catalog_items = query_bedmap_catalog(\n",
    "    geometry=radar_extent,\n",
    "    collections=['bedmap3']\n",
    ")\n",
    "\n",
    "if not catalog_items.empty:\n",
    "    print(f\"\\nFound {len(catalog_items)} matching files:\")\n",
    "    for _, row in catalog_items.head(10).iterrows():\n",
    "        props = row['properties'] if 'properties' in row else {}\n",
    "        name = props.get('name', row.get('id', 'unknown'))\n",
    "        row_count = props.get('row_count', 0)\n",
    "        print(f\"  - {name}: {row_count:,} rows\")\n",
    "    if len(catalog_items) > 10:\n",
    "        print(f\"  ... and {len(catalog_items) - 10} more\")\n",
    "else:\n",
    "    print(\"No matching files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f5dad-a7f5-4230-a8d7-2fa72f6beaa1",
   "metadata": {},
   "source": [
    "The cell above is a metadata query, and is optional-- you can just call `query_bedmap` by itself, which will automatically call `query_bedmap_catalog` to determine which files to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nz6h83qmwt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query bedmap data matching the radar data extent for comparison\n",
    "print(\"Querying bedmap data from cloud GeoParquet files...\")\n",
    "print(f\"Query region bounds: {radar_extent.bounds}\")\n",
    "\n",
    "print(\"Timing cloud-based query...\")\n",
    "t0 = time.time()\n",
    "\n",
    "cloud_result = query_bedmap(\n",
    "    geometry=radar_extent,\n",
    "    collections=['bedmap3'],\n",
    "    exclude_geometry=True\n",
    ")\n",
    "\n",
    "cloud_time = time.time() - t0\n",
    "print(f\"Cloud query: {cloud_time:.2f}s for {len(cloud_result):,} rows\")\n",
    "\n",
    "if not cloud_result.empty:\n",
    "    print(f\"\\nRetrieved {len(cloud_result):,} points from bedmap\")\n",
    "    print(\"\\nColumns available:\")\n",
    "    print(cloud_result.columns.tolist())\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(cloud_result.head())\n",
    "else:\n",
    "    print(\"No bedmap data found in query region.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169f7fdc-b5ac-4ed6-8dea-6ff764e714ad",
   "metadata": {},
   "source": [
    "### Cloud vs Local Cache Performance\n",
    "\n",
    "For repeated queries or large datasets, you can use the `local_cache` option to download the GeoParquet files once and query them locally. This can provide significant speedups for subsequent queries-- though the exact speed up depends on your network connection and local disk speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n9am97j2b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download bedmap STAC catalogs to local cache for faster subsequent queries\n",
    "# This only needs to be done once - catalog files are cached locally\n",
    "print(\"Fetching bedmap catalogs to local cache...\")\n",
    "t0 = time.time()\n",
    "fetch_bedmap('bedmap3') # Default is all versions\n",
    "download_time = time.time() - t0\n",
    "print(f\"Local cache: {download_time:.2f}s to download\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67yfqoghy2x",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time the same query using local cache\n",
    "print(\"Timing local cache query...\")\n",
    "t0 = time.time()\n",
    "local_result = query_bedmap(\n",
    "    geometry=radar_extent,\n",
    "    collections=['bedmap3'],\n",
    "    exclude_geometry=True,\n",
    "    local_cache=True  # Use locally cached files\n",
    ")\n",
    "local_time = time.time() - t0\n",
    "print(f\"Local query: {local_time:.2f}s for {len(local_result):,} rows\")\n",
    "\n",
    "# Compare performance\n",
    "print(f\"\\nSpeedup: {cloud_time / local_time:.1f}x faster with local cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wdxlz159t6p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bedmap bed elevation alongside OPR bed picks\n",
    "# Filter to rows with valid bed elevation for apples-to-apples comparison\n",
    "bedmap_with_bed = local_result.dropna(subset=['bedrock_altitude (m)'])\n",
    "print(f\"Points with bed elevation: {len(bedmap_with_bed):,} of {len(local_result):,}\")\n",
    "\n",
    "# Create GeoDataFrame for bedmap data\n",
    "bedmap_gdf = gpd.GeoDataFrame(\n",
    "    bedmap_with_bed,\n",
    "    geometry=gpd.points_from_xy(bedmap_with_bed['lon'], bedmap_with_bed['lat']),\n",
    "    crs='EPSG:4326'\n",
    ").to_crs('EPSG:3031')\n",
    "\n",
    "# Extract x/y coordinates for plotting\n",
    "bedmap_gdf['x'] = bedmap_gdf.geometry.x\n",
    "bedmap_gdf['y'] = bedmap_gdf.geometry.y\n",
    "\n",
    "# Plot bedmap bed elevation\n",
    "bedmap_hv = bedmap_gdf.hvplot.points(\n",
    "    x='x', y='y', c='bedrock_altitude (m)',\n",
    "    cmap='turbo', s=5, alpha=0.7\n",
    ").opts(clabel='Bed Elevation (m)')\n",
    "\n",
    "# Combine with OPR bed picks\n",
    "(velocity_map.opts(colorbar=False) * coastline * region_hv * \n",
    " radar_frames_hv * bedmap_hv).opts(\n",
    "    aspect='equal', legend_position='top_left', xlim=xlim, ylim=ylim,\n",
    "    title='Bedmap bed elevation in radar extent'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
