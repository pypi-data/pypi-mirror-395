{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf36e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn.attention.flex_attention import (\n",
    "    create_block_mask,\n",
    "    flex_attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa9cfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x28290066fc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ad424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "from world_machine.profile import profile_range\n",
    "\n",
    "from world_machine.layers.positional_encoder import create_positional_encoder\n",
    "\n",
    "def apply_score_mod(score, score_mode, batch_index, head_index, query_index, key_index):\n",
    "    return score+score_mode[batch_index, head_index, query_index, key_index]\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim: int, n_head: int, is_causal: bool, positional_encoder_type: str | None = None, fast: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = MultiHeadAttention(\n",
    "            embed_dim, n_head, is_causal, positional_encoder_type, fast)\n",
    "\n",
    "    @profile_range(\"multi_head_self_attention_forward\", domain=\"world_machine\")\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.attention(x, x, x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_head: int, is_causal: bool, positional_encoder_type: str | None = None, fast: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Creates the layer.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): size of the embedding in the layer input and output.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = embed_dim//n_head\n",
    "\n",
    "        self.is_causal = is_causal\n",
    "        self.fast = fast\n",
    "\n",
    "        if self.head_dim * n_head != embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads ({embed_dim}/{n_head} is not integer).\")\n",
    "\n",
    "        # Initialize weights\n",
    "\n",
    "        # d_model = dv = dk = embed_dim\n",
    "        # h = 1\n",
    "\n",
    "        wQ = torch.Tensor(embed_dim, embed_dim)  # embed, embed\n",
    "        wK = torch.Tensor(embed_dim, embed_dim)  # embed, dk\n",
    "        wV = torch.Tensor(embed_dim, embed_dim)  # embed, dv\n",
    "        w0 = torch.Tensor(embed_dim, embed_dim)  # embed, embed\n",
    "\n",
    "        self.wQ = torch.nn.Parameter(wQ)\n",
    "        self.wK = torch.nn.Parameter(wK)\n",
    "        self.wV = torch.nn.Parameter(wV)\n",
    "        self.w0 = torch.nn.Parameter(w0)\n",
    "\n",
    "        self.register_buffer(\"dk_root\", torch.sqrt(\n",
    "            torch.tensor(self.head_dim, dtype=torch.float32)))\n",
    "        self.dk_root: torch.Tensor\n",
    "\n",
    "        self._positional_encoder = create_positional_encoder(\n",
    "            positional_encoder_type, embed_dim, 0, n_head)\n",
    "\n",
    "        for w in [self.wQ, self.wK, self.wV, self.w0]:\n",
    "            torch.nn.init.kaiming_normal_(w)\n",
    "\n",
    "        self.fast2 = False\n",
    "\n",
    "    @profile_range(\"multi_head_attention_forward\", domain=\"world_machine\")\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Process the inputs using the attention process.\n",
    "\n",
    "        Input tensors must be in [batch, sentence, embed] order.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): queries tensor, are compared against the keys.\n",
    "            key (torch.Tensor): keys tensor, represents the keys.\n",
    "            value (torch.Tensor): values tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the layer output, the values pondered by the compability between the keys and queries.\n",
    "        \"\"\"\n",
    "\n",
    "        # Check input\n",
    "        if query.shape[2] != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"Inputs must have embed dimension of {self.embed_dim} ({query.shape[2]} != {self.embed_dim})\")\n",
    "\n",
    "        # Get dimensions\n",
    "        batch_size = query.shape[0]\n",
    "        context_size = query.shape[1]\n",
    "\n",
    "        # Linear input transformation\n",
    "        # Transpose weights because PyTorch does that\n",
    "        with profile_range(\"linear_input_transformation\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            Q = query @ self.wQ.T\n",
    "            K = key @ self.wK.T\n",
    "            V = value @ self.wV.T\n",
    "\n",
    "        # Compute bias\n",
    "        with profile_range(\"compute_attention_bias\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            attention_bias = torch.zeros(\n",
    "                (context_size, context_size), device=Q.device)\n",
    "\n",
    "            if self.is_causal:\n",
    "                with profile_range(\"causal_bias\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "\n",
    "                    mask = torch.ones(\n",
    "                        (context_size, context_size), dtype=torch.bool, device=Q.device)\n",
    "                    mask = mask.tril()  # Lower triangular is one\n",
    "                    # Upper triangular without diagonal is ones\n",
    "                    mask = torch.bitwise_not(mask)\n",
    "\n",
    "                    attention_bias = torch.zeros(\n",
    "                        (context_size, context_size), device=Q.device)\n",
    "                    attention_bias[mask] = -torch.inf\n",
    "\n",
    "            attention_bias = attention_bias.unsqueeze(\n",
    "                0).repeat([batch_size*self.n_head, 1, 1])\n",
    "\n",
    "            with profile_range(\"positional_encoder\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "                attention_bias = self._positional_encoder.apply_attention_bias_pe(\n",
    "                    attention_bias)\n",
    "\n",
    "        # attention bias: [head*batch, context, context]\n",
    "\n",
    "        if self.fast:\n",
    "            E = self._fast_attention(Q, K, V, attention_bias)\n",
    "        else:\n",
    "            E = self._manual_attention(Q, K, V, attention_bias)\n",
    "\n",
    "        result = E @ self.w0.T\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _fast_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, attention_bias: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = Q.shape[0]\n",
    "        context_size = Q.shape[1]\n",
    "        embed_size = Q.shape[2]\n",
    "\n",
    "        with profile_range(\"pre_reshape\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            Q = Q.view(batch_size, -1, self.n_head,\n",
    "                       self.head_dim).transpose(1, 2)\n",
    "            K = K.view(batch_size, -1, self.n_head,\n",
    "                       self.head_dim).transpose(1, 2)\n",
    "            V = V.view(batch_size, -1, self.n_head,\n",
    "                       self.head_dim).transpose(1, 2)\n",
    "\n",
    "            # attention_bias: [head*batch, seq, seq]\n",
    "            # attention_bias2: [bath, head, seq, seq]\n",
    "            attention_bias = attention_bias.reshape(\n",
    "                [batch_size, self.n_head, context_size, context_size])\n",
    "    \n",
    "        score_mod = lambda score, batch_index, head_index, query_index, key_index : apply_score_mod(score, attention_bias, batch_index, head_index, query_index, key_index)\n",
    "        with profile_range(\"scaled_dot_product_attention\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            if self.fast2:\n",
    "                E = torch.nn.functional.scaled_dot_product_attention(Q, K, V, attn_mask=attention_bias, scale=1/self.dk_root)\n",
    "            else:\n",
    "                E = flex_attention(Q, K, V, score_mod, scale=1/self.dk_root)\n",
    "            #result = flex_attention(Q, K, V, scale=1/self.dk_root)\n",
    "        \n",
    "        with profile_range(\"post_reshape\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            E = E.transpose(1, 2).view(\n",
    "                batch_size, context_size, embed_size)\n",
    "\n",
    "        return E\n",
    "\n",
    "    def _manual_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, attention_bias: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = Q.shape[0]\n",
    "        context_size = Q.shape[1]\n",
    "\n",
    "        # batch_size, sentence, embed\n",
    "        # to\n",
    "        # batch_size,  n_head, sentence, head_dim\n",
    "        with profile_range(\"pre_reshape\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            Q = Q.transpose(0, 1).reshape(context_size, batch_size *\n",
    "                                          self.n_head, self.head_dim).transpose(0, 1)\n",
    "            K = K.transpose(0, 1).reshape(context_size, batch_size *\n",
    "                                          self.n_head, self.head_dim).transpose(0, 1)\n",
    "            V = V.transpose(0, 1).reshape(context_size, batch_size *\n",
    "                                          self.n_head, self.head_dim).transpose(0, 1)\n",
    "        # Now we have [\n",
    "        # [batch0word0part0, batch0word1part0],\n",
    "        # [batch0word0part1, batch0word1part1],\n",
    "        # [batch1word0part0, batch1word1part0],\n",
    "        # [batch1word0part1, batch1word1part1],\n",
    "        # ]\n",
    "\n",
    "        with profile_range(\"scores_computation\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            scores = Q @ K.transpose(-2, -1)  # K.permute(0,1,3,2)\n",
    "            scores /= self.dk_root\n",
    "\n",
    "        with profile_range(\"add_attention_bias\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            scores += attention_bias\n",
    "\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        E = probs @ V\n",
    "\n",
    "        # Return elements to correct place\n",
    "        with profile_range(\"post_reshape\", category=\"multi_head_attention\", domain=\"world_machine\"):\n",
    "            E = E.reshape(batch_size, self.n_head, context_size, self.head_dim)\n",
    "            E = E.transpose(-3, -2)\n",
    "            E = E.reshape(batch_size, context_size, self.embed_dim)\n",
    "        # Now we have [\n",
    "        # [batch0word0, batch0word1],\n",
    "        # [batch1word0, batch1word1]\n",
    "        # ]\n",
    "\n",
    "        return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4280188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "n_head = 4\n",
    "is_causal = True\n",
    "positional_encoder_type =\"alibi\"\n",
    "\n",
    "mhsa_manual = MultiHeadSelfAttention(embed_dim, n_head, is_causal, positional_encoder_type, False)\n",
    "mhsa_fast = MultiHeadSelfAttention(embed_dim, n_head, is_causal, positional_encoder_type, True)\n",
    "mhsa_fast2 = MultiHeadSelfAttention(embed_dim, n_head, is_causal, positional_encoder_type, True)\n",
    "torch_version = torch.nn.MultiheadAttention(embed_dim, num_heads=n_head, bias=False, batch_first=True).eval()\n",
    "\n",
    "mhsa_fast2.attention.fast2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacd787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mhsa_fast.attention.wQ = mhsa_manual.attention.wQ\n",
    "mhsa_fast.attention.wK = mhsa_manual.attention.wK\n",
    "mhsa_fast.attention.wV = mhsa_manual.attention.wV\n",
    "mhsa_fast.attention.w0 = mhsa_manual.attention.w0\n",
    "\n",
    "\n",
    "mhsa_fast2.attention.wQ = mhsa_manual.attention.wQ\n",
    "mhsa_fast2.attention.wK = mhsa_manual.attention.wK\n",
    "mhsa_fast2.attention.wV = mhsa_manual.attention.wV\n",
    "mhsa_fast2.attention.w0 = mhsa_manual.attention.w0\n",
    "\n",
    "torch_version.in_proj_weight = torch.nn.Parameter(torch.concat((mhsa_manual.attention.wQ, mhsa_manual.attention.wK, mhsa_manual.attention.wV)))\n",
    "torch_version.out_proj.weight = mhsa_manual.attention.w0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1890084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([32, 10, embed_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eddb4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_manual = mhsa_manual(x)\n",
    "y_fast = mhsa_fast(x)\n",
    "y_fast2 = mhsa_fast2(x)\n",
    "result_torch, _ = torch_version(x, x, x, need_weights=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e026c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_manual-y_fast).abs() > 1e-3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f8d054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_manual-y_fast2).abs() > 1e-3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e3d25bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1276)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_manual-result_torch) > 1e-4).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "76d952fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1102)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_fast-result_torch) > 1e-4).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c7053021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_manual.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d87ff85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4316, -1.0001,  0.9387,  ...,  4.1635, -6.3521, 12.4690])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_manual/y_fast).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40512ead",
   "metadata": {},
   "source": [
    "m = a/x\n",
    "\n",
    "\n",
    "f = a/y\n",
    "\n",
    "m/f = a/x * y/a = y/x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26f4bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_scale = 1/np.sqrt(embed_dim)\n",
    "manual_scale = 1/np.sqrt(mhsa_manual.attention.head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "27da3d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.35355339059327373, 0.7071067811865475)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_scale, manual_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1927f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_scale/manual_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "23775232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2615)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_manual/y_fast).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1204cc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6754, -0.7069,  0.6798,  1.5112, -0.8935, -1.7815, -0.8695,  1.2731],\n",
       "        [-0.6679, -0.7051,  0.6177,  1.4932, -0.8417, -1.8086, -0.7701,  1.3000],\n",
       "        [-0.7154, -0.6486,  0.5826,  1.4753, -0.8954, -1.7991, -0.8409,  1.2300],\n",
       "        [-0.7138, -0.6970,  0.6227,  1.5187, -0.8986, -1.8443, -0.8403,  1.2671],\n",
       "        [-0.7241, -0.6508,  0.5309,  1.5137, -0.8776, -1.7682, -0.7733,  1.2858],\n",
       "        [-0.6522, -0.6995,  0.6463,  1.4729, -0.8497, -1.7742, -0.8005,  1.2941],\n",
       "        [-0.6666, -0.6871,  0.6764,  1.4478, -0.8675, -1.7611, -0.8445,  1.2663],\n",
       "        [-0.6634, -0.6942,  0.6372,  1.4637, -0.8470, -1.7935, -0.7980,  1.2871],\n",
       "        [-0.6590, -0.6966,  0.6276,  1.5074, -0.8627, -1.7871, -0.8209,  1.2677],\n",
       "        [-0.6464, -0.6991,  0.6569,  1.4561, -0.8415, -1.7620, -0.7948,  1.2885]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_manual[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e71abccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(434)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin((y_fast-y_manual[1,1,0]).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b10bd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "852"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "426*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "220042cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "426-434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9656bc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 8])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac0846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0aedd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_head = 4\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alibi_bias():\n",
    "    alibi_bias = []\n",
    "    for h in range(n_head):\n",
    "        alibi_bias.append(-((h + 1) * 8.0 / n_head))\n",
    "    alibi_bias = torch.tensor(alibi_bias, device=device)\n",
    "    alibi_bias = torch.exp2(alibi_bias)\n",
    "    return alibi_bias\n",
    "\n",
    "\n",
    "alibi_bias = generate_alibi_bias()\n",
    "\n",
    "\n",
    "# In this case we are going to use a mask_mod and a score_mod\n",
    "def causal_mask(b, h, q_idx, kv_idx):\n",
    "    return q_idx >= kv_idx\n",
    "\n",
    "\n",
    "def alibi_and_causal_closure(score, b, h, q_idx, kv_idx):\n",
    "    bias = alibi_bias[h] * (kv_idx - q_idx)\n",
    "    return score + bias\n",
    "\n",
    "score_mod = alibi_and_causal_closure\n",
    "mask_mod = causal_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "419d17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshape = x.view(32, -1, n_head,\n",
    "                       128//n_head).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "678b0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshape = x_reshape.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "675c5a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4794, 0.5454, 0.5413,  ..., 0.4885, 0.4858, 0.5556],\n",
       "          [0.4698, 0.5364, 0.5326,  ..., 0.4786, 0.4887, 0.5545],\n",
       "          [0.4737, 0.5467, 0.5350,  ..., 0.4942, 0.4928, 0.5496],\n",
       "          ...,\n",
       "          [0.4863, 0.5527, 0.5354,  ..., 0.4890, 0.4874, 0.5446],\n",
       "          [0.4736, 0.5417, 0.5275,  ..., 0.4963, 0.4962, 0.5477],\n",
       "          [0.4844, 0.5622, 0.5359,  ..., 0.4822, 0.4975, 0.5528]],\n",
       "\n",
       "         [[0.4946, 0.5040, 0.4788,  ..., 0.4799, 0.4680, 0.4833],\n",
       "          [0.4910, 0.4968, 0.4859,  ..., 0.4832, 0.4657, 0.4918],\n",
       "          [0.5061, 0.4946, 0.4749,  ..., 0.4867, 0.4639, 0.4976],\n",
       "          ...,\n",
       "          [0.5104, 0.5015, 0.4819,  ..., 0.4799, 0.4835, 0.4827],\n",
       "          [0.5015, 0.5021, 0.4742,  ..., 0.4920, 0.4677, 0.4832],\n",
       "          [0.5074, 0.5034, 0.4832,  ..., 0.4800, 0.4755, 0.4806]],\n",
       "\n",
       "         [[0.4929, 0.4862, 0.5478,  ..., 0.4798, 0.4643, 0.5114],\n",
       "          [0.4829, 0.4821, 0.5508,  ..., 0.4802, 0.4802, 0.4880],\n",
       "          [0.4908, 0.4804, 0.5514,  ..., 0.4824, 0.4821, 0.4965],\n",
       "          ...,\n",
       "          [0.4811, 0.4929, 0.5580,  ..., 0.4832, 0.4681, 0.5038],\n",
       "          [0.4876, 0.4789, 0.5499,  ..., 0.4894, 0.4677, 0.4892],\n",
       "          [0.4869, 0.4812, 0.5481,  ..., 0.4857, 0.4749, 0.4872]],\n",
       "\n",
       "         [[0.5195, 0.5076, 0.5276,  ..., 0.5303, 0.5074, 0.5121],\n",
       "          [0.5232, 0.5195, 0.5171,  ..., 0.5204, 0.5097, 0.5182],\n",
       "          [0.5192, 0.5098, 0.5183,  ..., 0.5325, 0.5049, 0.5088],\n",
       "          ...,\n",
       "          [0.5210, 0.5158, 0.5315,  ..., 0.5287, 0.5031, 0.5113],\n",
       "          [0.5284, 0.5113, 0.5256,  ..., 0.5212, 0.5089, 0.5194],\n",
       "          [0.5163, 0.5080, 0.5151,  ..., 0.5322, 0.4990, 0.5208]]],\n",
       "\n",
       "\n",
       "        [[[0.5021, 0.4911, 0.4982,  ..., 0.4235, 0.4685, 0.4967],\n",
       "          [0.4959, 0.4908, 0.4928,  ..., 0.4209, 0.4766, 0.4887],\n",
       "          [0.4846, 0.4998, 0.4904,  ..., 0.4311, 0.4678, 0.4877],\n",
       "          ...,\n",
       "          [0.5069, 0.4865, 0.4795,  ..., 0.4225, 0.4710, 0.4942],\n",
       "          [0.5015, 0.4937, 0.4872,  ..., 0.4307, 0.4836, 0.5115],\n",
       "          [0.5023, 0.4967, 0.4790,  ..., 0.4228, 0.4839, 0.4986]],\n",
       "\n",
       "         [[0.5374, 0.4273, 0.5180,  ..., 0.5476, 0.5047, 0.5697],\n",
       "          [0.5488, 0.4249, 0.5137,  ..., 0.5411, 0.4994, 0.5593],\n",
       "          [0.5536, 0.4226, 0.5265,  ..., 0.5471, 0.4951, 0.5694],\n",
       "          ...,\n",
       "          [0.5481, 0.4264, 0.5173,  ..., 0.5470, 0.4933, 0.5594],\n",
       "          [0.5518, 0.4295, 0.5236,  ..., 0.5423, 0.4906, 0.5613],\n",
       "          [0.5538, 0.4353, 0.5126,  ..., 0.5390, 0.5092, 0.5596]],\n",
       "\n",
       "         [[0.4846, 0.5042, 0.5411,  ..., 0.5090, 0.5285, 0.5079],\n",
       "          [0.4855, 0.5099, 0.5291,  ..., 0.5092, 0.5447, 0.4896],\n",
       "          [0.4768, 0.5131, 0.5294,  ..., 0.5201, 0.5417, 0.4885],\n",
       "          ...,\n",
       "          [0.4756, 0.5197, 0.5214,  ..., 0.5173, 0.5245, 0.4961],\n",
       "          [0.4849, 0.5251, 0.5293,  ..., 0.5247, 0.5385, 0.5120],\n",
       "          [0.4776, 0.5102, 0.5247,  ..., 0.5085, 0.5320, 0.5017]],\n",
       "\n",
       "         [[0.5305, 0.5664, 0.5318,  ..., 0.5472, 0.5022, 0.4754],\n",
       "          [0.5222, 0.5599, 0.5296,  ..., 0.5409, 0.5146, 0.4781],\n",
       "          [0.5151, 0.5730, 0.5329,  ..., 0.5441, 0.5188, 0.4823],\n",
       "          ...,\n",
       "          [0.5357, 0.5731, 0.5344,  ..., 0.5456, 0.5062, 0.4742],\n",
       "          [0.5270, 0.5735, 0.5364,  ..., 0.5300, 0.5087, 0.4763],\n",
       "          [0.5270, 0.5698, 0.5290,  ..., 0.5373, 0.5061, 0.4752]]],\n",
       "\n",
       "\n",
       "        [[[0.5092, 0.5061, 0.4663,  ..., 0.5352, 0.5867, 0.5011],\n",
       "          [0.5189, 0.4991, 0.4694,  ..., 0.5210, 0.5771, 0.4910],\n",
       "          [0.4953, 0.5167, 0.4729,  ..., 0.5155, 0.5796, 0.5064],\n",
       "          ...,\n",
       "          [0.5016, 0.5036, 0.4664,  ..., 0.5230, 0.5950, 0.4896],\n",
       "          [0.5178, 0.5006, 0.4669,  ..., 0.5395, 0.5863, 0.5080],\n",
       "          [0.5061, 0.5040, 0.4775,  ..., 0.5332, 0.5942, 0.5078]],\n",
       "\n",
       "         [[0.5260, 0.5357, 0.4593,  ..., 0.5507, 0.4804, 0.5247],\n",
       "          [0.5351, 0.5438, 0.4617,  ..., 0.5683, 0.4866, 0.5208],\n",
       "          [0.5243, 0.5472, 0.4611,  ..., 0.5602, 0.4917, 0.5155],\n",
       "          ...,\n",
       "          [0.5288, 0.5392, 0.4574,  ..., 0.5598, 0.4819, 0.5123],\n",
       "          [0.5393, 0.5367, 0.4569,  ..., 0.5655, 0.4828, 0.5196],\n",
       "          [0.5388, 0.5446, 0.4617,  ..., 0.5698, 0.5003, 0.5156]],\n",
       "\n",
       "         [[0.5077, 0.4627, 0.4688,  ..., 0.4419, 0.5281, 0.5593],\n",
       "          [0.5130, 0.4598, 0.4733,  ..., 0.4332, 0.5215, 0.5640],\n",
       "          [0.5206, 0.4615, 0.4737,  ..., 0.4409, 0.5270, 0.5475],\n",
       "          ...,\n",
       "          [0.5050, 0.4691, 0.4684,  ..., 0.4561, 0.5201, 0.5528],\n",
       "          [0.5168, 0.4598, 0.4902,  ..., 0.4446, 0.5308, 0.5531],\n",
       "          [0.5112, 0.4573, 0.4757,  ..., 0.4385, 0.5248, 0.5484]],\n",
       "\n",
       "         [[0.4687, 0.4872, 0.5284,  ..., 0.5152, 0.5200, 0.4860],\n",
       "          [0.4672, 0.5044, 0.5281,  ..., 0.5165, 0.5288, 0.4865],\n",
       "          [0.4738, 0.4957, 0.5365,  ..., 0.5190, 0.5194, 0.4865],\n",
       "          ...,\n",
       "          [0.4669, 0.4956, 0.5374,  ..., 0.5225, 0.5162, 0.4863],\n",
       "          [0.4686, 0.4966, 0.5286,  ..., 0.5134, 0.5281, 0.5023],\n",
       "          [0.4740, 0.4882, 0.5445,  ..., 0.5139, 0.5218, 0.4903]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.5132, 0.5208, 0.5431,  ..., 0.4700, 0.4881, 0.4890],\n",
       "          [0.5016, 0.5104, 0.5452,  ..., 0.4900, 0.4893, 0.4764],\n",
       "          [0.5132, 0.5123, 0.5519,  ..., 0.4870, 0.4940, 0.4804],\n",
       "          ...,\n",
       "          [0.5095, 0.5156, 0.5568,  ..., 0.4694, 0.4920, 0.4937],\n",
       "          [0.5177, 0.5169, 0.5456,  ..., 0.4823, 0.5010, 0.4769],\n",
       "          [0.5024, 0.5120, 0.5619,  ..., 0.4735, 0.4927, 0.4850]],\n",
       "\n",
       "         [[0.4972, 0.5237, 0.5245,  ..., 0.5625, 0.5001, 0.4719],\n",
       "          [0.5006, 0.5181, 0.5276,  ..., 0.5679, 0.4924, 0.4718],\n",
       "          [0.4964, 0.5192, 0.5242,  ..., 0.5724, 0.4967, 0.4698],\n",
       "          ...,\n",
       "          [0.5109, 0.5231, 0.5205,  ..., 0.5672, 0.5047, 0.4682],\n",
       "          [0.4958, 0.5284, 0.5265,  ..., 0.5584, 0.5035, 0.4756],\n",
       "          [0.4996, 0.5324, 0.5216,  ..., 0.5626, 0.5038, 0.4791]],\n",
       "\n",
       "         [[0.4406, 0.5196, 0.5037,  ..., 0.4935, 0.4694, 0.4948],\n",
       "          [0.4352, 0.5162, 0.5061,  ..., 0.5002, 0.4657, 0.5009],\n",
       "          [0.4367, 0.5167, 0.5027,  ..., 0.4917, 0.4850, 0.5037],\n",
       "          ...,\n",
       "          [0.4388, 0.5161, 0.5027,  ..., 0.4949, 0.4671, 0.5031],\n",
       "          [0.4394, 0.5025, 0.5059,  ..., 0.4982, 0.4761, 0.5006],\n",
       "          [0.4418, 0.5176, 0.5032,  ..., 0.5008, 0.4688, 0.4967]],\n",
       "\n",
       "         [[0.5192, 0.4725, 0.5548,  ..., 0.5370, 0.5268, 0.4997],\n",
       "          [0.5194, 0.4727, 0.5546,  ..., 0.5356, 0.5318, 0.5025],\n",
       "          [0.5106, 0.4822, 0.5525,  ..., 0.5340, 0.5380, 0.5078],\n",
       "          ...,\n",
       "          [0.5170, 0.4809, 0.5442,  ..., 0.5461, 0.5300, 0.5167],\n",
       "          [0.5135, 0.4717, 0.5523,  ..., 0.5446, 0.5302, 0.5185],\n",
       "          [0.5274, 0.4839, 0.5585,  ..., 0.5454, 0.5307, 0.4982]]],\n",
       "\n",
       "\n",
       "        [[[0.5316, 0.5110, 0.4881,  ..., 0.5056, 0.5388, 0.5356],\n",
       "          [0.5423, 0.5106, 0.4834,  ..., 0.5101, 0.5427, 0.5434],\n",
       "          [0.5368, 0.5053, 0.4766,  ..., 0.5058, 0.5281, 0.5484],\n",
       "          ...,\n",
       "          [0.5317, 0.5158, 0.4917,  ..., 0.5097, 0.5348, 0.5375],\n",
       "          [0.5488, 0.5171, 0.4829,  ..., 0.5064, 0.5319, 0.5370],\n",
       "          [0.5469, 0.5175, 0.4793,  ..., 0.5071, 0.5236, 0.5543]],\n",
       "\n",
       "         [[0.5204, 0.4951, 0.5397,  ..., 0.5559, 0.5408, 0.4626],\n",
       "          [0.5254, 0.4967, 0.5459,  ..., 0.5470, 0.5508, 0.4695],\n",
       "          [0.5123, 0.4907, 0.5649,  ..., 0.5553, 0.5349, 0.4779],\n",
       "          ...,\n",
       "          [0.5267, 0.4980, 0.5457,  ..., 0.5587, 0.5455, 0.4672],\n",
       "          [0.5159, 0.4916, 0.5479,  ..., 0.5628, 0.5417, 0.4703],\n",
       "          [0.5161, 0.4893, 0.5549,  ..., 0.5557, 0.5344, 0.4608]],\n",
       "\n",
       "         [[0.5395, 0.5481, 0.4875,  ..., 0.5453, 0.4539, 0.5126],\n",
       "          [0.5271, 0.5479, 0.4865,  ..., 0.5313, 0.4594, 0.5189],\n",
       "          [0.5300, 0.5496, 0.4831,  ..., 0.5296, 0.4580, 0.5101],\n",
       "          ...,\n",
       "          [0.5316, 0.5562, 0.4940,  ..., 0.5372, 0.4679, 0.5029],\n",
       "          [0.5428, 0.5499, 0.4971,  ..., 0.5366, 0.4676, 0.5152],\n",
       "          [0.5355, 0.5537, 0.5022,  ..., 0.5455, 0.4595, 0.5006]],\n",
       "\n",
       "         [[0.4783, 0.4846, 0.5240,  ..., 0.5495, 0.4936, 0.4707],\n",
       "          [0.4902, 0.4841, 0.5162,  ..., 0.5485, 0.5087, 0.4653],\n",
       "          [0.4727, 0.4882, 0.5278,  ..., 0.5550, 0.4959, 0.4783],\n",
       "          ...,\n",
       "          [0.4803, 0.4861, 0.5310,  ..., 0.5484, 0.4853, 0.4822],\n",
       "          [0.4786, 0.5007, 0.5312,  ..., 0.5546, 0.4847, 0.4778],\n",
       "          [0.4646, 0.4889, 0.5271,  ..., 0.5576, 0.4934, 0.4833]]],\n",
       "\n",
       "\n",
       "        [[[0.4915, 0.4855, 0.4718,  ..., 0.5507, 0.4810, 0.4715],\n",
       "          [0.4755, 0.4812, 0.4675,  ..., 0.5348, 0.4780, 0.4801],\n",
       "          [0.4889, 0.4781, 0.4658,  ..., 0.5325, 0.4842, 0.4784],\n",
       "          ...,\n",
       "          [0.4744, 0.4798, 0.4652,  ..., 0.5413, 0.4682, 0.4791],\n",
       "          [0.4814, 0.4762, 0.4595,  ..., 0.5305, 0.4829, 0.4882],\n",
       "          [0.4812, 0.4854, 0.4622,  ..., 0.5326, 0.4756, 0.4819]],\n",
       "\n",
       "         [[0.5038, 0.5666, 0.5185,  ..., 0.4862, 0.4776, 0.4308],\n",
       "          [0.5069, 0.5852, 0.5412,  ..., 0.4904, 0.4606, 0.4347],\n",
       "          [0.5090, 0.5730, 0.5192,  ..., 0.4883, 0.4736, 0.4370],\n",
       "          ...,\n",
       "          [0.4983, 0.5736, 0.5269,  ..., 0.4807, 0.4756, 0.4307],\n",
       "          [0.5057, 0.5768, 0.5326,  ..., 0.4919, 0.4546, 0.4344],\n",
       "          [0.4970, 0.5823, 0.5345,  ..., 0.4874, 0.4709, 0.4341]],\n",
       "\n",
       "         [[0.5715, 0.4996, 0.4943,  ..., 0.5537, 0.5462, 0.4796],\n",
       "          [0.5487, 0.5112, 0.4915,  ..., 0.5519, 0.5419, 0.4757],\n",
       "          [0.5549, 0.5152, 0.5054,  ..., 0.5530, 0.5472, 0.4656],\n",
       "          ...,\n",
       "          [0.5524, 0.5063, 0.4897,  ..., 0.5463, 0.5512, 0.4732],\n",
       "          [0.5578, 0.5053, 0.4888,  ..., 0.5487, 0.5452, 0.4598],\n",
       "          [0.5677, 0.5070, 0.5026,  ..., 0.5357, 0.5439, 0.4741]],\n",
       "\n",
       "         [[0.5564, 0.5346, 0.5277,  ..., 0.4997, 0.5158, 0.5278],\n",
       "          [0.5458, 0.5212, 0.5268,  ..., 0.5017, 0.5187, 0.5218],\n",
       "          [0.5604, 0.5320, 0.5133,  ..., 0.5055, 0.5162, 0.5334],\n",
       "          ...,\n",
       "          [0.5607, 0.5392, 0.5257,  ..., 0.4986, 0.5152, 0.5372],\n",
       "          [0.5561, 0.5350, 0.5184,  ..., 0.4937, 0.5189, 0.5336],\n",
       "          [0.5590, 0.5272, 0.5218,  ..., 0.5055, 0.5142, 0.5240]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flex_attention(x_reshape, x_reshape, x_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5f844f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_mask = create_block_mask(mask_mod, 1, 1, embed_dim, embed_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a66569",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unsupported",
     "evalue": "builtin: print [<class 'torch._dynamo.variables.lists.SizeVariable'>] False\n\nfrom user code:\n   File \"c:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\attention\\flex_attention.py\", line 1033, in _flex_attention_hop_wrapper\n    return flex_attention_hop(*args, **kwargs)\n  File \"C:\\Users\\eltsu\\AppData\\Local\\Temp\\ipykernel_41424\\1960214997.py\", line 20, in alibi_and_causal_closure\n    print(score.shape)\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupported\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mflex_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_reshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_reshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_reshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\attention\\flex_attention.py:1038\u001b[0m, in \u001b[0;36mflex_attention\u001b[1;34m(query, key, value, score_mod, block_mask, scale, enable_gqa, return_lse, kernel_options)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdisable_cache_limit():\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _temp_remove_pre_dispatch_torch_function_mode():\n\u001b[1;32m-> 1038\u001b[0m         out, lse \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_flex_attention_hop_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meager\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mblock_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkernel_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1049\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m return_lse:\n\u001b[0;32m   1050\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m out, lse \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[0;32m    462\u001b[0m )\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[0;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[0;32m    470\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[1;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[0;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[0;32m   1265\u001b[0m             )\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[1;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[1;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[0;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[0;32m    512\u001b[0m signpost_event(\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m     },\n\u001b[0;32m    524\u001b[0m )\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[0;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[1;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[0;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     91\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[1;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[0;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[1;34m(code, transformations, safe)\u001b[0m\n\u001b[0;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[0;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[1;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[0;32m    217\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[1;34m(instructions, code_options)\u001b[0m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[1;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[0;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:1680\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL_FUNCTION_EX\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;66;03m# Map to a dictionary of str -> VariableTracker\u001b[39;00m\n\u001b[0;32m   1679\u001b[0m kwargsvars \u001b[38;5;241m=\u001b[39m kwargsvars\u001b[38;5;241m.\u001b[39mkeys_as_python_constant()\n\u001b[1;32m-> 1680\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margsvars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargsvars\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\higher_order_ops.py:1882\u001b[0m, in \u001b[0;36mFlexAttentionHigherOrderVariable.call_function\u001b[1;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wrap_fx_proxy\n\u001b[0;32m   1872\u001b[0m (\n\u001b[0;32m   1873\u001b[0m     query,\n\u001b[0;32m   1874\u001b[0m     key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1879\u001b[0m     kernel_options,\n\u001b[0;32m   1880\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_to_args(args, kwargs)\n\u001b[1;32m-> 1882\u001b[0m score_mod_node, score_mod_lifted_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_wrapped_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore_mod\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1885\u001b[0m mask_fn \u001b[38;5;241m=\u001b[39m block_mask\u001b[38;5;241m.\u001b[39mitems[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_fn, ConstantVariable):\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\higher_order_ops.py:1836\u001b[0m, in \u001b[0;36mFlexAttentionHigherOrderVariable.create_wrapped_node\u001b[1;34m(self, tx, query, fn, fn_name)\u001b[0m\n\u001b[0;32m   1829\u001b[0m     new_args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39mbhmn]\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TransformGetItemToIndex():\n\u001b[0;32m   1832\u001b[0m     (\n\u001b[0;32m   1833\u001b[0m         (body_output, body_treespec),\n\u001b[0;32m   1834\u001b[0m         body_graph,\n\u001b[0;32m   1835\u001b[0m         body_lifted_freevars,\n\u001b[1;32m-> 1836\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mspeculate_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1840\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# expect only args no kwargs for now\u001b[39;49;00m\n\u001b[0;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mset_subgraph_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflatten_manual\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1846\u001b[0m body_name \u001b[38;5;241m=\u001b[39m add_subgraph(\n\u001b[0;32m   1847\u001b[0m     tx,\n\u001b[0;32m   1848\u001b[0m     fn_name,\n\u001b[0;32m   1849\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule(tx\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mnn_modules, body_graph),\n\u001b[0;32m   1850\u001b[0m )\n\u001b[0;32m   1852\u001b[0m body_node \u001b[38;5;241m=\u001b[39m make_attr(tx, body_name)\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\higher_order_ops.py:533\u001b[0m, in \u001b[0;36mspeculate_subgraph\u001b[1;34m(tx, f, sub_args, sub_kwargs, description, source_target, always_restore, enable_grad, set_subgraph_inputs, restore_side_effects, should_flatten_outputs, tracer)\u001b[0m\n\u001b[0;32m    531\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(msg)\n\u001b[0;32m    532\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(ex)\n\u001b[1;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\higher_order_ops.py:462\u001b[0m, in \u001b[0;36mspeculate_subgraph\u001b[1;34m(tx, f, sub_args, sub_kwargs, description, source_target, always_restore, enable_grad, set_subgraph_inputs, restore_side_effects, should_flatten_outputs, tracer)\u001b[0m\n\u001b[0;32m    459\u001b[0m     prev_side_effects \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autograd_ctx:\n\u001b[1;32m--> 462\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m restore_side_effects:\n\u001b[0;32m    465\u001b[0m     new_side_effects \u001b[38;5;241m=\u001b[39m tx\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mside_effects\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\functions.py:324\u001b[0m, in \u001b[0;36mUserFunctionVariable.call_function\u001b[1;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_constant:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invoke_and_store_as_constant(\n\u001b[0;32m    321\u001b[0m         tx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(), args, kwargs\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\functions.py:111\u001b[0m, in \u001b[0;36mBaseUserFunctionVariable.call_function\u001b[1;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_function\u001b[39m(\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    107\u001b[0m     tx: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstructionTranslator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     args: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList[VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m     kwargs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDict[str, VariableTracker]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariableTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_user_function_return\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:836\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.inline_user_function_return\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_user_function_return\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m    833\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;124;03m    A call to some user defined function by inlining it.\u001b[39;00m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInliningInstructionTranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:3011\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call\u001b[1;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[0;32m   3008\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   3009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minline_call\u001b[39m(\u001b[38;5;28mcls\u001b[39m, parent, func, args, kwargs):\n\u001b[0;32m   3010\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m patch\u001b[38;5;241m.\u001b[39mdict(counters, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munimplemented\u001b[39m\u001b[38;5;124m\"\u001b[39m: counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minline_call\u001b[39m\u001b[38;5;124m\"\u001b[39m]}):\n\u001b[1;32m-> 3011\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minline_call_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:3139\u001b[0m, in \u001b[0;36mInliningInstructionTranslator.inline_call_\u001b[1;34m(parent, func, args, kwargs)\u001b[0m\n\u001b[0;32m   3137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3138\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strict_ctx:\n\u001b[1;32m-> 3139\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3141\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObserved exception DURING INLING \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2279\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[1;34m(self, inst)\u001b[0m\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[1;32m-> 2279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2273\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[1;34m(self, inst, call_kw)\u001b[0m\n\u001b[0;32m   2268\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2271\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[1;32m-> 2273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\builtin.py:967\u001b[0m, in \u001b[0;36mBuiltinVariable.call_function\u001b[1;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handler:\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_function_handler_cache[key] \u001b[38;5;241m=\u001b[39m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_handler(\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, [\u001b[38;5;28mtype\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m args], \u001b[38;5;28mbool\u001b[39m(kwargs)\n\u001b[0;32m    966\u001b[0m     )\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\variables\\builtin.py:834\u001b[0m, in \u001b[0;36mBuiltinVariable._make_handler.<locals>.<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    832\u001b[0m error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltin: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhas_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(handlers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: \u001b[43munimplemented\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(handlers) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    836\u001b[0m     (handler,) \u001b[38;5;241m=\u001b[39m handlers\n",
      "File \u001b[1;32mc:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_dynamo\\exc.py:297\u001b[0m, in \u001b[0;36munimplemented\u001b[1;34m(msg, from_exc, case_name)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _NOTHING:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg, case_name\u001b[38;5;241m=\u001b[39mcase_name) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfrom_exc\u001b[39;00m\n\u001b[1;32m--> 297\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Unsupported(msg, case_name\u001b[38;5;241m=\u001b[39mcase_name)\n",
      "\u001b[1;31mUnsupported\u001b[0m: builtin: print [<class 'torch._dynamo.variables.lists.SizeVariable'>] False\n\nfrom user code:\n   File \"c:\\Users\\eltsu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\attention\\flex_attention.py\", line 1033, in _flex_attention_hop_wrapper\n    return flex_attention_hop(*args, **kwargs)\n  File \"C:\\Users\\eltsu\\AppData\\Local\\Temp\\ipykernel_41424\\1960214997.py\", line 20, in alibi_and_causal_closure\n    print(score.shape)\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "flex_attention(x_reshape, x_reshape, x_reshape, score_mod, block_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "823100b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyModule(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"a\", torch.ones(100, dtype=torch.float32), False)\n",
    "        self.a *= 2\n",
    "\n",
    "    def test(self):\n",
    "\n",
    "        self.a = torch.ones(200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x+self.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eadc5f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MyModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0ef22821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "        2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8476da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "215b70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a, \"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "958ce958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eltsu\\AppData\\Local\\Temp\\ipykernel_63068\\2147831974.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  b = torch.load(\"test.pt\")\n"
     ]
    }
   ],
   "source": [
    "b = torch.load(\"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b04237e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d935fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34723895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([]).dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050d9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from world_machine.profile import profile_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7d1374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(a:int):\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822505c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = profile_range(\"my_message\")(func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8536b787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac64ed8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\eltsu\\\\Documentos\\\\Projetos\\\\WorldMachine\\\\WorldMachine\\\\benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e33db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_benchmark_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f2880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_benchmark_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4041273c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WorldMachine(\n",
       "   (_blocks): ModuleList(\n",
       "     (0-1): 2 x BlockContainer(\n",
       "       (block): AdaLNZeroBlock(\n",
       "         (conditioning_mlp): Sequential(\n",
       "           (0): SiLU()\n",
       "           (1): Linear(in_features=128, out_features=768, bias=True)\n",
       "         )\n",
       "         (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (modulate1): Modulate()\n",
       "         (attention): MultiHeadSelfAttention(\n",
       "           (attention): MultiHeadAttention(\n",
       "             (_positional_encoder): AlibiPositionalEncoder()\n",
       "             (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "           )\n",
       "         )\n",
       "         (dropout_attention): Dropout(p=0.0, inplace=False)\n",
       "         (modulate2): Modulate()\n",
       "         (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "         (modulate3): Modulate()\n",
       "         (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "         (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "         (act): GELU(approximate='tanh')\n",
       "         (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "         (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "         (modulate4): Modulate()\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (_sensorial_encoders): ModuleDict(\n",
       "     (dim0): PointwiseFeedforward(\n",
       "       (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "       (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "       (relu): ReLU()\n",
       "       (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "       (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (dim1): PointwiseFeedforward(\n",
       "       (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "       (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "       (relu): ReLU()\n",
       "       (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "       (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (_sensorial_decoders): ModuleDict(\n",
       "     (dim0): PointwiseFeedforward(\n",
       "       (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "       (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "       (relu): ReLU()\n",
       "       (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "       (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "     (dim1): PointwiseFeedforward(\n",
       "       (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "       (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "       (relu): ReLU()\n",
       "       (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "       (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "     )\n",
       "   )\n",
       "   (_state_encoder): Identity()\n",
       "   (_state_decoder): Identity()\n",
       "   (_positional_encoder): AlibiPositionalEncoder()\n",
       "   (_state_activation): Tanh()\n",
       " ),\n",
       " ModuleList(\n",
       "   (0-1): 2 x BlockContainer(\n",
       "     (block): AdaLNZeroBlock(\n",
       "       (conditioning_mlp): Sequential(\n",
       "         (0): SiLU()\n",
       "         (1): Linear(in_features=128, out_features=768, bias=True)\n",
       "       )\n",
       "       (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (modulate1): Modulate()\n",
       "       (attention): MultiHeadSelfAttention(\n",
       "         (attention): MultiHeadAttention(\n",
       "           (_positional_encoder): AlibiPositionalEncoder()\n",
       "           (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "         )\n",
       "       )\n",
       "       (dropout_attention): Dropout(p=0.0, inplace=False)\n",
       "       (modulate2): Modulate()\n",
       "       (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "       (modulate3): Modulate()\n",
       "       (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "       (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "       (act): GELU(approximate='tanh')\n",
       "       (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "       (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "       (modulate4): Modulate()\n",
       "     )\n",
       "   )\n",
       " ),\n",
       " BlockContainer(\n",
       "   (block): AdaLNZeroBlock(\n",
       "     (conditioning_mlp): Sequential(\n",
       "       (0): SiLU()\n",
       "       (1): Linear(in_features=128, out_features=768, bias=True)\n",
       "     )\n",
       "     (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "     (modulate1): Modulate()\n",
       "     (attention): MultiHeadSelfAttention(\n",
       "       (attention): MultiHeadAttention(\n",
       "         (_positional_encoder): AlibiPositionalEncoder()\n",
       "         (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "       )\n",
       "     )\n",
       "     (dropout_attention): Dropout(p=0.0, inplace=False)\n",
       "     (modulate2): Modulate()\n",
       "     (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "     (modulate3): Modulate()\n",
       "     (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "     (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "     (act): GELU(approximate='tanh')\n",
       "     (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "     (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "     (modulate4): Modulate()\n",
       "   )\n",
       " ),\n",
       " AdaLNZeroBlock(\n",
       "   (conditioning_mlp): Sequential(\n",
       "     (0): SiLU()\n",
       "     (1): Linear(in_features=128, out_features=768, bias=True)\n",
       "   )\n",
       "   (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "   (modulate1): Modulate()\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (attention): MultiHeadAttention(\n",
       "       (_positional_encoder): AlibiPositionalEncoder()\n",
       "       (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "     )\n",
       "   )\n",
       "   (dropout_attention): Dropout(p=0.0, inplace=False)\n",
       "   (modulate2): Modulate()\n",
       "   (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "   (modulate3): Modulate()\n",
       "   (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "   (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "   (act): GELU(approximate='tanh')\n",
       "   (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "   (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "   (modulate4): Modulate()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): SiLU()\n",
       "   (1): Linear(in_features=128, out_features=768, bias=True)\n",
       " ),\n",
       " SiLU(),\n",
       " Linear(in_features=128, out_features=768, bias=True),\n",
       " LayerNorm((128,), eps=1e-05, elementwise_affine=True),\n",
       " Modulate(),\n",
       " MultiHeadSelfAttention(\n",
       "   (attention): MultiHeadAttention(\n",
       "     (_positional_encoder): AlibiPositionalEncoder()\n",
       "     (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "   )\n",
       " ),\n",
       " MultiHeadAttention(\n",
       "   (_positional_encoder): AlibiPositionalEncoder()\n",
       "   (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       " ),\n",
       " AlibiPositionalEncoder(),\n",
       " Linear(in_features=128, out_features=384, bias=False),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Modulate(),\n",
       " LayerNorm((128,), eps=1e-05, elementwise_affine=True),\n",
       " Modulate(),\n",
       " Linear(in_features=128, out_features=512, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " GELU(approximate='tanh'),\n",
       " Linear(in_features=512, out_features=128, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Modulate(),\n",
       " BlockContainer(\n",
       "   (block): AdaLNZeroBlock(\n",
       "     (conditioning_mlp): Sequential(\n",
       "       (0): SiLU()\n",
       "       (1): Linear(in_features=128, out_features=768, bias=True)\n",
       "     )\n",
       "     (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "     (modulate1): Modulate()\n",
       "     (attention): MultiHeadSelfAttention(\n",
       "       (attention): MultiHeadAttention(\n",
       "         (_positional_encoder): AlibiPositionalEncoder()\n",
       "         (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "       )\n",
       "     )\n",
       "     (dropout_attention): Dropout(p=0.0, inplace=False)\n",
       "     (modulate2): Modulate()\n",
       "     (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "     (modulate3): Modulate()\n",
       "     (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "     (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "     (act): GELU(approximate='tanh')\n",
       "     (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "     (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "     (modulate4): Modulate()\n",
       "   )\n",
       " ),\n",
       " AdaLNZeroBlock(\n",
       "   (conditioning_mlp): Sequential(\n",
       "     (0): SiLU()\n",
       "     (1): Linear(in_features=128, out_features=768, bias=True)\n",
       "   )\n",
       "   (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "   (modulate1): Modulate()\n",
       "   (attention): MultiHeadSelfAttention(\n",
       "     (attention): MultiHeadAttention(\n",
       "       (_positional_encoder): AlibiPositionalEncoder()\n",
       "       (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "     )\n",
       "   )\n",
       "   (dropout_attention): Dropout(p=0.0, inplace=False)\n",
       "   (modulate2): Modulate()\n",
       "   (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "   (modulate3): Modulate()\n",
       "   (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "   (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "   (act): GELU(approximate='tanh')\n",
       "   (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "   (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "   (modulate4): Modulate()\n",
       " ),\n",
       " Sequential(\n",
       "   (0): SiLU()\n",
       "   (1): Linear(in_features=128, out_features=768, bias=True)\n",
       " ),\n",
       " SiLU(),\n",
       " Linear(in_features=128, out_features=768, bias=True),\n",
       " LayerNorm((128,), eps=1e-05, elementwise_affine=True),\n",
       " Modulate(),\n",
       " MultiHeadSelfAttention(\n",
       "   (attention): MultiHeadAttention(\n",
       "     (_positional_encoder): AlibiPositionalEncoder()\n",
       "     (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       "   )\n",
       " ),\n",
       " MultiHeadAttention(\n",
       "   (_positional_encoder): AlibiPositionalEncoder()\n",
       "   (input_projection): Linear(in_features=128, out_features=384, bias=False)\n",
       " ),\n",
       " AlibiPositionalEncoder(),\n",
       " Linear(in_features=128, out_features=384, bias=False),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Modulate(),\n",
       " LayerNorm((128,), eps=1e-05, elementwise_affine=True),\n",
       " Modulate(),\n",
       " Linear(in_features=128, out_features=512, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " GELU(approximate='tanh'),\n",
       " Linear(in_features=512, out_features=128, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Modulate(),\n",
       " ModuleDict(\n",
       "   (dim0): PointwiseFeedforward(\n",
       "     (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "     (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "     (relu): ReLU()\n",
       "     (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "     (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (dim1): PointwiseFeedforward(\n",
       "     (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "     (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "     (relu): ReLU()\n",
       "     (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "     (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       " ),\n",
       " PointwiseFeedforward(\n",
       "   (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "   (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "   (relu): ReLU()\n",
       "   (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "   (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3, out_features=256, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " ReLU(),\n",
       " Linear(in_features=256, out_features=128, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " PointwiseFeedforward(\n",
       "   (linear1): Linear(in_features=3, out_features=256, bias=True)\n",
       "   (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "   (relu): ReLU()\n",
       "   (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
       "   (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " Linear(in_features=3, out_features=256, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " ReLU(),\n",
       " Linear(in_features=256, out_features=128, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " ModuleDict(\n",
       "   (dim0): PointwiseFeedforward(\n",
       "     (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "     (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "     (relu): ReLU()\n",
       "     (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "     (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       "   (dim1): PointwiseFeedforward(\n",
       "     (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "     (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "     (relu): ReLU()\n",
       "     (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "     (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       "   )\n",
       " ),\n",
       " PointwiseFeedforward(\n",
       "   (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "   (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "   (relu): ReLU()\n",
       "   (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "   (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " Linear(in_features=128, out_features=256, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " ReLU(),\n",
       " Linear(in_features=256, out_features=3, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " PointwiseFeedforward(\n",
       "   (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "   (dropout_linear1): Dropout(p=0.0, inplace=False)\n",
       "   (relu): ReLU()\n",
       "   (linear2): Linear(in_features=256, out_features=3, bias=True)\n",
       "   (dropout_linear2): Dropout(p=0.0, inplace=False)\n",
       " ),\n",
       " Linear(in_features=128, out_features=256, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " ReLU(),\n",
       " Linear(in_features=256, out_features=3, bias=True),\n",
       " Dropout(p=0.0, inplace=False),\n",
       " Identity(),\n",
       " Identity(),\n",
       " AlibiPositionalEncoder(),\n",
       " Tanh()]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.modules())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8b2d33",
   "metadata": {},
   "source": [
    "# Trainer ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6a01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from world_machine.train.stages import PrepareModel, TrainStage\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681573fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = PrepareModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca3a58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<code object inner at 0x7f1244092c10, file \"/usr/lib/python3.12/contextlib.py\", line 78>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm.pre_train == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51cb58d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'world_machine.train.stages.prepare_model',\n",
       "              '__init__': <function world_machine.train.stages.prepare_model.PrepareModel.__init__(self)>,\n",
       "              'pre_batch': <function world_machine.train.stages.prepare_model.PrepareModel.pre_batch(self, model: world_machine.world_machine.WorldMachine, mode: world_machine.train.mode.DatasetPassMode, criterions: dict[str, dict[str, torch.nn.modules.module.Module]], optimizer: torch.optim.optimizer.Optimizer, device: torch.device, losses: dict, train_criterions: dict[str, dict[str, float]]) -> None>,\n",
       "              'post_batch': <function world_machine.train.stages.prepare_model.PrepareModel.post_batch(self, model: world_machine.world_machine.WorldMachine, losses: dict, criterions: dict[str, dict[str, torch.nn.modules.module.Module]], train_criterions: dict[str, dict[str, float]]) -> None>,\n",
       "              '__doc__': None,\n",
       "              '__abstractmethods__': frozenset(),\n",
       "              '_abc_impl': <_abc._abc_data at 0x7f103d030c00>})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PrepareModel.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f016cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
