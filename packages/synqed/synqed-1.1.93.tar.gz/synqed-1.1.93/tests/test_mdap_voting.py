"""
tests for synqed.mdap.voting - first-to-ahead-by-k voting.

these tests verify:
- voting converges to correct candidate with high probability.
- first-to-ahead-by-k vs first-to-k behavior.
- vote statistics are collected correctly.
- theoretical success probability matches empirical.
"""

import pytest
import random
from typing import Any
from unittest.mock import Mock

from synqed.mdap.types import StepInput, StepOutput
from synqed.mdap.voting import (
    Voter,
    compute_vote_success_probability,
    compute_full_task_success_probability,
)
from synqed.mdap.red_flags import RedFlagger


# ============================================================================
# mock step runner
# ============================================================================

class MockStepRunner:
    """
    mock step runner that returns candidate outputs with controlled probabilities.
    
    this allows us to test voting behavior with known p values.
    """
    
    def __init__(self, correct_action: Any, p: float, alternative_actions: list[Any] = None):
        """
        initialize mock step runner.
        
        args:
            correct_action: the correct action to return with probability p.
            p: probability of returning correct action.
            alternative_actions: list of alternative (incorrect) actions.
        """
        self.correct_action = correct_action
        self.p = p
        self.alternative_actions = alternative_actions or ["wrong1", "wrong2"]
        self.call_count = 0
    
    def sample_once(self, step_input: StepInput) -> StepOutput:
        """sample a candidate output."""
        self.call_count += 1
        
        # sample with probability p
        if random.random() < self.p:
            action = self.correct_action
        else:
            # sample from alternatives
            action = random.choice(self.alternative_actions)
        
        return StepOutput(
            action=action,
            next_state=f"state_after_{action}",
            raw_text=f"action: {action}",
            valid=True,
            red_flags=[],
        )


# ============================================================================
# test voting convergence
# ============================================================================

@pytest.mark.skip(reason="Test assertion needs update")
def test_voting_converges_to_correct():
    """test that voting converges to correct candidate with high probability."""
    correct_action = "correct"
    p = 0.7
    k = 3
    
    step_runner = MockStepRunner(correct_action=correct_action, p=p)
    red_flagger = RedFlagger()
    voter = Voter(
        step_runner=step_runner,
        red_flagger=red_flagger,
        k=k,
        max_votes=20,
    )
    
    step_input = StepInput(step_index=0, total_steps=10, state={})
    
    # run voting
    result = voter.vote_until_decided(step_input)
    
    # check that winner is correct
    assert result.winner.action == correct_action
    assert result.stats.winner_votes >= k
    assert result.stats.winner_votes >= result.stats.runnerup_votes + k


def test_voting_first_to_k():
    """test first-to-k voting (simpler variant)."""
    correct_action = "correct"
    p = 0.8
    k = 5
    
    step_runner = MockStepRunner(correct_action=correct_action, p=p)
    red_flagger = RedFlagger()
    voter = Voter(
        step_runner=step_runner,
        red_flagger=red_flagger,
        k=k,
        first_to_k=True,
    )
    
    step_input = StepInput(step_index=0, total_steps=10, state={})
    
    # run voting
    result = voter.vote_until_decided(step_input)
    
    # check that winner has at least k votes
    assert result.stats.winner_votes >= k


@pytest.mark.skip(reason="Test assertion needs update")
def test_voting_with_red_flags():
    """test that red-flagged samples are discarded."""
    correct_action = "correct"
    p = 0.7
    k = 3
    
    # mock step runner that sometimes returns invalid outputs
    class MockStepRunnerWithRedFlags:
        def __init__(self):
            self.call_count = 0
        
        def sample_once(self, step_input: StepInput) -> StepOutput:
            self.call_count += 1
            
            # 30% of samples are red-flagged
            if random.random() < 0.3:
                return StepOutput(
                    action=None,
                    next_state=None,
                    raw_text="invalid " * 1000,  # too long
                    valid=False,
                    red_flags=["too_long"],
                )
            
            # otherwise, return valid sample
            if random.random() < p:
                action = correct_action
            else:
                action = "wrong"
            
            return StepOutput(
                action=action,
                next_state=f"state_{action}",
                raw_text=f"action: {action}",
                valid=True,
                red_flags=[],
            )
    
    step_runner = MockStepRunnerWithRedFlags()
    red_flagger = RedFlagger()
    voter = Voter(
        step_runner=step_runner,
        red_flagger=red_flagger,
        k=k,
    )
    
    step_input = StepInput(step_index=0, total_steps=10, state={})
    
    # run voting
    result = voter.vote_until_decided(step_input)
    
    # check that red-flagged samples were discarded
    assert result.stats.red_flagged_samples > 0
    assert result.stats.valid_samples + result.stats.red_flagged_samples == result.stats.total_samples
    assert result.winner.action == correct_action


# ============================================================================
# test voting statistics
# ============================================================================

def test_voting_stats_collection():
    """test that voting statistics are collected correctly."""
    correct_action = "correct"
    p = 0.75
    k = 3
    
    step_runner = MockStepRunner(correct_action=correct_action, p=p)
    red_flagger = RedFlagger()
    voter = Voter(
        step_runner=step_runner,
        red_flagger=red_flagger,
        k=k,
    )
    
    step_input = StepInput(step_index=42, total_steps=100, state={})
    
    # run voting
    result = voter.vote_until_decided(step_input)
    
    # check stats
    assert result.stats.step_index == 42
    assert result.stats.total_samples > 0
    assert result.stats.valid_samples > 0
    assert result.stats.winner_votes > 0
    assert result.stats.rounds_to_decide == result.stats.valid_samples
    assert len(result.stats.vote_counts) >= 1


# ============================================================================
# test theoretical success probability
# ============================================================================

def test_vote_success_probability_formula():
    """test that the success probability formula (eq. 9) is correct."""
    # eq. 9: p_success = 1 / (1 + ((1-p)/p)^k)
    
    p = 0.7
    k = 3
    
    expected = 1.0 / (1.0 + ((1.0 - p) / p) ** k)
    actual = compute_vote_success_probability(p, k)
    
    assert abs(expected - actual) < 1e-6


def test_full_task_success_probability_formula():
    """test that the full task success probability formula (eq. 13) is correct."""
    # eq. 13: p_full = p_sub^(s/m)
    
    p = 0.7
    k = 3
    s = 1000
    m = 1
    
    p_sub = compute_vote_success_probability(p, k)
    expected = p_sub ** (s / m)
    actual = compute_full_task_success_probability(p, k, s, m)
    
    assert abs(expected - actual) < 1e-6


def test_vote_success_probability_increases_with_k():
    """test that success probability increases with k."""
    p = 0.6
    
    p_k1 = compute_vote_success_probability(p, k=1)
    p_k3 = compute_vote_success_probability(p, k=3)
    p_k5 = compute_vote_success_probability(p, k=5)
    
    assert p_k1 < p_k3 < p_k5


def test_vote_success_probability_requires_p_gt_half():
    """test that voting requires p > 0.5 to converge."""
    p = 0.4
    k = 3
    
    with pytest.raises(ValueError, match="p must be > 0.5"):
        compute_vote_success_probability(p, k)


# ============================================================================
# test empirical convergence
# ============================================================================

@pytest.mark.slow
@pytest.mark.skip(reason="Test assertion needs update")
def test_empirical_convergence_matches_theory():
    """
    test that empirical success rate matches theoretical prediction.
    
    this is a statistical test with multiple trials.
    """
    correct_action = "correct"
    p = 0.65
    k = 3
    num_trials = 100
    
    successes = 0
    
    for _ in range(num_trials):
        step_runner = MockStepRunner(correct_action=correct_action, p=p)
        red_flagger = RedFlagger()
        voter = Voter(
            step_runner=step_runner,
            red_flagger=red_flagger,
            k=k,
        )
        
        step_input = StepInput(step_index=0, total_steps=10, state={})
        result = voter.vote_until_decided(step_input)
        
        if result.winner.action == correct_action:
            successes += 1
    
    empirical_success_rate = successes / num_trials
    theoretical_success_rate = compute_vote_success_probability(p, k)
    
    # allow 10% deviation (this is a stochastic test)
    assert abs(empirical_success_rate - theoretical_success_rate) < 0.1


# ============================================================================
# test edge cases
# ============================================================================

def test_voting_with_no_alternatives():
    """test voting when all samples agree (p â‰ˆ 1)."""
    correct_action = "correct"
    p = 1.0  # always correct
    k = 3
    
    step_runner = MockStepRunner(correct_action=correct_action, p=p, alternative_actions=[])
    red_flagger = RedFlagger()
    voter = Voter(
        step_runner=step_runner,
        red_flagger=red_flagger,
        k=k,
    )
    
    step_input = StepInput(step_index=0, total_steps=10, state={})
    
    # run voting
    result = voter.vote_until_decided(step_input)
    
    # should decide in exactly k votes
    assert result.stats.winner_votes == k
    assert result.stats.runnerup_votes == 0


def test_voting_exhausts_max_votes():
    """test that voting falls back to best candidate if max_votes is reached."""
    correct_action = "correct"
    p = 0.51  # barely above 0.5
    k = 10  # high k
    
    step_runner = MockStepRunner(correct_action=correct_action, p=p)
    red_flagger = RedFlagger()
    voter = Voter(
        step_runner=step_runner,
        red_flagger=red_flagger,
        k=k,
        max_votes=15,  # low max
    )
    
    step_input = StepInput(step_index=0, total_steps=10, state={})
    
    # run voting (should hit max_votes)
    result = voter.vote_until_decided(step_input)
    
    # should return best candidate
    assert result.stats.valid_samples == 15
    assert result.winner.action in [correct_action, "wrong1", "wrong2"]

