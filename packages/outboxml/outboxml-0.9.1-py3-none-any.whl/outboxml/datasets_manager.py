import json
import os
import pickle
from copy import deepcopy
from itertools import chain
from pathlib import Path

from pydantic import ValidationError
from sklearn.base import is_classifier
import pandas as pd
import numpy as np
from loguru import logger
from typing import List, Dict, Any, Optional, Union, Literal
from sklearn.preprocessing import LabelEncoder

from outboxml.monitoring_result import DataContext
from outboxml.core.enums import ModelsParams
from outboxml.data_subsets import DataPreprocessor, ModelDataSubset
from outboxml.dataset_retro import RetroDataset
from outboxml.datadrift import DataDrift
from outboxml.core.data_prepare import prepare_dataset
from outboxml.core.pydantic_models import AllModelsConfig, DataModelConfig, ModelConfig
from outboxml.extractors import Extractor, BaseExtractor, SimpleExtractor
from outboxml.metrics.base_metrics import BaseMetric, BaseMetrics
from outboxml.core.prepared_datasets import PrepareDataset, TrainTestIndexes, PrepareDatasetPl
from outboxml.metrics.processor import ModelMetrics
from outboxml.models import DefaultModels
from outboxml import config


class DSManagerResult:
    """Класс контейнер результатов

    Parameters
    ___________
    model_name: имя модели
    config: конфиг файл с исходными данными
    model: обученная модель
    datasubset: Объект ModelDataSubset, содержащий вектора X_train/test, Y_train/test, имена числовых и категориальных фичей, экспозицию
    model config: конфиг модели
    __________
    Methods
    dict_for_prod_export - Возвращает словаь для формирования pickle файла для сервиса
    from_pickle_model_result - Конвертер словаря из пикл сервиса в объект (class method)
    ______
    Properties:
    ______
    X - вектор X
    y_pred - Predictions
    y - y_true
    exposure - вектор экспозиции
    """

    def __init__(self,
                 model_name: str,
                 model: Any,
                 data_subset: ModelDataSubset,
                 model_config: ModelConfig = None,
                 config: AllModelsConfig = None,
                 predictions: dict = None,
                 metrics: dict = None,
                 ):
        if predictions is None:
            self.predictions = {'train': None, 'test': None}
        else:
            self.predictions = predictions
        self.model_name = model_name
        self.config = config
        if metrics is None:
            self.metrics = {'train': None, 'test': None}
        else:
            self.metrics = metrics
        self.model = model
        self.data_subset = data_subset
        self.model_config = model_config

    def load_metrics(self, metrics: dict, ds_type: str = None):
        if ds_type is not None:
            self.metrics[ds_type] = metrics
        else:
            self.metrics = metrics

    def load_predictions(self, df: Union[pd.DataFrame, pd.Series], ds_type: str):
        self.predictions[ds_type] = df

    def dict_for_prod_export(self, ):
        """Конверте DSManagerResult в пикл"""
        model = self.model
        model_results = {
            "model_config": self.model_config.model_dump(),
            "model": model,
            "min_max_scaler": None,
            "features_numerical": self.data_subset.features_numerical,
            "features_categorical": self.data_subset.features_categorical,
        }

        return model_results

    @classmethod
    def from_pickle_model_result(cls, model_result: dict, all_model_config: AllModelsConfig, ):
        """Конвертер пикла в DSManagerResult. Используется библиотечный вид модели (wrapper)"""

        model_config = model_result['model_config']
        model_name = model_result['model_config']['name']
        model_config = ModelConfig.model_validate(model_config)
        model = model_result['model']
        features_numerical = model_result['features_numerical']
        features_categorical = model_result['features_categorical']
        return cls(model_name=model_name,
                   config=all_model_config,
                   data_subset=ModelDataSubset(model_name=model_name,
                                               features_numerical=features_numerical,
                                               features_categorical=features_categorical,
                                               ),
                   model=model,
                   model_config=model_config)

    @property
    def X(self):
        return pd.concat([self.data_subset.X_train, self.data_subset.X_test])

    @property
    def y(self):
        return pd.concat([self.data_subset.y_train, self.data_subset.y_test])

    @property
    def y_pred(self):
        y_pred = pd.concat([self.predictions['train'], self.predictions['test']])
        return y_pred

    @property
    def exposure(self):
        exposure = pd.concat([self.data_subset.exposure_train, self.data_subset.exposure_test])
        return exposure


class DataSetsManager:
    """Основной класс для работы с моделями.

    Для работы с фреймоврком необходим заполенный по правилам config файл.
    Из коробки работа производится по параметрам конфига DataSetsManager(config_name = config).
    
    Для пользовательской настройки фреймоворка необходимо импортировать модули:
    from outboxml.extractors.extractor import Extractor, RTDMExtractor, ActuarExtractor
    from outboxml.metrics.metrics import BaseMetric
    from outboxml.models import Model
    from outboxml.dataset_retro import RetroDataset
    from outboxml.export_results import ResultsExport

    Работа с фрейморком начинается с создания объекта с параметрами.
    Обязательный параметр на входе: путь к конфиг файлу или сам валидированный конифиг-файл AllModesConfig
    Остальные параметры могут быть установлены автоматически "из коробки",
    Информация о работе выводится в виде лог-файла.
    Результаты моделирования выводятся в контейнере результатов DSManagerResult


    Parameters:
    ----------
    config_name: path to config or validated config file

    extractor: User-defined extractor object inheritanced by Extractor interface.
               main method - extract_dataset() should return Pandas Dataframe
               extractor should contain check_object() method with data validation and verification
               Use library RTDMExtractor or ActuarExtractor for working with databases

    modified_data: dict {name: PreparedDataset} with models preparation objects. Wrapper of prepare_dataset function
                        PreparedDataset by default uses model_config features and has no prep and post prep functions


    models_dict: dict {name: Model} with models for train and prediction inheritanced by Model class.
                class should have fit(), predict() and models_dict() methods.
                By default model is chosen by group and project name. You can import models from library

    business_metric: user-defined business metric.
                    The inheritace of BaseMetric. Main method is calculate_metric()

    use_baseline_model: bool , Выбор Baseline. 1 - RandomForestRegressor, 2 - DummyRegressor median; 3 - mean

    retroChanges: RetroDataset object for retro analysis.

    Methods:
    __________
    load_dataset() - loading due to source and path from config file or user-defined extractor class
    get_trainDfs(model_name: str) - return prepared datasubset to train user model
    get_testDfs(model_name: str) - return prepared datasubset to test user model
    fit_models({model_name: model, ...}, need_fit=True) - fit< predict and get metrics for all models in model_dict
    get_result() - return container of results DSManagerResult
    check_datadrift(model_name: str) - returns dataframe with datadrift analysis result
    ----------
    Examples:
    ----------
    To see more examples go to outboxml/examples repository


    Examples:
    _______
    1. Titanic

        #post prep function
        def data_post_prep_func(data: pd.DataFrame):
            data["SEX"] = pd.to_numeric(data["SEX"])
            return data

        titanic_ds_manager =  DataSetsManager(config_name=config_name,
                                         extractor=TitanicExampleExtractor(path_to_file=path_to_data),
                                         prepared_datasets={
                                                        'first': PrepareDataset(group_name='survived1',
                                                                                    data_post_prep_func=data_post_prep_func,
                                                                                    check_prepared=True,
                                                                                    calc_corr=True),
                                                        'second': PrepareDataset(group_name='survived2',)
                                                            },
                                         business_metric=TitanicExampleMetric()

                                         )
        titanic_TrainDs = titanic_ds_manager.get_TrainDfs(model_name='first')
        titanic_results = titanic_ds_manager.fit_models()

    """

    def __init__(
            self,
            config_name: Union[str, Dict],
            extractor: Optional[Extractor] = None,
            prepared_datasets: Optional[Dict[str, PrepareDataset]] = None,
            models_dict: Optional[Dict] = None,
            business_metric: Optional[BaseMetric] = None,
            use_baseline_model: int = 0,
            retro_changes: Optional[RetroDataset] = None,
            external_config = None,
            use_temp_files: bool = False,
            prepare_engine: Literal['pandas', 'polars'] = 'pandas',
    ):
        if external_config is None:
            self._external_config = config
        else:
            self._external_config = external_config
        self._work_type_fit = self._external_config.work_type_fit
        self._work_type_hptune = self._external_config.work_type_hptune
        self._use_temp_files = use_temp_files
        self._prepare_engine = prepare_engine
        self._exposure = {}
        self._all_models_config_name: Union[str, Dict] = config_name
        self._results: Dict[str, DSManagerResult] = {}
        self._extractor: Optional[Extractor] = extractor
        self._prepare_datasets: Optional[Dict[str, PrepareDataset]] = prepared_datasets
        self._models_dict: Optional[Dict] = models_dict
        self._use_baseline_model = use_baseline_model
        self._business_metric: Optional[BaseMetric] = business_metric
        self._data_preprocessor: Optional[DataPreprocessor] = None
        self.X: Optional[pd.DataFrame] = None
        self.Y: Optional[pd.DataFrame] = None
        self.index_train: Optional[pd.Index] = None
        self.index_test: Optional[pd.Index] = None
        self.targets_columns_names = []
        self.extra_columns: Optional[pd.DataFrame] = None
        self.all_models_config: Optional[AllModelsConfig] = None
        self.group_name = 'general'
        self.data_config: Optional[DataModelConfig] = None
        self._models_configs: List[ModelConfig] = []
        self._retro = False
        self.business_metric_value = {}

        self._retro_changes = retro_changes
        self._retro_dataset = None

        self._default_name = None
        self._init_dsmanager()

    @property
    def dataset(self):
        return self._data_preprocessor.dataset

    @property
    def config(self):
        config_to_return = deepcopy(self._all_models_config)
        if self._results != {}:
            updated_models_configs = []
            for result in self._results.values():
                updated_models_configs.append(result.model_config)
            config_to_return.models_configs = updated_models_configs

        return config_to_return

    def get_result(self) -> Dict[str, DSManagerResult]:
        return self._results

    def load_dataset(self, data: pd.DataFrame = None) -> pd.DataFrame:
        """Load data from source due to config or user-defined extractor object. Use .env file or external config for extracor
        Also you can load dataset by parameter data"""

        logger.debug("Dataset loading")
        if data is not None:
            self._extractor = SimpleExtractor(data=data)
        data = self._extractor.extract_dataset()
        logger.debug('DataSet is extracted')
        return data

    def get_subset(self, model_name):
        if model_name is None: model_name = self._default_name
        logger.debug('Model ' + model_name + ' || Subset export')
        return self._data_preprocessor.get_subset(model_name)

    @property
    def data_subsets(self, ):
        return self._data_preprocessor.data_subsets()

    def fit_models(self, models_dict: dict = None, need_fit: bool = False, model_name: str = None,
                  ) -> dict:
        """Fitting and calculating metrics for models. If 'need_fit' option then fit methods are calling for models
        Uf load_subsets_from_pickle option then loading previously saved datasubsets in enviroment"""

        fitted = False
        logger.debug('Fitting model started')
        if models_dict is not None:
            models = models_dict
            if need_fit:
                fitted = False
            logger.info('User-defined models')
        else:
            if self._models_dict is None:
                logger.info('Setting default models')
                self.__load_models()
                fitted = False
            models = self._models_dict

        if model_name is not None:
            try:
                chosen_model = models[model_name]
                models = {chosen_model: models[model_name]}
            except KeyError:
                logger.error('Wrong model name in input')

        fitted_models = self.__get_fitted_models(models=models, fitted=fitted)
        metrics = {}
        for model_name, model in fitted_models.items():
            data_subset = self.get_subset(model_name)
            predictions_train = self._predict(model, data_subset.X_train)
            predictions_test = self._predict(model, data_subset.X_test)

            metrics[model_name] = ModelMetrics(data_config=self.data_config,
                                               model_config=self._prepare_datasets[model_name].get_model_config(),
                                               data_subset=data_subset,
                                               ).result_dict(predictions={'train': predictions_train,
                                                                          'test': predictions_test})



            self._results[model_name] = DSManagerResult(model_name=model_name,
                                                        model=model,
                                                        config=self.config,
                                                        data_subset=data_subset,
                                                        model_config=self._prepare_datasets[
                                                            model_name].get_model_config(),
                                                        predictions={'train': predictions_train,
                                                                     'test': predictions_test},
                                                        metrics=metrics[model_name])
        try:
            metrics.update(self._calculate_business_metric())
        except Exception as exc:
            logger.error('Error while calculation business metric||'+ str(exc))
        return metrics


    def check_datadrift(self, model_name: str) -> pd.DataFrame:
        """Method for checking datadrift between train and test. Using DataDrift library"""
        subset = self.get_subset(model_name)
        report = DataDrift(full_calc=False).review(DataContext(X_train=subset.X_train, X_test=subset.X_test))

        return report

    def model_predict(self,
                      data: pd.DataFrame,
                      model_name: str,
                      model_result=None,
                      full_output: bool = True,
                      ) -> DSManagerResult:
        """Method for constructing DSManagerResult for external model or data.
        Use model_result as dict from service or DSManagerResult
        Use full_output option to get only prediction vector of full output"""
        logger.debug('Prediction for external data||' + model_name)
        if model_result is None:
            logger.info('No external model||Using inner results')
            result = self.get_result()
            model_result = result[model_name]
        else:
            if isinstance(model_result, dict):
                try:
                    model_result = model_result[model_name]
                except KeyError as e:
                    # logger.error(e)
                    logger.debug('Converting pickle to DSManagerResult')
                    model_result = DSManagerResult.from_pickle_model_result(model_result=model_result,
                                                                            all_model_config=self._all_models_config)

        model_config = deepcopy(model_result.model_config)
        model_config.column_exposure = None
        model = model_result.model
        features_numerical = model_result.data_subset.features_numerical
        features_categorical = model_result.data_subset.features_categorical
        data_to_predict = data.copy()

        preproc = DataPreprocessor(prepare_dataset_interface_dict={model_name:
                                                                           PrepareDataset(model_config=model_config,
                                                   check_prepared=False,
                                                   )},
                                       dataset=data_to_predict,
                                       data_config=self.data_config,
                                       prepare_engine='pandas',)
        data_subset = preproc.get_subset(model_name, from_pickle=False)
        output_model = model
        prediction = model.predict(data_subset.X[chain(features_numerical, features_categorical)])
        if isinstance(prediction, np.ndarray):
            prediction = pd.Series(prediction, index=data_subset.X.index)

        metrics = ModelMetrics(model_config=model_config,
                               data_subset=data_subset,
                               data_config=None).result_dict(
            predictions={'train': prediction[prediction.index.isin(preproc.index_train)],
                         'test': prediction[prediction.index.isin(preproc.index_test)]},
        )

        res = DSManagerResult(model_name=model_name,
                              config=model_result.config,
                              model=output_model,
                              data_subset=data_subset,
                              model_config=model_config,
                              predictions={'train': prediction.loc[prediction.index.isin(preproc.index_train)],
                                           'test': prediction.loc[prediction.index.isin(preproc.index_test)]},
                              metrics=metrics,
                              )

        logger.debug('Prediction for external data finished')
        return res

    def __get_fitted_models(self, models: dict, fitted: bool = False) -> dict:
        return models

    def _predict(self, model, X):
        if X is None: return None
        if type(model).__name__ == 'Prophet':
            logger.info('Prophet in work..')
            data = model.predict(X)
            prediction_series = pd.Series(data=np.expm1(data['yhat']), index=data.index)
            logger.info('Prophet finished')
        elif is_classifier(model):
            prediction_series = pd.Series(data=model.predict_proba(X)[:, 1], index=X.index)
        else:
            try:
                data = model.predict(X)
                prediction_series = pd.Series(data=data, index=X.index)
            except:
                logger.info('Using label encoder for prediction')
                le = LabelEncoder()
                for column_name in X.columns:
                    X[column_name] = le.fit_transform(X[column_name])
                prediction_series = pd.Series(data=model.predict(X), index=X.index)
        return prediction_series

    def _calculate_business_metric(self,) -> dict:
        metric = {}
        try:
            if self._business_metric is not None:
                metric = self._business_metric.calculate_metric(self.get_result())
                self.business_metric_value = metric
                logger.info('Business metric value||'+ str(metric))
        except ModuleNotFoundError as e:
            logger.info('No business metrics')

        logger.debug('Calculating metrics finished')
        return metric

    def __load_all_models_config(self):

        if isinstance(self._all_models_config_name, dict):
            logger.info("All models config from dict")
            all_models_config = json.dumps(self._all_models_config_name)

        else:
            logger.info("All models config from path")
            try:
                with open(self._all_models_config_name, "r", encoding='utf-8') as f:
                    all_models_config = f.read()
            except FileNotFoundError:
                logger.error("Invalid all models config name")
                raise FileNotFoundError("Invalid config name")

        try:
            self._all_models_config = AllModelsConfig.model_validate_json(all_models_config)
        except ValidationError as e:
            logger.error("Config validation error")
            raise ValidationError(e)
        self.data_config = self._all_models_config.data_config
        self._models_configs = self._all_models_config.models_configs
        self.version = self._all_models_config.version
        for model in self._models_configs:

            file_path = os.path.join(self._external_config.results_path, model.name + '_v' + self.version + '_subset.pickle')
            if os.path.exists(file_path):
                if not self._retro:
                    logger.warning(f'{model.name}||File {file_path} already exists. Change version in config file to for new data prepare')
                else:
                    logger.warning(
                        f'{model.name}||File {file_path} already exists. Changing version in config file for A/B test')
                    self.version = self.version + '_new'
        self.group_name = f"{self._all_models_config.project}_{self._all_models_config.version}"

        logger.info("Config is loaded")

    def __load_targets_names(self):

        self.random_state = self.data_config.separation.random_state

        if self.random_state:
            np.random.seed(self.random_state)

        self.targets_columns_names = list(set(
            [model.column_target for model in self._models_configs if model.column_target]
            + [model.column_exposure for model in self._models_configs if model.column_exposure]
        ))

    def __load_prepare_datasets(self):

        i = 0
        if self._prepare_datasets is None and self._prepare_engine == "pandas":
            self._prepare_datasets = {}
            logger.info("Load models prepare datasets")
            for model_config in self._models_configs:
                self._prepare_datasets[model_config.name] = PrepareDataset(model_config=model_config,
                                                                           check_prepared=True,
                                                                           group_name=self.group_name)

        elif self._prepare_datasets is None and self._prepare_engine == "polars":
            self._prepare_datasets = {}
            logger.info("Load models prepare datasets with polars")
            for model_config in self._models_configs:
                self._prepare_datasets[model_config.name] = PrepareDatasetPl(
                    group_name=self.group_name, model_config=model_config, check_prepared=True
                )

        elif self._prepare_datasets is None:
            logger.error("Unknown prepare engine")
            raise ValueError("Unknown prepare engine")

        else:
            logger.info("User models prepare datasets")
            for value in self._prepare_datasets.values():
                if value.get_model_config() is None:
                    logger.info(f"Load config  from DS Manager")
                    value.load_model_config(model_config=self._models_configs[i])
                    i += 1
        self._default_name = list(self._prepare_datasets.keys())[0]

    def __load_models(self):
        if self._models_dict is None:
            self._models_dict = DefaultModels(dataset=self.dataset,
                                              data_subsets=self._data_preprocessor.data_subsets(),
                                              models_configs=self._models_configs,
                                              group_name=self.group_name,
                                              baseline_model=self._use_baseline_model,
                                              work_type_fit=self._work_type_fit).load_default()

    def __init_retro(self):
        logger.debug('Initializing retro')
        self._retro_dataset = self._retro_changes.get_retro_dataset()
        self._models_configs = self._retro_changes.models_config_for_retro(models_config=self._models_configs,
                                                                           target_columns_names=self.targets_columns_names)

    def _init_dsmanager(self):
        logger.debug('Initializing DSManager')
        self.__load_all_models_config()
        self.__load_targets_names()
        if self._retro_changes is not None:
            self.__init_retro()
        self.__load_prepare_datasets()

        if self._extractor is not None:
            logger.info("Reading user extractor")
            if self._extractor.load_config_from_env:
                logger.info("Reading config from env")
                self._extractor.load_config(connection_config=config)


        else:
            self._extractor = BaseExtractor(data_config=self.data_config)
        logger.debug('Initializing completed')
        self._data_preprocessor = DataPreprocessor(prepare_dataset_interface_dict=self._prepare_datasets,
                                                   dataset=self._extractor,
                                                   external_config=self._external_config,
                                                   version=self.version,
                                                   prepare_engine=self._prepare_engine,
                                                   use_saved_files=self._use_temp_files,
                                                   data_config=self.data_config,
                                                   retro=self._retro
                                                   )
