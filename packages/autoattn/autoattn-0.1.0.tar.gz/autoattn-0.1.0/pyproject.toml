[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "autoattn"
version = "0.1.0"
description = "Automatic routing between attention backends (dense, flash, sparse)"
authors = [
    { name = "Achintya Paningapalli" }
]
readme = "README.md"
license = { text = "MIT" }
requires-python = ">=3.9"
keywords = ["attention", "transformer", "llm", "flash-attention", "pytorch"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dependencies = [
    "torch>=2.0"
]

[project.optional-dependencies]
dev = [
    "pytest",
    "pytest-cov",
]

[project.urls]
Homepage = "https://github.com/achintya-p/auto_attn"
Repository = "https://github.com/achintya-p/auto_attn"

[tool.setuptools]
packages = ["autoattn", "autoattn.backends"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --tb=short"
