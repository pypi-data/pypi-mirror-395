---
title: pipelines
content_type: event
source_url: https://motherduck.com/glossary/pipelines
indexed_at: '2025-11-25T20:02:23.790934'
content_hash: 516ca89d51b20012
---

Hands-on Lab: Agentic Data Engineering with MotherDuck and Ascend[December 3, 10am PT / 1pm ET](https://www.ascend.io/events/hands-on-lab-agentic-data-engineering-with-motherduck)

[Motherduck home](https://motherduck.com/)

[START FREE](https://app.motherduck.com/?auth_flow=signup)

# pipelines

_[Back to DuckDB Data Engineering Glossary](https://motherduck.com/glossary/)_

Data pipelines are automated workflows that move and transform data from various sources to one or more destinations. They typically consist of interconnected steps or tasks that extract data from its origin, apply transformations or cleansing operations, and load the processed data into a target system for analysis or storage. Modern data pipelines often leverage tools like [Apache Airflow](https://airflow.apache.org/), [Dagster](https://dagster.io/), or [Prefect](https://www.prefect.io/) to orchestrate these workflows, ensuring data flows smoothly and reliably through an organization's data infrastructure. Pipelines can handle batch processing of large datasets or facilitate real-time streaming of data, depending on the requirements. They play a crucial role in maintaining data quality, consistency, and timeliness across different systems and applications within a data ecosystem.

Authorization Response