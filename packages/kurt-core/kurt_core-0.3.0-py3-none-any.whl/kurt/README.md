# Kurt Core CLI

Document intelligence CLI - Fetch web content and extract metadata using Trafilatura.

## Installation

```bash
# From project root
uv sync
```

## Quick Start

```bash
# 1. Initialize a new Kurt project (creates .kurt/ directory and SQLite database)
uv run kurt init --ide claude   # For Claude Code
uv run kurt init --ide cursor   # For Cursor

# 2. Map sitemap to discover URLs (fast, no downloads)
uv run kurt ingest map https://example.com

# 3a. Fetch single document
uv run kurt ingest fetch <document-id>

# 3b. Or batch fetch all discovered URLs (parallel, recommended)
uv run kurt ingest fetch --url-prefix https://example.com/

# 4. List documents
uv run kurt document list

# 5. Get document details
uv run kurt document get <document-id>
```

## Configuration

Kurt stores project configuration in a `.kurt` file in your project directory:

```
KURT_PROJECT_PATH="."
KURT_DB=".kurt/kurt.sqlite"
```

This file is auto-generated by `kurt init`. You can customize:
- **KURT_PROJECT_PATH**: Root directory of your Kurt project
- **KURT_DB**: Path to SQLite database (relative to project path)

Example custom initialization:
```bash
# Use a custom database path
uv run kurt init --db-path data/my-project.db

# Initialize in a specific directory
cd /path/to/my-project
uv run kurt init
```

## Commands

### Content Ingestion

**Map-Then-Fetch Workflow** (recommended):
1. **Map**: Discover URLs from sitemap (fast, creates `NOT_FETCHED` records)
2. **Review**: List and examine discovered URLs
3. **Fetch**: Selectively download content (batch or single)

```bash
# Discover URLs from sitemap
uv run kurt ingest map https://example.com
uv run kurt ingest map https://example.com --limit 10      # Test with first 10 URLs
uv run kurt ingest map https://example.com --fetch         # Map + fetch in one step

# Fetch content
uv run kurt ingest fetch <doc-id>                          # Single document
uv run kurt ingest fetch https://example.com/page          # By URL (creates if needed)
uv run kurt ingest fetch --url-prefix https://example.com/ # Batch: all matching prefix
uv run kurt ingest fetch --url-contains /blog/             # Batch: URLs containing string
uv run kurt ingest fetch --all                             # Batch: all NOT_FETCHED docs
uv run kurt ingest fetch --url-prefix https://example.com/ --max-concurrent 10  # 10 parallel downloads
uv run kurt ingest fetch --url-prefix https://example.com/ --status ERROR       # Retry failed docs

# Add single URL without sitemap
uv run kurt ingest add https://example.com/page
```

### Document Management

```bash
# List documents
uv run kurt document list
uv run kurt document list --status FETCHED --limit 20
uv run kurt document list --status NOT_FETCHED

# Get document details
uv run kurt document get <document-id>          # Full UUID
uv run kurt document get 44ea066e               # Partial UUID (min 8 chars)

# Delete document
uv run kurt document delete <document-id>
uv run kurt document delete <document-id> --delete-content  # Also delete markdown file
uv run kurt document delete <document-id> --yes             # Skip confirmation

# Document statistics
uv run kurt document stats
```

### Project Management

```bash
# Initialize project for Claude Code (default)
uv run kurt init --ide claude                      # Current directory

# Initialize project for Cursor
uv run kurt init --ide cursor

# Initialize with custom paths
uv run kurt init --ide claude --db-path data/my-db.sqlite

# What gets created:
# - .kurt/ directory with SQLite database
# - .claude/ or .cursor/ directory with IDE-specific instructions
# - kurt/ directory with content templates
```

## Architecture

**Content Storage**:
- Metadata stored in SQLite (`Document` table)
- Content stored as markdown files in `sources/{domain}/{path}/`
- Metadata extracted with Trafilatura (title, author, date, categories, language)

**Batch Fetching**:
- Uses `httpx` with async/await for parallel downloads
- Semaphore-based concurrency control (default: 5 concurrent)
- Graceful error handling (continues on individual failures)

**Database Schema**:
```sql
CREATE TABLE documents (
    id TEXT PRIMARY KEY,              -- UUID
    title TEXT NOT NULL,
    source_type TEXT,                 -- URL, FILE_UPLOAD, API
    source_url TEXT UNIQUE,
    content_path TEXT,                -- Relative path to markdown file
    ingestion_status TEXT,            -- NOT_FETCHED, FETCHED, ERROR
    content_hash TEXT,                -- Trafilatura fingerprint (deduplication)
    description TEXT,                 -- Meta description
    author JSON,                      -- List of authors
    published_date DATETIME,
    categories JSON,                  -- Tags/categories
    language TEXT,                    -- ISO 639-1 code
    created_at DATETIME,
    updated_at DATETIME
);
```

## Use Cases

**Content Aggregation**:
```bash
# Ingest entire blog
uv run kurt ingest map https://blog.example.com
uv run kurt ingest fetch --url-prefix https://blog.example.com/
```

**Selective Ingestion**:
```bash
# Map all URLs, fetch only specific category
uv run kurt ingest map https://example.com
uv run kurt ingest fetch --url-contains /tutorials/
```

**Retry Failed Downloads**:
```bash
# Retry documents that failed to fetch
uv run kurt ingest fetch --url-prefix https://example.com/ --status ERROR --max-concurrent 10
```

**Find Duplicate Content**:
```bash
# Trafilatura extracts content_hash (fingerprint) for deduplication
sqlite3 .kurt/kurt.sqlite "
  SELECT content_hash, GROUP_CONCAT(source_url)
  FROM documents
  WHERE content_hash IS NOT NULL
  GROUP BY content_hash
  HAVING COUNT(*) > 1
"
```

## Dependencies

Core:
- `trafilatura>=2.0.0` - Web scraping and metadata extraction
- `httpx>=0.27.0` - Async HTTP client for batch fetching
- `sqlmodel>=0.0.14` - SQLite ORM
- `click>=8.1.0` - CLI framework
- `rich>=13.0.0` - Terminal output formatting
