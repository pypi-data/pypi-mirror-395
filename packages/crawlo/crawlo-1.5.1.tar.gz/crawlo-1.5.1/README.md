<p align="center">
  <img src="assets/logo.svg" alt="Crawlo Logo" width="150"/>
</p>

<h1 align="center">Crawlo</h1>

<p align="center">
  <strong>ä¸€ä¸ªåŸºäº asyncio çš„ç°ä»£åŒ–ã€é«˜æ€§èƒ½ Python å¼‚æ­¥çˆ¬è™«æ¡†æ¶ã€‚</strong>
</p>

<p align="center">
  <a href="#æ ¸å¿ƒç‰¹æ€§">æ ¸å¿ƒç‰¹æ€§</a> â€¢
  <a href="#é¡¹ç›®æ¶æ„">æ¶æ„</a> â€¢
  <a href="#å®‰è£…">å®‰è£…</a> â€¢
  <a href="#é…ç½®æ¨¡å¼è¯¦è§£">é…ç½®æ¨¡å¼</a> â€¢
  <a href="https://github.com/yourusername/crawlo">æ–‡æ¡£</a>
</p>

## æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½å¼‚æ­¥æ¶æ„**ï¼šåŸºäº asyncio å’Œ aiohttpï¼Œå……åˆ†åˆ©ç”¨å¼‚æ­¥ I/O æå‡çˆ¬å–æ•ˆç‡
- ğŸ¯ **æ™ºèƒ½è°ƒåº¦ç³»ç»Ÿ**ï¼šä¼˜å…ˆçº§é˜Ÿåˆ—ã€å¹¶å‘æ§åˆ¶ã€è‡ªåŠ¨é‡è¯•ã€æ™ºèƒ½é™é€Ÿ
- ğŸ”„ **çµæ´»çš„é…ç½®æ¨¡å¼**ï¼š
  - **Standalone æ¨¡å¼**ï¼šå•æœºå¼€å‘æµ‹è¯•ï¼Œä½¿ç”¨å†…å­˜é˜Ÿåˆ—
  - **Distributed æ¨¡å¼**ï¼šå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼éƒ¨ç½²ï¼Œä¸¥æ ¼è¦æ±‚ Redisï¼ˆä¸å…è®¸é™çº§ï¼‰
  - **Auto æ¨¡å¼**ï¼šæ™ºèƒ½æ£€æµ‹ Redis å¯ç”¨æ€§ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®ï¼ˆæ¨èï¼‰
- ğŸ“¦ **ä¸°å¯Œçš„ç»„ä»¶ç”Ÿæ€**ï¼š
  - å†…ç½® Redis å’Œ MongoDB æ”¯æŒ
  - MySQL å¼‚æ­¥è¿æ¥æ± ï¼ˆåŸºäº asyncmyå’Œaiomysqlåˆ†åˆ«å®ç°ï¼‰
  - å¤šç§è¿‡æ»¤å™¨å’Œå»é‡ç®¡é“ï¼ˆMemory/Redisï¼‰
  - ä»£ç†ä¸­é—´ä»¶æ”¯æŒï¼ˆç®€å•ä»£ç†/åŠ¨æ€ä»£ç†ï¼‰
  - å¤šç§ä¸‹è½½å™¨ï¼ˆaiohttpã€httpxã€curl-cffiï¼‰
- ğŸ›  **å¼€å‘å‹å¥½**ï¼š
  - ç±» Scrapy çš„é¡¹ç›®ç»“æ„å’Œ API è®¾è®¡
  - é…ç½®å·¥å‚æ¨¡å¼ï¼ˆ`CrawloConfig.auto()`ï¼‰
  - è‡ªåŠ¨çˆ¬è™«å‘ç°æœºåˆ¶
  - å®Œå–„çš„æ—¥å¿—ç³»ç»Ÿ

## é¡¹ç›®æ¶æ„

Crawlo æ¡†æ¶é‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

![Crawlo æ¡†æ¶æ¶æ„å›¾](assets/Crawlo%20æ¡†æ¶æ¶æ„å›¾.png)

- **Engine**ï¼šæ ¸å¿ƒå¼•æ“ï¼Œåè°ƒå„ä¸ªç»„ä»¶å·¥ä½œ
- **Scheduler**ï¼šè°ƒåº¦å™¨ï¼Œç®¡ç†è¯·æ±‚é˜Ÿåˆ—å’Œå»é‡
- **Downloader**ï¼šä¸‹è½½å™¨ï¼Œæ”¯æŒå¤šç§ HTTP å®¢æˆ·ç«¯
- **Spider**ï¼šçˆ¬è™«åŸºç±»ï¼Œå®šä¹‰æ•°æ®æå–é€»è¾‘
- **Pipeline**ï¼šæ•°æ®ç®¡é“ï¼Œå¤„ç†å’Œå­˜å‚¨æ•°æ®
- **Middleware**ï¼šä¸­é—´ä»¶ï¼Œå¤„ç†è¯·æ±‚å’Œå“åº”

![Crawlo æ•°æ®æµå›¾](assets/Crawlo%20æ•°æ®æµå›¾.png)

## ç¤ºä¾‹é¡¹ç›®

æŸ¥çœ‹ [`examples/`](examples/) ç›®å½•ä¸‹çš„å®Œæ•´ç¤ºä¾‹é¡¹ç›®ï¼š

- **ofweek_standalone** - Auto æ¨¡å¼ç¤ºä¾‹ï¼ˆæ™ºèƒ½æ£€æµ‹ï¼‰
- **ofweek_spider** - Auto æ¨¡å¼ç¤ºä¾‹
- **ofweek_distributed** - Distributed æ¨¡å¼ç¤ºä¾‹ï¼ˆä¸¥æ ¼åˆ†å¸ƒå¼ï¼‰

## å®‰è£…

```
# åŸºç¡€å®‰è£…
pip install crawlo
```

## é…ç½®æ¨¡å¼è¯¦è§£

> âš ï¸ **é‡è¦**ï¼šé…ç½®æ¨¡å¼çš„é€‰æ‹©ç›´æ¥å½±å“çˆ¬è™«çš„è¿è¡Œæ–¹å¼ã€æ€§èƒ½å’Œå¯é æ€§ï¼Œè¯·ä»”ç»†é˜…è¯»æœ¬èŠ‚å†…å®¹ã€‚

Crawlo æä¾›ä¸‰ç§é…ç½®æ¨¡å¼ï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚ï¼š

### ä¸‰ç§æ¨¡å¼å¯¹æ¯”

| é…ç½®é¡¹ | Standalone | Distributed | Auto |
|--------|-----------|-------------|------|
| **RUN_MODE** | `standalone` | `distributed` | `auto` |
| **é˜Ÿåˆ—ç±»å‹** | å†…å­˜é˜Ÿåˆ— | Redis é˜Ÿåˆ— | è‡ªåŠ¨æ£€æµ‹ |
| **Redis è¦æ±‚** | ä¸éœ€è¦ | **å¿…éœ€** | å¯é€‰ |
| **Redis ä¸å¯ç”¨æ—¶** | N/A | ğŸš« **æŠ¥é”™é€€å‡º** | âœ… é™çº§åˆ°å†…å­˜ |
| **é…ç½®è‡ªåŠ¨æ›´æ–°** | âŒ å¦ | âŒ å¦ | âœ… æ˜¯ |
| **è¿‡æ»¤å™¨** | Memory | Redis | Redis/Memory |
| **å»é‡ç®¡é“** | Memory | Redis | Redis/Memory |
| **é€‚ç”¨åœºæ™¯** | å¼€å‘æµ‹è¯• | å¤šèŠ‚ç‚¹éƒ¨ç½² | ç”Ÿäº§ç¯å¢ƒ |
| **å¹¶å‘æ•°é»˜è®¤å€¼** | 8 | 16 | 12 |
| **æ¨èæŒ‡æ•°** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |

### 1. Auto æ¨¡å¼ï¼ˆæ¨èï¼‰

**æ™ºèƒ½æ£€æµ‹ï¼Œè‡ªåŠ¨é€‚é…ï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒã€‚**

``python
from crawlo.config import CrawloConfig

config = CrawloConfig.auto(
    project_name='myproject',
    concurrency=12,
    download_delay=1.0
)
locals().update(config.to_dict())
```

**è¿è¡Œæœºåˆ¶**ï¼š
- é…ç½®é˜¶æ®µä¸ä¾èµ– Redis
- è¿è¡Œæ—¶æ‰æ£€æµ‹ Redis å¯ç”¨æ€§
- Redis å¯ç”¨ â†’ ä½¿ç”¨ `RedisPriorityQueue` + `AioRedisFilter`
- Redis ä¸å¯ç”¨ â†’ é™çº§åˆ° `MemoryQueue` + `MemoryFilter`
- è‡ªåŠ¨æ›´æ–°é…ç½®ï¼ˆ`QUEUE_TYPE`ã€`FILTER_CLASS`ã€`DEFAULT_DEDUP_PIPELINE`ï¼‰

**ä¼˜åŠ¿**ï¼š
- âœ… å¼€å‘ç¯å¢ƒæ— éœ€é…ç½® Redisï¼Œç›´æ¥å¯åŠ¨
- âœ… ç”Ÿäº§ç¯å¢ƒ Redis æ•…éšœæ—¶è‡ªåŠ¨é™çº§ï¼Œä¿è¯ç³»ç»Ÿå¯ç”¨æ€§
- âœ… åŒä¸€ä»½ä»£ç å¯åœ¨ä¸åŒç¯å¢ƒè¿è¡Œï¼Œæ— éœ€ä¿®æ”¹é…ç½®
- âœ… æœ€ä½³çš„çµæ´»æ€§å’Œå¯é æ€§

**é€‚ç”¨åœºæ™¯**ï¼š
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ˆé¦–é€‰ï¼‰
- éœ€è¦åœ¨å¤šç§ç¯å¢ƒè¿è¡Œçš„é¡¹ç›®
- å¸Œæœ›ç³»ç»Ÿå…·å¤‡å®¹é”™èƒ½åŠ›

### 2. Standalone æ¨¡å¼

**å•æœºæ¨¡å¼ï¼Œé€‚åˆå¼€å‘æµ‹è¯•å’Œä¸­å°è§„æ¨¡çˆ¬å–ã€‚**

``python
config = CrawloConfig.standalone(
    project_name='myproject',
    concurrency=8
)
locals().update(config.to_dict())
```

**è¿è¡Œæœºåˆ¶**ï¼š
- å›ºå®šä½¿ç”¨ `MemoryQueue`ï¼ˆå†…å­˜é˜Ÿåˆ—ï¼‰
- å›ºå®šä½¿ç”¨ `MemoryFilter`ï¼ˆå†…å­˜è¿‡æ»¤å™¨ï¼‰
- å›ºå®šä½¿ç”¨ `MemoryDedupPipeline`ï¼ˆå†…å­˜å»é‡ï¼‰
- ä¸è¿›è¡Œ Redis æ£€æµ‹
- é…ç½®ä¸ä¼šè‡ªåŠ¨æ›´æ–°

**ä¼˜åŠ¿**ï¼š
- âœ… æ— éœ€ä»»ä½•å¤–éƒ¨ä¾èµ–
- âœ… å¯åŠ¨é€Ÿåº¦å¿«
- âœ… é€‚åˆå¿«é€Ÿå¼€å‘è°ƒè¯•

**é™åˆ¶**ï¼š
- âŒ ä¸æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²
- âŒ é‡å¯åé˜Ÿåˆ—æ•°æ®ä¸¢å¤±
- âŒ ä¸é€‚åˆå¤§è§„æ¨¡æ•°æ®é‡‡é›†

**é€‚ç”¨åœºæ™¯**ï¼š
- æœ¬åœ°å¼€å‘è°ƒè¯•
- å­¦ä¹ æ¡†æ¶ç‰¹æ€§
- ä¸­å°è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆ< 10ä¸‡æ¡ï¼‰
- å•æœºè¿è¡Œçš„ç®€å•çˆ¬è™«

### 3. Distributed æ¨¡å¼

**åˆ†å¸ƒå¼æ¨¡å¼ï¼Œä¸¥æ ¼è¦æ±‚ Redis å¯ç”¨ï¼Œé€‚åˆå¤šèŠ‚ç‚¹ååŒå·¥ä½œã€‚**

``python
config = CrawloConfig.distributed(
    project_name='myproject',
    redis_host='redis.example.com',
    redis_port=6379,
    redis_password='your_password',
    concurrency=16
)
locals().update(config.to_dict())
```

**è¿è¡Œæœºåˆ¶**ï¼š
- å¿…é¡»ä½¿ç”¨ `RedisPriorityQueue`
- å¿…é¡»ä½¿ç”¨ `AioRedisFilter`
- å¿…é¡»ä½¿ç”¨ `RedisDedupPipeline`
- å¯åŠ¨æ—¶å¼ºåˆ¶æ£€æŸ¥ Redis è¿æ¥
- **Redis ä¸å¯ç”¨æ—¶æŠ›å‡º `RuntimeError` å¹¶é€€å‡ºï¼ˆä¸å…è®¸é™çº§ï¼‰**

**ä¸ºä»€ä¹ˆè¦ä¸¥æ ¼è¦æ±‚ Redisï¼Ÿ**

1. **æ•°æ®ä¸€è‡´æ€§**ï¼šé˜²æ­¢ä¸åŒèŠ‚ç‚¹ä½¿ç”¨ä¸åŒçš„é˜Ÿåˆ—ç±»å‹
2. **å»é‡æœ‰æ•ˆæ€§**ï¼šç¡®ä¿å¤šèŠ‚ç‚¹é—´çš„å»é‡åŠŸèƒ½æ­£å¸¸å·¥ä½œ
3. **ä»»åŠ¡åˆ†é…**ï¼šé˜²æ­¢ä»»åŠ¡è¢«é‡å¤æ‰§è¡Œ
4. **é—®é¢˜æ—©å‘ç°**ï¼šå¯åŠ¨å¤±è´¥æ¯”è¿è¡Œæ—¶å¤±è´¥æ›´å®¹æ˜“å‘ç°å’Œä¿®å¤
5. **æ˜ç¡®çš„æ„å›¾**ï¼šåˆ†å¸ƒå¼æ¨¡å¼å°±åº”è¯¥æ˜¯åˆ†å¸ƒå¼çš„ï¼Œä¸åº”è¯¥é™é»˜é™çº§

**Redis ä¸å¯ç”¨æ—¶çš„é”™è¯¯ä¿¡æ¯**ï¼š

```
$ crawlo run my_spider

2025-10-25 22:00:00 - [queue_manager] - ERROR: 
Distributed æ¨¡å¼è¦æ±‚ Redis å¯ç”¨ï¼Œä½†æ— æ³•è¿æ¥åˆ° Redis æœåŠ¡å™¨ã€‚
é”™è¯¯ä¿¡æ¯: Connection refused
Redis URL: redis://127.0.0.1:6379/0
è¯·æ£€æŸ¥ï¼š
  1. Redis æœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ
  2. Redis è¿æ¥é…ç½®æ˜¯å¦æ­£ç¡®
  3. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸

RuntimeError: Distributed æ¨¡å¼è¦æ±‚ Redis å¯ç”¨ï¼Œä½†æ— æ³•è¿æ¥åˆ° Redis æœåŠ¡å™¨ã€‚
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ”¯æŒå¤šèŠ‚ç‚¹ååŒçˆ¬å–
- âœ… æ•°æ®æŒä¹…åŒ–ï¼Œé‡å¯åå¯ç»§ç»­
- âœ… ä¸¥æ ¼çš„åˆ†å¸ƒå¼ä¸€è‡´æ€§ä¿è¯
- âœ… é€‚åˆå¤§è§„æ¨¡æ•°æ®é‡‡é›†

**é€‚ç”¨åœºæ™¯**ï¼š
- å¤šæœåŠ¡å™¨ååŒé‡‡é›†
- å¤§è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆ> ç™¾ä¸‡æ¡ï¼‰
- éœ€è¦ä¸¥æ ¼ä¿è¯åˆ†å¸ƒå¼ä¸€è‡´æ€§
- ç”Ÿäº§ç¯å¢ƒå¤šèŠ‚ç‚¹éƒ¨ç½²

### æ¨¡å¼é€‰æ‹©å»ºè®®

| åœºæ™¯ | æ¨èæ¨¡å¼ | åŸå›  |
|------|---------|------|
| ç”Ÿäº§ç¯å¢ƒï¼ˆå•èŠ‚ç‚¹æˆ–å¤šèŠ‚ç‚¹ï¼‰ | **Auto** | è‡ªåŠ¨é€‚é…ï¼Œå®¹é”™èƒ½åŠ›å¼º |
| å¼€å‘ç¯å¢ƒ | **Standalone** æˆ– **Auto** | æ— éœ€é…ç½® Redis |
| ä¸¥æ ¼çš„å¤šèŠ‚ç‚¹åˆ†å¸ƒå¼éƒ¨ç½² | **Distributed** | ä¿è¯åˆ†å¸ƒå¼ä¸€è‡´æ€§ |
| å­¦ä¹ å’Œæµ‹è¯• | **Standalone** | æœ€ç®€å•ï¼Œæ— ä¾èµ– |
| ä¸­å°è§„æ¨¡çˆ¬å– | **Standalone** æˆ– **Auto** | ç®€å•é«˜æ•ˆ |
| å¤§è§„æ¨¡çˆ¬å– | **Auto** æˆ– **Distributed** | æ€§èƒ½å’Œå¯é æ€§ |

> ğŸ“– **å®Œæ•´æ–‡æ¡£**ï¼šæ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒ [é…ç½®æ¨¡å¼å®Œå…¨æŒ‡å—](docs/tutorials/configuration_modes.md)

## Redis æ•°æ®ç»“æ„è¯´æ˜

åœ¨ä½¿ç”¨ Distributed æ¨¡å¼æˆ– Auto æ¨¡å¼ä¸” Redis å¯ç”¨æ—¶ï¼ŒCrawlo æ¡†æ¶ä¼šåœ¨ Redis ä¸­åˆ›å»ºä»¥ä¸‹æ•°æ®ç»“æ„ç”¨äºç®¡ç†å’Œè·Ÿè¸ªçˆ¬è™«çŠ¶æ€ï¼š

### æ ¸å¿ƒ Redis Keys

1. **`{project_name}:filter:fingerprint`** - è¯·æ±‚å»é‡è¿‡æ»¤å™¨
   - ç±»å‹ï¼šRedis Set
   - ç”¨é€”ï¼šå­˜å‚¨å·²å¤„ç†è¯·æ±‚çš„æŒ‡çº¹ï¼Œé¿å…é‡å¤æŠ“å–ç›¸åŒURL
   - ç¤ºä¾‹ï¼š`crawlo:ofweek_standalone:filter:fingerprint`

2. **`{project_name}:item:fingerprint`** - æ•°æ®é¡¹å»é‡é›†åˆ
   - ç±»å‹ï¼šRedis Set
   - ç”¨é€”ï¼šå­˜å‚¨å·²å¤„ç†æ•°æ®é¡¹çš„æŒ‡çº¹ï¼Œé¿å…é‡å¤å¤„ç†ç›¸åŒçš„æ•°æ®
   - ç¤ºä¾‹ï¼š`crawlo:ofweek_standalone:item:fingerprint`

3. **`{project_name}:queue:requests`** - ä¸»è¯·æ±‚é˜Ÿåˆ—
   - ç±»å‹ï¼šRedis Sorted Set
   - ç”¨é€”ï¼šå­˜å‚¨å¾…å¤„ç†çš„çˆ¬è™«è¯·æ±‚ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
   - ç¤ºä¾‹ï¼š`crawlo:ofweek_standalone:queue:requests`

4. **`{project_name}:queue:requests:data`** - ä¸»è¯·æ±‚é˜Ÿåˆ—æ•°æ®
   - ç±»å‹ï¼šRedis Hash
   - ç”¨é€”ï¼šä¿å­˜è¯·æ±‚é˜Ÿåˆ—ä¸­æ¯ä¸ªè¯·æ±‚çš„è¯¦ç»†åºåˆ—åŒ–æ•°æ®
   - ç¤ºä¾‹ï¼š`crawlo:ofweek_standalone:queue:requests:data`

### æ•°æ®æ ¸éªŒæ–¹æ³•

åœ¨çˆ¬è™«é‡‡é›†å®Œæˆåï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº› Redis key æ¥æ ¸éªŒæ•°æ®å’Œç›‘æ§çˆ¬è™«çŠ¶æ€ï¼š

```bash
# è¿æ¥åˆ° Redis
redis-cli

# æŸ¥çœ‹è¯·æ±‚å»é‡æ•°é‡ï¼ˆå·²å¤„ç†çš„å”¯ä¸€URLæ•°ï¼‰
SCARD crawlo:ofweek_standalone:filter:fingerprint

# æŸ¥çœ‹æ•°æ®é¡¹å»é‡æ•°é‡ï¼ˆå·²å¤„ç†çš„å”¯ä¸€æ•°æ®é¡¹æ•°ï¼‰
SCARD crawlo:ofweek_standalone:item:fingerprint

# æŸ¥çœ‹å¾…å¤„ç†é˜Ÿåˆ—é•¿åº¦
ZCARD crawlo:ofweek_standalone:queue:requests

# è·å–éƒ¨åˆ†æŒ‡çº¹æ•°æ®è¿›è¡Œæ£€æŸ¥
SMEMBERS crawlo:ofweek_standalone:filter:fingerprint LIMIT 10

# è·å–é˜Ÿåˆ—ä¸­çš„è¯·æ±‚ä¿¡æ¯
ZRANGE crawlo:ofweek_standalone:queue:requests 0 -1 WITHSCORES LIMIT 10
```

### æ³¨æ„äº‹é¡¹

1. **æ•°æ®æ¸…ç†**ï¼šçˆ¬è™«ä»»åŠ¡å®Œæˆåï¼Œå»ºè®®æ¸…ç†è¿™äº› Redis keys ä»¥é‡Šæ”¾å†…å­˜ï¼š
   ```bash
   DEL crawlo:ofweek_standalone:filter:fingerprint
   DEL crawlo:ofweek_standalone:item:fingerprint
   DEL crawlo:ofweek_standalone:queue:requests
   DEL crawlo:ofweek_standalone:queue:requests:data
   ```

2. **å‘½åç©ºé—´éš”ç¦»**ï¼šä¸åŒé¡¹ç›®ä½¿ç”¨ä¸åŒçš„ `{project_name}` å‰ç¼€ï¼Œç¡®ä¿æ•°æ®éš”ç¦»ã€‚å¯¹äºåŒä¸€é¡¹ç›®ä¸‹çš„ä¸åŒçˆ¬è™«ï¼Œè¿˜å¯ä»¥é€šè¿‡ `{spider_name}` è¿›ä¸€æ­¥åŒºåˆ†ï¼Œç¡®ä¿æ›´ç»†ç²’åº¦çš„æ•°æ®éš”ç¦»ã€‚

3. **æŒä¹…åŒ–è€ƒè™‘**ï¼šå¦‚æœéœ€è¦æŒä¹…åŒ–è¿™äº›æ•°æ®ï¼Œç¡®ä¿ Redis é…ç½®äº†åˆé€‚çš„æŒä¹…åŒ–ç­–ç•¥

## é…ç½®ä¼˜å…ˆçº§

Crawlo æ¡†æ¶æ”¯æŒå¤šå±‚çº§çš„é…ç½®ç³»ç»Ÿï¼Œäº†è§£é…ç½®ä¼˜å…ˆçº§å¯¹äºæ­£ç¡®ä½¿ç”¨æ¡†æ¶è‡³å…³é‡è¦ã€‚

### é…ç½®æ¥æºä¸ä¼˜å…ˆçº§

ä»**ä½åˆ°é«˜**çš„ä¼˜å…ˆçº§é¡ºåºï¼š

```
1. default_settings.py (æ¡†æ¶é»˜è®¤é…ç½®)                    â­
   â†“
2. ç¯å¢ƒå˜é‡ (CRAWLO_*)                                   â­â­
   (åœ¨ default_settings.py ä¸­é€šè¿‡ EnvConfigManager è¯»å–)
   â†“
3. ç”¨æˆ· settings.py (é¡¹ç›®é…ç½®æ–‡ä»¶)                       â­â­â­
   â†“
4. Spider.custom_settings (Spider è‡ªå®šä¹‰é…ç½®)            â­â­â­â­
   â†“
5. è¿è¡Œæ—¶ settings å‚æ•° (crawl() ä¼ å…¥çš„é…ç½®)             â­â­â­â­â­
```

### ç¯å¢ƒå˜é‡é…ç½®

æ‰€æœ‰ç¯å¢ƒå˜é‡éƒ½ä½¿ç”¨ `CRAWLO_` å‰ç¼€ï¼š

```bash
# åŸºç¡€é…ç½®
export CRAWLO_MODE=auto                    # è¿è¡Œæ¨¡å¼
export CRAWLO_PROJECT_NAME=myproject       # é¡¹ç›®åç§°
export CRAWLO_CONCURRENCY=16               # å¹¶å‘æ•°

# Redis é…ç½®
export CRAWLO_REDIS_HOST=127.0.0.1         # Redis ä¸»æœº
export CRAWLO_REDIS_PORT=6379              # Redis ç«¯å£
export CRAWLO_REDIS_PASSWORD=your_password # Redis å¯†ç 
export CRAWLO_REDIS_DB=0                   # Redis æ•°æ®åº“
```

### é…ç½®åˆå¹¶ç­–ç•¥

**æ™®é€šé…ç½®**ï¼ˆå¦‚ `CONCURRENCY`ï¼‰ï¼šé‡‡ç”¨**è¦†ç›–ç­–ç•¥**
```python
# å‡è®¾å„å¤„éƒ½æœ‰å®šä¹‰
default_settings.py:  8   â†’
ç¯å¢ƒå˜é‡:  12  â†’
settings.py:  16  â†’
Spider.custom_settings:  24  â†’
crawl(settings={...}):  32  âœ… æœ€ç»ˆå€¼ = 32
```

**åˆ—è¡¨é…ç½®**ï¼ˆå¦‚ `MIDDLEWARES`ã€`PIPELINES`ã€`EXTENSIONS`ï¼‰ï¼šé‡‡ç”¨**åˆå¹¶ç­–ç•¥**
```python
# default_settings.py
PIPELINES = ['crawlo.pipelines.console_pipeline.ConsolePipeline']

# settings.py
PIPELINES = ['myproject.pipelines.MySQLPipeline']

# æœ€ç»ˆç»“æœï¼ˆåˆå¹¶ï¼‰
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # ä¿ç•™é»˜è®¤
    'myproject.pipelines.MySQLPipeline',                   # è¿½åŠ ç”¨æˆ·
]
```

### Spider çº§åˆ«é…ç½®

åœ¨ Spider ç±»ä¸­å¯ä»¥è¦†ç›–é¡¹ç›®é…ç½®ï¼š

```python
class MySpider(Spider):
    name = 'myspider'
    
    custom_settings = {
        'CONCURRENCY': 32,           # è¦†ç›–é¡¹ç›®é…ç½®
        'DOWNLOAD_DELAY': 2.0,       # è¦†ç›–é¡¹ç›®é…ç½®
        'PIPELINES': [               # ä¼šä¸é»˜è®¤ç®¡é“åˆå¹¶
            'myproject.pipelines.SpecialPipeline',
        ]
    }
```

### è¿è¡Œæ—¶åŠ¨æ€é…ç½®

```
from crawlo import CrawlerProcess

process = CrawlerProcess()
await process.crawl(
    MySpider,
    settings={
        'CONCURRENCY': 64,        # æœ€é«˜ä¼˜å…ˆçº§
        'DOWNLOAD_DELAY': 0.1,
    }
)
```

### âš ï¸ å¸¸è§é™·é˜±

**é™·é˜±1ï¼šç¯å¢ƒå˜é‡è¢«é¡¹ç›®é…ç½®è¦†ç›–**
```python
# ç¯å¢ƒå˜é‡
export CRAWLO_REDIS_HOST=192.168.1.100

# settings.pyï¼ˆè¿™ä¼šè¦†ç›–ç¯å¢ƒå˜é‡ï¼ï¼‰
REDIS_HOST = 'localhost'  # âŒ ä¼šè¦†ç›–ç¯å¢ƒå˜é‡

# è§£å†³æ–¹æ¡ˆï¼šä¸åœ¨ settings.py ä¸­é‡å¤è®¾ç½®ï¼Œæˆ–ä½¿ç”¨ CrawloConfig.auto()
```

**é™·é˜±2ï¼šè¯¯ä»¥ä¸ºåˆ—è¡¨é…ç½®ä¼šè¢«æ¸…ç©º**
```python
# settings.py
PIPELINES = ['myproject.pipelines.MySQLPipeline']

# å®é™…ç»“æœï¼ˆé»˜è®¤ç®¡é“ä¼šè¢«ä¿ç•™å¹¶åˆå¹¶ï¼‰
PIPELINES = [
    'crawlo.pipelines.console_pipeline.ConsolePipeline',  # é»˜è®¤ä¿ç•™
    'myproject.pipelines.MySQLPipeline',                   # ç”¨æˆ·è¿½åŠ 
]

# å¦‚æœæƒ³å®Œå…¨æ›¿æ¢ï¼Œéœ€è¦å…ˆæ¸…ç©º
PIPELINES = []  # æ¸…ç©º
PIPELINES.append('myproject.pipelines.MySQLPipeline')
```

> ğŸ“– **è¯¦ç»†æ–‡æ¡£**ï¼šå®Œæ•´çš„é…ç½®ä¼˜å…ˆçº§è¯´æ˜è¯·å‚è€ƒ [é…ç½®ä¼˜å…ˆçº§è¯¦è§£](docs/é…ç½®ä¼˜å…ˆçº§è¯¦è§£.md)

## å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»ºé¡¹ç›®

```
# åˆ›å»ºæ–°é¡¹ç›®
crawlo startproject myproject
cd myproject

# åˆ›å»ºçˆ¬è™«
crawlo genspider example example.com
```

### 2. é…ç½®é¡¹ç›®ï¼ˆæ¨èä½¿ç”¨ Auto æ¨¡å¼ï¼‰

```
# myproject/settings.py
from crawlo.config import CrawloConfig

# ä½¿ç”¨ Auto æ¨¡å¼ï¼šæ™ºèƒ½æ£€æµ‹ Redisï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®
config = CrawloConfig.auto(
    project_name='myproject',
    concurrency=12,          # å¹¶å‘æ•°
    download_delay=1.0       # ä¸‹è½½å»¶è¿Ÿï¼ˆç§’ï¼‰
)

# å°†é…ç½®åº”ç”¨åˆ°å½“å‰æ¨¡å—
locals().update(config.to_dict())

# çˆ¬è™«æ¨¡å—é…ç½®
SPIDER_MODULES = ['myproject.spiders']

# æ—¥å¿—é…ç½®
LOG_LEVEL = 'INFO'
LOG_FILE = 'logs/myproject.log'

# å¯é€‰ï¼šæ·»åŠ æ•°æ®ç®¡é“
# PIPELINES = [
#     'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',
# ]

# å¯é€‰ï¼šRedis é…ç½®ï¼ˆAuto æ¨¡å¼ä¼šè‡ªåŠ¨æ£€æµ‹ï¼‰
# REDIS_HOST = '127.0.0.1'
# REDIS_PORT = 6379
```

**å…¶ä»–é…ç½®æ¨¡å¼ï¼š**

```python
# Standalone æ¨¡å¼ï¼šå•æœºå¼€å‘æµ‹è¯•
config = CrawloConfig.standalone(
    project_name='myproject',
    concurrency=8
)

# Distributed æ¨¡å¼ï¼šå¤šèŠ‚ç‚¹åˆ†å¸ƒå¼ï¼ˆå¿…é¡»é…ç½® Redisï¼‰
config = CrawloConfig.distributed(
    project_name='myproject',
    redis_host='redis.example.com',
    redis_port=6379,
    redis_password='your_password',
    concurrency=16
)
```

### 3. ç¼–å†™çˆ¬è™«

```
# myproject/spiders/example.py
from crawlo import Spider
from crawlo.http import Request

class ExampleSpider(Spider):
    name = 'example'
    start_urls = ['https://example.com']
    
    async def parse(self, response):
        # æå–æ•°æ®
        title = response.css('h1::text').get()
        
        # è¿”å›æ•°æ®
        yield {
            'title': title,
            'url': response.url
        }
        
        # è·Ÿè¿›é“¾æ¥
        for href in response.css('a::attr(href)').getall():
            yield Request(
                url=response.urljoin(href),
                callback=self.parse
            )
```

### 4. è¿è¡Œçˆ¬è™«

```
# è¿è¡ŒæŒ‡å®šçˆ¬è™«
crawlo run example

# æŒ‡å®šæ—¥å¿—çº§åˆ«
crawlo run example --log-level DEBUG
```

## æ ¸å¿ƒåŠŸèƒ½

### Response å¯¹è±¡

Crawlo çš„ [`Response`](crawlo/http/response.py) å¯¹è±¡æä¾›äº†å¼ºå¤§çš„ç½‘é¡µå¤„ç†èƒ½åŠ›ï¼š

**1. æ™ºèƒ½ç¼–ç æ£€æµ‹**

```
# è‡ªåŠ¨æ£€æµ‹å¹¶æ­£ç¡®è§£ç é¡µé¢å†…å®¹
# ä¼˜å…ˆçº§ï¼šContent-Type â†’ HTML meta â†’ chardet â†’ utf-8
response.text      # å·²æ­£ç¡®è§£ç çš„æ–‡æœ¬
response.encoding  # æ£€æµ‹åˆ°çš„ç¼–ç 
```

**2. CSS/XPath é€‰æ‹©å™¨**

```
# CSS é€‰æ‹©å™¨ï¼ˆæ¨èï¼‰
title = response.css('h1::text').get()
links = response.css('a::attr(href)').getall()

# XPath é€‰æ‹©å™¨
title = response.xpath('//title/text()').get()
links = response.xpath('//a/@href').getall()

# æ”¯æŒé»˜è®¤å€¼
title = response.css('h1::text').get(default='æ— æ ‡é¢˜')
```

**3. URL å¤„ç†**

```
response.url          # è‡ªåŠ¨è§„èŒƒåŒ–ï¼ˆç§»é™¤ fragmentï¼‰
response.original_url # ä¿ç•™åŸå§‹ URL

# æ™ºèƒ½ URL æ‹¼æ¥
response.urljoin('/path')           # ç»å¯¹è·¯å¾„
response.urljoin('../path')         # ç›¸å¯¹è·¯å¾„
response.urljoin('//cdn.com/img')   # åè®®ç›¸å¯¹è·¯å¾„
```

**4. ä¾¿æ·æå–æ–¹æ³•**

```
# æå–å•ä¸ª/å¤šä¸ªå…ƒç´ æ–‡æœ¬
title = response.extract_text('h1')
paragraphs = response.extract_texts('.content p')

# æå–å•ä¸ª/å¤šä¸ªå…ƒç´ å±æ€§
link = response.extract_attr('a', 'href')
all_links = response.extract_attrs('a', 'href')
```

### é…ç½®å·¥å‚æ¨¡å¼

Crawlo æä¾›äº†ä¾¿æ·çš„é…ç½®å·¥å‚æ–¹æ³•ï¼Œæ— éœ€æ‰‹åŠ¨é…ç½®ç¹ççš„å‚æ•°ï¼š

```
from crawlo.config import CrawloConfig

# Auto æ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šæ™ºèƒ½æ£€æµ‹ï¼Œè‡ªåŠ¨é€‚é…
config = CrawloConfig.auto(
    project_name='myproject',
    concurrency=12,
    download_delay=1.0
)

# Standalone æ¨¡å¼ï¼šå•æœºå¼€å‘
config = CrawloConfig.standalone(
    project_name='myproject',
    concurrency=8
)

# Distributed æ¨¡å¼ï¼šä¸¥æ ¼åˆ†å¸ƒå¼
config = CrawloConfig.distributed(
    project_name='myproject',
    redis_host='localhost',
    redis_port=6379,
    concurrency=16
)

# åº”ç”¨åˆ° settings.py
locals().update(config.to_dict())
```

**ä¸‰ç§æ¨¡å¼çš„æ ¸å¿ƒåŒºåˆ«**ï¼š

- **Auto**ï¼šæ™ºèƒ½æ£€æµ‹ Redisï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³é…ç½®ï¼Œ**æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ**
- **Standalone**ï¼šå›ºå®šä½¿ç”¨å†…å­˜é˜Ÿåˆ—ï¼Œé€‚åˆå¼€å‘æµ‹è¯•ï¼Œæ— å¤–éƒ¨ä¾èµ–
- **Distributed**ï¼šä¸¥æ ¼è¦æ±‚ Redisï¼Œä¸å…è®¸é™çº§ï¼Œä¿è¯åˆ†å¸ƒå¼ä¸€è‡´æ€§

> ğŸ’¡ è¯¦ç»†é…ç½®è¯´æ˜è¯·æŸ¥çœ‹å‰é¢çš„ [é…ç½®æ¨¡å¼è¯¦è§£](#é…ç½®æ¨¡å¼è¯¦è§£) ç« èŠ‚

### æ—¥å¿—ç³»ç»Ÿ

Crawlo æä¾›äº†å®Œå–„çš„æ—¥å¿—ç³»ç»Ÿï¼Œæ”¯æŒæ§åˆ¶å°å’Œæ–‡ä»¶åŒè¾“å‡ºï¼š

```
from crawlo.logging import get_logger

logger = get_logger(__name__)

logger.debug('è°ƒè¯•ä¿¡æ¯')
logger.info('æ™®é€šä¿¡æ¯')
logger.warning('è­¦å‘Šä¿¡æ¯')
logger.error('é”™è¯¯ä¿¡æ¯')
```

**æ—¥å¿—é…ç½®ï¼š**

```
# settings.py
LOG_LEVEL = 'INFO'          # DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE = 'logs/spider.log'
LOG_ENCODING = 'utf-8'      # æ˜ç¡®æŒ‡å®šæ—¥å¿—æ–‡ä»¶ç¼–ç 
STATS_DUMP = True           # æ˜¯å¦è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
```

**é«˜çº§åŠŸèƒ½ï¼š**

```
from crawlo.logging import configure_logging

# åˆ†åˆ«é…ç½®æ§åˆ¶å°å’Œæ–‡ä»¶æ—¥å¿—çº§åˆ«
configure_logging(
    LOG_LEVEL='INFO',
    LOG_CONSOLE_LEVEL='WARNING',  # æ§åˆ¶å°åªæ˜¾ç¤º WARNING åŠä»¥ä¸Š
    LOG_FILE_LEVEL='DEBUG',       # æ–‡ä»¶è®°å½• DEBUG åŠä»¥ä¸Š
    LOG_FILE='logs/app.log',
    LOG_MAX_BYTES=10*1024*1024,   # 10MB
    LOG_BACKUP_COUNT=5
)
```

### çˆ¬è™«è‡ªåŠ¨å‘ç°

Crawlo æ”¯æŒè‡ªåŠ¨å‘ç°çˆ¬è™«ï¼Œæ— éœ€æ‰‹åŠ¨å¯¼å…¥ï¼š

```
# è‡ªåŠ¨å‘ç°å¹¶è¿è¡Œï¼ˆæ¨èï¼‰
crawlo run spider_name

# æŒ‡å®šæ–‡ä»¶è·¯å¾„è¿è¡Œ
crawlo run -f path/to/spider.py -s SpiderClassName
```

æ¡†æ¶ä¼šè‡ªåŠ¨åœ¨ `SPIDER_MODULES` é…ç½®çš„æ¨¡å—ä¸­æŸ¥æ‰¾çˆ¬è™«ã€‚

### è·¨å¹³å°æ”¯æŒ

Crawlo åœ¨ Windowsã€macOSã€Linux ä¸Šå‡å¯æ— ç¼è¿è¡Œï¼š

- **Windows**ï¼šè‡ªåŠ¨ä½¿ç”¨ ProactorEventLoopï¼Œæ­£ç¡®å¤„ç†æ§åˆ¶å°ç¼–ç 
- **macOS/Linux**ï¼šä½¿ç”¨é»˜è®¤çš„ SelectorEventLoop
- å…¼å®¹ä¸åŒå¹³å°çš„è·¯å¾„æ ¼å¼

> ğŸ’¡ **Windows ç”¨æˆ·æç¤º**ï¼šæ¡†æ¶é»˜è®¤å·²ç¦ç”¨æ—¥å¿—è½®è½¬åŠŸèƒ½ä»¥é¿å…æ–‡ä»¶é”å®šé—®é¢˜ã€‚å¦‚éœ€å¯ç”¨æ—¥å¿—è½®è½¬ï¼Œå»ºè®®å®‰è£… `concurrent-log-handler`ï¼š
> ```bash
> pip install concurrent-log-handler
> ```
> ç„¶ååœ¨ settings.py ä¸­è®¾ç½®ï¼š
> ```python
> LOG_MAX_BYTES = 10 * 1024 * 1024  # 10MB
> LOG_BACKUP_COUNT = 5
> ```

![Crawlo æ ¸å¿ƒæ¶æ„å›¾](assets/Crawlo%20æ ¸å¿ƒæ¶æ„å›¾.png)

## æ–‡æ¡£

å®Œæ•´æ–‡æ¡£è¯·æŸ¥çœ‹ [`docs/`](docs/) ç›®å½•ï¼š

### ğŸ“š æ ¸å¿ƒæ•™ç¨‹

- [é…ç½®æ¨¡å¼å®Œå…¨æŒ‡å—](docs/tutorials/configuration_modes.md) - **å¼ºçƒˆæ¨èé˜…è¯»**
- [æ¶æ„æ¦‚è¿°](docs/modules/architecture/index.md)
- [è¿è¡Œæ¨¡å¼](docs/modules/architecture/modes.md)
- [é…ç½®ç³»ç»Ÿ](docs/modules/configuration/index.md)

### ğŸ”§ æ ¸å¿ƒæ¨¡å—

- [å¼•æ“ (Engine)](docs/modules/core/engine.md)
- [è°ƒåº¦å™¨ (Scheduler)](docs/modules/core/scheduler.md)
- [å¤„ç†å™¨ (Processor)](docs/modules/core/processor.md)
- [çˆ¬è™«åŸºç±» (Spider)](docs/modules/core/spider.md)

### ğŸ“¦ åŠŸèƒ½æ¨¡å—

- [ä¸‹è½½å™¨ (Downloader)](docs/modules/downloader/index.md)
- [é˜Ÿåˆ— (Queue)](docs/modules/queue/index.md)
- [è¿‡æ»¤å™¨ (Filter)](docs/modules/filter/index.md)
- [ä¸­é—´ä»¶ (Middleware)](docs/modules/middleware/index.md)
- [ç®¡é“ (Pipeline)](docs/modules/pipeline/index.md)
- [æ‰©å±• (Extension)](docs/modules/extension/index.md)

### ğŸ›  å‘½ä»¤è¡Œå·¥å…·

- [CLI æ¦‚è¿°](docs/modules/cli/index.md)
- [startproject](docs/modules/cli/startproject.md) - é¡¹ç›®åˆå§‹åŒ–
- [genspider](docs/modules/cli/genspider.md) - çˆ¬è™«ç”Ÿæˆ
- [run](docs/modules/cli/run.md) - çˆ¬è™«è¿è¡Œ
- [list](docs/modules/cli/list.md) - æŸ¥çœ‹çˆ¬è™«åˆ—è¡¨
- [check](docs/modules/cli/check.md) - é…ç½®æ£€æŸ¥
- [stats](docs/modules/cli/stats.md) - ç»Ÿè®¡ä¿¡æ¯

### ğŸš€ é«˜çº§ä¸»é¢˜

- [åˆ†å¸ƒå¼éƒ¨ç½²](docs/modules/advanced/distributed.md)
- [æ€§èƒ½ä¼˜åŒ–](docs/modules/advanced/performance.md)
- [æ•…éšœæ’é™¤](docs/modules/advanced/troubleshooting.md)
- [æœ€ä½³å®è·µ](docs/modules/advanced/best_practices.md)

### ğŸ“ æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š

- [åˆå§‹åŒ–ä¼˜åŒ–æŠ¥å‘Š](docs/initialization_optimization_report.md)
- [MySQL è¿æ¥æ± ä¼˜åŒ–](docs/mysql_connection_pool_optimization.md)
- [MongoDB è¿æ¥æ± ä¼˜åŒ–](docs/mongo_connection_pool_optimization.md)

### ğŸ“– API å‚è€ƒ

- [å®Œæ•´ API æ–‡æ¡£](docs/api/)

---

**åœ¨çº¿æ–‡æ¡£**ï¼š
- [ä¸­æ–‡æ–‡æ¡£](https://crawlo.readthedocs.io/en/latest/README_zh/)
- [English Documentation](https://crawlo.readthedocs.io/en/latest/)

**æœ¬åœ°æ„å»ºæ–‡æ¡£**ï¼š
```
mkdocs serve
# æµè§ˆå™¨è®¿é—® http://localhost:8000
```

## å¸¸è§é—®é¢˜

### 1. å¦‚ä½•é€‰æ‹©é…ç½®æ¨¡å¼ï¼Ÿ

- **å¼€å‘æµ‹è¯•**ï¼šä½¿ç”¨ `CrawloConfig.standalone()`
- **ç”Ÿäº§ç¯å¢ƒ**ï¼šä½¿ç”¨ `CrawloConfig.auto()`ï¼ˆæ¨èï¼‰
- **å¤šèŠ‚ç‚¹éƒ¨ç½²**ï¼šä½¿ç”¨ `CrawloConfig.distributed()`

### 2. Distributed æ¨¡å¼ Redis ä¸å¯ç”¨æ€ä¹ˆåŠï¼Ÿ

Distributed æ¨¡å¼**ä¸¥æ ¼è¦æ±‚ Redis**ï¼Œä¸å¯ç”¨æ—¶ä¼šæŠ›å‡º `RuntimeError` å¹¶é€€å‡ºã€‚è¿™æ˜¯ä¸ºäº†ä¿è¯åˆ†å¸ƒå¼ä¸€è‡´æ€§å’Œæ•°æ®å®‰å…¨ã€‚

å¦‚æœå¸Œæœ› Redis ä¸å¯ç”¨æ—¶è‡ªåŠ¨é™çº§ï¼Œè¯·ä½¿ç”¨ **Auto æ¨¡å¼**ã€‚

### 3. Auto æ¨¡å¼å¦‚ä½•å·¥ä½œï¼Ÿ

Auto æ¨¡å¼åœ¨è¿è¡Œæ—¶æ™ºèƒ½æ£€æµ‹ï¼š
- Redis å¯ç”¨ â†’ ä½¿ç”¨ RedisPriorityQueue + AioRedisFilter
- Redis ä¸å¯ç”¨ â†’ é™çº§åˆ° MemoryQueue + MemoryFilter

è¯¦è§ [é…ç½®æ¨¡å¼å®Œå…¨æŒ‡å—](docs/tutorials/configuration_modes.md)ã€‚

### 4. å¦‚ä½•å¯ç”¨ MySQL æˆ– MongoDB æ”¯æŒï¼Ÿ

```
# settings.py
PIPELINES = [
    'crawlo.pipelines.mysql_pipeline.AsyncmyMySQLPipeline',  # MySQL
    # æˆ–
    'crawlo.pipelines.mongo_pipeline.MongoDBPipeline',       # MongoDB
]

# MySQL é…ç½®
MYSQL_HOST = '127.0.0.1'
MYSQL_USER = 'root'
MYSQL_PASSWORD = 'password'
MYSQL_DB = 'mydb'
MYSQL_TABLE = 'items'

# MongoDB é…ç½®
MONGO_URI = 'mongodb://localhost:27017'
MONGO_DATABASE = 'mydb'
MONGO_COLLECTION = 'items'
```

### 5. å¦‚ä½•ä½¿ç”¨ä»£ç†ï¼Ÿ

```
# settings.py

# ç®€å•ä»£ç†åˆ—è¡¨
PROXY_LIST = [
    "http://proxy1:8080",
    "http://proxy2:8080"
]

# æˆ–ä½¿ç”¨åŠ¨æ€ä»£ç† API
PROXY_API_URL = "http://your-proxy-api.com/get-proxy"
```

## å­¦ä¹ è·¯å¾„

å¦‚æœæ‚¨æ˜¯ Crawlo çš„æ–°ç”¨æˆ·ï¼Œå»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå­¦ä¹ ï¼š

1. **å…¥é—¨** - é˜…è¯»å¿«é€Ÿå¼€å§‹æŒ‡å—ï¼Œè¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹
2. **é…ç½®æ¨¡å¼** - å­¦ä¹ ä¸‰ç§é…ç½®æ¨¡å¼ï¼Œé€‰æ‹©é€‚åˆçš„æ¨¡å¼ï¼ˆ[é…ç½®æ¨¡å¼æŒ‡å—](docs/tutorials/configuration_modes.md)ï¼‰
3. **æ ¸å¿ƒæ¦‚å¿µ** - äº†è§£æ¡†æ¶æ¶æ„å’ŒåŸºæœ¬æ¦‚å¿µ
4. **æ ¸å¿ƒæ¨¡å—** - æ·±å…¥å­¦ä¹ å¼•æ“ã€è°ƒåº¦å™¨ã€å¤„ç†å™¨ç­‰æ ¸å˜¿ç»„ä»¶
5. **åŠŸèƒ½æ¨¡å—** - æ ¹æ®éœ€æ±‚å­¦ä¹ ä¸‹è½½å™¨ã€é˜Ÿåˆ—ã€è¿‡æ»¤å™¨ç­‰æ¨¡å—
6. **é«˜çº§ä¸»é¢˜** - æŒæ¡åˆ†å¸ƒå¼éƒ¨ç½²ã€æ€§èƒ½ä¼˜åŒ–ç­‰é«˜çº§åŠŸèƒ½

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼å¦‚æœæ‚¨æƒ³ä¸º Crawlo åšå‡ºè´¡çŒ®ï¼š

1. Fork é¡¹ç›®ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ‚¨çš„æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å‘èµ· Pull Request

## è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## å˜æ›´æ—¥å¿—

### v1.2.0

- **Redis Key é‡æ„**ï¼šå¼•å…¥ `RedisKeyManager` ç»Ÿä¸€ç®¡ç† Redis Key çš„ç”Ÿæˆå’ŒéªŒè¯
  - æ”¯æŒé¡¹ç›®çº§åˆ«å’Œçˆ¬è™«çº§åˆ«çš„ Key å‘½åè§„èŒƒ
  - æ”¯æŒåœ¨åŒä¸€ä¸ªé¡¹ç›®ä¸‹åŒºåˆ†ä¸åŒçš„çˆ¬è™«
  - é›†æˆ `RedisKeyValidator` ç¡®ä¿ Key å‘½åè§„èŒƒä¸€è‡´æ€§
  - è¯¦ç»†æ–‡æ¡£è¯·å‚è§ [Redis Key é‡æ„è¯´æ˜](docs/redis_key_refactor.md)

---

<p align="center">
  <i>å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ <a href="https://github.com/crawl-coder/Crawlo/issues">Issue</a></i>
</p>