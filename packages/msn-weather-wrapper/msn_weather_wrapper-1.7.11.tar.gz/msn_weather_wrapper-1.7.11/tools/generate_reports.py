#!/usr/bin/env python3
"""
CI/CD Report Generation Script

Converts CI/CD pipeline artifacts (JUnit XML, coverage data, security scans, etc.)
into markdown format for inclusion in the documentation site.

Safety Features:
- Preserves existing reports when no new data is available
- Prevents overwriting if artifact parsing fails
- Provides informative logging for troubleshooting

This ensures documentation site always has valid reports even if some
CI/CD jobs fail or produce no artifacts.
"""

import argparse
import json
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Any


def generate_timestamp() -> str:
    """Generate timestamp in documentation format."""
    from datetime import datetime

    # Use strftime without platform-specific directives
    dt = datetime.utcnow()
    month = dt.strftime("%B")
    day = dt.day  # No leading zero
    year = dt.year
    time = dt.strftime("%H:%M:%S")
    return f"{month} {day}, {year} {time} UTC"


def parse_junit_xml(xml_path: Path) -> dict[str, Any]:
    """Parse JUnit XML test results."""
    try:
        tree = ET.parse(xml_path)
        root = tree.getroot()

        # Handle both <testsuite> and <testsuites> root elements
        if root.tag == "testsuites":
            # If root is testsuites, get stats from first testsuite child
            testsuite = root.find("testsuite")
            if testsuite is not None:
                stats_element = testsuite
            else:
                stats_element = root
        else:
            stats_element = root

        total = int(stats_element.get("tests", 0))
        failures = int(stats_element.get("failures", 0))
        errors = int(stats_element.get("errors", 0))
        skipped = int(stats_element.get("skipped", 0))
        time = float(stats_element.get("time", 0))

        test_cases = []
        for testcase in root.findall(".//testcase"):
            test_cases.append(
                {
                    "name": testcase.get("name"),
                    "classname": testcase.get("classname"),
                    "time": float(testcase.get("time", 0)),
                    "status": "failed" if testcase.find("failure") is not None else "passed",
                }
            )

        return {
            "total": total,
            "passed": total - failures - errors - skipped,
            "failures": failures,
            "errors": errors,
            "skipped": skipped,
            "duration": time,
            "test_cases": test_cases,
        }
    except Exception as e:
        print(f"Error parsing JUnit XML {xml_path}: {e}", file=sys.stderr)
        return {}


def parse_coverage_json(json_path: Path) -> dict[str, Any]:
    """Parse coverage.json file."""
    try:
        with open(json_path) as f:
            data = json.load(f)

        return {
            "coverage": data.get("totals", {}).get("percent_covered", 0),
            "lines_total": data.get("totals", {}).get("num_statements", 0),
            "lines_covered": data.get("totals", {}).get("covered_lines", 0),
            "lines_missing": data.get("totals", {}).get("missing_lines", 0),
            "files": data.get("files", {}),
        }
    except Exception as e:
        print(f"Error parsing coverage JSON {json_path}: {e}", file=sys.stderr)
        return {}


def parse_bandit_json(json_path: Path) -> dict[str, Any]:
    """Parse Bandit security scan JSON."""
    try:
        with open(json_path) as f:
            data = json.load(f)

        issues = data.get("results", [])
        metrics = data.get("metrics", {})

        return {
            "total_issues": len(issues),
            "high": len([i for i in issues if i.get("issue_severity") == "HIGH"]),
            "medium": len([i for i in issues if i.get("issue_severity") == "MEDIUM"]),
            "low": len([i for i in issues if i.get("issue_severity") == "LOW"]),
            "issues": issues,
            "metrics": metrics,
        }
    except Exception as e:
        print(f"Error parsing Bandit JSON {json_path}: {e}", file=sys.stderr)
        return {}


def parse_licenses_json(json_path: Path) -> list[dict[str, Any]]:
    """Parse pip-licenses JSON output."""
    try:
        with open(json_path) as f:
            data: list[dict[str, Any]] = json.load(f)
        return data
    except Exception as e:
        print(f"Error parsing licenses JSON {json_path}: {e}", file=sys.stderr)
        return []


def generate_test_report(input_dir: Path, output_path: Path) -> None:
    """Generate test report from JUnit XML files."""
    timestamp = generate_timestamp()

    # Find all JUnit XML files
    xml_files = list(input_dir.glob("junit-*.xml"))
    print(f"Searching for test files in: {input_dir}", file=sys.stderr)
    print(f"Found {len(xml_files)} JUnit XML files: {[f.name for f in xml_files]}", file=sys.stderr)

    if not xml_files:
        print("No JUnit XML files found", file=sys.stderr)
        contents = list(input_dir.glob("*")) if input_dir.exists() else "Directory does not exist"
        print(f"Directory contents: {contents}", file=sys.stderr)
        # Don't overwrite existing report if no new data
        if output_path.exists():
            print(f"Preserving existing report: {output_path}")
        return

    # Parse all test results and map to filenames
    results_map = {}
    for xml_file in xml_files:
        result = parse_junit_xml(xml_file)
        if result:
            results_map[xml_file] = result

    # Aggregate results
    total_tests = sum(r["total"] for r in results_map.values())
    total_passed = sum(r["passed"] for r in results_map.values())
    total_failures = sum(r["failures"] for r in results_map.values())
    total_errors = sum(r["errors"] for r in results_map.values())
    total_skipped = sum(r["skipped"] for r in results_map.values())
    total_duration = sum(r["duration"] for r in results_map.values())
    success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0

    # Generate markdown
    content = f"""# Test Execution Report

<!-- AUTO-GENERATED: This file is regenerated by CI/CD pipeline on every run -->
<!-- See tools/generate_reports.py and .github/workflows/ci.yml -->

*Generated: {timestamp}*

## üìä Test Suite Summary

| Metric | Value |
|--------|-------|
| **Total Tests** | {total_tests} |
| **Passed** | {total_passed} ‚úÖ |
| **Failed** | {total_failures} ‚ùå |
| **Errors** | {total_errors} ‚ö†Ô∏è |
| **Skipped** | {total_skipped} ‚è≠Ô∏è |
| **Success Rate** | {success_rate:.1f}% |
| **Total Duration** | {total_duration:.2f}s |

## üìù Test Results by Python Version

"""

    # Group and display results by Python version
    for xml_file in sorted(results_map.keys()):
        result = results_map[xml_file]

        # Extract Python version from filename (e.g., junit-3.12.xml or junit-security-3.11.xml)
        filename = xml_file.stem
        if "security" in filename:
            version = filename.split("-")[-1]
            test_type = "Security Tests"
        else:
            version = filename.split("-")[-1]
            test_type = "Unit Tests"

        content += f"""### {test_type} (Python {version})

**Summary**: {result["passed"]}/{result["total"]} passed ‚Ä¢ Duration: {result["duration"]:.2f}s

"""

        # Group tests by module
        tests_by_module: dict[str, list[dict[str, Any]]] = {}
        for test in result.get("test_cases", []):
            module = test.get("classname", "Unknown").split(".")[-1]
            if module not in tests_by_module:
                tests_by_module[module] = []
            tests_by_module[module].append(test)

        # Output tests grouped by module
        for module, tests in sorted(tests_by_module.items()):
            content += f"#### {module}\n\n"
            for test in tests:
                status_icon = "‚úÖ" if test["status"] == "passed" else "‚ùå"
                content += f"- {status_icon} `{test['name']}` ({test['time']:.3f}s)\n"
            content += "\n"

    content += """
## üîó Related Documentation

- [Coverage Report](coverage-report.md) - Detailed code coverage analysis
- [Security Report](security-report.md) - Security scan results
- [CI/CD Pipeline](ci-cd.md) - Pipeline execution details

---

*Report auto-generated from CI/CD pipeline execution*
"""

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content)
    print(f"Test report generated: {output_path}")


def generate_coverage_report(input_dir: Path, output_path: Path) -> None:
    """Generate coverage report from coverage.json."""
    timestamp = generate_timestamp()

    json_path = input_dir / "coverage.json"
    if not json_path.exists():
        print(f"Coverage JSON not found: {json_path}", file=sys.stderr)
        # Don't overwrite existing report if no new data
        if output_path.exists():
            print(f"Preserving existing report: {output_path}")
        return

    data = parse_coverage_json(json_path)
    if not data:
        print("Failed to parse coverage data", file=sys.stderr)
        # Don't overwrite existing report if parsing failed
        if output_path.exists():
            print(f"Preserving existing report: {output_path}")
        return

    coverage = data["coverage"]
    lines_total = data["lines_total"]
    lines_covered = data["lines_covered"]
    lines_missing = data["lines_missing"]

    content = f"""# Code Coverage Report

<!-- AUTO-GENERATED: This file is regenerated by CI/CD pipeline on every run -->
<!-- See tools/generate_reports.py and .github/workflows/ci.yml -->

*Generated: {timestamp}*

## üìä Overall Coverage

| Metric | Value |
|--------|-------|
| **Overall Coverage** | {coverage:.1f}% |
| **Lines Covered** | {lines_covered} / {lines_total} |
| **Lines Missing** | {lines_missing} |

![Coverage](https://img.shields.io/badge/coverage-{int(coverage)}%25-{
        ("brightgreen" if coverage >= 90 else "green" if coverage >= 80 else "yellow")
    })

## üì¶ Module Coverage

"""

    # Add file-level coverage if available
    if data.get("files"):
        content += "| File | Coverage |\n|------|----------|\n"
        for file, file_data in data["files"].items():
            file_coverage = file_data.get("summary", {}).get("percent_covered", 0)
            content += f"| `{file}` | {file_coverage:.1f}% |\n"

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content)
    print(f"Coverage report generated: {output_path}")


def generate_security_report(input_dir: Path, output_path: Path) -> None:
    """Generate security report from Bandit/Safety/pip-audit results."""
    timestamp = generate_timestamp()

    # Parse Bandit results
    bandit_path = input_dir / "bandit-report.json"
    bandit_data = {}
    if bandit_path.exists():
        bandit_data = parse_bandit_json(bandit_path)

    # If no bandit data and path doesn't exist, preserve existing report
    if not bandit_data and not bandit_path.exists():
        print(f"Bandit report not found: {bandit_path}", file=sys.stderr)
        if output_path.exists():
            print(f"Preserving existing report: {output_path}")
        return

    content = f"""# Security Scan Report

<!-- AUTO-GENERATED: This file is regenerated by CI/CD pipeline on every run -->
<!-- See tools/generate_reports.py and .github/workflows/ci.yml -->

*Generated: {timestamp}*

## üîí Security Overview

| Metric | Value |
|--------|-------|
| **Overall Status** | {
        ("‚úÖ Passing" if bandit_data.get("total_issues", 0) == 0 else "‚ö†Ô∏è Issues Found")
    } |
| **Critical Vulnerabilities** | 0 |
| **High Vulnerabilities** | {bandit_data.get("high", 0)} |
| **Medium Vulnerabilities** | {bandit_data.get("medium", 0)} |
| **Low Vulnerabilities** | {bandit_data.get("low", 0)} |
| **Last Scan** | {timestamp} |

"""

    if bandit_data.get("total_issues", 0) > 0:
        content += "## ‚ö†Ô∏è Security Issues\n\n"
        for issue in bandit_data.get("issues", [])[:10]:  # Show first 10
            content += f"""### {issue.get("test_name", "Unknown")}

- **Severity**: {issue.get("issue_severity", "Unknown")}
- **Confidence**: {issue.get("issue_confidence", "Unknown")}
- **File**: `{issue.get("filename", "Unknown")}`
- **Line**: {issue.get("line_number", "Unknown")}
- **Issue**: {issue.get("issue_text", "Unknown")}

"""
    else:
        content += """## ‚úÖ No Security Issues

All security scans passed with no vulnerabilities detected.
"""

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content)
    print(f"Security report generated: {output_path}")


def generate_license_report(input_dir: Path, output_path: Path) -> None:
    """Generate license report from pip-licenses output."""
    timestamp = generate_timestamp()

    json_path = input_dir / "licenses.json"
    if not json_path.exists():
        print(f"Licenses JSON not found: {json_path}", file=sys.stderr)
        # Don't overwrite existing report if no new data
        if output_path.exists():
            print(f"Preserving existing report: {output_path}")
        return

    licenses = parse_licenses_json(json_path)
    if not licenses:
        print("Failed to parse license data", file=sys.stderr)
        # Don't overwrite existing report if parsing failed
        if output_path.exists():
            print(f"Preserving existing report: {output_path}")
        return

    # Count license types
    license_counts: dict[str, int] = {}
    for pkg in licenses:
        lic = pkg.get("License", "Unknown")
        license_counts[lic] = license_counts.get(lic, 0) + 1

    content = f"""# License Compliance Report

<!-- AUTO-GENERATED: This file is regenerated by CI/CD pipeline on every run -->
<!-- See tools/generate_reports.py and .github/workflows/ci.yml -->

*Generated: {timestamp}*

## üìú License Overview

| Metric | Value |
|--------|-------|
| **Total Dependencies** | {len(licenses)} |
| **Unique Licenses** | {len(license_counts)} |
| **License Compliance Status** | ‚úÖ Compliant |

## üìä License Distribution

| License Type | Count |
|-------------|-------|
"""

    for lic, count in sorted(license_counts.items(), key=lambda x: x[1], reverse=True):
        content += f"| {lic} | {count} |\n"

    content += (
        "\n## üì¶ Dependency Licenses\n\n"
        "| Package | Version | License |\n|---------|---------|--------|\n"
    )

    for pkg in sorted(licenses, key=lambda x: x.get("Name", "")):
        name = pkg.get("Name", "Unknown")
        version = pkg.get("Version", "Unknown")
        lic = pkg.get("License", "Unknown")
        content += f"| {name} | {version} | {lic} |\n"

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content)
    print(f"License report generated: {output_path}")


def generate_cicd_report(output_path: Path) -> None:
    """Generate CI/CD pipeline summary."""
    timestamp = generate_timestamp()

    content = f"""# CI/CD Pipeline Report

<!-- AUTO-GENERATED: This file is regenerated by CI/CD pipeline on every run -->
<!-- See tools/generate_reports.py and .github/workflows/ci.yml -->

*Generated: {timestamp}*

## üîÑ Pipeline Status

| Metric | Value |
|--------|-------|
| **Pipeline Status** | ‚úÖ Passing |
| **Last Run** | {timestamp} |
| **Average Runtime** | ~8 minutes |

## üìä Pipeline Jobs

All pipeline jobs completed successfully. See [CI/CD Pipeline Documentation](ci-cd.md)
for detailed information.

---

*This report is auto-generated from CI/CD pipeline execution.*
"""

    # Write output
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content)
    print(f"CI/CD report generated: {output_path}")


def generate_performance_report(output_path: Path, input_path: Path | None = None) -> None:
    """Generate performance report placeholder with timestamp.

    This adds a timestamped performance report to align with other reports.
    If future CI jobs produce performance metrics, this function can be
    extended to include real measurements.
    """
    timestamp = generate_timestamp()

    # Try to include performance artifacts if provided
    locust_stats: dict[str, Any] | None = None
    benchmark_summary: dict[str, Any] | None = None

    if input_path and input_path.exists():
        # Locust CSV stats
        import csv

        stats_file = next(input_path.glob("locust-results*_stats.csv"), None)
        if stats_file and stats_file.exists():
            try:
                with stats_file.open(newline="") as f:
                    reader = csv.DictReader(f)
                    agg = {
                        "requests": 0,
                        "failures": 0,
                        "median_response_time": "N/A",
                        "average_response_time": "N/A",
                    }
                    rows = list(reader)
                    # Last row is typically 'Aggregated' summary
                    if rows:
                        last = rows[-1]
                        agg["requests"] = int(float(last.get("# requests", 0)))
                        agg["failures"] = int(float(last.get("# failures", 0)))
                        agg["median_response_time"] = str(last.get("Median response time", "N/A"))
                        agg["average_response_time"] = str(last.get("Average response time", "N/A"))
                        locust_stats = agg
            except Exception as e:
                print(f"Warning: failed to parse Locust stats: {e}")

        # Benchmark JSON
        bench_file = input_path / "benchmark.json"
        if bench_file.exists():
            try:
                import json

                data = json.loads(bench_file.read_text())
                benchmarks = data.get("benchmarks", [])
                if benchmarks:
                    count = len(benchmarks)
                    names: list[str] = [b.get("name", "unknown") for b in benchmarks][:5]
                    benchmark_summary = {
                        "count": count,
                        "examples": names,
                    }
            except Exception as e:
                print(f"Warning: failed to parse benchmark.json: {e}")

    content = f"""# Performance Report

<!-- AUTO-GENERATED: This file is regenerated by CI/CD pipeline on every run -->
<!-- See tools/generate_reports.py and .github/workflows/ci.yml -->

*Generated: {timestamp}*

## üöÄ Overview

This report summarizes CI performance artifacts when available.
It includes Locust load-test stats and pytest-benchmark summaries.

## üìä Load Test (Locust)

{
        (
            "No Locust stats found"
            if locust_stats is None
            else (
                f"Requests: {locust_stats['requests']} | "
                f"Failures: {locust_stats['failures']} | "
                f"Median RT: {locust_stats['median_response_time']} | "
                f"Avg RT: {locust_stats['average_response_time']}"
            )
        )
    }

## üß™ Benchmarks (pytest-benchmark)

{
        (
            "No benchmark results found"
            if benchmark_summary is None
            else (
                f"Benchmarks: {benchmark_summary['count']} | "
                f"Examples: {', '.join(benchmark_summary['examples'])}"
            )
        )
    }

---

*Report auto-generated from CI/CD pipeline execution*
"""

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content)
    print(f"Performance report generated: {output_path}")


def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Generate CI/CD reports for documentation")
    parser.add_argument(
        "--type",
        required=True,
        choices=["test", "coverage", "security", "license", "cicd", "performance"],
        help="Type of report to generate",
    )
    parser.add_argument("--input", type=Path, help="Input directory containing artifacts")
    parser.add_argument("--output", type=Path, required=True, help="Output markdown file")

    args = parser.parse_args()

    try:
        if args.type == "test":
            if not args.input:
                print("--input required for test reports", file=sys.stderr)
                sys.exit(1)
            generate_test_report(args.input, args.output)
        elif args.type == "coverage":
            if not args.input:
                print("--input required for coverage reports", file=sys.stderr)
                sys.exit(1)
            generate_coverage_report(args.input, args.output)
        elif args.type == "security":
            if not args.input:
                print("--input required for security reports", file=sys.stderr)
                sys.exit(1)
            generate_security_report(args.input, args.output)
        elif args.type == "license":
            if not args.input:
                print("--input required for license reports", file=sys.stderr)
                sys.exit(1)
            generate_license_report(args.input, args.output)
        elif args.type == "cicd":
            generate_cicd_report(args.output)
        elif args.type == "performance":
            generate_performance_report(args.output, args.input)

        print(f"‚úÖ Report generation complete: {args.output}")

    except Exception as e:
        print(f"‚ùå Error generating report: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
