"""This module provides the assets for extracting the behavior data from the .npz log archives generated by the
microcontrollers (hardware modules) used in the Sun lab.
"""

from typing import TypedDict
from pathlib import Path  # noqa: TC003
from collections.abc import Callable  # noqa: TC003
from concurrent.futures import ProcessPoolExecutor, as_completed

from tqdm import tqdm
import numpy as np
import polars as pl
from numpy.typing import NDArray  # noqa: TC002
from sl_shared_assets import SessionData, ProcessingTracker, MesoscopeHardwareState
from ataraxis_base_utilities import LogLevel, console
from sl_forgery.shared_assets import ProcessingTrackers, interpolate_data
from ataraxis_communication_interface import (
    ExtractedModuleData,
    ExtractedMessageData,
    extract_logged_hardware_module_data,
)

# Microcontroller log file identifiers used by the Mesoscope-VR data acquisition system.
_ACTOR_LOG_ID: int = 101
"""Log file identifier for the Actor microcontroller."""
_SENSOR_LOG_ID: int = 152
"""Log file identifier for the Sensor microcontroller."""
_ENCODER_LOG_ID: int = 203
"""Log file identifier for the Encoder microcontroller."""


class _ParseTask(TypedDict):
    """An internal typing class used to enforce static typing while supporting processing microcontroller data in
    parallel.
    """

    func: Callable[..., None]
    output: Path
    kwargs: dict[str, bool | np.float64 | np.uint16 | int]


def _parse_encoder_data(
    extracted_module_data: ExtractedModuleData, output_file: Path, cm_per_pulse: np.float64
) -> None:
    """Extracts and saves the data acquired by the EncoderModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        cm_per_pulse: The conversion factor to translate raw encoder pulses into distance in centimeters.
    """
    event_data = extracted_module_data.event_data

    # Looks for event-codes 51 (kRotatedCCW) and event-codes 52 (kRotatedCW).

    # Gets the data, defaulting to an empty tuple if the data is missing
    ccw_data = event_data.get(np.uint8(51), ())
    cw_data = event_data.get(np.uint8(52), ())

    # The way EncoderModule is implemented guarantees there is at least one CW code message with the displacement
    # of 0 that is received by the PC. In the worst case scenario, there will be no CCW codes and the parsing will
    # not work. To avoid that issue, generates an artificial zero-code CCW value at the same timestamp + 1
    # microsecond as the original CW zero-code value. This does not affect the accuracy of our data, just makes the
    # code work for edge-cases.
    if not ccw_data:
        first_timestamp = cw_data[0].timestamp
        ccw_data = (ExtractedMessageData(timestamp=first_timestamp + 1, command=np.uint8(51), data=np.float64(0)),)
    elif not cw_data:
        first_timestamp = ccw_data[0].timestamp
        cw_data = (ExtractedMessageData(timestamp=first_timestamp + 1, command=np.uint8(52), data=np.float64(0)),)

    # Pre-creates the output arrays, based on the number of recorded CW and CCW displacements.
    n_ccw = len(ccw_data)
    n_cw = len(cw_data)
    total_length = n_ccw + n_cw
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    displacements: NDArray[np.float64] = np.empty(total_length, dtype=np.float64)

    # Processes CCW rotations (Code 51). CCW rotation is interpreted as positive displacement
    timestamps[:n_ccw] = np.array([msg.timestamp for msg in ccw_data], dtype=np.uint64)
    displacements[:n_ccw] = np.array([msg.data for msg in ccw_data], dtype=np.float64)

    # Processes CW rotations (Code 52). CW rotation is interpreted as negative displacement
    timestamps[n_ccw:] = np.array([msg.timestamp for msg in cw_data], dtype=np.uint64)
    displacements[n_ccw:] = -np.array([msg.data for msg in cw_data], dtype=np.float64)

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    displacements = displacements[sort_indices]

    # Converts individual displacement vectors into aggregated absolute position of the animal in the task environment.
    # The position is also translated from encoder pulse counts into centimeters. The position is referenced to the
    # start of the experimental trial (beginning of the VR track) as 0-value. Positive positions mean moving forward
    # along the track, negative positions mean moving backward along the track.
    # noinspection PyTypeChecker
    positions = np.cumsum(displacements * cm_per_pulse)
    positions = np.round(positions, decimals=8)

    # Replaces -0.0 values with 0.0. This is a convenience adjustment to improve the visual appearance of the data.
    positions[np.isclose(positions, -0.0) & np.signbit(positions)] = 0.0

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "traveled_distance_cm": positions,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_ttl_data(extracted_module_data: ExtractedModuleData, output_file: Path) -> None:
    """Extracts and saves the data acquired by the TTLModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
    """
    event_data = extracted_module_data.event_data

    # Looks for event-codes 51 (kInputOn) and event-codes 52 (kInputOff).

    # Gets the data for both message types. The way the module is written guarantees that the PC receives code 52
    # at least once. No such guarantee is made for code 51, however.
    on_data = event_data.get(np.uint8(51), ())
    off_data = event_data.get(np.uint8(52), ())

    # Since this function ultimately looks for rising edges, it will not find any unless there is at least one ON and
    # one OFF message. Therefore, if any of the codes is actually missing, aborts data extraction early.
    if len(on_data) == 0 or len(off_data) == 0:
        return

    # Pre-creates the storage numpy arrays for both message types. Timestamps use uint64 datatype, and the trigger
    # values are boolean.
    n_on = len(on_data)
    n_off = len(off_data)
    total_length = n_on + n_off
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    triggers: NDArray[np.uint8] = np.empty(total_length, dtype=np.uint8)

    # Extracts ON (Code 51) trigger codes. Statically assigns the value '1' to denote ON signals.
    timestamps[:n_on] = np.array([msg.timestamp for msg in on_data], dtype=np.uint64)
    triggers[:n_on] = 1  # All ON signals

    # Extracts OFF (Code 52) trigger codes.
    timestamps[n_on:] = np.array([msg.timestamp for msg in off_data], dtype=np.uint64)
    triggers[n_on:] = 0  # All OFF signals

    # Sorts both arrays based on the timestamps, so that the data is in the chronological order.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    triggers = triggers[sort_indices]

    # If the last value is not 0, adds a zero-value to the end of the data sequence, one microsecond
    # after the last readout. This is to properly mark the end of the monitoring sequence.
    if triggers[-1] != 0:
        timestamps = np.append(timestamps, timestamps[-1] + 1)
        triggers = np.append(triggers, 0)

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "ttl_state": triggers,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_brake_data(
    extracted_module_data: ExtractedModuleData,
    output_file: Path,
    maximum_brake_strength: np.float64,
    minimum_brake_strength: np.float64,
) -> None:
    """Extracts and saves the data acquired by the BrakeModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        maximum_brake_strength: The maximum torque of the brake in Newton centimeters.
        minimum_brake_strength: The minimum torque of the brake in Newton centimeters.

    Notes:
        This method assumes that the brake was used in the absolute force mode. Currently, it does not support
        extracting variable brake power data.
    """
    event_data = extracted_module_data.event_data

    # This function looks for event-codes 51 (kEngaged) and event-codes 52 (kDisengaged) as, currently, no experiment
    # requires variable braking power. In the future, to add support for parsing variable braking power, this function
    # needs to be expanded to parse code 53 (kVariable) events.

    # Gets the data, defaulting to an empty tuple if the data is missing
    engaged_data = event_data.get(np.uint8(51), ())
    disengaged_data = event_data.get(np.uint8(52), ())

    # Pre-creates the storage numpy arrays for both message types. Timestamps use uint64 datatype. Although trigger
    # values are boolean, they are translated into the actual torque applied by the brake in Newton centimeters and
    # stored as float64 values.
    n_engaged = len(engaged_data)
    n_disengaged = len(disengaged_data)
    total_length = n_engaged + n_disengaged
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    torques: NDArray[np.float64] = np.empty(total_length, dtype=np.float64)

    # Processes Engaged (code 51) triggers. When the motor is engaged, it applies the maximum possible torque to
    # the brake.
    timestamps[:n_engaged] = np.array([msg.timestamp for msg in engaged_data], dtype=np.uint64)
    torques[:n_engaged] = maximum_brake_strength  # Broadcasting scalar value

    # Processes Disengaged (code 52) triggers. Contrary to naive expectation, the torque of a disengaged brake is
    # NOT zero. Instead, it is at least the same as the minimum brake strength, likely larger due to all mechanical
    # couplings in the system.
    timestamps[n_engaged:] = np.array([msg.timestamp for msg in disengaged_data], dtype=np.uint64)
    torques[n_engaged:] = minimum_brake_strength  # Broadcasting scalar value

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    torques = torques[sort_indices]

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "brake_torque_N_cm": torques,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_valve_data(
    extracted_module_data: ExtractedModuleData,
    output_file: Path,
    scale_coefficient: np.float64,
    nonlinearity_exponent: np.float64,
) -> None:
    """Extracts and saves the data acquired by the ValveModule during runtime as a .feather file.

    Notes:
        Unlike other processing methods, this method generates a .feather dataset with 3 columns: time, dispensed
        water volume, and the state of the tone buzzer.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        scale_coefficient: Stores the scale coefficient used in the fitted power law equation that translates valve
            pulses into dispensed water volumes.
        nonlinearity_exponent: Stores the nonlinearity exponent used in the fitted power law equation that
            translates valve pulses into dispensed water volumes.
    """
    event_data = extracted_module_data.event_data

    # This function looks for event-codes 51 (kOpen) and event-codes 52 (kClosed). It also looks for codes 54
    # (kToneOn) and 55 (kToneOff), however, and these codes are parsed similar to the ttl state codes.

    # The way this module is implemented guarantees there is at least one code 52 message, but there may be no code
    # 51 messages.
    open_data = event_data.get(np.uint8(51), ())
    closed_data = event_data[np.uint8(52)]

    # If there were no valve open events, no water was dispensed. In this case, uses the first code 52 timestamp
    # to report a zero-volume reward and ends the runtime early. If the valve was never opened, there were no
    # tones, so aborts both tone-parsing and valve-parsing early.
    if not open_data:
        module_dataframe = pl.DataFrame(
            {
                "time_us": np.array([closed_data[0].timestamp], dtype=np.uint64),
                "dispensed_water_volume_uL": np.array([0], dtype=np.float64),
                "tone_state": np.array([0], dtype=np.uint8),
            }
        )
        module_dataframe.write_ipc(file=output_file, compression="uncompressed")
        return

    # Pre-creates the storage numpy arrays for both message types. Timestamps use uint64 datatype. Although valve
    # trigger values are boolean, they are translated into the total volume of water, in microliters, dispensed to the
    # animal at each time-point and store that value as a float64.
    n_on = len(open_data)
    n_off = len(closed_data)
    total_length = n_on + n_off
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    volume: NDArray[np.float64] = np.empty(total_length, dtype=np.float64)

    # The water is dispensed gradually while the valve stays open. Therefore, the full reward volume is dispensed
    # when the valve goes from open to closed. Based on calibration data, uses a conversion factor to translate
    # the time the valve remains open into the fluid volume dispensed to the animal, which is then used to convert each
    # Open/Close cycle duration into the dispensed volume.

    # Extracts Open (Code 51) trigger codes. Statically assigns the value '1' to denote Open signals.
    timestamps[:n_on] = np.array([msg.timestamp for msg in open_data], dtype=np.uint64)
    volume[:n_on] = 1  # Open state

    # Extracts Closed (Code 52) trigger codes.
    timestamps[n_on:] = np.array([msg.timestamp for msg in closed_data], dtype=np.uint64)
    volume[n_on:] = 0  # Closed state

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    volume = volume[sort_indices]

    # Find falling and rising edges. Falling edges are valve-closing events, rising edges are valve-opening events.
    edges = np.diff(volume, prepend=volume[0])
    rising_edges = np.where(edges == 1)[0]
    falling_edges = np.where(edges == -1)[0]

    # Samples the timestamp array to only include timestamps for the falling edges. That is, when the valve has
    # finished delivering water
    reward_timestamps = timestamps[falling_edges]

    # Calculates pulse durations in microseconds for each open-close cycle. Since the original timestamp array
    # contains alternating HIGH / LOW edges, each falling edge has to match to a rising edge.
    pulse_durations: NDArray[np.float64] = (timestamps[falling_edges] - timestamps[rising_edges]).astype(np.float64)

    # Converts the time the Valve stayed open into the dispensed water volume, in microliters.
    # noinspection PyTypeChecker
    volumes = np.cumsum(scale_coefficient * np.power(pulse_durations, nonlinearity_exponent))
    volumes = np.round(volumes, decimals=8)

    # The processing logic above removes the initial water volume of 0. This re-adds the initial volume using the
    # first timestamp of the module data. That timestamp communicates the initial valve state, which should be 0.
    reward_timestamps = np.insert(reward_timestamps, 0, timestamps[0])
    volumes = np.insert(volumes, 0, 0.0)

    # Now carries out similar processing for the Tone signals
    # Same logic as with code 51 applies to code 54
    tone_on_data = event_data.get(np.uint8(54), ())
    tone_off_data = event_data.get(np.uint8(55), ())  # The empty default is to appease mypy

    tone_on_n = len(tone_on_data)
    tone_off_n = len(tone_off_data)
    tone_length = tone_on_n + tone_off_n
    tone_timestamps: NDArray[np.uint64] = np.empty(tone_length, dtype=np.uint64)
    tone_states: NDArray[np.uint8] = np.empty(tone_length, dtype=np.uint8)

    # Extracts ON (Code 54) Tone codes. Statically assigns the value '1' to denote On signals.
    tone_timestamps[:tone_on_n] = np.array([msg.timestamp for msg in tone_on_data], dtype=np.uint64)
    tone_states[:tone_on_n] = 1

    # Extracts OFF (Code 55) trigger codes.
    tone_timestamps[tone_on_n:] = np.array([msg.timestamp for msg in tone_off_data], dtype=np.uint64)
    tone_states[tone_on_n:] = 0

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(tone_timestamps)
    tone_timestamps = tone_timestamps[sort_indices]
    tone_states = tone_states[sort_indices]

    # If the last value is not 0, adds a zero-value to the end of the data sequence, one microsecond
    # after the last readout. This is to properly mark the end of the monitoring sequence.
    if tone_states[-1] != 0:
        tone_timestamps = np.append(tone_timestamps, tone_timestamps[-1] + 1)
        tone_states = np.append(tone_states, 0)

    # Constructs a shared array that includes all reward and tone timestamps. This is used to interpolate tone
    # and timestamp values. Sorts the generated array to arrange all timestamps in monotonically ascending order
    shared_stamps = np.unique(np.concatenate([tone_timestamps, reward_timestamps]))

    # Interpolates the reward volumes for each tone state and tone states for each reward volume.
    out_reward = interpolate_data(
        source_coordinates=reward_timestamps, source_values=volumes, target_coordinates=shared_stamps, is_discrete=True
    )
    out_tones = interpolate_data(
        source_coordinates=tone_timestamps,
        source_values=tone_states,
        target_coordinates=shared_stamps,
        is_discrete=True,
    )

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": shared_stamps,
            "dispensed_water_volume_uL": out_reward,
            "tone_state": out_tones,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_lick_data(extracted_module_data: ExtractedModuleData, output_file: Path, lick_threshold: np.uint16) -> None:
    """Extracts and saves the data acquired by the LickModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        lick_threshold: The voltage threshold for detecting the interaction with the sensor as a lick.

    Notes:
        The extraction classifies lick events based on the lick threshold used during runtime. The
        time-difference between consecutive ON and OFF event edges corresponds to the time, in microseconds, the
        tongue maintained contact with the lick tube.

        In addition to classifying the licks and providing binary lick state data, the extraction preserves the raw
        12-bit ADC voltages associated with each lick.
    """
    event_data = extracted_module_data.event_data

    # LickModule only sends messages with code 51 (kChanged). Therefore, this extraction pipeline has
    # to apply the threshold filter, similar to how the real-time processing method.

    # Unlike the other parsing methods, this one will always work as expected since it only deals with one code and
    # that code is guaranteed to be received for each runtime.

    # Extract timestamps and voltage levels. Timestamps use uint64 datatype. Lick sensor
    # voltage levels come in as uint16, but they are later used to generate a binary uint8 lick classification mask.
    voltage_data = event_data[np.uint8(51)]
    timestamps = np.array([msg.timestamp for msg in voltage_data], dtype=np.uint64)
    voltages = np.array([msg.data for msg in voltage_data], dtype=np.uint16)

    # Sorts all arrays by timestamp. This is technically not needed as the extracted values are already sorted by
    # timestamp, but this is still done for additional safety.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    voltages = voltages[sort_indices]

    # Creates a lick binary classification column based on the class threshold. Note, the threshold is inclusive.
    licks = (voltages >= lick_threshold).astype(np.uint8)

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "voltage_12_bit_adc": voltages,
            "lick_state": licks,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_torque_data(
    extracted_module_data: ExtractedModuleData, output_file: Path, torque_per_adc_unit: np.float64
) -> None:
    """Extracts and saves the data acquired by the TorqueModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        torque_per_adc_unit: The conversion actor used to translate ADC units recorded by the torque sensor into
            the torque in Newton centimeter, applied by the animal to the wheel.
    """
    event_data = extracted_module_data.event_data

    # Looks for event-codes 51 (kCCWTorque) and event-codes 52 (kCWTorque). CCW torque is interpreted
    # as torque in the positive direction, and CW torque is interpreted as torque in the negative direction.

    # Gets the data, defaulting to an empty tuple if the data is missing
    ccw_data = event_data.get(np.uint8(51), ())
    cw_data = event_data.get(np.uint8(52), ())

    # The way TorqueModule is implemented guarantees there is at least one CW code message with the displacement
    # of 0 that is received by the PC. In the worst case scenario, there will be no CCW codes and the parsing will
    # not work. To avoid that issue, generates an artificial zero-code CCW value at the same timestamp + 1
    # microsecond as the original CW zero-code value. This does not affect the accuracy of our data, just makes the
    # code work for edge-cases.
    if not ccw_data:
        first_timestamp = cw_data[0].timestamp
        ccw_data = (ExtractedMessageData(timestamp=first_timestamp + 1, command=np.uint8(51), data=np.float64(0)),)
    elif not cw_data:
        first_timestamp = ccw_data[0].timestamp
        cw_data = (ExtractedMessageData(timestamp=first_timestamp + 1, command=np.uint8(52), data=np.float64(0)),)

    # Pre-creates the storage numpy arrays for both message types. Timestamps use uint64 datatype. Although torque
    # values are uint16, they are translated into the actual torque applied by the animal in Newton centimeters and
    # store them as float 64 values.
    n_ccw = len(ccw_data)
    n_cw = len(cw_data)
    total_length = n_ccw + n_cw
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)

    # Processes CCW torques (Code 51). CCW torque is interpreted as positive torque
    timestamps[:n_ccw] = np.array([msg.timestamp for msg in ccw_data], dtype=np.uint64)
    ccw_values = np.array([msg.data for msg in ccw_data], dtype=np.float64) * torque_per_adc_unit

    # Processes CW torques (Code 52). CW torque is interpreted as negative torque
    timestamps[n_ccw:] = np.array([msg.timestamp for msg in cw_data], dtype=np.uint64)
    cw_values = -np.array([msg.data for msg in cw_data], dtype=np.float64) * torque_per_adc_unit

    # Combine torques into a unified time-based array
    torques = np.concatenate([ccw_values, cw_values])
    torques = np.round(torques, decimals=8)

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    torques = torques[sort_indices]

    # If the last value is not 0, adds a zero-value to the end of the data sequence, one microsecond
    # after the last readout. This is to properly mark the end of the monitoring sequence.
    if torques[-1] != 0:
        timestamps = np.append(timestamps, timestamps[-1] + 1)
        torques = np.append(torques, 0)

    # Replaces -0.0 values with 0.0. This is a convenience conversion used to improve the visual appearance of the data.
    torques[np.isclose(torques, -0.0) & np.signbit(torques)] = 0.0

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "torque_N_cm": torques,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_screen_data(extracted_module_data: ExtractedModuleData, output_file: Path, *, initially_on: bool) -> None:
    """Extracts and saves the data acquired by the ScreenModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        initially_on: Communicates the initial state of the screen at module interface initialization. This is used
            to determine the state of the screens after each processed screen toggle signal.

    Notes:
        This extraction method works similar to the TTLModule method. This is intentional, as ScreenInterface is
        essentially a group of 3 TTLModules.
    """
    event_data = extracted_module_data.event_data
    # Looks for event-codes 51 (kOn) and event-codes 52 (kOff).

    # The way the module is implemented guarantees there is at least one code 52 message. However, if the screen state
    # is never toggled, there may be no code 51 messages.
    on_data = event_data.get(np.uint8(51), ())
    off_data = event_data[np.uint8(52)]

    # If there were no ON pulses, screens never changed state. In this case, shorts to returning the data for the
    # initial screen state using the initial Off timestamp. Otherwise, parses the data
    if not on_data:
        module_dataframe = pl.DataFrame(
            {
                "time_us": np.array([off_data[0].timestamp], dtype=np.uint64),
                "screen_state": np.array([initially_on], dtype=np.uint8),
            }
        )
        module_dataframe.write_ipc(file=output_file, compression="uncompressed")
        return

    # Pre-creates the storage numpy arrays for both message types. Timestamps use uint64 datatype, and the trigger
    # values are boolean.
    n_on = len(on_data)
    n_off = len(off_data)
    total_length = n_on + n_off
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    triggers: NDArray[np.uint8] = np.empty(total_length, dtype=np.uint8)

    # Extracts ON (Code 51) trigger codes. Statically assigns the value '1' to denote ON signals.
    timestamps[:n_on] = np.array([msg.timestamp for msg in on_data], dtype=np.uint64)
    triggers[:n_on] = 1

    # Extracts OFF (Code 52) trigger codes.
    timestamps[n_on:] = np.array([msg.timestamp for msg in off_data], dtype=np.uint64)
    triggers[n_on:] = 0

    # Sorts both arrays based on the timestamps, so that the data is in the chronological order.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    triggers = triggers[sort_indices]

    # Finds rising edges (where the signal goes from 0 to 1). Then uses the indices for such events to extract the
    # timestamps associated with each rising edge, before returning them to the caller.
    edges = np.diff(triggers, prepend=0)
    rising_edges = np.where(edges == 1)[0]
    screen_timestamps = timestamps[rising_edges]

    # Adds the initial state of the screen using the first recorded timestamp. The module is configured to send the
    # initial state of the relay (Off) during Setup, so the first recorded timestamp will always be 0 and correspond
    # to the initial state of the screen.
    screen_timestamps = np.concatenate(([timestamps[0]], screen_timestamps))

    # Builds an array of screen states. Starts with the initial screen state and then flips the state for each
    # consecutive timestamp matching a rising edge of the toggle pulse.
    n_states = len(screen_timestamps)
    screen_states = np.empty(n_states, dtype=np.uint8)
    screen_states[0] = initially_on
    if n_states > 1:
        screen_states[1:] = (initially_on + np.arange(1, n_states)) % 2

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": screen_timestamps,
            "screen_state": screen_states,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_module_data(
    parse_func: Callable[..., None],
    extracted_data: ExtractedModuleData | None,
    output_file: Path,
    **kwargs: bool | np.float64 | np.uint16 | int,
) -> Exception | None:
    """Runs a hardware module data parsing function with error handling.

    This helper function is used to parse hardware module data in parallel.

    Args:
        parse_func: The parsing function to execute.
        extracted_data: The extracted module data to parse.
        output_file: The output file path.
        **kwargs: Additional arguments for the parsing function.

    Returns:
        None if successful, Exception if an error occurs during runtime.
    """
    if extracted_data is None:
        return None
    try:
        parse_func(extracted_data, output_file, **kwargs)
    except Exception as e:
        return e
    else:
        return None


def _extract_mesoscope_vr_actor_data(
    log_path: Path, output_directory: Path, hardware_state: MesoscopeHardwareState, workers: int
) -> None:
    """Extracts the data logged by the Actor microcontroller hardware modules used by the Mesoscope-VR data acquisition
    system and saves it as multiple .feather files.

    Args:
        log_path: The path to the .npz archive containing the Actor microcontroller data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        hardware_state: The HardwareState instance that stores the hardware configuration of the Mesoscope-VR
            acquisition system.
        workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            less than 1 uses all available CPU cores. Setting this to 1 conducts the processing sequentially.
    """
    # Resolves the modules for which to extract the data. Not all runtimes use all the modules supported by the AMC.
    module_type_id = []  # Determines the data to parse
    data_indices: list[int | None] = []  # Tracks the index under which module's data is returned
    parse_tasks: list[_ParseTask] = []  # Stores parsing tasks to execute in parallel

    # Brake
    # Note: Uses 'brake' (correct spelling) and falls back to legacy 'break' (typo) for backward compatibility with
    # older hardware state YAML files.
    index = 0
    minimum_brake_strength = hardware_state.minimum_brake_strength
    if minimum_brake_strength is None:
        minimum_brake_strength = getattr(hardware_state, "minimum_break_strength", None)
    maximum_brake_strength = hardware_state.maximum_brake_strength
    if maximum_brake_strength is None:
        maximum_brake_strength = getattr(hardware_state, "maximum_break_strength", None)

    if minimum_brake_strength is not None and maximum_brake_strength is not None:
        module_type_id.append((3, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_brake_data,
                "output": output_directory.joinpath("brake_data.feather"),
                "kwargs": {
                    "minimum_brake_strength": np.float64(minimum_brake_strength),
                    "maximum_brake_strength": np.float64(maximum_brake_strength),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Valve
    if hardware_state.valve_nonlinearity_exponent is not None and hardware_state.valve_scale_coefficient is not None:
        module_type_id.append((5, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_valve_data,
                "output": output_directory.joinpath("valve_data.feather"),
                "kwargs": {
                    "scale_coefficient": np.float64(hardware_state.valve_scale_coefficient),
                    "nonlinearity_exponent": np.float64(hardware_state.valve_nonlinearity_exponent),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Screens
    if hardware_state.screens_initially_on is not None:
        module_type_id.append((7, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_screen_data,
                "output": output_directory.joinpath("screen_data.feather"),
                "kwargs": {
                    "initially_on": hardware_state.screens_initially_on,
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Aborts early if no module data needs to be parsed.
    if set(data_indices) == {None}:
        return

    # Extracts the data for the requested hardware modules from the log file.
    log_data_tuple = extract_logged_hardware_module_data(
        log_path=log_path, module_type_id=tuple(module_type_id), n_workers=workers
    )

    # Depending on configuration, executes the parsing tasks in-parallel or sequentially.
    if workers == 1 or len(parse_tasks) == 1:
        # Sequential execution
        for i, task in enumerate(parse_tasks):
            idx = [d for d in data_indices if d is not None][i]
            task["func"](extracted_module_data=log_data_tuple[idx], output_file=task["output"], **task["kwargs"])
    else:
        # Parallel execution
        n_workers = workers if workers > 0 else None  # None uses all available cores
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = []
            for i, task in enumerate(parse_tasks):
                idx = [d for d in data_indices if d is not None][i]
                future = executor.submit(
                    _parse_module_data, task["func"], log_data_tuple[idx], task["output"], **task["kwargs"]
                )
                futures.append(future)

            # Waits for all parsing tasks to complete and check for errors
            with tqdm(
                total=len(futures), desc="Parsing Actor microcontroller hardware modules data", unit="module"
            ) as pbar:
                for future in as_completed(futures):
                    error = future.result()
                    if error is not None:
                        pbar.close()  # Closes progress bar before raising error
                        raise error
                    pbar.update(1)


def _extract_mesoscope_vr_sensor_data(
    log_path: Path, output_directory: Path, hardware_state: MesoscopeHardwareState, workers: int
) -> None:
    """Extracts the data logged by the Sensor microcontroller hardware modules used by the Mesoscope-VR data acquisition
    system and saves it as multiple .feather files.

    Args:
        log_path: The path to the .npz archive containing the Sensor microcontroller data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        hardware_state: The HardwareState instance that stores the hardware configuration of the Mesoscope-VR
            acquisition system.
        workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            less than 1 uses all available CPU cores. Setting this to 1 conducts the processing sequentially.
    """
    # Resolves the modules for which to extract the data
    module_type_id = []  # Determines the data to parse
    data_indices: list[int | None] = []  # Tracks the index under which module's data is returned
    parse_tasks: list[_ParseTask] = []  # Stores parsing tasks to execute in parallel

    # Lick Sensor
    index = 0
    if hardware_state.lick_threshold is not None:
        module_type_id.append((4, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_lick_data,
                "output": output_directory.joinpath("lick_data.feather"),
                "kwargs": {
                    "lick_threshold": np.uint16(hardware_state.lick_threshold),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Torque Sensor
    if hardware_state.torque_per_adc_unit is not None:
        module_type_id.append((6, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_torque_data,
                "output": output_directory.joinpath("torque_data.feather"),
                "kwargs": {
                    "torque_per_adc_unit": np.float64(hardware_state.torque_per_adc_unit),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Mesoscope Frame TTL module.
    if hardware_state.recorded_mesoscope_ttl:
        module_type_id.append((1, 1))
        data_indices.append(index)
        parse_tasks.append(
            {"func": _parse_ttl_data, "output": output_directory.joinpath("mesoscope_frame_data.feather"), "kwargs": {}}
        )
        index += 1
    else:
        data_indices.append(None)

    # Aborts early if no module data needs to be parsed.
    if set(data_indices) == {None}:
        return

    # Extracts the data for all requested modules in parallel
    log_data_tuple = extract_logged_hardware_module_data(
        log_path=log_path, module_type_id=tuple(module_type_id), n_workers=workers
    )

    # Execute module data parsing tasks in parallel
    if workers == 1 or len(parse_tasks) == 1:
        # Sequential execution
        for i, task in enumerate(parse_tasks):
            idx = [d for d in data_indices if d is not None][i]
            task["func"](extracted_module_data=log_data_tuple[idx], output_file=task["output"], **task["kwargs"])
    else:
        # Parallel execution
        n_workers = workers if workers > 0 else None  # None uses all available cores
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = []
            for i, task in enumerate(parse_tasks):
                idx = [d for d in data_indices if d is not None][i]
                future = executor.submit(
                    _parse_module_data, task["func"], log_data_tuple[idx], task["output"], **task["kwargs"]
                )
                futures.append(future)

            # Waits for all parsing tasks to complete and check for errors
            with tqdm(
                total=len(futures), desc="Parsing Sensor microcontroller hardware modules data", unit="module"
            ) as pbar:
                for future in as_completed(futures):
                    error = future.result()
                    if error is not None:
                        pbar.close()  # Closes progress bar before raising error
                        raise error
                    pbar.update(1)


def _extract_mesoscope_vr_encoder_data(
    log_path: Path, output_directory: Path, hardware_state: MesoscopeHardwareState, workers: int
) -> None:
    """Extracts the data logged by the Encoder microcontroller hardware module used by the Mesoscope-VR data
    acquisition system and saves it as a .feather file.

    Args:
        log_path: The path to the .npz archive containing the Encoder microcontroller data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        hardware_state: The HardwareState instance that stores the hardware configuration of the Mesoscope-VR
            acquisition system.
        workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            less than 1 uses all available CPU cores. Setting this to 1 conducts the processing sequentially.
    """
    # Resolves the module for which to extract the data
    module_type_id = []  # Determines the data to parse
    data_indices: list[int | None] = []  # Tracks the index under which module's data is returned
    parse_tasks: list[_ParseTask] = []  # Stores parsing tasks to execute in parallel

    # Encoder
    index = 0
    if hardware_state.cm_per_pulse is not None:
        module_type_id.append((2, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_encoder_data,
                "output": output_directory.joinpath("encoder_data.feather"),
                "kwargs": {
                    "cm_per_pulse": np.float64(hardware_state.cm_per_pulse),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Aborts early if no module data needs to be parsed.
    if set(data_indices) == {None}:
        return

    # Extracts module data in parallel
    log_data_tuple = extract_logged_hardware_module_data(
        log_path=log_path, module_type_id=tuple(module_type_id), n_workers=workers
    )

    # Parses extracted module data in parallel
    if workers == 1 or len(parse_tasks) == 1:
        # Sequential execution
        for i, task in enumerate(parse_tasks):
            idx = [d for d in data_indices if d is not None][i]
            task["func"](extracted_module_data=log_data_tuple[idx], output_file=task["output"], **task["kwargs"])
    else:
        n_workers = workers if workers > 0 else None
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = []
            for i, task in enumerate(parse_tasks):
                idx = [d for d in data_indices if d is not None][i]
                future = executor.submit(
                    _parse_module_data, task["func"], log_data_tuple[idx], task["output"], **task["kwargs"]
                )
                futures.append(future)

            # Waits for all parsing tasks to complete and check for errors
            with tqdm(
                total=len(futures), desc="Parsing Encoder microcontroller hardware modules data", unit="module"
            ) as pbar:
                for future in as_completed(futures):
                    error = future.result()
                    if error is not None:
                        pbar.close()  # Closes progress bar before raising error
                        raise error
                    pbar.update(1)


def process_microcontroller_data(
    session_path: Path,
    log_id: int,
    job_id: str,
    workers: int = -1,
) -> None:
    """Reads the specified microcontroller log .npz file and extracts the behavior data recorded by the hardware modules
    managed by the microcontroller as uncompressed .feather files.

    This function is used to process the log archives generated by the microcontrollers used in the Sun lab.
    It assumes that the data was logged using the assets from the ataraxis-communication-interface library.

    Args:
        session_path: The path to the session directory for which to process the microcontroller log file.
        log_id: The unique identifier of the microcontroller log file to process: 101 (actor), 152 (sensor),
            or 203 (encoder).
        job_id: The unique hexadecimal identifier for this processing job.
        workers: The number of worker processes to use for extracting the hardware module messages in parallel. Setting
            this argument to a value less than 1 uses all available CPU cores. Setting this to a value of 1 conducts
            the processing sequentially.
    """
    # Loads the target session's data hierarchy into memory
    session = SessionData.load(session_path=session_path)

    # Resolves the path to the processed log file
    log_path = session.raw_data.behavior_data_path.joinpath(f"{log_id}_log.npz")

    # Initializes the processing tracker for this pipeline.
    tracker = ProcessingTracker(
        file_path=session.tracking_data.tracking_data_path.joinpath(ProcessingTrackers.BEHAVIOR)
    )

    # Marks the job as running.
    tracker.start_job(job_id=job_id)

    try:
        # Loads the hardware state configuration
        hardware_state = MesoscopeHardwareState.from_yaml(session.raw_data.hardware_state_path)

        console.echo(f"Extracting microcontroller hardware module data from the '{log_id}' log file...")

        # Depending on the target log ID, calls the appropriate extraction function.
        if log_id == _ACTOR_LOG_ID:
            _extract_mesoscope_vr_actor_data(
                log_path=log_path,
                output_directory=session.processed_data.behavior_data_path,
                hardware_state=hardware_state,
                workers=workers,
            )
        elif log_id == _SENSOR_LOG_ID:
            _extract_mesoscope_vr_sensor_data(
                log_path=log_path,
                output_directory=session.processed_data.behavior_data_path,
                hardware_state=hardware_state,
                workers=workers,
            )
        elif log_id == _ENCODER_LOG_ID:
            _extract_mesoscope_vr_encoder_data(
                log_path=log_path,
                output_directory=session.processed_data.behavior_data_path,
                hardware_state=hardware_state,
                workers=workers,
            )

        # Marks the job as successfully completed.
        tracker.complete_job(job_id=job_id)
        console.echo("MicroController hardware module data processing: Complete.", level=LogLevel.SUCCESS)

    # If the runtime encounters an error, marks the job as failed.
    except Exception:
        tracker.fail_job(job_id=job_id)
        raise
