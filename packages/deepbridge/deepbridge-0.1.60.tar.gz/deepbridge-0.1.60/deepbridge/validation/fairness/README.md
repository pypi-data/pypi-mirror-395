# Fairness Testing Module - DeepBridge

## üìã Vis√£o Geral

O m√≥dulo de Fairness Testing do DeepBridge fornece ferramentas abrangentes para avaliar se modelos de Machine Learning apresentam discrimina√ß√£o contra grupos protegidos (protected groups).

**Compliance**: Segue padr√µes regulat√≥rios de Banking (EEOC, ECOA), Healthcare (HIPAA), e Fair Lending Act.

**Baseado em**: AI Fairness 360 (IBM), Fairlearn (Microsoft), Aequitas, e frameworks acad√™micos state-of-the-art.

---

## üéØ Quando Usar

### Casos de Uso Cr√≠ticos

1. **Banking & Finance**
   - Aprova√ß√£o de cr√©dito
   - Limite de cart√£o de cr√©dito
   - Taxas de juros
   - Detec√ß√£o de fraude

2. **Healthcare**
   - Diagn√≥stico m√©dico
   - Prioriza√ß√£o de tratamento
   - Aloca√ß√£o de recursos

3. **Insurance**
   - C√°lculo de pr√™mios
   - Aprova√ß√£o de cobertura
   - Avalia√ß√£o de risco

4. **Employment**
   - Sele√ß√£o de candidatos
   - Promo√ß√µes
   - Avalia√ß√µes de performance

5. **Lending**
   - Empr√©stimos pessoais
   - Hipotecas
   - Financiamento de ve√≠culos

### Quando √© Obrigat√≥rio

- ‚úÖ Aplica√ß√µes em setores altamente regulados
- ‚úÖ Modelos que impactam decis√µes sobre pessoas
- ‚úÖ Sistemas com atributos protegidos (ra√ßa, g√™nero, idade, etc.)
- ‚úÖ Contextos onde discrimina√ß√£o √© ilegal ou anti√©tica

---

## üöÄ Quick Start

### Instala√ß√£o

J√° inclu√≠do no DeepBridge! Nenhuma instala√ß√£o adicional necess√°ria.

### Exemplo B√°sico

```python
from deepbridge.core.db_data import DBDataset
from deepbridge.core.experiment import Experiment
import pandas as pd

# 1. Preparar dados com atributos protegidos
X = pd.DataFrame({
    'income': [...],
    'age': [...],
    'credit_score': [...],
    'gender': ['M', 'F', ...],  # Protected
    'race': ['White', 'Black', ...]  # Protected
})
y = [...]  # Target binary

# 2. Treinar modelo
model = LogisticRegression()
model.fit(X, y)

# 3. Criar dataset
dataset = DBDataset(features=X, target=y, model=model)

# 4. Executar testes de fairness
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["fairness"],
    protected_attributes=['gender', 'race']  # ‚Üê Especificar atributos protegidos
)

fairness_results = experiment.run_fairness_tests(config='full')

# 5. Analisar resultados
print(f"Fairness Score: {fairness_results.overall_fairness_score:.3f}")
print(f"Critical Issues: {len(fairness_results.critical_issues)}")

for issue in fairness_results.critical_issues:
    print(f"  - {issue}")
```

---

## üìä M√©tricas de Fairness Implementadas

### 1. Statistical Parity (Demographic Parity)

**Defini√ß√£o**: Taxa de predi√ß√µes positivas deve ser igual entre grupos.

**F√≥rmula**: `P(≈∂=1 | A=a) = P(≈∂=1 | A=b)` para todos os grupos a, b

**Quando usar**:
- Garantir representa√ß√£o igual em outcomes positivos
- Contextos onde "equal treatment" √© esperado

**Compliance**: Regra dos 80% da EEOC

```python
result = fairness_results.results['metrics']['gender']['statistical_parity']
print(f"Ratio: {result['ratio']:.3f}")
print(f"Passes 80% rule: {result['passes_80_rule']}")  # Must be >= 0.8
```

**Interpreta√ß√£o**:
- `ratio >= 0.95`: Excelente
- `ratio >= 0.80`: Compliant com EEOC
- `ratio < 0.80`: ‚ö†Ô∏è Evid√™ncia de discrimina√ß√£o

---

### 2. Equal Opportunity

**Defini√ß√£o**: True Positive Rate (TPR/Recall) deve ser igual entre grupos.

**F√≥rmula**: `P(≈∂=1 | Y=1, A=a) = P(≈∂=1 | Y=1, A=b)`

**Quando usar**:
- Focar em "benef√≠cios" (outcomes positivos)
- Garantir que grupos t√™m igual chance de serem identificados quando qualificados

**Exemplo**: Aprova√ß√£o de cr√©dito - qualificados devem ter mesma taxa de aprova√ß√£o independente de ra√ßa.

```python
result = fairness_results.results['metrics']['race']['equal_opportunity']
print(f"TPR Disparity: {result['disparity']:.3f}")
print(f"Group TPRs: {result['group_tpr']}")
```

**Interpreta√ß√£o**:
- `disparity < 0.05`: Excelente
- `disparity < 0.10`: Aceit√°vel
- `disparity >= 0.20`: ‚ö†Ô∏è Cr√≠tico

---

### 3. Equalized Odds

**Defini√ß√£o**: TPR E FPR devem ser iguais entre grupos.

**F√≥rmula**: `P(≈∂=1 | Y=y, A=a) = P(≈∂=1 | Y=y, A=b)` para y ‚àà {0,1}

**Quando usar**:
- Garantir fairness em benef√≠cios E harms
- Aplica√ß√µes onde false positives s√£o cr√≠ticos (ex: justi√ßa criminal)

**Mais rigoroso** que Equal Opportunity.

```python
result = fairness_results.results['metrics']['gender']['equalized_odds']
print(f"TPR Disparity: {result['tpr_disparity']:.3f}")
print(f"FPR Disparity: {result['fpr_disparity']:.3f}")
print(f"Combined: {result['combined_disparity']:.3f}")
```

---

### 4. Disparate Impact

**Defini√ß√£o**: Raz√£o entre taxa de sele√ß√£o do grupo menos/mais favorecido.

**F√≥rmula**: `DI = P(≈∂=1 | A=unprivileged) / P(≈∂=1 | A=privileged)`

**Legal Threshold**: Ratio < 0.8 = evid√™ncia de discrimina√ß√£o (EEOC)

**Quando usar**:
- Compliance legal mandat√≥rio
- Banking, lending, employment

```python
result = fairness_results.results['metrics']['race']['disparate_impact']
print(f"Disparate Impact Ratio: {result['ratio']:.3f}")
print(f"Passes Threshold (0.8): {result['passes_threshold']}")  # CR√çTICO!
```

**Legal Risk**:
- `ratio < 0.8`: üö® ALTO RISCO LEGAL - a√ß√£o necess√°ria
- `ratio >= 0.8`: Compliant com EEOC

---

## ‚öôÔ∏è Configura√ß√µes de Teste

### Quick (2 m√©tricas)
```python
fairness_results = experiment.run_fairness_tests(config='quick')
```
- Statistical Parity
- Disparate Impact
- **Tempo**: ~5-10s
- **Uso**: Explora√ß√£o r√°pida

### Medium (3 m√©tricas)
```python
fairness_results = experiment.run_fairness_tests(config='medium')
```
- Statistical Parity
- Equal Opportunity
- Disparate Impact
- **Tempo**: ~10-20s
- **Uso**: Valida√ß√£o padr√£o

### Full (4 m√©tricas - RECOMENDADO)
```python
fairness_results = experiment.run_fairness_tests(config='full')
```
- Statistical Parity
- Equal Opportunity
- Equalized Odds
- Disparate Impact
- **Tempo**: ~15-30s
- **Uso**: Auditoria completa, compliance

---

## üîç An√°lise de Resultados

### Overall Fairness Score

Score agregado (0-1, higher = more fair) combinando todas as m√©tricas:

```python
score = fairness_results.overall_fairness_score

if score >= 0.95:
    print("EXCELENTE - Modelo altamente fair")
elif score >= 0.85:
    print("BOM - Adequado para produ√ß√£o")
elif score >= 0.70:
    print("MODERADO - Requer aten√ß√£o")
else:
    print("CR√çTICO - Interven√ß√£o necess√°ria")
```

**Pesos**:
- Disparate Impact: 30% (mais cr√≠tico legalmente)
- Statistical Parity: 25%
- Equal Opportunity: 25%
- Equalized Odds: 20%

### Critical Issues

Issues que violam thresholds legais ou √©ticos:

```python
for issue in fairness_results.critical_issues:
    print(f"üö® {issue}")

# Exemplo de output:
# üö® race: Disparate Impact CR√çTICO (ratio=0.65 < 0.8) - RISCO LEGAL
```

### Warnings

Issues que n√£o s√£o cr√≠ticos mas requerem aten√ß√£o:

```python
for warning in fairness_results.warnings:
    print(f"‚ö†Ô∏è  {warning}")

# Exemplo de output:
# ‚ö†Ô∏è  gender: Falha na regra dos 80% (ratio=0.78)
```

### Detailed Per-Attribute Analysis

```python
# Analisar atributo espec√≠fico
gender_metrics = fairness_results.results['metrics']['gender']

# Statistical Parity
sp = gender_metrics['statistical_parity']
print(f"Group rates: {sp['group_rates']}")
# Exemplo: {'M': 0.65, 'F': 0.48}

print(f"Interpretation: {sp['interpretation']}")
# "MODERADO: Alguma disparidade presente - requer investiga√ß√£o"
```

---

## üõ†Ô∏è API Reference

### FairnessMetrics

Classe com m√©todos est√°ticos para c√°lculo de m√©tricas:

```python
from deepbridge.validation.fairness.metrics import FairnessMetrics

# Statistical Parity
result = FairnessMetrics.statistical_parity(y_pred, sensitive_feature)

# Equal Opportunity
result = FairnessMetrics.equal_opportunity(y_true, y_pred, sensitive_feature)

# Equalized Odds
result = FairnessMetrics.equalized_odds(y_true, y_pred, sensitive_feature)

# Disparate Impact
result = FairnessMetrics.disparate_impact(y_pred, sensitive_feature, threshold=0.8)
```

### FairnessSuite

Suite completa de testes:

```python
from deepbridge.validation.wrappers.fairness_suite import FairnessSuite

suite = FairnessSuite(
    dataset=dataset,
    protected_attributes=['gender', 'race'],
    verbose=True
)

results = suite.config('full').run()
```

### FairnessResult

Object com resultados estruturados:

```python
# Properties
fairness_results.overall_fairness_score  # float: 0-1
fairness_results.critical_issues  # list
fairness_results.warnings  # list
fairness_results.protected_attributes  # list

# Methods
fairness_results.to_dict()  # Convert to dict
fairness_results.results  # Access raw results dict
```

---

## üìö Exemplos Avan√ßados

### Exemplo 1: Comparar Fairness de M√∫ltiplos Modelos

```python
models = {
    'LogisticRegression': LogisticRegression(),
    'RandomForest': RandomForestClassifier(),
    'XGBoost': XGBClassifier()
}

fairness_scores = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    dataset = DBDataset(features=X_test, target=y_test, model=model)

    exp = Experiment(
        dataset=dataset,
        experiment_type="binary_classification",
        tests=["fairness"],
        protected_attributes=['gender', 'race']
    )

    results = exp.run_fairness_tests(config='full')
    fairness_scores[name] = results.overall_fairness_score

# Identificar modelo mais fair
best_model = max(fairness_scores, key=fairness_scores.get)
print(f"Modelo mais fair: {best_model} (score={fairness_scores[best_model]:.3f})")
```

### Exemplo 2: Integrar com Robustness Testing

```python
experiment = Experiment(
    dataset=dataset,
    experiment_type="binary_classification",
    tests=["robustness", "fairness"],  # M√∫ltiplos testes
    protected_attributes=['gender', 'race']
)

# Run both tests
robustness_results = experiment.run_tests(config='full')
fairness_results = experiment.run_fairness_tests(config='full')

# Modelo deve ser robusto E fair
print(f"Robustness Score: {robustness_results.robustness_score:.3f}")
print(f"Fairness Score: {fairness_results.overall_fairness_score:.3f}")
```

---

## ‚ö†Ô∏è Limita√ß√µes e Considera√ß√µes

### 1. Remover Protected Attributes N√ÉO √© suficiente

```python
# ‚ùå INCORRETO: Apenas remover 'gender' e 'race' do treinamento
X_train = df[['income', 'credit_score']]  # Remove protected attrs
model.fit(X_train, y)

# ‚ö†Ô∏è Modelo ainda pode ter bias devido a PROXY FEATURES
# Ex: 'income' correlacionado com 'race' por desigualdade hist√≥rica
```

**Solu√ß√£o**: Usar t√©cnicas de de-biasing (fairness-aware ML).

### 2. Trade-off entre Fairness e Accuracy

Aumentar fairness pode reduzir accuracy:

```python
# Pode precisar aceitar accuracy ligeiramente menor para ganhar fairness
# Ex: Accuracy 0.92 ‚Üí 0.90, mas Fairness 0.70 ‚Üí 0.90
```

### 3. Choice de M√©trica Depende do Contexto

- **Banking**: Disparate Impact (mandat√≥rio por lei)
- **Healthcare**: Equal Opportunity (benef√≠cios iguais)
- **Criminal Justice**: Equalized Odds (harms e benefits)

### 4. Interseccionalidade

Testar m√∫ltiplos atributos pode n√£o capturar intersec√ß√£o:

```python
# Pode passar em 'gender' E 'race' separadamente,
# mas falhar em 'Black women' especificamente
```

---

## üìñ Refer√™ncias

### Frameworks e Libraries
- [AI Fairness 360 (IBM)](https://github.com/Trusted-AI/AIF360)
- [Fairlearn (Microsoft)](https://github.com/fairlearn/fairlearn)
- [Aequitas (U. Chicago)](https://github.com/dssg/aequitas)

### Regula√ß√µes
- EEOC Uniform Guidelines (1978)
- Equal Credit Opportunity Act (ECOA)
- Fair Lending Act
- GDPR Article 22 (EU)

### Artigos Acad√™micos
- Mehrabi et al. (2021): "A Survey on Bias and Fairness in Machine Learning"
- Barocas & Selbst (2016): "Big Data's Disparate Impact"

---

## ü§ù Contribuindo

Encontrou um bug ou tem uma sugest√£o? Abra uma issue no GitHub!

---

**Vers√£o**: 1.0
**Data**: Outubro 2025
**Autor**: DeepBridge Team
